## **High-Order Error Bounds for Markovian LSA with Richardson-Romberg** **Extrapolation**
### Ilya Levin [1], Alexey Naumov [1], Sergey Samsonov [1]

1 HSE University
ivlevin@hse.ru, anaumov@hse.ru, svsamsonov@hse.ru


**Abstract**

In this paper, we study the bias and high-order error bounds of
the Linear Stochastic Approximation (LSA) algorithm with
Polyak–Ruppert (PR) averaging under Markovian noise. We
focus on the version of the algorithm with constant step size
*α* and propose a novel decomposition of the bias via a linearization technique. We analyze the structure of the bias
and show that the leading-order term is linear in *α* and cannot be eliminated by PR averaging. To address this, we apply the Richardson–Romberg (RR) extrapolation procedure,
which effectively cancels the leading bias term. We derive
high-order moment bounds for the RR iterates and show that
the leading error term aligns with the asymptotically optimal
covariance matrix of the vanilla averaged LSA iterates.
### **1 Introduction**

Stochastic approximation (SA) algorithms (Robbins and
Monro 1951) play a foundational role in modern machine
learning due to their various applications in reinforcement
learning (Sutton and Barto 2018) and empirical risk minimization. In this paper, we consider the simplified setting of
linear SA (LSA) algorithms, which estimate a solution of
the linear system **A** [¯] *θ* *[⋆]* = **b** [¯] . For a sequence of step sizes
*{α* *k* *}* *k∈* N, a burn-in period *n* 0 *∈* N, and an initialization
*θ* 0 *∈* R *[d]*, we consider the sequences of estimates *{θ* *k* *}* *k∈* N
and *{θ* [¯] *n* *}* *n≥n* 0 +1 given by

*θ* *k* = *θ* *k−* 1 *−* *α* *k* *{* **A** ( *Z* *k* ) *θ* *k−* 1 *−* **b** ( *Z* *k* ) *}, k ≥* 1 *,*


lot of recent contributions (Huo et al. 2024; Lauand and
Meyn 2022a) focus on the setting of constant step sizes
*α* *k* = *α >* 0. This setting is of particular interest because it enables geometrically fast forgetting of the initialization (Dieuleveut, Durmus, and Bach 2020) and is often
easier to use in practice. At the same time, the solution of
the SA problem obtained with a constant step size suffers
from an inevitable *bias*, which arises in non-linear problems
(Dieuleveut, Durmus, and Bach 2020) or even in linear SA
(1) when the sequence of noise variables *{Z* *k* *}* *k∈* N forms a
Markov chain, see e.g., (Lauand and Meyn 2022a; Durmus
et al. 2025; Huo, Chen, and Xie 2023a). This problem can
be partially mitigated using the Richardson-Romberg (RR)
extrapolation method. To formally define this method, we
denote the LSA iterations (1) with a constant step size *α* and
define the corresponding Polyak-Ruppert averaged iterates

as

*θ* *k* [(] *[α]* [)] = *θ* *k* [(] *[α]* *−* [)] 1 *[−]* *[α][{]* **[A]** [(] *[Z]* *[k]* [)] *[θ]* *k* [(] *[α]* *−* [)] 1 *[−]* **[b]** [(] *[Z]* *[k]* [)] *[}][,]* (2)


*n−* 1

¯
*θ* *n* [(] *[α]* [)] = ( *n −* *n* 0 ) *[−]* [1] � *θ* *k* [(] *[α]* [)] *.*

*k* = *n* 0


*n−* 1
¯ (1)
*θ* *n* = ( *n −* *n* 0 ) *[−]* [1] � *θ* *k* *, n ≥* *n* 0 + 1 *.*

*k* = *n* 0


The next steps of the Richardson-Romberg (RR) procedure
rely on the fact that the bias of *θ* [¯] *n* [(] *[α]* [)] is linear in *α* and is of order *O* ( *α* ), see e.g., (Huo, Chen, and Xie 2023a). To proceed
further, a learner considers two sequences *{θ* *k* [(] *[α]* [)] *[, k][ ∈]* [N] *[}]*
and *{θ* *k* [(2] *[α]* [)] *, k ∈* N *}* with the same noise sequence *{Z* *k* *}* *k∈* N .
Then for any *n ≥* *n* 0 + 1, one can set

¯
*θ* *n* [(] *[α,]* [RR][)] = 2 *θ* [¯] *n* [(] *[α]* [)] *−* *θ* [¯] *n* [(2] *[α]* [)] *.*

The non-asymptotic analysis of Richardson-Romberg extrapolation has recently attracted a lot of contributions in the
context of linear SA (Huo, Chen, and Xie 2023a), stochastic gradient descent (SGD) (Durmus et al. 2016; Dieuleveut,
Durmus, and Bach 2020), and non-linear SA problems (Huo
et al. 2024; Allmeier and Gast 2024a). At the same time, a
large and relatively unexplored gap is related to the question
of the optimality of the leading term of the error bounds for

¯
*θ* *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* . To properly define what ”optimality” means
in this context, note that in the context of linear SA problems with a decreasing step size (1), the sequence *{θ* [¯] *n* *}* *n∈* N
is asymptotically normal under appropriate conditions on


Here, *θ* [¯] *n* corresponds to the Polyak-Ruppert averaged estimator (Ruppert 1988; Polyak and Juditsky 1992), a popular instrument for accelerating the convergence of stochastic
approximation algorithms. In (1), *{Z* *k* *}* *k∈* N is a sequence of
random variables taking values in some measurable space
(Z *, Z* ), and **A** ( *Z* *k* ) and **b** ( *Z* *k* ) are stochastic estimates of **A** [¯]
and **b** [¯], respectively. In this paper, we focus on the setting
where *{Z* *k* *}* *k∈* N is a Markov chain.
One of the key questions related to the recurrence (1) is
the choice of step sizes *{α* *k* *}* *k∈* N . While the classical SA
schemes (Robbins and Monro 1951; Polyak and Juditsky
1992) correspond to the setting of decreasing step sizes, a


-----

*{α* *k* *}* *k∈* N, that is

*√n* (¯ *θ* *n* *−* *θ* *⋆* ) *−→N* *d* (0 *,* Σ *∞* ) *,* *n →∞* *.*

The covariance matrix Σ *∞* here is known to be asymptotically optimal both in a sense of the Rao-Cramer lower
bound and in a sense that it corresponds to the last iterate
of the modified process *θ* [˜] *k*, which uses the optimal preconditioner matrix ( *A* [¯] *[−]* [1] in the context of linear SA). Details
can be found in the papers (Polyak and Juditsky 1992; Fort,
Gersende 2015). A precise expression for Σ *∞* is given later
in the current paper, see (7). It is known for SGD methods
with i.i.d. noise and averaging that the Richardson-Romberg
estimator achieves mean-squared error bounds (MSE) with
the leading term, which aligns with Σ *∞* ; that is,


E [1] *[/]* [2] [ *∥θ* [¯] *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* *∥* [2] ] *≤* *√* Tr Σ *∞* + *O* 1
*√n* � *n* [1] *[/]* [2+] *[δ]*


*,*
�


for some *δ >* 0. This result is due to (Sheshukova et al.
2024). To the best of our knowledge, there is no result of
this kind available for the setting of Markovian SA. In this
paper, we aim to close this gap for the setting of linear SA,
yet we expect that the developed method can be useful for
a more general setting. The main contributions of this paper
are as follows:

 - We propose a novel technique to quantify the asymptotic
bias of *θ* *n* [(] *[α]* [)] [. Our approach considers the limiting distri-]
bution Π *α* of the joint Markov chain *{* ( *θ* *k* [(] *[α]* [)] *[, Z]* *[k]* [+1] [)] *[}]* *[k][∈]* [N]
and analyzes the bias Π *α* ( *θ* 0 ) *−* *θ* *[⋆]* . Then, we apply the
linearization method for *θ* *k* [(] *[α]* [)] from (Aguech, Moulines,
and Priouret 2000). This allows us to study the limiting
distribution of the components, whose average values are
shown to be ordered by powers of *α* .

 - We establish high-order moment error bounds for the
Richardson-Romberg method, where the leading term
aligns with the asymptotically optimal covariance Σ *∞* .
We analyze its dependence on the number of steps *n*, step
size *α*, and the mixing time *t* mix .
### **2 Related work**

The stochastic approximation scheme is widely studied
for reinforcement learning (RL) (Sutton 1988; Sutton and
Barto 2018). The well-known Temporal-Difference (TD)
algorithm with linear function approximation (Bertsekas
and Tsitsiklis 1996) can be represented as the LSA problem. Originally, this method was proposed in (Robbins and
Monro 1951) with a diminishing step size. While asymptotic convergence results were first studied, non-asymptotic
analysis later became of particular interest. For general SA,
non-asymptotic bounds were investigated in (Moulines and
Bach 2011; Gadat and Panloup 2023). For LSA with a constant step size, finite-time analysis was presented in (Mou
et al. 2020, 2024; Durmus et al. 2025).
The bias and MSE for non-linear problems with i.i.d.
noise have been studied for SGD in (Dieuleveut, Durmus,
and Bach 2020; Yu et al. 2021; Sheshukova et al. 2024), and,
recently, with both i.i.d. and Markovian noise in (Zhang and


Xie 2024; Zhang et al. 2024; Huo et al. 2024; Allmeier and
Gast 2024b). Another source of bias arises under Markovian noise and cannot be eliminated using averaging, as
shown in (Lauand and Meyn 2022b, 2023b). MSE bounds
for Markovian LSA have been studied in several works, including (Srikant and Ying 2019; Mou et al. 2024; Durmus
et al. 2025). In (Mou et al. 2024) and (Durmus et al. 2025),
the authors derive the leading term, which aligns with the
optimal covariance Σ *∞*, but they do not eliminate the effect
of the asymptotic bias.
Further, when studying Markovian LSA, in (Lauand and
Meyn 2022a) the authors address the problem of bias, which
can’t be eliminated using PR averaging. In the work (Lauand
and Meyn 2023a), the authors establish weak convergence
of the Markov chain ( *θ* *n* *, Z* *n* +1 ) and also provide a decomposition for the limiting covariance of the iterations. In our
work, we establish a similar result in Theorem 1. The work
(Lauand and Meyn 2024) extends results on bias and convergence of Polyak-Ruppert iterations to diminishing step sizes
*α* *k* = *α* 0 *k* *[−][ρ]* with *ρ ∈* (0 *,* 1 */* 2).
The non-asymptotic analysis of Richardson-Romberg has
been carried out in (Durmus et al. 2016; Huo et al. 2024;
Sheshukova et al. 2024; Allmeier and Gast 2024a) for general SA, with particular applications to SGD. Further, in
(Huo, Chen, and Xie 2023a) and (Huo, Chen, and Xie
2023b), the authors derive bounds for the LSA problem.
In the work (Huo et al. 2024), the authors establish a bias
decomposition for general SA up to the linear term in the
step size *α* and derive MSE bounds dependent on *α* and the
mixing time. For LSA, (Huo, Chen, and Xie 2023a) extends
this analysis by deriving a bias decomposition via an infinite series expansion in *α* and examining the MSE under
the RR procedure, which eliminates arbitrary leading-order
terms. Both works demonstrate that the RR technique accelerates convergence and maintains the proper scaling with
the mixing time. However, neither work explicitly identifies the leading-term coefficient, and their results primarily
address the improvement of higher-order terms in *α* . Additionally, (Huo, Chen, and Xie 2023a) imposes a restrictive
reversibility assumption on the underlying Markov chain,
limiting its applicability. Separately, (Huo, Chen, and Xie
2023b) explores the role of the RR procedure in statistical
inference, particularly in constructing confidence intervals.
Further, in (Zhang and Xie 2024; Kwon et al. 2025) authors
consider the application of the RR procedure for Q-learning
and two-timescale SA. A comparison of the bias decompositions known in the literature with our approach can be found
in Section 4.
### **3 Notations**

Consider a Polish space Z and a Markov kernel Q on
(Z *, Z* ) endowed with its Borel *σ* -field denoted by *Z* and let
(Z [N] *, Z* *[⊗]* [N] ) be the corresponding canonical space. Consider
a Markov kernel Q on Z *× Z* and denote by P *ξ* and E *ξ* the
corresponding probability distribution and expectation with
initial distribution *ξ* . Without loss of generality, assume that
( *Z* *k* ) *k∈* N is the associated canonical process. By construction, for any A *∈Z*, P *ξ* ( *Z* *k* *∈* A *| Z* *k−* 1 ) = Q( *Z* *k−* 1 *,* A),
P *ξ* -a.s. In the case *ξ* = δ *z*, *z ∈* Z, P *ξ* and E *ξ* are denoted


-----

by P *z* and E *z* . Also, for any measurable space (X *, G* ) with
the signed measure *µ*, we define the total variation norm
*∥µ∥* TV = *|µ|* (X).
Let (X *, G* ) be a complete separable metric space equipped
with its Borel *σ* -algebra *G* . We call *c* : X *×* X *→* R +
a distance-like function, if it is symmetric, lower semicontinuous and *c* ( *x, y* ) = 0 if and only if *x* = *y*, and there
exists *q ∈* N such that ( *d* ( *x, y* ) *∧* 1) *[q]* *≤* *c* ( *x, y* ). We denote by
*H* ( *ξ, ξ* *[′]* ) the set of couplings of probability measures *ξ* and
*ξ* *[′]*, that is, a set of probability measures on (X *×* X *, G ⊗G* ),
such that for any Γ *∈H* ( *ξ, ξ* *[′]* ) and any *A ∈G* it holds
Γ(X *× A* ) = *ξ* *[′]* ( *A* ) and Γ( *A ×* X) = *ξ* ( *A* ). We define the
Wasserstein semimetric associated to the distance-like functiton *c* *[p]* ( *·, ·* ), as


Gersende 2015), the asymptotically optimal covariance matrix Σ *∞* is defined as

Σ *∞* = ( **A** [¯] ) *[−]* [1] Σ [(M)] *ε* ( **A** [¯] ) *[−][T]* *.* (7)

In the considered setting when *{Z* *k* *}* *k∈* N is a Markov chain,
the sequence *{θ* *k* [(] *[α]* [)] *[}]* [ given by (4), considered separately]
from *{Z* *k* *}* *k∈* N, might fail to be a Markov chain. This is not
the case in the setting when *Z* *k* are i.i.d. random variables,
see e.g. (Mou et al. 2020; Durmus et al. 2025). That is why,
in the current paper we need to consider the joint process
( *θ* *k* [(] *[α]* [)] *[, Z]* *[k]* [+1] [)][, which is a Markov chain with the kernel][ ¯P] *[α]* [,]
specified below. For any measurable and bounded function
*f* : R *[d]* *×* Z *→* R +, ( *θ, z* ) *∈* R *[d]* *×* Z, we define P [¯] *α* as

¯P *α* *f* ( *θ, z* ) = Q( *z,* d *z* *[′]* ) *f* ( **F** *z* *′* ( *θ* ) *, z* *[′]* ) *,*
� Z

**F** *z* ( *θ* ) = (I *−* *α* **A** ( *z* )) *θ* + *α* **b** ( *z* ) *.*

Thus, our next aim is to perform a quantitative analysis of
¯P *α* . In particular, we show below that under appropriate regularity conditions, P [¯] *α* admits a unique invariant distribution
Π *α* . Specifically, we impose the following assumptions:

**A1.** C **A** = sup *z∈* Z *∥* **A** ( *z* ) *∥∨* sup *z∈* Z *∥* **A** [˜] ( *z* ) *∥* *< ∞* *and the*
*matrix −* **A** [¯] *is Hurwitz.*
In particular, the condition that *−* **A** [¯] is Hurwitz implies
that the linear system **A** [¯] *θ* = **b** [¯] has a unique solution *θ* *[⋆]* .
We further require the following assumptions on the noise
term *ε* ( *z* ) and the stationary distribution *π* of the sequence
*{Z* *k* *}* *k∈* N *∗* :
**A2.** � Z **[A]** [(] *[z]* [)d] *[π]* [(] *[z]* [) = ¯] **[A]** *[ and]* � Z **[b]** [(] *[z]* [)d] *[π]* [(] *[z]* [) = ¯] **[b]** *[. Moreover,]*

*∥ε∥* *∞* = sup *∥ε* ( *z* ) *∥* *<* + *∞.*
*z∈* Z

**Theorem 1.** *Assume A1, A2, and* ***UGE*** *1. Let* 2 *≤* *p ≤* *q.*
*Then, for any α ∈* (0 *,* ( *α* *q,* [(] *[M]* *∞* [)] *[∧]* *[a]* *[−]* [1] [)] *[t]* *[−]* mix [1] [)] *[, the Markov ker-]*
*nel* P [¯] *α* *admits a unique invariant distribution* Π *α* *, such that*
Π *α* ( *∥θ* 0 *−* *θ* *[⋆]* *∥* ) *< ∞. Here α* *q,* [(M)] *∞* *[is a constant depending]*
*upon q and other problem characteristics, and is defined in*
(30) *.*

*Proof sketch.* We consider two noise sequences, *{Z* *n* *, n ∈*
N *}* and *{Z* [˜] *n* *, n ∈* N *}*, with a coupling time *T* . They evolve
separately before time *T* and coincide afterwards. See more
details on coupling construction in Appendix B.1. To prove
the statement, we first establish the result on the contraction
of the Wasserstein semimetric (3) with the cost function c 0,
defined as

c 0 (( *θ, z* ) *,* ( *θ* *[′]* *, z* *[′]* )) = ( *∥θ −* *θ* *[′]* *∥* + **1** *{z* = *̸* *z* *′* *}* )

*×* �1 + *∥θ −* *θ* *[⋆]* *∥* + *∥θ* *[′]* *−* *θ* *[⋆]* *∥* � *,*

where ( *θ, z* ) *,* ( *θ* *[′]* *, z* *[′]* ) *∈* R *[d]* *×* Z. To do that, we consider
two coupled Markov chains *{* ( *θ* *k* [(] *[α]* [)] *[, Z]* *[k]* [+1] [)] *[, k][ ≥]* [0] *[}]* [ and]
*{* ( *θ* [˜] *k* [(] *[α]* [)] *[,]* [ ˜] *[Z]* *[k]* [+1] [)] *[, k][ ≥]* [0] *[}]* [, starting from][ (] *[θ, z]* [)][ and][ (˜] *[θ,]* [ ˜] *[z]* [)][ re-]
spectively. For *n ≥* 1 and *θ,* *θ* [˜] *∈* R, we define:

*θ* *n* [(] *[α]* [)] = *θ* *n* [(] *[α]* *−* [)] 1 *[−]* *[α][{]* **[A]** [(] *[Z]* *[n]* [)] *[θ]* *n* [(] *[α]* *−* [)] 1 *[−]* **[b]** [(] *[Z]* *[n]* [)] *[}][,]* *θ* 0 = *θ,*

˜
*θ* *n* [(] *[α]* [)] = *θ* [˜] *n* [(] *[α]* *−* [)] 1 *[−]* *[α][{]* **[A]** [( ˜] *[Z]* *[n]* [)˜] *[θ]* *n* [(] *[α]* *−* [)] 1 *[−]* **[b]** [( ˜] *[Z]* *[n]* [)] *[}][,]* *θ* 0 = *θ .* [˜]


**W** *c,p* ( *ξ, ξ* *[′]* ) = inf
Γ *∈H* ( *ξ,ξ* *[′]* )


*c* *[p]* ( *x, x* *[′]* )Γ( *dx, dx* *[′]* ) *.* (3)

� X *×* X


We also denote **W** *c* ( *ξ, ξ* *[′]* ) := **W** *c,* 1 ( *ξ, ξ* *[′]* ).
### **4 Bias of the LSA iterates**

In this section we aim to study the properties of the sequence
*θ* *k* [(] *[α]* [)] given by (2) based on theory of Markov chains. Using
the definition (2) and some elementary algebra, we obtain

*θ* *k* [(] *[α]* [)] *−* *θ* *[⋆]* = (I *−* *α* **A** ( *Z* *k* ))( *θ* *k* [(] *[α]* *−* [)] 1 *[−]* *[θ]* *[⋆]* [)] *[ −]* *[αε]* [(] *[Z]* *[k]* [)] *[,]* (4)

where we have set

˜ ¯
*ε* ( *z* ) = **A** [˜] ( *z* ) *θ* *[⋆]* *−* **b** [˜] ( *z* ) *,* **A** ( *z* ) = **A** ( *z* ) *−* **A** *,* (5)

˜ ¯
**b** ( *z* ) = **b** ( *z* ) *−* **b** *.*

We consider the following assumptions on the noise variables *{Z* *k* *}* :
**UGE1.** *{Z* *k* *}* *k∈* N *is a Markov chain with the Markov kernel*
Q *taking values in complete separable metric space* (Z *, Z* ) *.*
*Moreover,* Q *admits π as an invariant distribution and is*
*uniformly geometrically ergodic, that is, there exists t* mix *∈*
N *[∗]* *such that for all k ∈* N *[∗]* *,*

∆(Q *[k]* ) *≤* (1 */* 4) *[⌊][k/t]* [mix] *[⌋]* *,* (6)

*where* ∆(Q *[k]* ) *is Dobrushin coefficient defined as*

∆(Q *[k]* ) = sup (1 */* 2) *∥* Q *[k]* ( *z, ·* ) *−* Q *[k]* ( *z* *[′]* *, ·* ) *∥* TV *.*
*z,z* *[′]* *∈* Z

*Equivalently, there exist constants ζ >* 0 *and ρ ∈* (0 *,* 1) *such*
*that for all k ≥* 1 *,*

sup *∥* Q *[k]* ( *z, ·* ) *−* *π∥* TV *≤* *ζρ* *[k]* *.*
*z∈* Z

Here, *t* mix is the mixing time of Q. **UGE** 1 implies, in
particular, that *π* is the unique invariant distribution of Q.
We also define the noise covariance matrix


�


Z **[A]** [(] *[z]* [)d] *[π]* [(] *[z]* [) = ¯] **[A]** *[ and]* �


Σ [(M)] *ε* = E *π* [ *ε* ( *Z* 0 ) *ε* ( *Z* 0 ) *[T]* ] + 2


*∞*
� E *π* [ *ε* ( *Z* 0 ) *ε* ( *Z* *ℓ* ) *[T]* ] *.*

*ℓ* =1


This covariance is limiting for the sum *n* *[−]* [1] *[/]* [2] [ �] *[n]* *t* =0 *[−]* [1] *[ε]* [(] *[Z]* *[t]* [)][,]
see (Douc et al. 2018)[Theorem 21.2.10]. Due to (Fort,


-----

Then, for any *z, z* *[′]* *∈* Z, from the result in Proposition 4, we
get:

˜
E *z,z* ˜ [c 0 (( *θ* *n* [(] *[α]* [)] *[, Z]* *[n]* [)] *[,]* [ (˜] *[θ]* *n* [(] *[α]* [)] *[,]* *Z* [ ˜] *n* ))] (8)

≲ *ρ* *[n]* *α* [c] [0] [((] *[z, θ]* [)] *[,]* [ (˜] *[z,]* [ ˜] *[θ]* [))] *[,]*

where *ρ* *α* = e *[−][αa/]* [24] and the expectation is taken over the
coupling measure. Finally, the existence and uniqueness of
the invariant measure Π *α* follows from the contraction inequality (8) in conjunction with (Douc et al. 2018, Theorem
20.3.4). The detailed proof is provided in Appendix B.1.

Our next goal is to quantify the bias

Π *α* [ *θ* 0 ] *−* *θ* *[⋆]* *.*

Towards this aim, we consider the perturbation-expansion
framework of (Aguech, Moulines, and Priouret 2000), see
also (Durmus et al. 2025). We define the product of random
matrices

Γ [(] *m* *[α]* : [)] *n* [=][ �] *[n]* *i* = *m* [(I] *[ −]* *[α]* **[A]** [(] *[Z]* *[i]* [))] *[,]* *m ≤* *n,* (9)

with the convention, Γ [(] *m* *[α]* : [)] *n* [= I][ for] *[ m > n]* [. Then we con-]
sider the decomposition of the error into the transient and
fluctuation terms

*θ* *n* [(] *[α]* [)] *−* *θ* *[⋆]* = *θ* [˜] *n* [(][tr][)] + *θ* [˜] *n* [(][fl][)] *,* (10)

where

*θ* ˜ *n* [(][tr][)] = Γ [(] 1: *[α]* *n* [)] *[{][θ]* [0] *[ −]* *[θ]* *[⋆]* *[}][,]* (11)


where we set *J* 0 [(] *[l,α]* [)] = *H* 0 [(] *[l,α]* [)] = 0. It is easy to check that,
in this setting,

*H* *n* [(] *[l,α]* [)] = *J* *n* [(] *[l]* [+1] *[,α]* [)] + *H* *n* [(] *[l]* [+1] *[,α]* [)] *,*

and


˜
*θ* *n* [(][fl][)] =


*L*
� *J* *n* [(0] *[,α]* [)] + *H* *n* [(] *[L,α]* [)] *.* (16)

*ℓ* =0


To analyze the bias Π *α* [ *θ* 0 ] *−* *θ* *[⋆]*, we consider this expansion with *L* = 2. That is, combining (12), (13) and (14),
we obtain the decomposition which is the cornerstone of our
analysis:

*θ* *n* [(] *[α]* [)] *−θ* *[⋆]* = *θ* [˜] *n* [(][tr][)] + *J* *n* [(0] *[,α]* [)] + *J* *n* [(1] *[,α]* [)] + *J* *n* [(2] *[,α]* [)] + *H* *n* [(2] *[,α]* [)] *.* (17)

Following the arguments in (Durmus et al. 2021), this decomposition can be used to obtain sharp bounds on the *p* -th
moment of the final LSA iterate *θ* *n* [(] *[α]* [)] [.]

**Bias expansion for LSA** Similarly to (4), we can not consider the process *{J* *k* [(] *[ℓ,α]* [)] *}* separately, as it might fail to be a
Markov chain. Instead, we again consider the joint process

*Y* *t* = ( *Z* *t* +1 *, J* *t* [(0] *[,α]* [)] *, J* *t* [(1] *[,α]* [)] ) (18)

with the Markov kernel Q *J* (1), which can be defined formally in the similar way as P [¯] *α* . We need to refine our assumptions on the step-size compared to (31). More specifically, for any 2 *≤* *p < ∞*, we set


*t* mix *[−]* [1] *[,]* (19)
�


1
*α* *p,* [(][b] *∞* [)] [=] � *α* *p* [(M)] (1+log *d* ) *,∞* *[∧]* 1 + C **A** *∧* *ap* [1]


˜
*θ* *n* [(][fl][)] = *−α*


*n*
� Γ [(] *j* *[α]* +1: [)] *n* *[ε]* [(] *[Z]* *[j]* [)] *[ .]*

*j* =1


**Bounding the transient and fluctuation terms** To bound
the transient term, we apply the result on exponential stability of the random matrix product from (Durmus et al.
2025, Proposition 7). For the fluctuation term *θ* [˜] *n* [(][fl][)] we use the
perturbation expansion technique formalized in (Aguech,
Moulines, and Priouret 2000) and later applied to obtain the
high-probability bounds in (Durmus et al. 2025). For this
decomposition, we define for any *l ≥* 0 the vectors *{J* *n* [(] *[l,α]* [)],
*H* *n* [(] *[l,α]* [)] *}* which can be computed from the recursion relations

*J* *n* [(0] *[,α]* [)] = �I *−* *α* **A** [¯] � *J* *n* [(0] *−* *[,α]* 1 [)] *[−]* *[αε]* [(] *[Z]* *[n]* [)] *[,]* (12)

*H* *n* [(0] *[,α]* [)] = (I *−* *α* **A** ( *Z* *n* )) *H* *n* [(0] *−* *[,α]* 1 [)] *[−]* *[α]* [ ˜] **[A]** [(] *[Z]* *[n]* [)] *[J]* *n* [(0] *−* *[,α]* 1 [)] *[,]* [ (13)]

where *J* 0 [(0] *[,α]* [)] = *H* 0 [(0] *[,α]* [)] = 0. It is easy to check that

˜
*θ* *n* [(][fl][)] = *J* *n* [(0] *[,α]* [)] + *H* *n* [(0] *[,α]* [)] *.*

Moreover, the term *H* *n* [(0] *[,α]* [)] can be further decomposed similalry to (12) - (13). Precisely, for any *L ∈* N *[∗]* and *ℓ* *∈*
*{* 1 *, . . ., L}*, we consider

*J* *n* [(] *[l,α]* [)] = �I *−* *α* **A** [¯] � *J* *n* [(] *[l,α]* *−* 1 [)] *[−]* *[α]* [ ˜] **[A]** [(] *[Z]* *[n]* [)] *[J]* *n* [(] *[l]* *−* *[−]* 1 [1] *[,α]* [)] *,* (14)

and

*H* *n* [(] *[ℓ,α]* [)] = (I *−* *α* **A** ( *Z* *n* )) *H* *n* [(] *[ℓ,α]* *−* 1 [)] *[−]* *[α]* [ ˜] **[A]** [(] *[Z]* *[n]* [)] *[J]* *n* [(] *[ℓ,α]* *−* 1 [)] *[,]* (15)


where *α* *∞* [(M)] is defined in (31). For ease of notation, we set
*α* take smaller step sizes in order to control higher moments. *∞* [(][b][)] [:=] *[ α]* 2 [(][b] *,∞* [)] [. Note that the established step size suggests to]

**Proposition 1.** *Assume A 1, A 2 and* ***UGE*** *1. Let α ∈*
(0 *, α* *∞* [(][b][)] [)] *[. Then the process][ {][Y]* *t* *[}]* *t∈* N *[is a Markov chain with]*
*a unique stationary distribution* Π *J* (1) *,α* *.*

*Proof sketch.* We consider the Markov chain *{Y* *t* *, t ≥* 0 *}*
with kernel Q *J* (1), where *Y* *t* = ( *Z* *t* +1 *, J* *t* [(0] *[,α]* [)] *, J* *t* [(1] *[,α]* [)] ).
Our approach involves analyzing the convergence of this
Markov chain using the Wasserstein semimetric, defined in
(3), with a properly chosen cost function. Denoting *Y* =
( *z, J* [(0)] *, J* [(1)] ) and *Y* [˜] = (˜ *z,* *J* [˜] [(0)] *,* *J* [˜] [(1)] ), where *Y,* *Y* [˜] *∈* Z *×*
R *[d]* *×* R *[d]*, we define the cost function as:

c( *Y,* *Y* [˜] ) = *∥J* [(0)] *−* *J* [˜] [(0)] *∥* + *∥J* [(1)] *−* *J* [˜] [(1)] *∥* (20)

+ ( *∥J* [(0)] *∥* + *∥J* [˜] [(0)] *∥*

+ *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) **1** *{z* =˜ *̸* *z}* *.*

Note, the term *[√]* *αa∥ε∥* *∞* is introduced to account for the
fluctuations of *J* *n* [(0] *[,α]* [)] and *J* *n* [(1] *[,α]* [)], whose magnitudes do not
exceed the order of this term. Now, we introduce the result
on the contraction of the Wasserstein semimetric for two
coupled Markov chains *{Y* *t* *}* and *{Y* [˜] *t* *}* starting from different points. Choosing *J* [(0)] *,* *J* [˜] [(0)] *, J* [(1)] *,* *J* [˜] [(1)] *∈* R *[d]* and *z,* ˜ *z ∈*


-----

Z, we denote *y* = ( *z, J* [(0)] *, J* [(1)] ) and ˜ *y* = (˜ *z,* *J* [˜] [(0)] *,* *J* [˜] [(1)] )
such that *y ̸* = ˜ *y* . Then, by Lemma 3 with *p* = 1, for any
*n ≥* 1, we have
**W** c ( *δ* *y* Q *[n]* *J* [(1)] *[,δ]* *[y]* [˜] [Q] *[n]* *J* [(1)] [)] (21)


≲ *ρ* *[n]* 1 *,α* �


log (1 */αa* )c( *y,* ˜ *y* ) *,*


where *ρ* 1 *,α* = *e* *[−][αa/]* [12] . Thus, the existence of invariant
distribution Π *J* (1) *,α* directly follows from (21) and (Douc

                                    et al. 2018, Theorem 20.3.4); for more details, see A p
pendix B.3.

We denote random variables
( *Z* *∞* +1 *, J* *∞* [(0] *[,α]* [)] *, J* *∞* [(1] *[,α]* [)] )
with distribution Π *J* (1) *,α* . Under stationary distribution, we

have E[ *J* *∞* [(0] *[,α]* [)] ] = 0. Consider now the component that corresponds to *J* *∞* [(1] *[,α]* [)] . The following proposition holds:
**Proposition 2.** *Assume A1, A2 and* ***UGE*** *1. Then for α ∈*
(0 *, α* *∞* [(][b][)] [)] *[, it holds that]*

lim *n* ] = E[ *J* *∞* [(1] *[,α]* [)] ] = *α* ∆+ *R* ( *α* ) *,*
*n→∞* [E][[] *[J]* [(1] *[,α]* [)]

*where* ∆ *∈* R *[d]* *is defined as*

*∞*
∆= **A** [¯] *[−]* [1] � E[ **A** [˜] ( *Z* *∞* + *k* ) *ε* ( *Z* *∞* )] *,*

*k* =1

*and R* ( *α* ) *is a reminder term which can be bounded as*

*∥R* ( *α* ) *∥≤* 12 *∥* **A** [¯] *[−]* [1] *∥* C [2] **A** *[t]* mix [2] *[α]* [2] *[∥][ε][∥]* *[∞]* *[.]*

**Corollary 1.** *Under the setting of Proposition 2, we get the*
*following expansion for the asymptotic bias*

lim (22)
*n→∞* [E][[] *[θ]* *[n]* [] = Π] *[α]* [(] *[θ]* [0] [) =] *[ θ]* *[⋆]* [+] *[ α]* [∆+] *[ O]* [(] *[α]* [3] *[/]* [2] [)] *[ .]*

*Proof.* From Proposition 8, we deduce that
lim *n→∞* E� *∥J* *n* [(2] *[,α]* [)] *∥* � ≲ *α* [3] *[/]* [2] and lim *n→∞* E� *∥H* *n* [(2] *[,α]* [)] *∥* � ≲
*α* [3] *[/]* [2] . This implies that the term *J* *n* [(1] *[,α]* [)] should be the leading
term in the bias decomposition. Together with the analysis
of *J* *n* [(2] *[,α]* [)], this confirms that *{J* *n* [(] *[l]* [+1] *[,α]* [)] *, l ≥* 0 *}* provides
the proper linearization of the bias in powers of *α*, giving
rigorous justification for our decomposition approach. For
the complete proof, we refer to Appendix B.5.

**Remark** **1.** *By* *sequentially* *analyzing* *the* *terms*
*{J* *n* [(] *[k,α]* [)] *, k* *≥* 2 *} in the decomposition of θ* *n* *, we can*
*obtain the bias decomposition as a power series in α.*
*Additionally, in Proposition 6, we show that*

lim *n* ] = *α* [2] ∆ 2 + *R* 2 ( *α* ) *,*
*n→∞* [E][[] *[J]* [(2] *[,α]* [)]

*where*


*∞*
� E[ **A** [˜] ( *Z* *∞* + *k* + *i* +1 ) **A** [˜] ( *Z* *∞* + *i* +1 ) *ε* ( *Z* *∞* )]

*i* =0


∆ 2 = *−*


*∞*
�

*k* =1


*and ∥R* 2 ( *α* ) *∥* ≲ *α* [5] *[/]* [2] *. Unrolling further* (15) *for H* *n* [(2] *[,α]* [)] *, we*
*can sharpen the remainder term in the bias decomposition*
(22) *. Indeed, using a technique similar to the one used for*
*the p-th moment of J* *n* [(2] *[,α]* [)] *in Proposition 8, we can expect*
*that* E [1] *[/p]* [ *∥J* *n* [(3] *[,α]* [)] *∥* *[p]* ] ≲ *α* [2] *. Therefore, we conclude that the*
*rate O* ( *α* [2] ) *could be achieved in the remainder term of* (22) *.*


**Discussion** Our coefficient ∆ in the linear term matches
the representation derived in (Lauand and Meyn 2023a, Theorem 2.5), but that work does not analyze MSE with reduced bias. To observe the next result, we define an adjoint
kernel Q *[∗]* such that for the invariant measure *π*, we have
*π ⊗* Q( *A × B* ) = *π ⊗* Q *[∗]* ( *B × A* ). Additionally, we define the independent kernel Π such that for any *z ∈* Z and
*A ∈Z*, Π( *z, A* ) = *π* ( *A* ). Under these notations, the authors in (Huo, Chen, and Xie 2023a) considered the bias expansion arising from the Neumann series for the operator
(I *−* Q *[∗]* + Π) *[−]* [1] (Q *[∗]* *−* Π). Furthermore, adapting the proof
of Proposition 2, our result can be reformulated in terms of
Q *[∗]* . This representation is less desirable because it requires
reversibility of the Markov kernel Q, as discussed in (Huo,
Chen, and Xie 2023a).
### **5 Analysis of Richardson-Romberg** **procedure**

A natural way to reduce the bias in (22) is to use the
Richardson-Romberg extrapolation (Hildebrand 1987)

¯
*θ* *n* [(] *[α,]* [RR][)] = 2 *θ* [¯] *n* [(] *[α]* [)] *−* *θ* [¯] *n* [(2] *[α]* [)] *.* (23)

After this procedure the remainder term in the bias has order *O* ( *α* [3] *[/]* [2] ). Before the main theorem of this section, we
establish our key technical results. For that, we consider another Markov chain *{V* *t* *}* *t∈* N with kernel Q *J*, where we set
*V* *t* = ( *J* *t* *, Z* *t* +1 ). In fact, it is closely related to the one described in (18) and also converges geometrically fast to the
unique stationary distribution as stated by Corollary 2.

**Corollary 2.** *Assume A1, A2 and* ***UGE*** *1. Let α ∈* (0 *, α* *∞* [(][b][)] [)] *[.]*
*Then the process {V* *t* *}* *t∈* N *is a Markov chain with a unique*
*stationary distribution* Π *J,α* *.*

*Proof sketch.* We define the cost function c *J* : R *[d]* *×* Z *×*
R *[d]* *×* Z *→* R +, as:

c *J* (( *J, z* ) *,* ( *J,* [˜] ˜ *z* )) = *∥J −* *J* [˜] *∥*

+ ( *∥J∥* + *∥J* [˜] *∥* + *[√]* *αa∥ε∥* *∞* ) **1** *{z* =˜ *̸* *z}* *.*

The result on the contraction of the Wasserstein semimetric
with cost function c *J* for *{V* *t* *}* *t∈* N can be also obtained independently using the technique from Proposition 1. However,
we derive a weaker result directly from Proposition 1, showing that **W** c *J* *,p* ( *δ* *y* Q *[n]* *J* *[, δ]* *[y]* [˜] [Q] *[n]* *J* [)] *[ ≤]* **[W]** [c] *[,p]* [(] *[δ]* *[y]* [Q] *[n]* *J* [(1)] *[, δ]* *[y]* [˜] [Q] *[n]* *J* [(1)] [)][.]
Hence, using the similar arguments, we conclude that the
Markov chain *{V* *t* *, t ≥* 0 *}* admits invariant distribution
Π *J,α* . The proof can be found in Appendix C.

Note that the invariant distribution Π *J,α* coincides with
the distribution of ( *J* *∞* [(0] *[,α]* [)] *, Z* *∞* +1 ). For any *J ∈* R *[d]* *, z ∈* Z,
we define:

¯
*ψ* ( *J, z* ) = *ψ* ( *J, z* ) *−* E *π* *J* [ *ψ* 0 ] *,*

¯
*ψ* *t* = ¯ *ψ* ( *J* *t* [(0] *[,α]* [)] *, Z* *t* +1 ) *.*

The cost functions c *J* and c *J* (1) are designed such that the
function *ψ* ( *J, z* ) = **A** [˜] ( *z* ) *J* for *J ∈* R *[d]* *, z ∈* Z is Lipschitz,
specifically:

*∥ψ* ( *J, z* ) *−* *ψ* ( *J,* [˜] ˜ *z* ) *∥≤* 2 C **A** c *J* (( *J, z* ) *,* ( *J,* [˜] ˜ *z* )) *.*


-----

This Lipschitz property is necessary for our analysis of
Theorem 2. The following result concerns the magnitude
of [�] *t* *[n]* = *[−]* *n* [1] 0 *[ψ]* *[t]* [, which appears in the decomposition (26). It]
has a non-zero bias, and thus, a direct estimation leads to
non-optimal behavior. However, after centering, the result
in Proposition 3 suggests that it can be estimated effectively.
This provides a theoretical justification for the numerical experiments presented in Section 6.

**Proposition 3.** *Assume A1, A2, and* ***UGE*** *1. Then for any*
*probability measure ξ on* R *[d]* *×* Z *,* 2 *≤* *p < ∞* *and α ∈*
(0 *, α* *p,* [(][b] *∞* [)] [)] *[, we get]*


The leading term can be bounded using the result for the *p* th moment of the last iteration in Lemma 7. The last term
can be further decomposed using

*n−* 1
� *t* = *n* 0 *[e]* � *θ* *t* [(] *[α]* [)] *, Z* *t* +1 � = *E* *n* [(][tr] *[,α]* [)] + *E* *n* [(][fl] *[,α]* [)] *,* (27)

where we have set

*E* *n* [(][tr] *[,α]* [)] = [�] *t* *[n]* = *[−]* *n* [1] 0 **[A]** [˜] [(] *[Z]* *[t]* [+1] [)Γ] 1: [(] *[α]* *t* [)] *[{][θ]* [0] *[ −]* *[θ]* *[⋆]* *[}][,]*

*n−* 1
*E* *n* [(][fl] *[,α]* [)] = [�] *t* *[n]* = *[−]* *n* [1] 0 *[ε]* [(] *[Z]* *[t]* [+1] [) +][ �] *ℓ* [2] =0 � *t* = *n* 0 **[A]** [˜] [(] *[Z]* *[t]* [+1] [)] *[J]* *t* [(] *[ℓ,α]* [)]


+


*n−* 1
�

*t* = *n* 0


**A** ˜ ( *Z* *t* +1 ) *H* *t* [(2] *[,α]* [)] *.*


E [1] *ξ* *[/p]* [ *∥*


*n−* 1

¯

� *ψ* *t* *∥* *[p]* ] *≤* c [(2)] *W,* 1 *[p]* [3] *[/]* [2] [(] *[αn]* [)] [1] *[/]* [2]

*t* =0


+ c [(2)] *W,* 2 *[p]* [3] *[α]* *[−]* [1] *[/]* [2] ~~[�]~~ log (1 */αa* ) *,*

*where the constants* c [(2)] *W,* 1 *[,]* [ c] [(2)] *W,* 2 *[are defined in the supplement]*
*paper, see* (68) *.*

Now, we conclude the result on *p* -th moment for error of
the RR iteration (23).

**Theorem 2.** *Assume A1, A2 and* ***UGE*** *1. Fix* 2 *≤* *p < ∞,*
*then for any n ≥* *t* mix *, α ∈* (0 *, α* *p,* [(][b] *∞* [)] [)] *[ and initial probability]*
*measure ξ on* (Z *, Z* ) *, we have*

E [1] *ξ* *[/p]* [ *∥* **A** [¯] ( *θ* [¯] *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* ) *∥* *[p]* ] (24)

*≤* 2 C Rm *,* 1 *{* Tr Σ [(M)] *ε* *}* [1] *[/]* [2] *p* [1] *[/]* [2] *n* *[−]* [1] *[/]* [2] + *R* *n,p,α* [(][fl][)]
+ *R* *n,p,α* [(][tr][)] *[∥][θ]* [0] *[−]* *[θ]* *[⋆]* *[∥]* [exp] *[{−][αan/]* [24] *[}][,]*

*where R* *n,p,α* [(][tr][)] *[,][ R]* *n,p,α* [(][fl][)] *[are provided in]* [ (25)] *[, and]* [ C] Rm *,* 1 [= 60e]
*is obtained from the Rosenthal inequality(see Theorem 3).*

The quantities *R* *n,p,α* [(][tr][)] [and] *[ R]* *n,p,α* [(][fl][)] [correspond to the fluc-]
tuation and transient terms in the error decomposition. We
set them as follows

*R* *n,p,α* [(][fl][)] [≲] *[pn]* *[−]* [3] *[/]* [4] (25)

+ ( *p* [3] *[/]* [2] ( *αn* ) *[−]* [1] *[/]* [2] + *α* [1] *[/]* [2] ) *p* [3] *[/]* [2] *n* *[−]* [1] *[/]* [2]

+ *p* [7] *[/]* [2] *α* [3] *[/]* [2] log [3] *[/]* [2] (1 */αa* ) *,*

*R* *n,p,α* [(][tr][)] [≲] [(] *[αn]* [)] *[−]* [1] *[,]*

Here ≲ stands for the inequality up a constant which may
depend on *t* mix . Precise expressions for the terms *R* *n,p,α* [(][fl][)] [and]
*R* *n,p,α* [(][tr][)] [are given in the supplement paper, see equation (87).]

*Proof sketch of Theorem 2.* Using (1) and the definition of
the noise term *ε* ( *·* ) in (5), we can write the decomposition
for the Richardson-Romberg iterations

¯
**A** (¯ *θ* *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* ) (26)

= *{* 2 *α* ( *n −* *n* 0 ) *}* *[−]* [1] (4 *θ* *n* [(] *[α]* 0 [)] *[−]* *[θ]* *n* [(2] 0 *[α]* [)] *−* (4 *θ* *n* [(] *[α]* [)] *−* *θ* *n* [(2] *[α]* [)] ))

*n−* 1
+ *{n −* *n* 0 *}* *[−]* [1] � *{e* ( *θ* *t* [(2] *[α]* [)] *, Z* *t* +1 ) *−* 2 *e* ( *θ* *t* [(] *[α]* [)] *, Z* *t* +1 ) *} .*

*t* = *n* 0


The first term in *E* *n* [(][fl] *[,α]* [)] is linear statistics of Markov chain
*{Z* *k* *, k ∈* N *}* . Therefore, we can bound it using the version of Rosenthal inequality for Markov chains from (Durmus et al. 2023). For the term involving *J* *t* [(0] *[,α]* [)], we employ
the expansion from (88), yielding a centered random variable component plus a bias term. This decomposition allows
direct application of the inequality in Proposition 3 to the
sum of centered random variables, which yields the bound
*O* (( *α/n* ) [1] *[/]* [2] + *α* *[−]* [1] *[/]* [2] *n* *[−]* [1] ). Combining this with the result
from Proposition 2, we conclude that the remaining term is
*O* ( *α* [2] ).
Then, we apply Proposition 8 to control the statistic
� *tn* = *−n* 1 0 **[A]** [˜] [(] *[Z]* *[t]* [+1] [)] *[J]* *t* [(1] *[,α]* [)], which we express in terms of *J* *n* [(2] *[,α]* [)]
via the expansion in (14). For the analogous term involving
*H* *n* [(2] *[,α]* [)], we establish the required bound in Proposition 9.
The detailed proof can be found in Appendix D.1.

Note that the bound in Proposition 7 motivates the choice
*α* = *O* ( *n* *[−]* [1] *[/]* [2] ), aligning with the rate observed in the
i.i.d case. Optimization over *α* gives us the following highprobability bound. Also, the term with *p* [3] could be slightly
improved to *p* [2] through a more accurate analysis of the
Lemma 5. Additionally, following the discussion in Remark 1, we expect that the remainder term *O* ( *α* [3] *[/]* [2] ) in Theorem 2 could be improved to *O* ( *α* [2] ), though this would
require a technically complicated analysis of *J* *n* [(3] *[,α]* [)] . Using Markov’s inequality, we derive the following highprobability bound.

**Corollary 3.** *Assume A1, A2 and* ***UGE*** *1. For* 2 *≤* *p < ∞*
*and any n ≥* *t* mix *, we consider the step size*

*α* ( *n, d, t* mix *, p* ) = *α* *p,* [(][b] *∞* [)] *[n]* *[−]* [1] *[/]* [2] *[ .]* (28)

*Substituting* (28) *into* (24) *with p* = ln (3e */δ* ) *, it holds with*
*probability at least* 1 *−* *δ, that*


+ (1 + log [3] *[/]* [2] ( *n* ) log [5] *[/]* [2] (1 */δ* )) log (1 */δ* ) *n* *[−]* [3] *[/]* [4]

+ *n* *[−]* [1] *[/]* [2] log (1 */δ* ) *∥θ* 0 *−* *θ* *[⋆]* *∥* exp � *−α* 1+log [(][b][)] *d,∞* *[n]* [1] *[/]* [2] [�] *.*

**Discussion** Our analysis establishes high-order moment
bounds and, as a consequence, high-probability bounds for
RR iterations in Markovian LSA. Moreover, the leading


*∥* **A** [¯] ( *θ* [¯] *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* ) *∥* ≲ �


Tr Σ [(M)] *ε* log (1 */δ* ) *n* *[−]* [1] *[/]* [2]


-----

(a) (b) (c) (d)

Figure 1: Subfigure (a): error for RR iterations (23). Subfigure (b): error for PR iterations (2). Subfigure (c): error for RR,
multiplied by a factor corresponding to the leading term of (25) after substituting *α* . Subfigure (d): MSE of Polyak-Ruppert and
Richardson-Romberg iterations for different step sizes *α* .


term in (24) scales with *{* Tr Σ [(M)] *ε* *}* [1] *[/]* [2], which is known to
be locally asymptotically minimax optimal for the PolyakRuppert iterates (Mou et al. 2024) and aligns with the CLT
covariance matrix Σ *∞* (see (7)). In (Dieuleveut, Durmus, and
Bach 2020), the authors study the bias and MSE for SGD
with i.i.d noise, and propose the Richardson-Romberg extrapolation to reduce this bias. However, they only consider
MSE bounds and do not obtain the proper factor for the leading term. In the Markovian LSA literature, the authors similarly consider only MSE and do not explicitly emphasize the
leading term (Huo et al. 2024; Huo, Chen, and Xie 2023a,b;
Zhang and Xie 2024). The closest result, (Sheshukova et al.
2024), shows high-order bounds with the leading term properly aligned with the optimal covariance, but in this work,
the authors consider general SA with i.i.d. noise, the analysis of which differs significantly from our case.
### **6 Experiments**

In this section, we aim to demonstrate the effect of reduced
bias achieved through Richardson-Romberg extrapolation
and to validate the accuracy of the bound obtained in Theorem 2. For this purpose, we adopt an example introduced in
(Lauand and Meyn 2024). More precisely, we consider the
Markovian noise *{Z* *k* *, k ≥* 1 *}* on the space Z = *{* 0 *,* 1 *}* with

*a* 1 *−* *a*
transition matrix *P* = 1 *−* *a* *a* and *a ∈* (0 *,* 1). For
� �

any *z ∈{* 0 *,* 1 *}*, we consider the noisy observations

**A** ( *z* ) = *z · A* [(1)] + (1 *−* *z* ) *· A* [(0)] *,*

**b** ( *z* ) = *z · b* [(1)] + (1 *−* *z* ) *· b* [(0)] *,*

where we set


Figure 1d illustrates the significant reduction in bias
achieved by the Richardson-Romberg scheme, estimating
E[ *∥θ* [¯] *n* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* [2] ] and E[ *∥θ* [¯] *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* *∥* [2] ]. These results justify that, after a few iterations, the error of the RR procedure
starts to decrease faster than for PR averaging. Additionally,
in Figure 1, we show that the resulting dependence on *α*
and *n* in the bounds (25) is tight. To achieve this, for different sample size *n* we select different step sizes of the form
*α* = *n* *[−][β]* for *β ∈* [1 */* 2 *,* 1), substitute these into (25), and
compute the scaling of the term *R* *n,p,α* [(][fl][)] [w.r.t.] *[ n]* [. For] *[ β][ ≥]* [1] *[/]* [2][,]
with mentioned choice of *α*, *R* *n,p,α* [(][fl][)] [scales as] *[ n]* *[β][−]* [2] [.]
To verify numerically this rate, we consider the following procedure. We approximate the terms E[ *∥θ* [¯] *n* [(] *[α]* [)] *−* *θ* *[⋆]* +
(1 */n* ) [�] *[n]* *k* =1 *[ε]* [(] *[Z]* *[k]* [)] *[∥]* [2] []][ for PR averaging, and]


∆ [(] *[RR]* [)] = E[ *∥θ* [¯] *n* [(] *[α,]* [RR][)] *−* *θ* *[⋆]* + (1 */n* )


*n*
� *ε* ( *Z* *k* ) *∥* [2] ]

*k* =1


*−* 2 0 0
*A* [(0)] = *−* 2 1 *−* 2 *,* *b* [(0)] = 0 *,*
� � � �

1 0 1
*A* [(1)] = *−* 2 *−* 1 1 *,* *b* [(1)] = 2 1 *.*
� � � �


Hence, we have **A** [¯] = I and **b** [¯] = (1 */* 2) *b* [(1)] . In the following
experiments, we set *a* = 0 *.* 3 and ran *N* traj = 400 trajectories
from *θ* 0 = *θ* *[⋆]* following (2).


for Richardson-Romberg iterations. The moments of the latter term should scale with *n* *[β][−]* [2] . We verify this effect numerically setting *α* = *n* *[−][β]* for *β ∈{* 1 */* 2 *,* 2 */* 3 *,* 3 */* 4 *,* 5 */* 6 *}*
and providing the plots for ∆ [(] *n* *[RR]* [)] and *n* [2] *[−][β]* ∆ [(] *n* *[RR]* [)] in Figure 1a and Figure 1c, respectively. Additionally, in Figure 1a
and Figure 1b, we compare the error for different choices of
step *α* . We can see that the step *α* = *n* *[−]* [1] *[/]* [2] gives the smallest
error for Richardson-Romberg iterations, while for PolyakRuppert averaging this choice of step introduces a large bias
in the error.
### **7 Conclusion**

We studied the high-order error bounds for RichardsonRomberg extrapolation in the setting of Markovian linear
stochastic approximation. By applying the novel technique
for bias characterization, we were able to obtain the leading
term which aligns with the asymptotically optimal covariance matrix Σ *∞* . For further work, we consider the generalization of the obtained results to the setting of non-linear
Markovian SA and SA with state-dependent noise.


-----

### **8 Acknowledgments**

The authors are grateful to Eric Moulines for valuable discussions and feedback. This research was supported in part
through computational resources of HPC facilities at HSE
University (Kostenetskiy, Chulkevich, and Kozyrev 2021).
### **References**

Aguech, R.; Moulines, E.; and Priouret, P. 2000. On a perturbation approach for the analysis of stochastic tracking algorithms. *SIAM Journal on Control and Optimization*, 39(3):
872–899.

Allmeier, S.; and Gast, N. 2024a. Computing the Bias
of Constant-step Stochastic Approximation with Markovian Noise. In Globerson, A.; Mackey, L.; Belgrave, D.;
Fan, A.; Paquet, U.; Tomczak, J.; and Zhang, C., eds.,
*Advances in Neural Information Processing Systems*, volume 37, 137873–137902. Curran Associates, Inc.

Allmeier, S.; and Gast, N. 2024b. Computing the Bias
of Constant-step Stochastic Approximation with Markovian Noise. In Globerson, A.; Mackey, L.; Belgrave, D.;
Fan, A.; Paquet, U.; Tomczak, J.; and Zhang, C., eds.,
*Advances in Neural Information Processing Systems*, volume 37, 137873–137902. Curran Associates, Inc.

Bertsekas, D.; and Tsitsiklis, J. N. 1996. *Neuro-dynamic*
*programming* . Athena Scientific.

Dieuleveut, A.; Durmus, A.; and Bach, F. 2020. Bridging the
gap between constant step size stochastic gradient descent
and Markov chains. *The Annals of Statistics*, 48(3): 1348 –
1382.

Douc, R.; Moulines, E.; Priouret, P.; and Soulier, P. 2018.
*Markov chains* . Springer Series in Operations Research and
Financial Engineering. Springer. ISBN 978-3-319-97703-4.

Durmus, A.; Moulines, E.; Naumov, A.; and Samsonov,
S. 2025. Finite-time high-probability bounds for Polyak–
Ruppert averaged iterates of linear stochastic approximation. *Mathematics of Operations Research*, 50(2): 935–964.

Durmus, A.; Moulines, E.; Naumov, A.; Samsonov, S.;
Scaman, K.; and Wai, H.-T. 2021. Tight High Probability Bounds for Linear Stochastic Approximation with
Fixed Stepsize. In Ranzato, M.; Beygelzimer, A.; Nguyen,
K.; Liang, P. S.; Vaughan, J. W.; and Dauphin, Y., eds.,
*Advances in Neural Information Processing Systems*, volume 34, 30063–30074. Curran Associates, Inc.

Durmus, A.; Moulines, E.; Naumov, A.; Samsonov, S.;
and Sheshukova, M. 2023. Rosenthal-type inequalities
for linear statistics of Markov chains. *arXiv preprint*
*arXiv:2303.05838* .

Durmus, A.; Simsekli, U.; Moulines, E.; Badeau, R.; and
RICHARD, G. 2016. Stochastic Gradient RichardsonRomberg Markov Chain Monte Carlo. In Lee, D.;
Sugiyama, M.; Luxburg, U.; Guyon, I.; and Garnett, R., eds.,
*Advances in Neural Information Processing Systems*, volume 29. Curran Associates, Inc.

Fort, Gersende. 2015. Central limit theorems for stochastic approximation with controlled Markov chain dynamics.
*ESAIM: PS*, 19: 60–80.


Gadat, S.; and Panloup, F. 2023. Optimal non-asymptotic
analysis of the Ruppert–Polyak averaging stochastic algorithm. *Stochastic Processes and their Applications*, 156:
312–348.

Hildebrand, F. 1987. *Introduction to Numerical Analysis* .
Dover books on advanced mathematics. Dover Publications.

ISBN 9780486653631.

Huo, D.; Chen, Y.; and Xie, Q. 2023a. Bias and extrapolation
in Markovian linear stochastic approximation with constant
stepsizes. In *Abstract Proceedings of the 2023 ACM SIG-*
*METRICS International Conference on Measurement and*
*Modeling of Computer Systems*, 81–82.

Huo, D.; Chen, Y.; and Xie, Q. 2023b. Effectiveness of Constant Stepsize in Markovian LSA and Statistical Inference.
arXiv:2312.10894.

Huo, D. L.; Zhang, Y.; Chen, Y.; and Xie, Q. 2024. The collusion of memory and nonlinearity in stochastic approximation with constant stepsize. *Advances in Neural Information*
*Processing Systems*, 37: 21699–21762.

Kostenetskiy, P.; Chulkevich, R.; and Kozyrev, V. 2021.
HPC resources of the higher school of economics. In *Jour-*
*nal of Physics: Conference Series*, volume 1740, 012050.
IOP Publishing.

Kwon, J.; Dotson, L.; Chen, Y.; and Xie, Q. 2025. TwoTimescale Linear Stochastic Approximation: Constant Stepsizes Go a Long Way. In *The 28th International Conference*
*on Artificial Intelligence and Statistics* .

Lauand, C. K.; and Meyn, S. 2022a. Bias in stochastic approximation cannot be eliminated with averaging. In *2022*
*58th Annual Allerton Conference on Communication, Con-*
*trol, and Computing (Allerton)*, 1–4. IEEE.

Lauand, C. K.; and Meyn, S. 2022b. Bias in Stochastic Approximation Cannot Be Eliminated With Averaging. In *2022*
*58th Annual Allerton Conference on Communication, Con-*
*trol, and Computing (Allerton)*, 1–4.

Lauand, C. K.; and Meyn, S. 2023a. The curse of memory
in stochastic approximation. In *2023 62nd IEEE Conference*
*on Decision and Control (CDC)*, 7803–7809. IEEE.

Lauand, C. K.; and Meyn, S. 2023b. The Curse of
Memory in Stochastic Approximation: Extended Version.
arXiv:2309.02944.

Lauand, C. K.; and Meyn, S. 2024. Revisiting step-size
assumptions in stochastic approximation. *arXiv preprint*
*arXiv:2405.17834* .

Mou, W.; Li, C. J.; Wainwright, M. J.; Bartlett, P. L.; and Jordan, M. I. 2020. On linear stochastic approximation: Finegrained Polyak-Ruppert and non-asymptotic concentration.
In *Conference on Learning Theory*, 2947–2997. PMLR.

Mou, W.; Pananjady, A.; Wainwright, M.; and Bartlett,
P. 2024. Optimal and instance-dependent guarantees for
Markovian linear stochastic approximation. *Mathematical*
*Statistics and Learning*, 7.

Moulines, E.; and Bach, F. 2011. Non-asymptotic analysis
of stochastic approximation algorithms for machine learning. *Advances in neural information processing systems*, 24:
451–459.


-----

Pinelis, I. 1994. Optimum Bounds for the Distributions of
Martingales in Banach Spaces. *The Annals of Probability*,
22(4): 1679 – 1706.

Polyak, B. T.; and Juditsky, A. B. 1992. Acceleration of
stochastic approximation by averaging. *SIAM journal on*
*control and optimization*, 30(4): 838–855.

Rio, E. 2017. *Asymptotic Theory of Weakly Dependent Ran-*
*dom Processes*, volume 80. Springer.
Robbins, H.; and Monro, S. 1951. A stochastic approximation method. *The annals of mathematical statistics*, 400–
407.

Ruppert, D. 1988. Efficient estimations from a slowly convergent Robbins-Monro process. Technical report, Cornell
University Operations Research and Industrial Engineering.
Sheshukova, M.; Belomestny, D.; Durmus, A.; Moulines,
E.; Naumov, A.; and Samsonov, S. 2024. Nonasymptotic
analysis of stochastic gradient descent with the richardsonromberg extrapolation. *arXiv preprint arXiv:2410.05106* .
Srikant, R.; and Ying, L. 2019. Finite-time error bounds for
linear stochastic approximation and TD learning. In *Confer-*
*ence on Learning Theory*, 2803–2830. PMLR.

Sutton, R. S. 1988. Learning to predict by the methods of
temporal differences. *Machine Learning*, 3(1): 9–44.

Sutton, R. S.; and Barto, A. G. 2018. *Reinforcement Learn-*
*ing: An Introduction* . The MIT Press, second edition.

Villani, C. 2009. *Optimal transport : old and new* .
Grundlehren der mathematischen Wissenschaften. Berlin:
Springer. ISBN 978-3-540-71049-3.
Yu, L.; Balasubramanian, K.; Volgushev, S.; and Erdogdu,
M. A. 2021. An Analysis of Constant Step Size SGD in the
Non-convex Regime: Asymptotic Normality and Bias. In
Ranzato, M.; Beygelzimer, A.; Dauphin, Y.; Liang, P.; and
Vaughan, J. W., eds., *Advances in Neural Information Pro-*
*cessing Systems*, volume 34, 4234–4248. Curran Associates,
Inc.

Zhang, Y.; Huo, D.; Chen, Y.; and Xie, Q. 2024. Prelimit coupling and steady-state convergence of constantstepsize nonsmooth contractive sa. In *Abstracts of the 2024*
*ACM SIGMETRICS/IFIP PERFORMANCE Joint Interna-*
*tional Conference on Measurement and Modeling of Com-*
*puter Systems*, 35–36.

Zhang, Y.; and Xie, Q. 2024. Constant stepsize q-learning:
Distributional convergence, bias and extrapolation. *arXiv*
*preprint arXiv:2401.13884* .


-----

### **A Notations and Constants**

Denote N *[∗]* = N *\ {* 0 *}* and N *−* = Z *\* N *[∗]* . Let *d ∈* N *[∗]* and *Q* be a symmetric positive definite *d × d* matrix. For *x ∈* R *[d]*, we
denote *∥x∥* *Q* = *{x* *[⊤]* *Qx}* 1 */* 2 . For brevity, we set *∥x∥* = *∥x∥* I *d* . We denote *∥A∥* *Q* = max *∥x∥* *Q* =1 *∥Ax∥* *Q*, and the subscriptless
norm *∥A∥* = *∥A∥* I is the standard spectral norm. For a function *g* : Z *→* R *[d]*, we denote *∥g∥* *∞* = sup *z∈* Z *∥g* ( *z* ) *∥* . For a random
variable *ξ*, we denote its distribution by *L* ( *ξ* ).
We denote S *[d][−]* [1] = *{x ∈* R *[d]* : *∥x∥* = 1 *}* . Let *A* 1 *, . . ., A* *N* be *d* -dimensional matrices. We denote [�] *[j]* *ℓ* = *i* *[A]* *[ℓ]* [=] *[ A]* *[i]* *[ . . . A]* *[j]* [ if]
*i ≤* *j* and by convention [�] *[j]* *ℓ* = *i* *[A]* *[ℓ]* [= I][ if] *[ i > j]* [.]
The readers can refer to the Table 1 on the variables, constants and notations that are used across the paper for references.

Table 1: Constants, definitions, notations


**Variable** **Descri** **p** **tion** **Reference**
*Q* Solution of Lyapunov equation for **A** [¯] Proposition 10
*κ* *Q* *λ* *[−]* min [1] [(] *[Q]* [)] *[λ]* [max] [(] *[Q]* [)] Proposition 10
*a* Real part of minimum eigenvalue of **A** [¯] Proposition 10
Γ *m* [(] *[α]* : [)] *n* Product of random matrices with step size *α* (9)
*θε* ˜( *n* [(][tr] *Z* [)] *n* *[,]* [ ˜] ) *[θ]* *n* [(][fl][)] Noise in LSA procedureTransient and fluctuation terms of LSA error (5)(11)
*α* *p,∞* (resp. *α* *p,* [(] *[M]* *∞* [)] [)] Stability threshold for Γ [(] *m* *[α]* : [)] *n* [to have bounded] (31)
*p* -th moment under **UGE** 1


*α* *q,* [(][b] *∞* [)] Threshold for the existence of invariant distribution Π *J* (1) (19)
*J* *n* [(0)] Dominant term in *θ* [˜] *n* [(][fl][)] (12)
*H* *n* [(0)] Residual term *θ* [˜] *n* [(][fl][)] *−* *J* *n* [(0)] (12)
*J* *n* [(1)] *[, H]* *n* [(1)] [,] *[J]* *n* [(2)] *[, H]* *n* [(2)] Elements of the decomposition (17) (14)-(15)
Σ [(M)] *ε* Noise covariance E[ *ε* 1 *ε* *[⊤]* 1 []] A2
c 0 Cost function associated with the vector ( *θ, z* ) (35)
c *J* Cost function associated with the vector ( *J* [(0] *[,α]* [)] *, z* ) (60)
c Cost function associated with the vector ( *z, J* [(0] *[,α]* [)] *, J* [(1] *[,α]* [)] ) (20)
c *J* (2) Cost function associated with the vector ( *z, J* [(0] *[,α]* [)] *, J* [(1] *[,α]* [)] *, J* [(2] *[,α]* [)] ) (49)
Π *α* Invariant distribution of *{* ( *θ* *t* [(] *[α]* [)] *, Z* *t* +1 ) *, t ≥* 0 *}* Theorem 1
Π *J,α* Invariant distribution of *{* ( *J* *t* [(0] *[,α]* [)] *, Z* *t* +1 ) *, t ≥* 0 *}* Corollary 2
Π *J* (1) *,α* Invariant distribution of *{* ( *Z* *t* +1 *, J* *t* [(0] *[,α]* [)] *, J* *t* [(1] *[,α]* [)] ) *, t ≥* 0 *}* Proposition 1
C Rm *,* 1 = 60e Constant in martingale Rosenthal’s inequality (Pinelis 1994, Theorem 4.1)
C Rm *,* 2 = 60 Constant in martingale Rosenthal’s inequality (Pinelis 1994, Theorem 4.1)
C Ros *,* 1 = [16] 3 ~~*√*~~ ~~*√*~~ 19 3 [C] Rm [5] *[/]* [2] 1 [,]


19

3 [C] Rm [5] *[/]* [2] *,* 1 [,]



[16] ~~*√*~~ 19

3 ~~*√*~~ 3


C Ros *,* 2 = 64(C Rm [2] *,* 1 [C] [1] Rm *[/]* [2] *,* 2 [+ C] [Rm] *[,]* [2] [)] Constants in Rosenthal’s inequallity under **UGE** 1 Theorem 3
*{F* *t* *}* *t∈* N filtration *F* *t* = *σ* ( *Z* *s* : 1 *≤* *s ≤* *t* ) with *F* 0 = *{∅,* Z *}*
E *[F]* *[t]* the conditional ex p ectation with res p ect to *F* *t*
### **B Bias decomposition**

We define the constants
*κ* *Q* = *λ* max ( *Q* ) */λ* min ( *Q* ) *,* *b* *Q* = 2 *[√]* *κ* *Q* C **A** *.* (29)

Under A1, we define the quantity

*−* 1
*α* *∞* [(M)] = � *α* *∞* *∧* *κ* *[−]* *Q* [1] *[/]* [2] C *[−]* **A** [1] *[∧]* *[a/]* [(6e] *[κ]* *[Q]* [ C] **[A]** [)] � *× ⌈* 8 *κ* [1] *Q* *[/]* [2] [C] **[A]** *[ /a][⌉]* *,* (30)

C **Γ** = 4( *κ* [1] *Q* *[/]* [2] [C] **[A]** [ +] *[a/]* [6)] [2] *[ × ⌈]* [8] *[κ]* [1] *Q* *[/]* [2] [C] **[A]** *[ /a][⌉]* *[,]*

where *α* *∞*, *a, κ* *Q* are defined in (97) and (29), respectively. Now we use *α* *∞* [(M)] and C **Γ** to define, for *q ≥* 2,

*α* *q,* [(M)] *∞* [=] *[ α]* *∞* [(M)] *∧* c [(M)] **A** */q,* c [(M)] **A** = *a/{* 12 C **Γ** *} .* (31)
The upper bounds (30) and (31) on the step size are required for the result on product of random matrices under Markov
conditions **UGE** 1, which can be found in (Durmus et al. 2025). We formulate this result in the Appendix E.


-----

#### **B.1 Proof of Theorem 1**

We preface the proof by some definitions and properties of coupling. We follow Let (X *, X* ) be a measurable space. In all
this section, Q and Q *[′]* denote two probability measures on the canonical space (X [N] *, X* *[⊗]* [N] ). Fix *x* *[∗]* *∈* X. For any X-valued
stochastic process *X* = *{X* *n* *}* *n∈* N and any N [¯] -valued random variable *T*, define the X-valued stochastic process S *T* *X* by
S *T* *X* = *{X* *T* + *k* *, k ∈* N *}* on *{T < ∞}* and S *T* *X* = ( *x* *[∗]* *, x* *[∗]* *, x* *[∗]* *, . . .* ) on *{T* = *∞}* . For any measure Q on (X [N] *, X* *[⊗]* [N] )
and any *σ* -field *G ⊂X* *[⊗]* [N], we denote by ( *µ* ) *G* the restriction of the measure *µ* to *G* . Moreover, for all *n ∈* N, define the
*σ* -field *G* *n* = �S *[−]* *n* [1] [(] *[A]* [) :] *[ A][ ∈X]* *[ ⊗]* [N] [�] . We say that (Ω *, F,* P *, X, X* *[′]* *, T* ) is an *exact coupling* of (Q *,* Q *[′]* ) (see (Douc et al. 2018,
Definition 19.3.3)), if

 - for all *A ∈X* *[⊗]* [N], P( *X ∈* *A* ) = Q( *A* ) and P( *X* *[′]* *∈* *A* ) = Q *[′]* ( *A* ),

 - S *T* *X* = S *T* *X* *[′]* *,* P *−* a *.* s *.*

The integer-valued random variable *T* is a coupling time. An exact coupling
(Ω *, F,* P *, X, X* *[′]* *, T* ) of (Q *,* Q *[′]* ) is *maximal* (see (Douc et al. 2018, Definition 19.3.5)) if for all *n ∈* N,

��(Q) *G* *n* *−* (Q *′* ) *G* *n* �� TV [= 2][P][(] *[T > n]* [)] *[ .]*

Assume that (X *, X* ) is a complete separable metric space and let Q and Q *[′]* denote two probability measures on (X [N] *, X* *[⊗]* [N] ).
Then, there exists a maximal exact coupling of (Q *,* Q *[′]* ).
We now turn to the special case of Markov chains. Let P be a Markov kernel on (X *, X* ). Denote by *{X* *n* *}* *n∈* N the coordinate
process and define as before *G* *n* = �S *[−]* *n* [1] [(] *[A]* [) :] *[ A][ ∈X]* *[ ⊗]* [N] [�] . By (Douc et al. 2018, Lemma 19.3.6), for any probabilities *µ, ν* on
(X *, X* ), we have
���(P *µ* ) *G* *n* *−* (P *ν* ) *G* *n* ��� TV [=] *[ ∥][µ]* [P] *[n]* *[ −]* *[ν]* [P] *[n]* *[∥]* [TV] *[ .]* (32)

Moreover, if (X *, X* ) is Polish, then, there exists a maximal and exact coupling of (P *µ* *,* P *ν* ); see (Douc et al. 2018, Theorem 19.3.9).
We apply this construction for the Markov kernel Q defined on the complete separable metric space (Z *,* d Z ). For any two
probabilities *ξ, ξ* *[′]* on (Z *, Z* ), there exists a maximal exact coupling (Ω *, F,* P [˜] *ξ,ξ* *[′]* *, Z, Z* *[′]* *, T* ) of P [Q] *ξ* [and][ P] [Q] *ξ* *[′]* [, that is,]

*∥ξ* Q *[n]* *−* *ξ* *[′]* Q *[n]* *∥* TV = 2P( *T > n* ) *.* (33)

We write E [˜] *ξ,ξ* *′* for the expectation with respect to P [˜] *ξ,ξ* *′* .
Also, we note that from (6) it immediately follows that

� *∞k* =0 [∆][(Q] *[k]* [)] *[ ≤]* [(4] *[/]* [3)] *[t]* [mix] *[ .]* (34)

For ( *θ, z* ) *,* ( *θ* *[′]* *, z* *[′]* ) *∈* R *[d]* *×* Z, define the cost function

c 0 (( *θ, z* ) *,* ( *θ* *[′]* *, z* *[′]* )) = ( *∥θ −* *θ* *[′]* *∥* + **1** *{z* = *̸* *z* *′* *}* )�1 + *∥θ −* *θ* *[⋆]* *∥* + *∥θ* *[′]* *−* *θ* *[⋆]* *∥* � *,* (35)

which is symmetric, lower semi-continuous and distance-like(see (Douc et al. 2018, Chapter 20.1)). Note that it can be lower
bounded by the distance function
*d* 0 (( *θ, z* ) *,* ( *θ* *[′]* *, z* *[′]* )) = *∥θ −* *θ* *[′]* *∥* + **1** *{z* = *̸* *z* *′* *}*

Now, we consider two noise sequences *{Z* *n* *, n ∈* N *}* and *{Z* [˜] *n* *, n ∈* N *}* with coupling time *T* . For *n ≥* 1 and *θ,* *θ* [˜] *∈* R, we
define

*θ* *n* [(] *[α]* [)] = *θ* *n* [(] *[α]* *−* [)] 1 *[−]* *[α][{]* **[A]** [(] *[Z]* *[n]* [)] *[θ]* *n* [(] *[α]* *−* [)] 1 *[−]* **[b]** [(] *[Z]* *[n]* [)] *[}][,]* *θ* 0 = *θ,* (36)

˜
*θ* *n* [(] *[α]* [)] = *θ* [˜] *n* [(] *[α]* *−* [)] 1 *[−]* *[α][{]* **[A]** [( ˜] *[Z]* *[n]* [)˜] *[θ]* *n* [(] *[α]* *−* [)] 1 *[−]* **[b]** [( ˜] *[Z]* *[n]* [)] *[}][,]* *θ* 0 = *θ .* [˜]

**Proposition 4.** *Assume A1, A2, and* ***UGE*** *1. Let q ≥* 8 *. Then, for any α ∈* (0; ( *α* *q,* [(] *[M]* *∞* [)] *[∧]* *[a]* *[−]* [1] [)] *[t]* *[−]* mix [1] [)] *[ with][ α]* *q,* [(] *[M]* *∞* [)] *[defined in]* [ (31)] *[,]*
*starting points* ( *z, θ* ) *,* (˜ *z,* *θ* [˜] ) *∈* Z *×* R *such that* ( *z, θ* ) *̸* = (˜ *z,* *θ* [˜] ) *. Then for any n ∈* N *, we get*

˜
E *z,z* ˜ [c 0 (( *θ* *n* [(] *[α]* [)] *[, Z]* *[n]* [)] *[,]* [ (˜] *[θ]* *n* [(] *[α]* [)] *[,]* [ ˜] *Z* *n* ))] *≤* D *θ* *d* [2] *[/q]* *ρ* *[n]* *α* [c] [0] [((] *[z, θ]* [)] *[,]* [ (˜] *[z,]* [ ˜] *[θ]* [))] *[,]*

*where*

D *θ* = c *θ,* 6 �1 + 2 *κ* [1] *Q* *[/]* [2] *[e]* [2] *[d]* [1] *[/q]* [ + 4][D] [2] *[d]* [1] *[/q]* *[√][αat]* [mix] *[∥][ε][∥]* *[∞]* � *,*

*ρ* *α* = e *[−][αa/]* [24] *,*

*and* c *θ,* 6 *is defined in* (39) *.*


-----

*Proof.* Applying H¨older’s and then Minkowski’s inequalities, we get

˜
E *z,z* ˜ [c 0 (( *θ* *n* [(] *[α]* [)] *[, Z]* *[n]* [)] *[,]* [ (˜] *[θ]* *n* [(] *[α]* [)] *[,]* [ ˜] *Z* *n* ))] *≤{* E [˜] *z,z* ˜ [( *∥θ* *n* [(] *[α]* [)] *−* *θ* [˜] *n* [(] *[α]* [)] *[∥]* [+] **[ 1]** *{Z* *n* = *̸* *Z* [˜] *n* *}* [)] [2] []] *[}]* [1] *[/]* [2] (37)

*×* (1 + *{* E *z* [ *∥θ* *n* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* [2] ] *}* [1] *[/]* [2] + *{* E *z* ˜ [ *∥θ* [˜] *n* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* [2] ] *}* [1] *[/]* [2] ) *.*

We bound the first term on the right-hand side of (37). Using (11), definition of the coupling time (33), and S *T* *Z* = S *T* *Z* [˜], we
obtain


*n*
� *{* I *−* *α* **A** ( *Z* [˜] *i* ) *}* ( *θ* [˜] *−* *θ* *[⋆]* ) (38)

*i* =1


*θ* *n* [(] *[α]* [)] *−* *θ* [˜] *n* [(] *[α]* [)] =


*n*
� *{* I *−* *α* **A** ( *Z* *i* ) *}* ( *θ −* *θ* *[⋆]* ) *−*

*i* =1


*n*
� (I *−* *α* **A** ( *Z* *i* )) **b** ( *Z* *j* ) + *α*

*i* = *j* +1


*n*
� (I *−* *α* **A** ( *Z* [˜] *i* )) **b** ( *Z* [˜] *j* ) *,*

*i* = *j* +1


*n∧T*
�

*i* =1


+ *α*


*n∧T*
�

*i* =1


or, equivalently,


*n*
� *{* I *−* *α* **A** ( *Z* [˜] *i* ) *}* ( *θ* [˜] *n* [(] *[α]* *∧* [)] *T* *[−]* *[θ]* *[⋆]* [)] *[ .]*

*i* = *n∧T* +1


*θ* *n* [(] *[α]* [)] *−* *θ* [˜] *n* [(] *[α]* [)] =


*n*
� *{* I *−* *α* **A** ( *Z* *i* ) *}* ( *θ* *n* [(] *[α]* *∧* [)] *T* *[−]* *[θ]* *[⋆]* [)] *[ −]*

*i* = *n∧T* +1


Now we bound the two terms in the right-hand side of (38) separately. Using H¨older’s inequality, we get

˜ *n*
E *z,z* ˜ � *∥* � *i* = *n∧T* +1 *[{]* [I] *[ −]* *[α]* **[A]** [(] *[Z]* *[i]* [)] *[}∥]* [2] *[∥][θ]* *n* [(] *[α]* *∧* [)] *T* *[−]* *[θ]* *[⋆]* *[∥]* [2] []] *[ ≤]* [E] *z* [1] *[/]* [2] � *∥θ* *n* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* [4] [�] P [˜] [1] *z,* *[/]* *z* ˜ [2] [(] *[T][ ≥]* *[n]* [)]


*n*

*i* = *k* +1 *[{]* [I] *[ −]* *[α]* **[A]** [(] *[Z]* *[i]* [)] *[}∥]* [8] [�] E [1] *ξ* *[/]* [4] � *∥θ* *k* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* [8] [�] P [˜] [1] *ξ,* *[/]* *ξ* [˜][2] [(] *[T]* [ =] *[ k]* [) =:] *[ T]* [1] [ +] *[ T]* [2] *[ .]*


+


*n−* 1
�


*n*

� E [1] *ξ* *[/]* [4] � *∥* � *i* =

*k* =1


We begin with estimating the term *T* 2 . By definition of the maximal coupling (32), P [˜] [1] *ξ,* *[/]* *ξ* [˜][2] [(] *[T][ ≥]* *[k]* [)] *[ ≤]* *[ς]* [1] *[/]* [2] *[ρ]* *[k/]* [2] [. Note also that]
(Durmus et al. 2025, Proposition 7) implies

*n*
E [1] *ξ* *[/]* [4] � *∥* � *i* = *k* +1 *[{]* [I] *[ −]* *[α]* **[A]** [(] *[Z]* *[i]* [)] *[}∥]* [8] [�] *≤* *κ* *Q* e [4] *d* [2] *[/q]* *ρ* 1 [2(] *,α* *[n][−][k]* [)] *,*

where *ρ* 1 *,α* = *e* *[−][αa/]* [12] . Moreover, by Lemma 7, we get for any *k ∈* N, that

E [1] *ξ* *[/]* [4] � *∥θ* *k* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* [8] [�] *≤* 2 *κ* *Q* *e* [4] *d* [2] *[/q]* *ρ* [2] 1 *[k]* *,α* *[∥][θ][ −]* *[θ]* *[⋆]* *[∥]* [2] [ + 8][D] 2 [2] *[d]* [2] *[/q]* *[αat]* [mix] *[∥][ε][∥]* [2] *∞* *[,]*

Combining the bounds above, we obtain that


*T* 2 *≤* c *θ,* 1 *d* [4] *[/q]* *ρ* [2] 1 *[n]* *,α* [(] *[ρ]* [1] *[/]* [2] *[/]* [(1] *[ −]* *[ρ]* [1] *[/]* [2] [))] *[∥][θ][ −]* *[θ]* *[⋆]* *[∥]* [2] [ + c] *[θ,]* [2] *[d]* [4] *[/q]* *[αat]* [mix]

where
c *θ,* 1 = 2 *κ* [2] *Q* [e] [8] *[ς]* [1] *[/]* [2] *[,]* c *θ,* 2 = 8 *κ* *Q* e [4] *ς* [1] *[/]* [2] D [2] 2 *[.]*


*n−* 1
� *ρ* 1 [2(] *,α* *[n][−][k]* [)] *ρ* *[k/]* [2] *,*

*k* =1


Note also that the condition *α ≤* 3 *a* *[−]* [1] log *ρ* *[−]* [1] implies *ρ* [1] *[/]* [2] *≤* *ρ* [2] 1 *,α* [. Combining the above bounds yields]


*αa*

Hence, we obtain the final bound on *T* 2 as


*n−* 1
� *ρ* [2(] 1 *,α* *[n][−][k]* [)] *ρ* *[k/]* [2] *≤* *αanρ* [2] 1 *[n]* *,α* *[≤]* [12e] *[−]* [1] *[ρ]* *[n]* 1 *,α* *[.]*

*k* =1


*T* 2 *≤* c *θ,* 1 *d* [4] *[/q]* *ρ* [2] 1 *[n]* *,α* [(] *[ρ]* [1] *[/]* [2] *[/]* [(1] *[ −]* *[ρ]* [1] *[/]* [2] [))] *[∥][θ][ −]* *[θ]* *[⋆]* *[∥]* [2] [ + c] *[θ,]* [3] *[d]* [4] *[/q]* *[ρ]* *[n]* 1 *,α* *[,]*

where
c *θ,* 3 = 24 *κ* *Q* e [3] *ς* [1] *[/]* [2] D [2] 2 *[.]*

Similarly, using Lemma 7 and the definition of the coupling time *T*, we get

*T* 1 *≤* 2 *κ* *Q* e [4] *ς* [1] *[/]* [2] *d* [2] *[/q]* *ρ* [2] 1 *[n]* *,α* *[ρ]* *[n/]* [2] *[∥][θ][ −]* *[θ]* *[⋆]* *[∥]* [2] [ + 8] *[ς]* [1] *[/]* [2] [D] 2 [2] *[d]* [2] *[/q]* *[αat]* [mix] *[ρ]* *[n/]* [2] *[ .]*

The previous bounds imply

*T* 1 + *T* 2 *≤* c *θ,* 4 *d* [4] *[/q]* *ρ* [2] 1 *[n]* *,α* *[∥][θ][ −]* *[θ]* *[⋆]* *[∥]* [2] [ + c] *[θ,]* [5] *[d]* [4] *[/q]* *[ρ]* *[n]* 1 *,α* *[,]*


-----

where
c *θ,* 4 = 2 c *θ,* 1 *,* c *θ,* 5 = 32 *κ* *Q* e [3] *ς* [1] *[/]* [2] D [2] 2 *[.]*

Combining the bounds above and Minkowski’s inequality, we get

*{* E [˜] *ξ,ξ* ˜ [( *∥θ* *n* [(] *[α]* [)] *−* *θ* [˜] *n* [(] *[α]* [)] *[∥]* [+] **[ 1]** *{Z* *n* = *̸* *Z* [˜] *n* *}* [)] [2] []] *[}]* [1] *[/]* [2] *[ ≤]* [c] *[θ,]* [6] *[ d]* [2] *[/q]* *[ρ]* *α* *[n]* [(1 +] *[ ∥][θ][ −]* *[θ]* *[⋆]* *[∥]* [+] *[ ∥][θ]* [˜] *[ −]* *[θ]* *[⋆]* *[∥]* [)] *[,]*

where
c *θ,* 6 = *[√]* c *θ,* 4 + 2 *[√]* c *θ,* 5 + *ς* [1] *[/]* [2] *.* (39)

To conclude the proof, it remains to bound the second term in the right-hand side of (37) by using Lemma 7.

*Proof of Theorem 1.* We denote *y* = ( *θ, z* ) and ˜ *y* = ( *θ,* [˜] ˜ *z* ) for *θ,* *θ* [˜] *∈* R *[d]* *, z,* ˜ *z ∈* Z. Using the coupling construction (36) and the
contraction of c 0 in Proposition 4, we get

**W** c 0 ( *δ* *y* P [¯] *nα* *[, δ]* *θ* [˜] [¯P] *nα* [)] *[ ≤]* [D] *[θ]* *[d]* [2] *[/q]* *[ρ]* *[n]* *α* [c] [0] [((] *[z, θ]* [)] *[,]* [ (˜] *[z,]* [ ˜] *[θ]* [))] *[ .]*

Then, applying (Douc et al. 2018, Theorem 20.3.4), we conclude that the Markov chain *{* ( *θ* *k* [(] *[α]* [)] *[, Z]* *[k]* [+1] [)] *[, k][ ∈]* [N] *[}]* [ with the]
Markov kernel P [¯] *α* admits the unique invariant distribution Π *α* . Finally, from (Villani 2009, Theorem 6.9) we conclude that
Π *α* ( *∥θ* 0 *−* *θ* *[⋆]* *∥* ) *< ∞* .
#### **B.2 Contraction for Wasserstein semimetric**

Before the main result of Lemma 3, we should state a preliminary lemmas on contraction of *{J* *n* [(0] *[,α]* [)] *, n ≥* 0 *}* and *{J* *n* [(1] *[,α]* [)] *, n ≥*
0 *}* iterations.

**Lemma 1.** *Assume A1, A2, and* ***UGE*** *1. Fix J,* *J* [˜] *∈* R *[d]* *and z,* ˜ *z ∈* Z *. Denote pairs y* = ( *J, z* ) *and y* *[′]* = ( *J* *[′]* *, z* *[′]* ) *such that*
*y ̸* = *y* *[′]* *. Then, for any n ≥* 1 *, p ≥* 1 *and α ∈* (0 *, α* *∞* *∧* ( *ap* ) *[−]* [1] ln *ρ* *[−]* [1] ) *, we have*

˜
E [1] *y,y* *[/p]* *[′]* [[] *[∥][J]* *n* [(0] *[,α]* [)] *−* *J* [˜] *n* [(0] *[,α]* [)] *∥* *[p]* ] *≤* c *W,* 1 *t* [1] mix *[/]* [2] *[p]* [1] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(] *[∥][J][∥]* [+] *[ ∥][J]* *[′]* *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[,]*

*where* c *W,* 1 *is defined in* (44) *and ρ* 1 *,α* = *e* *[−][αa/]* [12] *.*

*Proof.* Using the definition of exact coupling time, we get the decomposition

*J* *n* [(0] *[,α]* [)] *−* *J* [˜] *n* [(0] *[,α]* [)] = (I *−* *α* **A** [¯] ) *[n][−]* [(] *[n][∧][T]* [ )] ( *J* *n* [(0] *∧* *[,α]* *T* [)] *[−]* *[J]* [˜] *n* [(0] *∧* *[,α]* *T* [)] [)] *[ .]*

Using Holder’s and Minkowski’s inequalities, we have

˜
E *y,y* *′* [ *∥* (I *−* *α* ¯ **A** ) *[n][−][n][∧][T]* *∥* *[p]* *· ∥J* *n* [(0] *∧* *[,α]* *T* [)] *[−]* *[J]* [˜] *n* [(0] *∧* *[,α]* *T* [)] *[∥]* *[p]* []] *[ ≤]* [E][˜] [1] *y,y* *[/]* [2] *[′]* [[] *[∥][J]* *n* [(0] *[,α]* [)] *−* *J* [˜] *n* [(0] *[,α]* [)] *∥* [2] *[p]* ]P [˜] [1] *z,z* *[/]* [2] *[′]* [(] *[T][ ≥]* *[n]* [)]


+ *κ* *[p/]* [2]
*Q*


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] E [˜] [1] *y,y* *[/]* [4] *[′]* [[] *[∥][J]* *j* [(0] *[,α]* [)] *−* *J* [˜] *j* [(0] *[,α]* [)] *∥* [4] *[p]* ]P [˜] *z,z* [1] *[/]* [2] *[′]* [(] *[T]* [ =] *[ j]* [) =] *[ T]* [ (2)] 1 + *T* 2 [(2)] *.*

*j* =1


First, note that using Lemma 4 and Minkowski’s inequality, we have uniform bound independent on *n, z* and *z* *[′]*

˜
E [1] *y,y* *[/p]* *[′]* [[] *[∥][J]* *n* [(0] *[,α]* [)] *−* *J* [˜] *n* [(0] *[,α]* [)] *∥* *[p]* ] *≤* *κ* [1] *Q* *[/]* [2] [(1] *[ −]* *[αa]* [)] *[n/]* [2] [(] *[∥][J][∥]* [+] *[ ∥][J]* *[′]* *[∥]* [) + 4][D] [1] *√αat* mix *p∥ε∥* *∞* *.* (40)

Then, using this observation, the definition of the maximal coupling (32), P [˜] [1] *ξ,ξ* *[/]* [2] *[′]* [(] *[T][ ≥]* *[k]* [)] *[ ≤]* *[ς]* [1] *[/]* [2] *[ρ]* *[k/]* [2] [, and Lemma 4, we get]

*ρ* [1] *[/]* [2]
*T* 2 [(2)] *≤* 2 [2] *[p]* *κ* *[p]* *Q* [(1] *[ −]* *[αa]* [)] *[np/]* [2] *[ς]* [1] *[/]* [2] 1 *−* *ρ* [1] *[/]* [2] [ (] *[∥][J][∥]* *[p]* [ +] *[ ∥][J]* *[′]* *[∥]* *[p]* [)]


+ 2 [4] *[p]* D *[p]* 1 *[ς]* [1] *[/]* [2] *[κ]* *[p/]* *Q* [2] [(] *[αat]* [mix] *[p]* [)] *[p/]* [2] *[∥][ε][∥]* *∞* *[p]*

Thus, the sum in the last term can be bounded as


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] *ρ* *[j/]* [2] *.*

*j* =1


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] *ρ* *[j/]* [2] *≤*

*j* =1


*n−* 1
� *ρ* *[p]* 1 *,α* [(] *[n][−][j]* [)] *ρ* [2] *[j]* *≤* *ρ* *[np]* 1 *,α*

*j* =1


*n−* 1
�( *ρ* [1] *[/]* [2] *ρ* *[−]* 1 *,α* *[p]* [)] *[j]* *[ ≤]* [2] *[ρ]* 1 *[np]* *,α* *[.]* (41)

*j* =1


*n−* 1
where we used that �


*ap* [12] [ln] *ρ* [1] 1 *[/]* [2] [ . Therefore, we have]


� ( *ρ* [1] *[/]* [2] *ρ* *[−]* 1 *,α* *[p]* [)] *[j]* *[ ≤]* [2][ whenever] *[ α][ ≤]* *ap* [12]

*j* =1


*T* 2 [(2)] *≤* 2 [2] *[p]* *κ* *[p]* *Q* *[ς]* [1] *[/]* [2] *[ρ]* 1 *[np]* *,α* [(] *[∥][J][∥]* *[p]* [ +] *[ ∥][J]* *[′]* *[∥]* *[p]* [) + 2] [4(] *[p]* [+1)] [D] 1 *[p]* *[ς]* [1] *[/]* [2] *[κ]* *[p/]* *Q* [2] [(] *[αa]* [)] *[p/]* [2] [(] *[t]* [mix] *[p]* [)] *[p/]* [2] *[∥][ε][∥]* *∞* *[p]* *[ρ]* *[np]* 1 *,α* *[.]* (42)


-----

In what follows, we use the inequality *ρ* [1] *[/]* [2] *≤* *ρ* [2] 1 *,α* [, which holds for] *[ α][ ≤]* [3] *[a]* *[−]* [1] [ log] *[ ρ]* *[−]* [1] [. For the first term] *[ T]* [ (2)] 1 we can again use
the inequality (40), and get

*T* 1 [(2)] *≤* 2 [2] *[p]* *κ* *[p/]* *Q* [2] *[ς]* [1] *[/]* [2] *[ρ]* 1 *[np]* *,α* [(] *[∥][J][∥]* *[p]* [ +] *[ ∥][J]* *[′]* *[∥]* *[p]* [) + 2] [4] *[p]* [D] 1 *[p]* *[ς]* [1] *[/]* [2] [(] *[αat]* [mix] *[p]* [)] *[p/]* [2] *[∥][ε][∥]* *∞* *[p]* *[ρ]* 1 *[n]* *,α* *[.]* (43)

Combining together (43) and (42), we obtain

˜
E [1] *y,y* *[/p]* *[′]* [[] *[∥][J]* *n* [(0] *[,α]* [)] *−* *J* [˜] *n* [(0] *[,α]* [)] *∥* *[p]* ] *≤* ( *T* 1 [(2)] ) [1] *[/p]* + ( *T* 2 [(2)] ) [1] *[/p]* *≤* c *W,* 1 *t* [1] mix *[/]* [2] *[p]* [1] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(] *[∥][J][∥]* [+] *[ ∥][J]* *[′]* *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[,]*

where we set
c *W,* 1 = *ς* [1] *[/]* [2] *[p]* (4 *κ* [1] *Q* *[/]* [2] [(] *[κ]* [1] *Q* *[/]* [2] + 1) + 2 [8] *κ* [1] *Q* *[/]* [2] [D] [1] [ + 2] [4] [D] [1] [)] *[ .]* (44)

**Lemma 2.** *Assume A1, A2, and* ***UGE*** *1. Fix J,* *J* [˜] *∈* R *[d]* *and z,* ˜ *z ∈* Z *. Denote pairs y* = ( *J, z* ) *and y* *[′]* = ( *J* *[′]* *, z* *[′]* ) *such that*
*y ̸* = *y* *[′]* *. Then, for any n ≥* 1 *, p ≥* 1 *and α ∈* (0 *, α* *∞* *∧* ( *ap* ) *[−]* [1] ln *ρ* *[−]* [1] ) *, we have*


˜
E [1] *y,y* *[/p]* *[′]* [[] *[∥][J]* *n* [(1] *[,α]* [)] *−* *J* [˜] *n* [(1] *[,α]* [)] *∥* *[p]* ] *≤* c [(1)] *W,* 1 *[p]* [2] *[t]* mix [3] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* �

*where* c [(1)] *W,* 1 *[is defined in]* [ (48)] *[ and][ ρ]* [1] *[,α]* [ =] *[ e]* *[−][αa/]* [12] *[.]*


log (1 */αa* )( *∥J* [(0)] *∥* + *∥J* [˜] [(0)] *∥* + *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) *,*


*Proof.* We use the exact coupling construction (33) for the Markov chains *{Z* *k* *, k ≥* 1 *}* and *{Z* [˜] *k* *, k ≥* 1 *}* with coupling time *T* .
We have the decomposition

*J* *n* [(1] *[,α]* [)] *−* *J* [˜] *n* [(1] *[,α]* [)] = (I *−* *α* **A** [¯] ) *[n][−][n][∧][T]* ( *J* *n* [(1] *∧* *[,α]* *T* [)] *[−]* *[J]* [˜] *n* [(1] *∧* *[,α]* *T* [)] [)] (45)


*−* *α* **1** *{T ≤n}*


*n−n∧T* +1
� (I *−* *α* **A** [¯] ) *[k][−]* [1] [ ˜] **A** ( *Z* *n−k* +1 )( *J* *n* [(0] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(0] *−* *[,α]* *k* [)] [)]

*k* =1


=: *T* [(1)]
*J* [(1)] [ +] *[ T]* [ (2)] *J* [(1)] *[ .]*

We bound the two terms separately. For the first term, we can proceed the similar steps as in Lemma 5. Thus, using Holder’s
and Minkowski’s inequalities, we get

˜
E *y,y* ˜ [ *∥* (I *−* *α* ¯ **A** ) *[n][−][n][∧][T]* *∥* *[p]* *· ∥J* *n* [(1] *∧* *[,α]* *T* [)] *[−]* *[J]* [˜] *n* [(1] *∧* *[,α]* *T* [)] *[∥]* *[p]* []] *[ ≤]* [E][˜] [1] *y,* *[/]* *y* ˜ [2] [[] *[∥][J]* *n* [(1] *[,α]* [)] *−* *J* [˜] *n* [(1] *[,α]* [)] *∥* [2] *[p]* ]P [˜] [1] *z,* *[/]* *z* ˜ [2] [(] *[T][ ≥]* *[n]* [)]


+ *κ* *[p/]* [2]
*Q*


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] E [˜] [1] *y,* *[/]* *y* ˜ [4] [[] *[∥][J]* *j* [(1] *[,α]* [)] *−* *J* [˜] *j* [(1] *[,α]* [)] *∥* [4] *[p]* ]P [˜] [1] *z,* *[/]* *z* ˜ [2] [(] *[T]* [ =] *[ j]* [) =] *[ T]* [ (3)] 1 + *T* 2 [(3)] *.*

*j* =1


To bound the term *T* 1 [(3)], we apply Lemma 8

˜
E [1] *y,* *[/p]* *y* ˜ [[] *[∥][J]* *n* [(1] *[,α]* [)] *−* *J* [˜] *n* [(1] *[,α]* [)] *∥* *[p]* ] *≤* *κ* [1] *Q* *[/]* [2] [(1] *[ −]* *[αa]* [)] *[n/]* [2] [(] *[∥][J]* [(1)] *[∥]* [+] *[ ∥][J]* [˜] [(1)] *[∥]* [) + 2(][D] [(M)] *J,* 1 [+][ D] [(M)] *J,* 2 [)] *[∥][ε][∥]* *[∞]* *[p]* [2] *[t]* mix [3] *[/]* [2] *[αa]* �

Using (46), we get

*ρ* [1] *[/]* [2]
*T* 2 [(3)] *≤* 4 *[p]* *κ* *[p]* *Q* [(1] *[ −]* *[αa]* [)] *[np/]* [2] *[ζ]* [1] *[/]* [2] 1 *−* *ρ* [1] *[/]* [2] [ (] *[∥][J]* [(1)] *[∥]* *[p]* [ +] *[ ∥][J]* [˜] [(1)] *[∥]* *[p]* [)]


log(1 */αa* ) *.* (46)


+ 2 [6] *[p]* (D [(] *J,* *[M]* 3 [)] [)] *[p]* *[κ]* *[p/]* *Q* [2] *[ζ]* [1] *[/]* [2] *[p]* [2] *[p]* *[t]* mix [3] *[p/]* [2] [(] *[αa]* [)] *[p]* [(log (1] *[/αa]* [))] *[p/]* [2] *[∥][ε][∥]* *∞* *[p]*


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] *ρ* *[j/]* [2] *,*

*j* =1


where we set D [(] *J,* *[M]* 3 [)] [=][ D] [(] *J,* *[M]* 1 [)] [+][ D] [(] *J,* *[M]* 2 [)] [. Now, the bound for] *[ T]* [ (3)] 2 follows from (41). We conclude that

*ρ* [1] *[/]* [2]
*T* 2 [(3)] *≤* 4 *[p]* *κ* *[p]* *Q* [(1] *[ −]* *[αa]* [)] *[np/]* [2] *[ζ]* [1] *[/]* [2] 1 *−* *ρ* [1] *[/]* [2] [ (] *[∥][J]* [(1)] *[∥]* *[p]* [ +] *[ ∥][J]* [˜] [(1)] *[∥]* *[p]* [)]

+ 2 [6] *[p]* [+1] (D [(] *J,* *[M]* 3 [)] [)] *[p]* *[κ]* *[p/]* *Q* [2] *[ζ]* [1] *[/]* [2] *[p]* [2] *[p]* *[t]* mix [3] *[p/]* [2] [(] *[αa]* [)] *[p]* [(log (1] *[/αa]* [))] *[p/]* [2] *[ρ]* 1 *[np]* *,α* *[∥][ε][∥]* *∞* *[p]* *[.]*

Applying again (46) and the fact that *ρ* [1] *[/]* [2] *≤* *ρ* [2] 1 *,α* [, we get]

*T* 1 [(3)] *≤* 2 [2] *[p]* *κ* *[p/]* *Q* [2] *[ς]* [1] *[/]* [2] *[ρ]* 1 *[np]* *,α* [(] *[∥][J]* [(1)] *[∥]* *[p]* [ +] *[ ∥][J]* [˜] [(1)] *[∥]* *[p]* [)]

+ 2 [4] *[p]* (D [(] *J,* *[M]* 3 [)] [)] *[p]* *[ς]* [1] *[/]* [2] *[p]* [2] *[p]* *[t]* mix [3] *[p/]* [2] [(] *[αa]* [)] *[p]* [(log (1] *[/αa]* [))] *[p/]* [2] *[ρ]* 1 *[n]* *,α* *[∥][ε][∥]* *∞* *[p]* *[.]*


-----

Now, we bound the term *T* *J* [(2)] [(1)] [. Firstly, we note that for any] *[ j][ ≥]* [1][, using Lemma 1 and Minkowski’s inequality, we get]


˜
E [1] *y,* *[/p]* *y* ˜ [[] *[∥]*


*n−j* +1
� (I *−* *α* **A** [¯] ) *[k][−]* [1] [ ˜] **A** ( *Z* *n−k* +1 )( *J* *n* [(0] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(0] *−* *[,α]* *k* [)] [)] *[∥]* *[p]* []] (47)

*k* =1


*≤* C **A** *κ* [1] *Q* *[/]* [2]


*n−j* +1
� (1 *−* *αa* ) [(] *[k][−]* [1)] *[/]* [2] E [1] *y,* *[/p]* *y* ˜ [[] *[∥][J]* *n* [(0] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(0] *−* *[,α]* *k* [)] *[∥]* *[p]* []]

*k* =1


*≤* 4 c *W,* 1 C **A** *κ* [1] *Q* *[/]* [2] *[t]* mix [1] *[/]* [2] *[p]* [1] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(] *[αa]* [)] *[−]* [1] [(] *[∥][J]* [(0)] *[∥]* [+] *[ ∥][J]* [˜] [(0)] *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[ .]*

Thus, using (47) and Holder’s inequality, we obtain


*n*

˜
E *y,y* ˜ [ *∥T* *J* [(2)] [(1)] *[∥]* *[p]* []] *[ ≤]* *[α]* *[p]* � E [1] *y,* *[/]* *y* ˜ [2] [[] *[∥]*

*j* =1


*n−j* +1
� (I *−* *α* **A** [¯] ) *[k][−]* [1] [ ˜] **A** ( *Z* *n−k* +1 )( *J* *n* [(0] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(0] *−* *[,α]* *k* [)] [)] *[∥]* [2] *[p]* []˜][P] [1] *z,* *[/]* *z* ˜ [2] [(] *[T]* [ =] *[ j]* [)]

*k* =1


*n*
*≤* 2 [3] *[p]* c *[p]* *W,* 1 [C] *[p]* **A** *[ζ]* [1] *[/]* [2] *[κ]* *[p/]* *Q* [2] [(] *[t]* [mix] *[p]* [)] *[p/]* [2] *[ρ]* 1 *[n]* *,α* *[a]* *[−][p]* [(] *[∥][J]* [(0)] *[∥]* [+] *[ ∥][J]* [˜] [(0)] *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[p]* � *ρ* *[j/]* [2]

*j* =1


*ρ* [1] *[/]* [2]
*≤* 2 [3] *[p]* c *[p]* *W,* 1 [C] *[p]* **A** *[ζ]* [1] *[/]* [2] *[κ]* *[p/]* *Q* [2] 1 *−* *ρ* [1] *[/]* [2] [ (] *[t]* [mix] *[p]* [)] *[p/]* [2] *[ρ]* 1 *[n]* *,α* *[a]* *[−][p]* [(] *[∥][J]* [(0)] *[∥]* [+] *[ ∥][J]* [˜] [(0)] *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[p]* *[ .]*


Thus, we get the bound for (45), that is

˜
E [1] *y,* *[/p]* *y* ˜ [[] *[∥][J]* *n* [(1] *[,α]* [)] *−* *J* *n* [(1] *[,α]* [)] *∥* *[p]* ] *≤* E [1] *y,* *[/p]* *y* ˜ [[] *[∥][T]* [ (1)] *J* [(1)] *[∥]* *[p]* [] +][ E] [1] *y,* *[/p]* *y* ˜ [[] *[∥][T]* [ (2)] *J* [(1)] *[∥]* *[p]* []] *[ ≤]* [(] *[T]* [ (3)] 1 ) [1] *[/p]* + ( *T* 2 [(3)] ) [1] *[/p]* + E [1] *y,* *[/p]* *y* ˜ [[] *[∥][T]* [ (2)] *J* [(1)] *[∥]* *[p]* []]


*≤* c [(1)] *W,* 1 *[p]* [2] *[t]* mix [3] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* �


log (1 */αa* )( *∥J* [(0)] *∥* + *∥J* [˜] [(0)] *∥* + *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) *,*


where we set
c [(1)] *W,* 1 [=] *[ ζ]* [1] *[/]* [2] *[p]* [(148(] *[κ]* *Q* [1] *[/]* [2] [(1 +] *[ κ]* [1] *Q* *[/]* [2] [) +][ D] [(M)] *J,* 3 [) + 8 c] *[W,]* [1] *[ κ]* [1] *Q* *[/]* [2] *[a]* *[−]* [1] [)] *[ .]* (48)

Now, we are going to establish the result about asymptotic bias. As we will show, this bias is closely related to the limiting
distribution of the sequences *{J* *t* [(1] *[,α]* [)] *, t ≥* 0 *}* and *{J* *t* [(2] *[,α]* [)] *, t ≥* 0 *}* . In order to accurately define these distributions, we consider
*Y* the Markov chain˜ = (˜ *z,* ˜ *J* [(0)] *,* ˜ *J* [(1)] *Y,* ˜ *J* *t* = ( [(2)] ), we define the cost function *Z* *t* +1 *, J* *t* [(0] *[,α]* [)] *, J* *t* [(1] *[,α]* [)] *, J* *t* [(2] *[,α]* [)] ) for any *t ≥* 0 with kernel Q *J* (2) . Denoting *Y* = ( *z, J* [(0)] *, J* [(1)] *, J* [(2)] ) and

c *J* (2) ( *Y,* *Y* [˜] ) = *∥J* [(0)] *−* *J* [˜] [(0)] *∥* + *∥J* [(1)] *−* *J* [˜] [(1)] *∥* + *∥J* [(2)] *−* *J* [˜] [(2)] *∥* (49)

+ ( *∥J* [(0)] *∥* + *∥J* [˜] [(0)] *∥* + *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *∥J* [(2)] *∥* + *∥J* [˜] [(2)] *∥* + *[√]* *αa∥ε∥* *∞* ) **1** *{z* =˜ *̸* *z}* *.*

Now, we introduce the main result of this section on contraction of Wasserstein distance for the coupling of *Y* *t* and *Y* [˜] *t* .

**Proposition 5.** *Assume A 1, A 2, and* ***UGE*** *1. Fix J* [(0)] *,* *J* [˜] [(0)] *, J* [(1)] *,* *J* [˜] [(1)] *, J* [(2)] *,* *J* [˜] [(2)] *∈* R *[d]* *and z,* ˜ *z ∈* Z *. Denote y* =
( *z, J* [(0)] *, J* [(1)] *, J* [(2)] ) *and* ˜ *y* = (˜ *z,* *J* [˜] [(0)] *,* *J* [˜] [(1)] *,* *J* [˜] [(2)] ) *such that y ̸* = ˜ *y. Then, for any n ≥* 1 *, p ≥* 1 *and α ∈* (0 *, α* *∞* *∧* ( *ap* ) *[−]* [1] ln *ρ* *[−]* [1] ) *,*
*we have*

**W** c [1] *J* *[/p]* (2) *,p* [(] *[δ]* *[y]* [Q] *J* *[n]* [(2)] *[, δ]* *[y]* [˜] [Q] *J* *[n]* [(2)] [)] *[ ≤]* [c] [(2)] *W,* 3 *[p]* [7] *[/]* [2] *[t]* mix [5] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(log (1] *[/αa]* [))] [3] *[/]* [2] [c] *J* [(2)] [(] *[y,]* [ ˜] *[y]* [)] *[,]*

*where* c [(2)] *W,* 3 *[is defined in]* [ (55)] *[.]*

*Proof.* We use the similar construction with exact coupling as in Lemma 2. We have the decomposition

*J* *n* [(2] *[,α]* [)] *−* *J* [˜] *n* [(2] *[,α]* [)] = (I *−* *α* **A** [¯] ) *[n][−][n][∧][T]* ( *J* *n* [(2] *∧* *[,α]* *T* [)] *[−]* *[J]* [˜] *n* [(2] *∧* *[,α]* *T* [)] [)] (50)


*−* *α* **1** *{T ≤n}*


*n−n∧T* +1
� (I *−* *α* **A** [¯] ) *[k][−]* [1] [ ˜] **A** ( *Z* *n−k* +1 )( *J* *n* [(1] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(1] *−* *[,α]* *k* [)] [) =] *[ T]* [ (1)] *J* [(2)] [ +] *[ T]* [ (2)] *J* [(2)] *[ .]*

*k* =1


We bound the two terms separately. For the first term, we can proceed the similar steps as in Lemma 5. Thus, using Holder’s
and Minkowski’s inequalities, we get

˜
E *y,y* ˜ [ *∥* (I *−* *α* ¯ **A** ) *[n][−][n][∧][T]* *∥* *[p]* *· ∥J* *n* [(2] *∧* *[,α]* *T* [)] *[−]* *[J]* [˜] *n* [(2] *∧* *[,α]* *T* [)] *[∥]* *[p]* []] *[ ≤]* [E][˜] [1] *y,* *[/]* *y* ˜ [2] [[] *[∥][J]* *n* [(2] *[,α]* [)] *−* *J* [˜] *n* [(2] *[,α]* [)] *∥* [2] *[p]* ]P [˜] [1] *z,* *[/]* *z* ˜ [2] [(] *[T][ ≥]* *[n]* [)]


+ *κ* *[p/]* [2]
*Q*


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] E [˜] [1] *y,* *[/]* *y* ˜ [4] [[] *[∥][J]* *j* [(2] *[,α]* [)] *−* *J* [˜] *j* [(2] *[,α]* [)] *∥* [4] *[p]* ]P [˜] [1] *z,* *[/]* *z* ˜ [2] [(] *[T]* [ =] *[ j]* [) =] *[ T]* [ (4)] 1 + *T* 2 [(4)] *.*

*j* =1


-----

To bound the term *T* 1 [(4)], we apply Proposition 8

˜
E [1] *y,* *[/p]* *y* ˜ [[] *[∥][J]* *n* [(2] *[,α]* [)] *−* *J* [˜] *n* [(2] *[,α]* [)] *∥* *[p]* ] *≤* *κ* [1] *Q* *[/]* [2] [(1] *[ −]* *[αa]* [)] *[n/]* [2] [(] *[∥][J]* [(2)] *[∥]* [+] *[ ∥][J]* [˜] [(2)] *[∥]* [) + 2][D] *[J]* *[t]* mix [5] *[/]* [2] *[p]* [7] *[/]* [2] *[α]* [3] *[/]* [2] [ log] [3] *[/]* [2] [(1] *[/αa]* [)] *[ .]* (51)

Using (51), we get


*ρ* [1] *[/]* [2]
*T* 2 [(4)] *≤* 4 *[p]* *κ* *[p]* *Q* [(1] *[ −]* *[αa]* [)] *[np/]* [2] *[ζ]* [1] *[/]* [2] 1 *−* *ρ* [1] *[/]* [2] [ (] *[∥][J]* [(2)] *[∥]* *[p]* [ +] *[ ∥][J]* [˜] [(2)] *[∥]* *[p]* [)]

+ 2 [6] *[p]* (D *J* ) *[p]* *κ* *[p/]* *Q* [2] *[ζ]* [1] *[/]* [2] *[p]* [7] *[p/]* [2] *[t]* mix [5] *[p/]* [2] *[α]* [3] *[p/]* [2] [(log (1] *[/αa]* [))] [3] *[p/]* [2] *[∥][ε][∥]* *∞* *[p]*

Now, the bound for *T* 2 [(4)] follows from (41). We conclude that


*n−* 1
�(1 *−* *αa* ) *[p]* [(] *[n][−][j]* [)] *[/]* [2] *ρ* *[j/]* [2] *.*

*j* =1


*ρ* [1] *[/]* [2]
*T* 2 [(4)] *≤* 4 *[p]* *κ* *[p]* *Q* [(1] *[ −]* *[αa]* [)] *[np/]* [2] *[ζ]* [1] *[/]* [2] 1 *−* *ρ* [1] *[/]* [2] [ (] *[∥][J]* [(2)] *[∥]* *[p]* [ +] *[ ∥][J]* [˜] [(2)] *[∥]* *[p]* [)]

+ 2 [6] *[p]* [+1] (D *J* ) *[p]* *κ* *[p/]* *Q* [2] *[ζ]* [1] *[/]* [2] *[p]* [7] *[p/]* [2] *[t]* mix [5] *[p/]* [2] *[α]* [3] *[p/]* [2] [(log (1] *[/αa]* [))] [3] *[p/]* [2] *[ρ]* 1 *[np]* *,α* *[∥][ε][∥]* *∞* *[p]* *[.]*

Applying again (51) and the fact that *ρ* [1] *[/]* [2] *≤* *ρ* [2] 1 *,α* [, we get]

*T* 1 [(4)] *≤* 2 [2] *[p]* *κ* *[p/]* *Q* [2] *[ς]* [1] *[/]* [2] *[ρ]* 1 *[np]* *,α* [(] *[∥][J]* [(2)] *[∥]* *[p]* [ +] *[ ∥][J]* [˜] [(2)] *[∥]* *[p]* [)]

+ 2 [4] *[p]* (D *J* ) *[p]* *ς* [1] *[/]* [2] *p* [7] *[p/]* [2] *t* [5] mix *[p/]* [2] *[α]* [3] *[p/]* [2] [(log (1] *[/αa]* [))] [3] *[p/]* [2] *[ρ]* 1 *[n]* *,α* *[∥][ε][∥]* *∞* *[p]* *[.]*

Now, we bound the term *T* *J* [(2)] [(2)] [. Firstly, we note that for any] *[ j][ ≥]* [1][, using Lemma 2 and Minkowski’s inequality, we get]


˜
E [1] *y,* *[/p]* *y* ˜ [[] *[∥]*


*n−j* +1
� (I *−* *α* **A** [¯] ) *[k][−]* [1] [ ˜] **A** ( *Z* *n−k* +1 )( *J* *n* [(1] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(1] *−* *[,α]* *k* [)] [)] *[∥]* *[p]* []] (52)

*k* =1


*≤* C **A** *κ* [1] *Q* *[/]* [2]


*n−j* +1
� (1 *−* *αa* ) [(] *[k][−]* [1)] *[/]* [2] E [1] *y,* *[/p]* *y* ˜ [[] *[∥][J]* *n* [(1] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(1] *−* *[,α]* *k* [)] *[∥]* *[p]* []]

*k* =1


*≤* 4 c [(1)] *W,* 1 [C] **[A]** *[ κ]* [1] *Q* *[/]* [2] *[p]* [2] *[t]* mix [3] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(] *[αa]* [)] *[−]* [1] [�]

Thus, using (52) and Holder’s inequality, we obtain


log (1 */αa* )( *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) *.*


*n*

˜
E *y,y* ˜ [ *∥T* *J* [(2)] [(2)] *[∥]* *[p]* []] *[ ≤]* *[α]* *[p]* � E [1] *y,* *[/]* *y* ˜ [2] [[] *[∥]*

*j* =1


*n−j* +1
� (I *−* *α* **A** [¯] ) *[k][−]* [1] [ ˜] **A** ( *Z* *n−k* +1 )( *J* *n* [(1] *−* *[,α]* *k* [)] *[−]* *[J]* [˜] *n* [(1] *−* *[,α]* *k* [)] [)] *[∥]* [2] *[p]* []˜][P] *z,* [1] *[/]* *z* ˜ [2] [(] *[T]* [ =] *[ j]* [)]

*k* =1


*n*
log (1 */αa* )( *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) *[p]* �


*≤* 2 [3] *[p]* (c [(1)] *W,* 1 [)] *[p]* [ C] *[p]* **A** *[ζ]* [1] *[/]* [2] *[κ]* *[p/]* *Q* [2] *[p]* [7] *[p/]* [2] *[t]* mix [5] *[p/]* [2] *[ρ]* 1 *[n]* *,α* *[a]* *[−][p]* ~~[�]~~


� *ρ* *[j/]* [2]

*j* =1


*ρ* [1] *[/]* [2]
*≤* 2 [3] *[p]* (c [(1)] *W,* 1 [)] *[p]* [ C] *[p]* **A** *[ζ]* [1] *[/]* [2] *[κ]* *[p/]* *Q* [2] 1 [1]


*ρ*

mix *[ρ]* 1 *[n]* *,α* *[a]* *[−][p]* ~~[�]~~
1 *−* *ρ* [1] *[/]* [2] *[ p]* [7] *[p/]* [2] *[t]* [5] *[p/]* [2]


log (1 */αa* )( *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) *[p]* *.*


Thus, we obtain the bound for (50), that is

˜
E [1] *y,* *[/p]* *y* ˜ [[] *[∥][J]* *n* [(2] *[,α]* [)] *−* *J* *n* [(2] *[,α]* [)] *∥* *[p]* ] *≤* E [1] *y,* *[/p]* *y* ˜ [[] *[∥][T]* [ (1)] *J* [(2)] *[∥]* *[p]* [] +][ E] [1] *y,* *[/p]* *y* ˜ [[] *[∥][T]* [ (2)] *J* [(2)] *[∥]* *[p]* []] *[ ≤]* [(] *[T]* [ (4)] 1 ) [1] *[/p]* + ( *T* 2 [(4)] ) [1] *[/p]* + E [1] *y,* *[/p]* *y* ˜ [[] *[∥][T]* [ (2)] *J* [(2)] *[∥]* *[p]* []] (53)

*≤* c [(2)] *W,* 1 *[p]* [7] *[/]* [2] *[t]* mix [5] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(log (1] *[/αa]* [))] [3] *[/]* [2] [(] *[∥][J]* [(0)] *[∥]* [+] *[ ∥][J]* [˜] [(0)] *[∥]* [+] *[ ∥][J]* [(1)] *[∥]* [+] *[ ∥][J]* [˜] [(1)] *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[,]*

where we set
c [(2)] *W,* 1 [=] *[ ζ]* [1] *[/]* [2] *[p]* [(148(] *[κ]* [1] *Q* *[/]* [2] [(1 +] *[ κ]* [1] *Q* *[/]* [2] [) +][ D] *[J]* [) + 8 c] [(1)] *W,* 1 *[κ]* [1] *Q* *[/]* [2] *[a]* *[−]* [1] [)] *[ .]*

Finally, using the Holder’s and Minkowski’s inequality, we get

˜
E [1] *y,* *[/p]* *y* ˜ [[(] *[∥][J]* *n* [(0] *[,α]* [)] *∥* + *∥J* [˜] *n* [(0] *[,α]* [)] *∥* + *∥J* *n* [(1] *[,α]* [)] *∥* + *∥J* [˜] *n* [(1] *[,α]* [)] *∥* + *∥J* *n* [(2] *[,α]* [)] *∥* + *∥J* [˜] *n* [(2] *[,α]* [)] *∥* + *[√]* *αa∥ε∥* *∞* ) *[p]* **1** *{Z* *n* = *̸* *Z* *n′* *[}]* []] (54)

*≤* (E [˜] [1] *y,* *[/]* *y* ˜ [2] *[p]* [[] *[∥][J]* *n* [(0] *[,α]* [)] *∥* [2] *[p]* ] + E [˜] [1] *y,* *[/]* *y* ˜ [2] *[p]* [[] *[∥][J]* [˜] *n* [(0] *[,α]* [)] *∥* [2] *[p]* ] + E [˜] [1] *y,* *[/]* *y* ˜ [2] *[p]* [[] *[∥][J]* *n* [(1] *[,α]* [)] *∥* [2] *[p]* ] + E [˜] [1] *y,* *[/]* *y* ˜ [2] *[p]* [[] *[∥][J]* [˜] *n* [(1] *[,α]* [)] *∥* [2] *[p]* ]

+ E [˜] [1] *y,* *[/]* *y* ˜ [2] *[p]* [[] *[∥][J]* *n* [(2] *[,α]* [)] *∥* [2] *[p]* ] + E [˜] *y,* [1] *[/]* *y* ˜ [2] *[p]* [[] *[∥][J]* [˜] *n* [(2] *[,α]* [)] *∥* + *[√]* *αa∥ε∥* *∞* )P [˜] [1] *z,z* *[/]* [2] *[′]* *[p]* [ (] *[T][ ≥]* *[n]* [)]

*≤* c [(2)] *W,* 2 *[p]* [7] *[/]* [2] *[t]* mix [5] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(log (1] *[/αa]* [))] [3] *[/]* [2] [(] *[∥][J]* [(0)] *[∥]* [+] *[ ∥][J]* [˜] [(0)] *[∥]* [+] *[ ∥][J]* [(1)] *[∥]* [+] *[ ∥][J]* [˜] [(1)] *[∥]* [+] *[ ∥][J]* [(2)] *[∥]* [+] *[ ∥][J]* [˜] [(2)] *[∥]* [+] *[ √][αa][∥][ε][∥]* *[∞]* [)] *[,]*


-----

where we define
c [(2)] *W,* 2 [=] *[ ζ]* [1] *[/]* [2] *[p]* [(2][D] [1] [ + 4] *[κ]* [1] *Q* *[/]* [2] + 8D *J* ) *.*

Finally, combining the results (53) and (54), we obtain

**W** c [1] *J* *[/p]* (2) *,p* [(] *[δ]* *[y]* [Q] *J* *[n]* [(1)] *[, δ]* *[y]* [˜] [Q] *[n]* *J* [(1)] [)] *[ ≤]* [E][˜] *[y,][y]* [˜] [[][c] *[p]* *J* [(2)] [((] *[Z]* *[n]* [+1] *[, J]* *n* [(0] *[,α]* [)] *, J* *n* [(1] *[,α]* [)] ) *,* ( *Z* [˜] *n* +1 *,* *J* [˜] *n* [(0] *[,α]* [)] *,* *J* [˜] *n* [(1] *[,α]* [)] ))]

*≤* c [(2)] *W,* 3 *[p]* [7] *[/]* [2] *[t]* mix [5] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* [(log (1] *[/αa]* [))] [3] *[/]* [2] [c] *J* [(2)] [(] *[y,]* [ ˜] *[y]* [)] *[,]*

where

c [(2)] *W,* 3 [= c] [(2)] *W,* 1 [+ c] [(2)] *W,* 2 *[.]* (55)

**Corollary 4.** *Assume A 1, A 2 and* ***UGE*** *1. Let α ∈* (0 *, α* *∞* [(][b][)] [)] *[. Then the process][ {][Y]* *t* *[}]* *t∈* N *[is a Markov chain with a unique]*
*stationary distribution* Π *J* (2) *,α* *.*

*Proof.* Using Proposition 5, we follow the lines of Appendix B.3.

The similar result as in Proposition 5 can be obtained for the Markov chain *{* ( *Z* *t* +1 *, J* *t* [(0] *[,α]* [)] *, J* *t* [(1] *[,α]* [)] ) *, t ≥* 0 *}* with kernel Q *J* (1),
but with a sharper bound. That is, we set *U* = ( *z, J* [(0)] *, J* [(1)] ), *U* [˜] = (˜ *z,* *J* [˜] [(0)] *,* *J* [˜] [(1)] ) for *J* [(0)] *,* *J* [˜] [(0)] *, J* [(1)] *,* *J* [˜] [(1)] *∈* R *[d]*, *z,* ˜ *z ∈* Z, and
consider another cost function

c( *U,* *U* [˜] ) = *∥J* [(0)] *−* *J* [˜] [(0)] *∥* + *∥J* [(1)] *−* *J* [˜] [(1)] *∥* (56)

+ ( *∥J* [(0)] *∥* + *∥J* [˜] [(0)] *∥* + *∥J* [(1)] *∥* + *∥J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* ) **1** *{z* =˜ *̸* *z}* *.*

We establish the result on contraction of the Wasserstein semimetric for this cost function.

**Lemma 3.** *Assume A 1, A 2, and* ***UGE*** *1. Fix J* [(0)] *,* *J* [˜] [(0)] *, J* [(1)] *,* *J* [˜] [(1)] *∈* R *[d]* *and z,* ˜ *z ∈* Z *. Denote y* = ( *z, J* [(0)] *, J* [(1)] ) *and*
*y* ˜ = (˜ *z,* *J* [˜] [(0)] *,* *J* [˜] [(1)] ) *such that y ̸* = ˜ *y. Then, for any n ≥* 1 *, p ≥* 1 *and α ∈* (0 *, α* *∞* *∧* ( *ap* ) *[−]* [1] ln *ρ* *[−]* [1] ) *, we have*


**W** c [1] *,p* *[/p]* [(] *[δ]* *[y]* [Q] *[n]* *J* [(1)] *[, δ]* *[y]* [˜] [Q] *J* *[n]* [(1)] [)] *[ ≤]* [c] [(1)] *W,* 3 *[p]* [2] *[t]* mix [3] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* �


log (1 */αa* )c( *y,* ˜ *y* ) *,* (57)


*where* c [(1)] *W,* 3 *[is defined in]* [ (58)] *[.]*

*Proof.* Following the proof lines of Proposition 5 but using Lemma 1 instead of Lemma 2, we can obtain the result (57) with

c [(1)] *W,* 3 [= c] [(1)] *W,* 1 [+ c] [(1)] *W,* 2 *[,]* (58)

c [(1)] *W,* 1 [=] *[ ζ]* [1] *[/]* [2] *[p]* [(148(] *[κ]* [1] *Q* *[/]* [2] [(1 +] *[ κ]* [1] *Q* *[/]* [2] [) +][ D] [(M)] *J,* 3 [) + 8 c] *[W,]* [1] *[ κ]* [1] *Q* *[/]* [2] *[a]* *[−]* [1] [)] *[,]*

c [(1)] *W,* 2 [=] *[ ζ]* [1] *[/]* [2] *[p]* [(2][D] [1] [ + 4] *[κ]* [1] *Q* *[/]* [2] + 8(D [(M)] *J,* 1 [+][ D] [(M)] *J,* 2 [))] *[ .]*
#### **B.3 Proof of Proposition 1**

*Proof.* For any *Y* = ( *z, J* [(0)] *, J* [(1)] ) *,* *Y* [˜] = (˜ *z,* *J* [˜] [(0)] *,* *J* [˜] [(1)] ), where *J* [(0)] *, J* [(1)] *,* *J* [˜] [(0)] *,* *J* [˜] [(1)] *∈* R *[d]* and *z,* ˜ *z ∈* Z, we consider the metric

*d* *J* ( *Y,* *Y* [˜] ) = *∥J* [(0)] *−* *J* [˜] [(0)] *∥* + *∥J* [(1)] *−* *J* [˜] [(1)] *∥* + *[√]* *αa∥ε∥* *∞* **1** *{z* =˜ *̸* *z}* *.*

This metric is upper bounded by the cost function, defined in (56), that is, *d* *J* *≤* c. Applying (Douc et al. 2018, Theorem 20.3.4)
together with Lemma 3, we get the result.
#### **B.4 Proof of Proposition 2**

*Proof.* We define the random variable *J* *∞* [(1] *[,α]* [)] with distribution Π *J* (1) *,α* . Then, from Lemma 3 it follows that lim *t→∞* [E][[] *[J]* *t* [(1)] ] =

E[ *J* *∞* [(1)] []][. We omit the parameter] *[ α]* [ in the notation for the sake of simplicity. However, we note that the limiting random variable]
depends on the parameter *α* . Thus, using (14), we get

E[ *J* *∞* [(1)] +1 [] =][ E][[] *[J]* *∞* [(1)] []] *[ −]* *[α]* [ ¯] **[A]** [E][[] *[J]* *∞* [(1)] []] *[ −]* *[α]* [E][[ ˜] **[A]** [(] *[Z]* *[∞]* [+1] [)] *[J]* *∞* [(0)] []] *[,]*


-----

which is equivalent to

**A** ¯ E[ *J* *∞* [(1)] [] =] *[ −]* [E][[ ˜] **[A]** [(] *[Z]* *[∞]* [+1] [)] *[J]* *∞* [(0)] [] =] *[ α]* [E]


*∞*
�
� *k* =1


� **A** ˜ ( *Z* *∞* +1 )(I *−* *α* ¯ **A** ) *[k][−]* [1] *ε* ( *Z* *∞−k* +1 )

*k* =1


�


*∞* *∞*
�( *−* 1) *[j]* *α* *[j]* [+1] �

*j* =1 *k* = *j* +1


*k* = *j* +1


*k −* 1
� *j*


E **A** ˜ ( *Z* *∞* +1 ) ¯ **A** *[j]* *ε* ( *Z* *∞−k* +1 ) *.*
� � �


= *α*


*∞*
� E[ **A** [˜] ( *Z* *∞* +1 ) *ε* ( *Z* *∞−k* +1 )] +

*k* =1


*∞*
�


For any *t ≥* 0, we define the *σ* -algebra *F* *t* *[−]* [=] *[ σ]* [(] *[Z]* *[∞−][t]* *[, Z]* *[∞−][t][−]* [1] *[, . . .]* [ )][. Note that]

E[ **A** [˜] ( *Z* *∞* +1 ) *ε* ( *Z* *∞−k* +1 )] = E[E[ **A** [˜] ( *Z* *∞* +1 ) *|F* *∞−* *[−]* *k* +1 []] *[ε]* [(] *[Z]* *[∞−][k]* [+1] [)] =][ E][[Q] *[k]* [ ˜] **[A]** [(] *[Z]* *[∞]* [)] *[ε]* [(] *[Z]* *[∞]* [)]] *[ .]*

Therefore, we have

E[ *J* *∞* [(1)] [] =] *[ α]* [∆+] *[ R]* [(] *[α]* [)] *[,]*

where we denote

*∞*
∆= **A** [¯] *[−]* [1] � E[ *{* Q *[k]* [ ˜] **A** ( *Z* *∞* ) *}ε* ( *Z* *∞* )] *,*

*k* =1


*∞*
*R* ( *α* ) = **A** [¯] *[−]* [1] �


*∞* *∞*
�( *−* 1) *[j]* *α* *[j]* [+1] �

*j* =1 *k* = *j* +1


*k* = *j* +1


*k −* 1

E *π* [ *{* Q *[k]* [ ˜] **A** ( *Z* *∞* ) *}* **A** [¯] *[j]* *ε* ( *Z* *∞* )] *.*

� *j* �


Now, we will prove that this decomposition is well defined. Setting *v* *j* ( *z* ) = **A** [¯] *[j]* *ε* ( *z* ), we obtain


*u* *[T]* Q *[k]* [ ˜] **A** ( *z* ) *v* ( *z* ) *π* ( *dz* ) *|* = sup
Z *u∈* S *[d][−]* [1] *[ |]* �


*∥* E *π* [ *{* Q *[k]* [ ˜] **A** ( *Z* *∞* ) *}* **A** [¯] *[j]* *ε* ( *Z* *∞* )] *∥* = sup
*u∈* S *[d][−]* [1] *[ |]* �


*u* *[T]* (Q *[k]* **A** ( *z* ) *−* **A** [¯] ) *v* ( *z* ) *π* ( *dz* ) *|*
Z


*≤* C *[j]* **A** [+1] *∥ε∥* *∞* ∆(Q *[k]* ) *,*

where we set **A** [¯] [0] = I and ∆(Q *[k]* ) is the Dobrushin’s coefficient. Therefore, using (34), we have

*∞*
� *∥* E *π* [ *{* Q *[k]* [ ˜] **A** ( *Z* *∞* ) *}ε* ( *Z* *∞* ) *∥≤* (4 */* 3) *t* mix *.*

*k* =1

Setting *q* = (1 */* 4) [1] *[/t]* [mix] and using (6), we get


*∞*

C *[j]* **A** [+1] *j* ! *α* *[j]* [+1] *k* = � *j* +1


*∞*
�


*∞* *∞*
� *α* *[j]* [+1] �

*j* =1 *k* = *j* +1


*∞*
�

*j* =1


*k* = *j* +1


*k −* 1

*∥* E *π* [ *{* Q *[k]* [ ˜] **A** ( *Z* *∞* ) *}* **A** [¯] *[j]* *ε* ( *Z* *∞* )] *∥≤∥ε∥* *∞*

� *j* �


( *k −* 1 ) !
( *k −* *j −* 1)! [(1] *[/]* [4)] *[⌊][k/t]* [mix] *[⌋]*


C *[j]* **A** [+1] *α* *[j]* [+1] *q* *[j]* [+1] *j* !

*j* ! (1 *−* *q* ) *[j]* [+1]


*≤* 4 *∥ε∥* *∞*

*≤* 4 *∥ε∥* *∞*


*∞*
�

*j* =1


*∞*
�( *α* C **A** *t* mix ) *[j]* [+1]

*j* =1


*≤* 4 *α* [2] C [2] **A** *[t]* [2] mix *[∥][ε][∥]* *[∞]* [+ 8] *[α]* [3] [ C] [3] **A** *[t]* [3] mix *[∥][ε][∥]* *[∞]* *[,]*

where we used that *q/* (1 *−* *q* ) *≤* *t* mix, which concludes the proof.

**Proposition 6.** *Assume A1, A2 and* ***UGE*** *1. Then for α ∈* (0 *, α* 1 [(][b] *,∞* [)] [)] *[, it holds that]*

lim *n* ] = E[ *J* *∞* [(2] *[,α]* [)] ] = *α* [2] ∆ 2 + *R* 2 ( *α* ) *,*
*n→∞* [E][[] *[J]* [(2] *[,α]* [)]

*where* ∆ 2 *∈* R *[d]* *is defined as*


*∞*
� E[ **A** [˜] ( *Z* *∞* + *k* + *i* +1 ) **A** [˜] ( *Z* *∞* + *i* +1 ) *ε* ( *Z* *∞* )] *,*

*i* =0


∆ 2 = *−*


*∞*
�

*k* =1


*and R* 2 ( *α* ) *is a reminder term which can be bounded as*

*∥R* 2 ( *α* ) *∥≤* D *b* *t* [4] mix *[α]* [5] *[/]* [2] *[∥][ε][∥]* *[∞]* *[,]*

*where we define*
D *b* = C [3] **A** [(12][D] [1] [C] **[A]** *[a]* [1] *[/]* [2] [ + 24(] *[e]* [2] *[/t]* [mix] *[ −]* [1))] *[ .]*


-----

*Proof.* Firstly, we introduce the random variable *J* *∞* [(2)] [with distribution][ Π] *J* [(2)] *,α* [. We again omit the parameter] *[ α]* [ in the notation]

for the sake of simplicity. However, we note that the distribution of *J* *∞* [(2)] [depends on the parameter] *[ α]* [. Using the recursion for]
*J* *n* [(2)] from (14), we have
E[ *J* *∞* [(2)] +1 [] =][ E][[] *[J]* *∞* [(2)] []] *[ −]* *[α]* [ ¯] **[A]** [E][[] *[J]* *∞* [(2)] []] *[ −]* *[α]* [E][[ ˜] **[A]** [(] *[Z]* *[∞]* [+1] [)] *[J]* *∞* [(1)] []] *[,]*

which in turn, using the recursion for *J* *n* [(1)] [, leads to]


�


**A** ¯ E[ *J* *∞* [(2)] [] =] *[ −]* [E][[ ˜] **[A]** [(] *[Z]* *[∞]* [+1] [)] *[J]* *∞* [(1)] [] =] *[ α]* [E]


*∞*
� **A** ˜ ( *Z* *∞* +1 )(I *−* *α* ¯ **A** ) *[k][−]* [1] **A** ˜ ( *Z* *∞−k* +1 ) *J* *∞−* [(0)] *k*
� *k* =1


*∞*

= *α* � E[ **A** [˜] ( *Z* *∞* +1 ) **A** [˜] ( *Z* *∞−k* +1 ) *J* *∞−* [(0)] *k* [] +]

*k* =1


*∞* *∞*
�( *−* 1) *[j]* *α* *[j]* [+1] �

*j* =1 *k* = *j* +1


*k* = *j* +1


*∞*
�


*k −* 1

E[ **A** [˜] ( *Z* *∞* +1 ) **A** [¯] *[j]* [ ˜] **A** ( *Z* *∞−k* +1 ) *J* *∞−* [(0)] *k* []]

� *j* �


= *T* *b,* 1 + *T* *b,* 2 *.*

We can further decompose the first term, that is,


*∞*
*T* *b,* 1 = *−α* [2] �

*k* =1

*∞*
= *−α* [2] �

*k* =1


*∞*
� E[ **A** [˜] ( *Z* *∞* +1 ) **A** [˜] ( *Z* *∞−k* +1 )(I *−* *α* **A** [¯] ) *[i]* *ε* ( *Z* *∞−k−i* )]

*i* =0


*∞*
� E[ **A** [˜] ( *Z* *∞* +1 ) **A** [˜] ( *Z* *∞−k* +1 ) *ε* ( *Z* *∞−k−i* )]

*i* =0


*∞*
�


*∞* *∞*
�( *−* 1) *[j]* *α* *[j]* [+2] �

*j* =1 *i* = *j*


*i* = *j*


*−*


*∞*
�

*k* =1


*i*

E[ **A** [˜] ( *Z* *∞* +1 ) **A** [˜] ( *Z* *∞−k* +1 ) **A** [¯] *[j]* *ε* ( *Z* *∞−k−i* )] = *T* *b,* 11 + *T* *b,* 12

� *j* �


For any *t ≥* 0, we define the *σ* -algebra *F* *t* *[−]* [=] *[ σ]* [(] *[Z]* *[∞−][t]* *[, Z]* *[∞−][t][−]* [1] *[, . . .]* [ )][. For] *[ T]* *[b,]* [11] [, denoting] *[ u]* *[k]* [(] *[z]* [) =] *[ {]* [Q] *[k]* [ ˜] **[A]** [(] *[z]* [)] *[}]* [ ˜] **[A]** [(] *[z]* [)][ for] *[ z][ ∈]* [Z][,]
we get

E[ **A** [˜] ( *Z* *∞* +1 ) **A** [˜] ( *Z* *∞−k* +1 ) *ε* ( *Z* *∞−k−i* )] = E[E[ **A** [˜] ( *Z* *∞* +1 ) *|F* *∞−* *[−]* *k* +1 [] ˜] **[A]** [(] *[Z]* *[∞−][k]* [+1] [)] *[ε]* [(] *[Z]* *[∞−][k][−][i]* [)]]

= E[ *{* Q *[k]* [ ˜] **A** ( *Z* *∞−k* +1 ) *}* **A** [˜] ( *Z* *∞−k* +1 ) *ε* ( *Z* *∞−k−i* )]

= E[ *{* Q *[i]* [+1] *u* *k* ( *Z* *∞* ) *}ε* ( *Z* *∞* )] = E[ *{* Q *[i]* [+1] *u* ¯ *k* ( *Z* *∞* ) *}ε* ( *Z* *∞* )] *,*

where we set ¯ *u* *k* ( *z* ) = *u* *k* ( *z* ) *−* E[ *u* *k* ( *Z* *∞* )]. Note that for any *z ∈* Z, we have *∥u* *k* ( *z* ) *∥≤* C [2] **A** [∆][(Q] *[k]* [)][. Then, using Minkowski’s]
inequality and (34), we can bound the first term, as


*∞*
*∥T* *b,* 11 *∥≤* 2 C [2] **A** *[α]* [2] �

*k* =1

Similarly, for the term *T* *b,* 12, we have


*∞*
� ∆(Q *[i]* [+1] )∆(Q *[k]* ) *∥ε∥* *∞* *≤* 8 C [2] **A** *[t]* mix [2] *[α]* [2] *[∥][ε][∥]* *[∞]* *[.]*

*i* =0


*∞*
�


E[ **A** [˜] ( *Z* *∞* +1 ) **A** [˜] ( *Z* *∞−k* +1 ) **A** [¯] *[j]* *ε* ( *Z* *∞−k−i* )] = E[ *{* Q *[k]* [ ˜] **A** ( *Z* *∞−k* +1 ) *}* **A** [˜] ( *Z* *∞−k* +1 ) **A** [¯] *[j]* *ε* ( *Z* *∞−k−i* )]

= E[ *{* Q *[i]* [+1] *v* ¯ *k,j* ( *Z* *∞* ) *}ε* ( *Z* *∞* )] *,*

where we define *v* *k,j* ( *z* ) = *{* Q *[k]* [ ˜] **A** ( *z* ) *}* **A** [˜] ( *z* ) **A** [¯] *[j]* and ¯ *v* *k,j* ( *z* ) = *v* *k,j* ( *z* ) *−* E[ *v* *k,j* ( *Z* *∞* )]. Thus, using the bound

*∥* E[ *{* Q *[i]* [+1] *v* ¯ *k,j* ( *Z* *∞* ) *}ε* ( *Z* *∞* )] *∥≤* C *[j]* **A** [+2] ∆(Q *[i]* [+1] )∆(Q *[k]* ) *∥ε∥* *∞* *,*

and setting *q* = (1 */* 4) [1] *[/t]* [mix], we get


*∞*
�

*i* = *j*


*i* !

*∥ε∥* *∞*

( *i −* *j* )! [∆][(Q] *[i]* [+1] [)] *[ ≤]* [41] *[ −]* *q* *[q]*


*∞*
�

*j* =1


C *[j]* **A** [+2] *α* *[j]* [+2] *q* *[j]* [+2] *j* !

*j* ! (1 *−* *q* ) *[j]* [+2]


*∞*
�

*j* =1


C *[j]* **A** [+2] *α* *[j]* [+2] ∆(Q *[k]* )

*j* !


*∞*
� ∆(Q *[k]* )

*k* =1


*∥T* *b,* 12 *∥≤∥ε∥* *∞*


*∞*
�

*k* =1


*≤* 8( *e* [2] *[/t]* [mix] *−* 1) *t* mix *∥ε∥* *∞*


*∞*
�(C **A** *t* mix *α* ) *[j]* [+2] *≤* 24 C [3] **A** [(] *[e]* [2] *[/t]* [mix] *[ −]* [1)] *[t]* [4] mix *[α]* [3] *[∥][ε][∥]* *[∞]* *[.]*

*j* =1


Now, we proceed with bounding the term *T* *b,* 2 . Note that

E[ **A** [˜] ( *Z* *∞* +1 ) **A** [¯] *[j]* [ ˜] **A** ( *Z* *∞−k* +1 ) *J* *∞−* [(0)] *k* [] =][ E][[] *[{]* [Q] *[k]* [ ˜] **[A]** [(] *[Z]* *[∞−][k]* [+1] [)] *[}]* **[A]** [ ¯] *[j]* [ ˜] **[A]** [(] *[Z]* *[∞−][k]* [+1] [)] *[J]* *∞−* [(0)] *k* []] *[,]*


-----

Thus, applying Lemma 4, for any *j ≥* 1 and *k ≥* *j* + 1, we get

*∥* E[ **A** [˜] ( *Z* *∞* +1 ) **A** [¯] *[j]* [ ˜] **A** ( *Z* *∞−k* +1 ) *J* *∞−* [(0)] *k* []] *[∥≤]* [C] **A** *[j]* [+2] ∆(Q *[k]* )E[ *∥J* *∞* [(0)] *[∥]* []] *[ ≤]* [D] [1] [C] *[j]* **A** [+2] ∆(Q *[k]* ) *[√]* *αat* mix *∥ε∥* *∞* *.*

Applying this result combined with Minkowski’s inequality to *T* *b,* 2, we get


*∞*
�

*k* = *j* +1


( *k −* 1 ) !
( *k −* *j −* 1)! [∆][(Q] *[k]* [)]


C *[j]* **A** [+1] *α* *[j]* [+1]

*j* !


*∥T* *b,* 2 *∥≤* D 1 C [2] **A** *√αat* mix *∥ε∥* *∞*


*∞*
�

*j* =1


*≤* 4D 1 C [2] **A** *√αat* mix *∥ε∥* *∞*
#### **B.5 Proof of Corollary 1**

*Proof.* Using (10) and (16), we get


*∞*
�(C **A** *t* mix *α* ) *[j]* [+1] *≤* 12D 1 C [4] **A** *[a]* [1] *[/]* [2] *[t]* mix [5] *[/]* [2] *[α]* [5] *[/]* [2] *[∥][ε][∥]* *[∞]* *[.]*

*j* =1


E[ *θ* *n* [(] *[α]* [)] []] *[ −]* *[θ]* *[⋆]* [=][ E][[˜] *[θ]* *n* [(][tr][)] [] +][ E][[] *[J]* *n* [(0] *[,α]* [)] ] + E[ *J* *n* [(1] *[,α]* [)] ] + E[ *J* *n* [(2] *[,α]* [)] ] + E[ *H* *n* [(2] *[,α]* [)] ] *.* (59)

Using Proposition 4 and (Villani 2009, Theorem 6.9), we get that lim
*n→∞* [E][[] *[θ]* *[n]* [] = Π] *[α]* [(] *[θ]* [0] [)][. Similarly, from Lemma 3 it fol-]

lows that lim *n* ) *, L* ( *J* *∞* [(1] *[,α]* [)] )) = 0, hence lim *n* ] = E[ *J* *∞* [(1] *[,α]* [)] ]. Due to (Durmus et al. 2025, Proposition
*n→∞* **[W]** *[p]* [(] *[L]* [(] *[J]* [(1] *[,α]* [)] *n→∞* [E][[] *[J]* [(1] *[,α]* [)]

7) the term E[ *θ* [˜] *n* [(][tr][)] []][ tends to][ 0][ geometrically fast. Since] *[ J]* *n* [(0] *[,α]* [)] is the linear statistics of *{ε* ( *Z* *k* ) *}*, using **UGE** 1 we get that
lim *n→∞* E[ *J* *n* [(0] *[,α]* [)] ] = 0. Now, we can rewrite the equation (59) as

E[ *θ* *n* [(] *[α]* [)] []] *[ −]* *[θ]* *[⋆]* *[−]* [E][[˜] *[θ]* *n* [(][tr][)] []] *[ −]* [E][[] *[J]* *n* [(0] *[,α]* [)] ] *−* E[ *J* *n* [(1] *[,α]* [)] ] = E[ *J* *n* [(2] *[,α]* [)] ] + E[ *H* *n* [(2] *[,α]* [)] ] *.*

From the arguments above, it follows that the left-hand side of this equation converges, hence, the right-hand side converges as
well. Applying Proposition 2 and using Proposition 8, Proposition 9, we get the result.
### **C Rosenthal-type inequality**

We begin with the preliminary fact on the boundness of iterations *{J* *n* [(0] *[,α]* [)] *}* .
**Lemma 4.** *Assume A1, A2 and* ***UGE*** *1. Let p ≥* 2 *. Then, for any α ∈* (0; *α* *∞* ) *, where α* *∞* *is defined in* (97) *, initial probability*
*distribution ξ on* (Z *, Z* ) *, n ∈* N *, it holds that*

E [1] *ξ* *[/p]* � *∥J* *n* [(0] *[,α]* [)] *∥* *[p]* [�] *≤* D 1 *√αapt* mix *∥ε∥* *∞* *,*


*where* D 1 *is defined as*
D 1 = 2 [7] *[/]* [2] *κ* [1] *Q* *[/]* [2] *[a]* *[−]* [1] *[{][e]* *[−]* [1] *[/]* [4] [ +] *√*

*Proof.* See (Durmus et al. 2025, Proposition 8).


2 *πe* C **A** *a* *[−]* [1] *} .*


**A** ˜ (In this section we consider a Markov Chain *z* ) *J* . In what follows, the Markov kernel Q (( *J* admits unique stationary distribution which we denote *J* *t* [(0] *[,α]* [)] *, Z* *t* +1 ) *, t ≥* 0) with a transition kernel Q *J* and a function Π *J,α* . Also, for any *ψ* ( *J, z* ) =
*t ≥* 0 we denote

¯ ¯
*ψ* ( *J, z* ) = *ψ* ( *J, z* ) *−* E Π *J,α* [ *ψ* 0 ] *,* *ψ* *t* = *ψ* ( *J* *t* [(0] *[,α]* [)] *, Z* *t* +1 ) *,* *ψ* *t* = ¯ *ψ* ( *J* *t* [(0] *[,α]* [)] *, Z* *t* +1 ) *,*

We define the cost function c *J* : R *[d]* *×* Z *×* R *[d]* *×* Z *→* R + as

c *J* (( *J, z* ) *,* ( *J,* [˜] ˜ *z* )) = *∥J −* *J* [˜] *∥* + ( *∥J∥* + *∥J* [˜] *∥* + *[√]* *αa∥ε∥* *∞* ) **1** *{z* =˜ *̸* *z}* *.* (60)

For this cost function, we get
*∥ψ* ( *J, z* ) *−* *ψ* ( *J,* [˜] ˜ *z* ) *∥≤* 2 C **A** c *J* (( *J, z* ) *,* ( *J,* [˜] ˜ *z* )) *.* (61)
Before the main result of this section, formulated in Proposition 7, we state additional lemmas. In the following results we use
the notation for pairs ( *J, z* ) where *J ∈* R *[d]* and *z ∈* Z. We denote the *n* -th step transition of our Markov Chain *Y* *n* = ( *J* *n* *, Z* *n* +1 )
starting from some distribution *ξ* and *Y* [˜] *n* = ( *J* [˜] *n* [(0] *[,α]* [)] *,* *Z* [˜] *n* +1 ) from distribution *ξ* [˜] . Also, due to (Douc et al. 2018, Theorem 20.1.3.),
we consider the optimal kernel coupling *K* *J* of (Q *J* *,* Q *J* ) defined as


**W** c *J* *,p* ( *δ* *y* Q *J* *, δ* *y* ˜ Q *J* ) =
�

R *[d]* *×* Z


c *[p]* *J* [(] *[x,]* [ ˜] *[x]* [)] *[K]* *[J]* [(] *[y,]* [ ˜] *[y]* [;] *[ dx, d][x]* [˜][)] *[ .]* (62)


Now, we prove the result on contraction of Wasserstein distance, which, in sequel, will give the existence of invariant measure.


-----

**Lemma 5.** *Assume A1, A2, and* ***UGE*** *1. Fix J,* *J* [˜] *∈* R *[d]* *and z,* ˜ *z ∈* Z *. Denote pairs y* = ( *J, z* ) *and* ˜ *y* = ( *J,* [˜] ˜ *z* ) *such that y ̸* = ˜ *y.*
*Then, for any n, p ≥* 1 *and α ∈* (0 *, α* *∞* [(M)] *∧* ( *ap* ) *[−]* [1] ln *ρ* *[−]* [1] ) *, we have*


**W** c [1] *J* *[/p]* *,p* [(] *[δ]* *[y]* [Q] *J* *[n]* *[, δ]* *[y]* [˜] [Q] *[n]* *J* [)] *[ ≤]* [c] [(1)] *W,* 3 *[p]* [2] *[t]* mix [3] *[/]* [2] *[ρ]* 1 *[n/p]* *,α* �log (1 */αa* )c *J* ( *y,* ˜ *y* ) *,*

*where* c [(1)] *W,* 3 *[is defined in]* [ (58)] *[ and][ ρ]* [1] *[,α]* [ =] *[ e]* *[−][αa/]* [12] *[.]*

*Proof.* Consider *u* = ( *J, J* [(1)] *, z* ) and ˜ *u* = ( *J,* [˜] *J* [˜] [(1)] *,* ˜ *z* ), where *J* [(1)] *,* *J* [˜] [(1)] *∈* R. Note that

c *J* ( *y,* ˜ *y* ) *≤* c( *u,* ˜ *u* ) *,* (63)

where c( *·, ·* ) is defined in (56). Let *µ ∈* Π( *δ* *u* Q *[n]* *J* [(1)] *[, δ]* *[u]* [˜] [Q] *[n]* *J* [(1)] [)][ be an arbitrary coupling. We can match it with some coupling]
*ν ∈* Π( *δ* *u* Q *[n]* *J* *[, δ]* *[u]* [˜] [Q] *[n]* *J* [)][ such that for any] *[ A, B, A]* *[′]* *[, B]* *[′]* *[ ∈B]* [(][R][)][ and] *[ C, C]* *[′]* *[ ∈Z]* [, we have]

*ν* ( *A × C, A* *[′]* *× C* *[′]* ) = *µ* ( *A ×* R *× C, A* *[′]* *×* R *× C* *[′]* ) *.*

Hence, taking expectation on both sides of (63), we get

E *ν* [c *J* ( *Y,* *Y* [˜] )] = E *µ* [c *J* ( *Y,* *Y* [˜] )] *≤* E *µ* [c( *U,* *U* [˜] )] *.*

Therefore, it follows that
**W** c *J* *,p* ( *δ* *y* Q *[n]* *J* *[, δ]* *[y]* [˜] [Q] *[n]* *J* [)] *[ ≤]* **[W]** [c] *[,p]* [(] *[δ]* *[y]* [Q] *[n]* *J* [(1)] *[, δ]* *[y]* [˜] [Q] *[n]* *J* [(1)] [)] *[ .]*
To conclude the proof, we apply Lemma 3.

**Corollary 5.** *stationary distribution Assume A1, A2 and* Π *J,α* *.* ***UGE*** *1. Let α ∈* (0 *, α* 1 [(][b] *,∞* [)] [)] *[. Then the process][ {][Y]* *[t]* *[}]* *[t][∈]* [N] *[ is a Markov chain with a unique]*

*Proof.* We can apply the similar arguments as in Proposition 1, but with Lemma 5 instead of Lemma 3.

**Proposition 7.** *Assume A1, A2, and* ***UGE*** *1. We set step size α ∈* (0 *, α* *∞* [(M)] *∧* ( *ap* ) *[−]* [1] ln *ρ* *[−]* [1] ) *. Then*


E Π [1] *[/p]* *J,α* [[] *[∥]*


*n−* 1
� *ψ* ¯ *t* *∥* *[p]* ] *≤* 64 C **A** *κ* [1] *Q* *[/]* [2] *[p]* [1] *[/]* [2] *[t]* mix [1] *[/]* [2] *[∥][ε][∥]* *[∞]* [(] *[pa]* *[−]* [1] *[/]* [2] [(] *[αn]* [)] [1] *[/]* [2] [ +] *[ t]* [1] mix *[/]* [2] *[αn]* [1] *[/]* [2] [ +] *[ a]* *[−]* [1] *[/]* [2] *[α]* [1] *[/]* [2] [)] *[ .]*

*t* =0


*Proof.* For any 1 *≤* *k ≤* *t*, we denote


*µ* *t,k* = E *π* [ **A** [˜] ( *Z* *t* +1 )(I *−* *α* **A** [¯] ) *[t][−][k]* *ε* ( *Z* *k* )] *,*


*n−k*
�


� **A** ˜ ( *Z* *l* +1 )(I *−* *α* ¯ **A** ) *[l][−]* [1] *ε* ( *Z* 1 )] *.*

*l* =1


*µ* *k* = E *π* [


We decompose our quantity into the three terms


*n−* 1

¯

� *ψ* *t* = *−α*

*t* =0


*n−* 1
�

*t* =0


*t*
� *{* **A** [˜] ( *Z* *t* +1 )(I *−* *α* **A** [¯] ) *[t][−][k]* *ε* ( *Z* *k* ) *−* *µ* *t,k* *}* = *−α*

*k* =1


*n−k*
� **A** ˜ ( *Z* *k* + *l* )(I *−* *α* ¯ **A** ) *[l][−]* [1]
� [�] *l* =1 �


*n−* 1
�

*k* =1


*ε* ( *Z* *k* ) *−* *µ* *k*


�


= *−α{H* 2 *ε* ( *Z* 1 ) *−* *µ* 1 *} −* *α*


~~�~~ � ~~�~~ �
*H* *k* +1

*n−* 1
� *{H* *k* +1 *ε* ( *Z* *k* ) *−* E[ *H* *k* +1 *ε* ( *Z* *k* ) *|F* *k−* 1 ] *}*

*k* =2


*−* *α*


*n−* 1
� *{* E[ *H* *k* +1 *ε* ( *Z* *k* ) *|F* *k−* 1 ] *−* *µ* *k* *}* = *−α* ( *U* 1 + *U* 2 + *U* 3 ) *,*

*k* =2


where we set *F* *k* = *σ* ( *X* 1 *, . . ., X* *k* ). For any *k ≥* 1, we denote


*υ* *k* ( *z* ) =


*n−k*
� Q *[l]* [ ˜] **A** ( *z* )(I *−* *α* **A** [¯] ) *[l][−]* [1] *,* *υ* *k* *[ε]* [(] *[z]* [) =] *[ υ]* *[k]* [(] *[z]* [)] *[ε]* [(] *[z]* [)] *[ .]*

*l* =1


*n−k*
Note that *∥υ* *k* *∥* *∞* *≤* C **A** *κ* [1] *Q* *[/]* [2] � *l* =1 [(1] *[ −]* *[αa]* [)] [(] *[l][−]* [1)] *[/]* [2] [∆][(Q] *[l]* [)][. Thus, using the tower property, we get]

E[ *H* *k* +1 *ε* ( *Z* *k* ) *|F* *k−* 1 ] = E[E[ *H* *k* +1 *ε* ( *Z* *k* ) *|F* *k* ] *|F* *k−* 1 ] = E[ *υ* *k* ( *Z* *k* ) *ε* ( *Z* *k* ) *|F* *k−* 1 ] = Q *υ* *k* *[ε]* [(] *[Z]* *[k][−]* [1] [)] *[ .]*


-----

Now, we bound the terms separately. Consider the term *U* 2, it is a sum of a martingale-difference sequence w.r.t. the filtration
*F* *k*, that is


E[ *H* *k* +1 *ε* ( *Z* *k* ) *−* E[ *H* *k* +1 *ε* ( *Z* *k* ) *|F* *k−* 1 ] *|F* *k−* 1 ] = 0 *.*
~~�~~ � ~~�~~ �
*M* *k*


Now, note that *H* *k* +1 is *σ* ( *Z* *k* +1 *, . . ., Z* *n* )-measurable. Then applying Minkowski’s and Holder’s inequalities, we obtain the
moment bound on the *M* *k*, that is

E *π* [1] *[/p]* [ *∥M* *k* *∥* *[p]* ] *≤* 2E [1] *π* *[/p]* [ *∥H* *k* +1 *ε* ( *Z* *k* ) *∥* *[p]* ] = 2E [1] *π* *[/p]* [ *∥ε* ( *Z* *k* ) *∥* *[p]* E *π* [ *∥H* *k* +1 *ε* ( *Z* *k* ) */∥ε* ( *Z* *k* ) *∥∥* *[p]* *|F* *k* ]] (64)

*≤* 2 *∥ε∥* *∞* sup E [1] *ξ* *[/p]* [ *∥H* *k* +1 *u∥* *[p]* ] *.*
*u∈* S *[d][−]* [1] *,ξ∈P* (Z)

Hence, applying Lemma 11 and (Durmus et al. 2025, Lemma 7), we get


E [1] *π* *[/p]* [ *∥M* *k* *∥* *[p]* ] *≤* 32 C **A** *κ* [1] *Q* *[/]* [2] *[p]* [1] *[/]* [2] *[t]* mix [1] *[/]* [2] *[∥][ε][∥]* *[∞]*


*n−k* 1 */* 2
�(1 *−* *αa* ) *[l][−]* [1]
� *l* =1 �


*≤* 64 C **A** *κ* *Q* [1] *[/]* [2] *[p]* [1] *[/]* [2] *[t]* mix [1] *[/]* [2] [(] *[αa]* [)] *[−]* [1] *[/]* [2] *[∥][ε][∥]* *[∞]* *[.]*


Therefore, applying Burkholder’s and Holder’s inequalities, we get


*n−* 1 *p/* 2 []
� *∥M* *k* *∥* [2]

*k* =2 � 


 *≤* *p*


*n−* 1
�
� *k* =2


1 */* 2
�


E [1] *π* *[/p]* [ *∥U* 2 *∥* *[p]* ] *≤* *p* E [1] *π* *[/p]*







*n−* 1
�
� *k* =2


� E [2] *π* *[/p]* [ *∥M* *k* *∥* *[p]* ]

*k* =2


(65)


*≤* 64 C **A** *κ* [1] *Q* *[/]* [2] *[p]* [3] *[/]* [2] *[t]* mix [1] *[/]* [2] *√n* ( *αa* ) *−* 1 */* 2 *∥ε∥* *∞* *.*

Now, to bound *U* 3 we denote *ϕ* *k* ( *z* ) = Q *υ* *k* *[ε]* [(] *[z]* [)][ and][ ¯] *[ϕ]* *[k]* [(] *[z]* [) =] *[ ϕ]* *[k]* [(] *[z]* [)] *[ −]* *[µ]* *[k]* [ for any] *[ k][ ≥]* [2][. We can seed that][ E] *[π]* [[¯] *[ϕ]* *[k]* [(] *[Z]* [)] = 0][ and]


*U* 3 =


*n−* 1

¯

� *ϕ* *k* ( *Z* *k−* 1 ) *.*

*k* =2


Also, using the previously obtained bound on *∥υ* *k* *∥* *∞*, we have


*∥ϕ* [¯] *k* *∥* *∞* *≤* 2 C **A** *κ* [1] *Q* *[/]* [2] *[∥][ε][∥]* *[∞]*


*n−k*
� ∆(Q *[l]* [+1] ) *≤* 4 C **A** *κ* [1] *Q* *[/]* [2] *[t]* [mix] *[∥][ε][∥]* *[∞]* *[.]*

*l* =1


Thus, applying Lemma 11 and (Durmus et al. 2025, Lemma 7), we get

E [1] *π* *[/p]* [ *∥U* 3 *∥* *[p]* ] *≤* 32 C **A** *κ* [1] *Q* *[/]* [2] *[p]* [1] *[/]* [2] *[t]* mix [3] *[/]* [2] *[n]* [1] *[/]* [2] *[∥][ε][∥]* *[∞]* *[.]* (66)

Finally, bound for *U* 1 can be obtained in the same way as provided in (64). Thus, combining (65) and (66) we get the result.

**Corollary 6.** *Assume A1, A2, and* ***UGE*** *1. Then for any probability measure ξ on* R *[d]* *×* Z *and α ∈* (0 *, α* 1 [(][b] *,∞* [)] [)] *[, we get:]*


E [1] *ξ* *[/p]* [ *∥*


*n−* 1

¯

� *ψ* *t* *∥* *[p]* ] *≤* c [(2)] *W,* 1 *[p]* [3] *[/]* [2] [(] *[αn]* [)] [1] *[/]* [2] [ + c] [(2)] *W,* 2 *[p]* [3] *[α]* *[−]* [1] *[/]* [2] [�]

*t* =0


log (1 */αa* ) *,* (67)


*where*

c [(2)] *W,* 1 [= 192 C] **[A]** *[ κ]* [1] *Q* *[/]* [2] *[t]* [mix] *[a]* *[−]* [1] *[/]* [2] *[∥][ε][∥]* *[∞]* *[,]* (68)

c [(2)] *W,* 2 [= 432 C] **[A]** [ D] [1] [ c] [(1)] *W,* 3 *[t]* mix [3] *[/]* [2] *[a]* *[−]* [1] *[/]* [2] *[∥][ε][∥]* *[∞]*

*Proof.* We use the optimal kernel coupling *K* *J* defined in (62). Then, using Minkowski’s inequality, we have


*n−* 1

¯

� *ψ* *t* *∥* *[p]* ] +

*t* =0


E *[K]* *[J]*
*ξ,* Π *J,α* [[] *[∥]*

�


*n−* 1
�


� *{ψ* ( *Y* *t* ) *−* *ψ* ( *Y* [˜] *t* ) *}∥* *[p]* ]

*t* =0


1 */p*
�


E [1] *ξ* *[/p]* [ *∥*


*n−* 1
� *ψ* ¯ *t* *∥* *[p]* ] *≤* E [1] Π *[/p]* *J,α* [[] *[∥]*

*t* =0


*.* (69)


Applying the result of Proposition 7, we can bound the first term. For the second term, we can apply Minkowski’s inequality
together with (61), thus

*n−* 1 1 */p* *n−* 1 *n−* 1

E *[K]* *ξ,* Π *[J]* *J,α* [[] *[∥]* � *{ψ* ( *Y* *t* ) *−* *ψ* ( *Y* [˜] *t* ) *}∥* *[p]* ] *≤* �(E *[K]* *ξ,* Π *[J]* *J,α* [[] *[∥][ψ]* [(] *[Y]* *[t]* [)] *[ −]* *[ψ]* [( ˜] *[Y]* *[t]* [)] *[∥]* *[p]* [])] [1] *[/p]* *[ ≤]* [2 C] **[A]** �(E *[K]* *ξ,* Π *[J]* *J,α* [[] *[c]* *[p]* [(] *[Y]* *[t]* *[,]* [ ˜] *[Y]* *[t]* [)])] [1] *[/p]* *[ .]*

� *t* =0 � *t* =0 *t* =0


1 */p*

]

�


*n−* 1
�(


�(E *[K]* *ξ,* Π *[J]* *J,α* [[] *[c]* *[p]* [(] *[Y]* *[t]* *[,]* [ ˜] *[Y]* *[t]* [)])] [1] *[/p]* *[ .]*

*t* =0


E *[K]* *[J]*
*ξ,* Π *J,α* [[] *[∥]*


*n−* 1
�


� *{ψ* ( *Y* *t* ) *−* *ψ* ( *Y* [˜] *t* ) *}∥* *[p]* ]

*t* =0


*≤*


*n−* 1
�


�(E *[K]* *ξ,* Π *[J]* *J,α* [[] *[∥][ψ]* [(] *[Y]* *[t]* [)] *[ −]* *[ψ]* [( ˜] *[Y]* *[t]* [)] *[∥]* *[p]* [])] [1] *[/p]* *[ ≤]* [2 C] **[A]**

*t* =0


-----

Therefore, using (62), (60) and applying Lemma 5, we get

(E *[K]* *ξ,* Π *[J]* *J,α* [[] *[c]* [(] *[Y]* *[t]* *[,]* [ ˜] *[Y]* *[t]* [)] *[p]* [])] [1] *[/p]* [ = (][E] *[ξ,]* [Π] *[J,α]* [[] **[W]** *[c,p]* [(] *[δ]* *[Y]* [0] [Q] *J* *[t]* *[, δ]* *Y* [ ˜] 0 [Q] *J* *[t]* [)])] [1] *[/p]*

*≤* c [(1)] *W,* 3 *[t]* mix [3] *[/]* [2] *[p]* [2] *[ρ]* 1 *[t/p]* *,α* ~~�~~ log (1 */αa* )(E *ξ,* Π *J,α* [ *c* *[p]* ( *Y* 0 *,* *Y* [˜] 0 )]) [1] *[/p]*

*≤* c [(1)] *W,* 3 *[t]* mix [3] *[/]* [2] *[p]* [2] *[ρ]* 1 *[t/p]* *,α* ~~�~~ log (1 */αa* )(2E [1] *ξ* *[/p]* [ *∥J* 0 [(0] *[,α]* [)] *∥* *[p]* ] + 2E [1] Π *[/p]* *J,α* [[] *[∥][J]* 0 [(0] *[,α]* [)] *∥* *[p]* ] + *[√]* *αa∥ε∥* *∞* )

*≤* 9D 1 c [(1)] *W,* 3 *[ρ]* 1 *[t/p]* *,α* *[t]* mix [3] *[/]* [2] *[p]* [2] [(] *[αa]* [)] [1] *[/]* [2] ~~[�]~~ log (1 */αa* ) *∥ε∥* *∞* *.*


Hence, since for our choice of *α* it holds that [�] *[n]* *t* =0 *[−]* [1] *[ρ]* 1 *[t/p]* *,α* *[≤]* [24] *[p]* [(] *[αa]* [)] *[−]* [1] [, we get]

*n−* 1 1 */p*

E *ξ,* *[K]* Π *[J]* *J,α* [[] *[∥]* � *{ψ* ( *Y* *t* ) *−* *ψ* ( *Y* [˜] *t* ) *}∥* *[p]* ] *≤* 432 C **A** D 1 c [(1)] *W,* 3 *[t]* mix [3] *[/]* [2]

� *t* =0 �


1 */p*
�


E *[K]* *[J]*
*ξ,* Π *J,α* [[] *[∥]*


*n−* 1
�


� *{ψ* ( *Y* *t* ) *−* *ψ* ( *Y* [˜] *t* ) *}∥* *[p]* ]

*t* =0


*≤* 432 C **A** D 1 c [(1)] *W,* 3 *[t]* mix [3] *[/]* [2] *[p]* [3] [(] *[αa]* [)] *[−]* [1] *[/]* [2] [�]


log (1 */αa* ) *∥ε∥* *∞* *.* (70)


Finally, to obtain the bound (67), we combine (69) and (70).
### **D Results for Richardson-Romberg procedure**

Define

D *J,* 1 = 10 c *J,* 5 +2 c *J,* 3 +24 c *J,* 5 +4 c *J,* 6 *,* D *J,* 2 = c *J,* 4 +13 *,*
D *J,* 3 = 2(c *J,* 1 + c *J,* 2 ) *,* D *J* = D *J,* 1 + D *J,* 2 + D *J,* 3 *,* (71)

where c *J,* 1 *,* c *J,* 2 *,* c *J,* 3 *,* c *J,* 4 *,* c *J,* 5 and c *J,* 6 are defined in (74), (77), (79), (82), (84) and (86).
For simplicity of notation, in this section we use *θ* [¯] *n* [(][RR][)] instead of *θ* [¯] *n* [(] *[α,]* [RR][)] . We preface the proof of Proposition 8 by giving
a statement of the Berbee lemma, which plays an essential role. Consider the extended measurable space Z [˜] N = Z [N] *×* [0 *,* 1],
equipped with theP˜ *ξ* = P *ξ* *⊗* **Unif** *σ* ([0-field *,* 1]) and denote by *Z* [˜] N = *Z* *[⊗]* [N] *⊗B* ˜E([0 *ξ* the corresponding expectated value. Finally, we denote by *,* 1]). For each probability measure *ξ* on (Z *, Z* ), we consider the probability measure ( ˜ *Z* *k* ) *k∈* N the canonical
process *Z* [˜] *k* : (( *z* *i* ) *i∈* N *, u* ) *∈* Z [˜] N *�→* *z* *k* and *U* : (( *z* *i* ) *i∈* N *, u* ) *∈* Z [˜] N *�→* *u* . Under P [˜] *ξ*, *{Z* [˜] *k* *}* *k∈* N is by construction a Markov chain
with initial distribution *ξ* and Markov kernel Q independent of *U* . The distribution of *U* under P [˜] *ξ* is uniform over [0 *,* 1].

**Lemma 6.** *Assume* ***UGE*** *1, let m ∈* N *[∗]* *and ξ be a probability measure on* (Z *, Z* ) *. Then, there exists a random process* ( *Z* [˜] *k* *[⋆]* [)] *[k][∈]* [N]
*defined on* (Z [˜] N *,* *Z* [˜] N *,* P [˜] *ξ* ) *such that for any k ∈* N *,*

*(a)* *Z* [˜] *k* *[⋆]* *[is independent of]* [ ˜] *[F]* *[k]* [+] *[m]* [ =] *[ σ][{][Z]* [ ˜] *[ℓ]* [:] *[ ℓ]* *[≥]* *[k]* [ +] *[ m][}][;]*
*(b)* P [˜] *ξ* ( *Z* [˜] *k* *[⋆]* *[̸]* [= ˜] *[Z]* *[k]* [)] *[ ≤]* [∆][(Q] *[m]* [)] *[;]*
*(c) the random variables* *Z* [˜] *k* *[⋆]* *[and]* [ ˜] *[Z]* *[k]* *[ have the same distribution under]* [ ˜][P] *[ξ]* *[.]*

*Proof.* Berbee’s lemma (Rio 2017, Lemma 5.1) ensures that for any *k*, there exists *Z* [˜] *k* *[⋆]* [satisfying (a), (c) and][ ˜][P] *[ξ]* [( ˜] *[Z]* *k* *[⋆]* *[̸]* [= ˜] *[Z]* *[k]* [) =]
*β* *ξ* ( *σ* ( *Z* [˜] *k* ) *,* *F* [˜] *k* + *m* ). Here for two sub *σ* -fields F, G of *Z* [˜] N,


*β* *ξ* (F *,* G) = (1 */* 2) sup [�] *i*


*i∈* I �


*j∈* J *[|]* [P][˜] *[ξ]* [(][A] *[i]* *[ ∩]* [B] *[j]* [)] *[ −]* [P][˜] *[ξ]* [(][A] *[i]* [)˜][P] *[ξ]* [(][B] *[j]* [)] *[|][,]*


and the supremum is taken over all pairs of partitions *{* A *i* *}* *i∈* I *∈* F [I] and *{* B *j* *}* *j∈* J *∈* G [J] of Z [˜] N with I and J finite. Applying (Douc
et al. 2018, Theorem 3.3) with **UGE** 1 completes the proof.

**Proposition 8.** *Assume A1, A2 and* ***UGE*** *1. Fix* 2 *≤* *p < ∞, α ∈* (0 *, α* *∞* ] *and initial probability measure ξ on* (Z *, Z* ) *, we have*
*the following bound*

E [1] *ξ* *[/p]* [ *∥J* *n* [(2] *[,α]* [)] *∥* *[p]* ] *≤* D *J* *t* [5] mix *[/]* [2] *[p]* [7] *[/]* [2] *[α]* [3] *[/]* [2] [ log] [3] *[/]* [2] [(1] *[/αa]* [)] *[,]*

*where* D *J* *is defined in* (71) *.*

*Proof.* To bound *J* *n* [(2] *[,α]* [)] we define


*S* *j* [(1)] +1: *i* [=]

*S* *j* [(2)] +1: *n* [=]


*i*
� (I *−* *α* **A** [¯] ) *[i][−][k]* [ ˜] **A** ( *Z* *k* )(I *−* *α* **A** [¯] ) *[k][−][j][−]* [1] *,* (72)

*k* = *j* +1

*n*
� (I *−* *α* **A** [¯] ) *[n][−][i]* [ ˜] **A** ( *Z* *i* ) *S* *j* [(1)] +1: *i* *[.]*

*i* = *j* +1


-----

Hence, following the definition (14) we have


*n−* 1
*J* *n* [(2] *[,α]* [)] = *−α* [3] � *S* *j* [(2)] +1: *n* *[ε]* [(] *[Z]* *[j]* [)] *[ .]*

*j* =1


Now, we form blocks of size *m* and let *N* = *⌊* *[n]* *m* *[−]* [1] *[⌋]* [be a number of blocks. Then we can decompose]


( *N* *−* 1) *m*
*J* *n* [(2] *[,α]* [)] = *−α* [3] �


� *S* *j* [(2)] +1: *n* *[ε]* [(] *[Z]* *[j]* [) =] *[ −][α]* [3] *[T]* [1] *[ −]* *[α]* [3] *[T]* [2] *[ .]*

*j* =( *N* *−* 1) *m* +1


*−* 1) *m* *n−* 1
� *S* *j* [(2)] +1: *n* *[ε]* [(] *[Z]* *[j]* [)] *[ −]* *[α]* [3] �

*j* =1 *j* =( *N* *−* 1)


First, we are going to bound *T* 2 . Using Lemma 10, we get


E [1] *ξ* *[/p]* [ *∥T* 2 *∥* *[p]* ] *≤*


*n−* 1
� E [1] *ξ* *[/p]* [ *∥S* *j* [(2)] +1: *n* *[ε]* [(] *[Z]* *[j]* [)] *[∥]* *[p]* []] *[ ≤]* [c] *[J,]* [1] *[ m]* [3] *[/]* [2] *[t]* [3] mix *[/]* [2] *[p]* [2] *[α]* *[−]* [1] *[,]* (73)

*j* =( *N* *−* 1) *m* +1

1 + D [(1)] 2 [)] *[∥]* *[ε]* *[∥]* *[∞]*
c *J,* 1 := [(] [D] [(1)] *,* (74)
*a*


where we set


and we used that *n −* ( *N −* 1) *m ≤* 2 *m* with log( *x* ) *≤* *x* [1] *[/]* [2] for *x >* 0. To bound *T* 1 we should note a decomposition for *S* *j* [(1)] +1: *i*

*S* *j* [(1)] +1: *i* [= (I] *[ −]* *[α]* [ ¯] **[A]** [)] *[i][−][m][−][j]* *[S]* *j* [(1)] +1: *j* + *m* [+] *[ S]* *j* [(1)] + *m* +1: *i* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[ .]* (75)

Substituting (75) into *S* *j* [(2)] +1: *n* [, we get]


*j* + *m*

*S* *j* [(2)] +1: *n* [=] � (I *−* *α* **A** [¯] ) *[n][−][i]* [ ˜] **A** ( *Z* *i* ) *S* *j* [(1)] +1: *i* [+]

*i* = *j* +1


*n*
� (I *−* *α* **A** [¯] ) *[n][−][i]* [ ˜] **A** ( *Z* *i* ) *S* *j* [(1)] +1: *i*

*i* = *j* + *m* +1


= (I *−* *α* **A** [¯] ) *[n][−][j][−][m]* *S* *j* [(2)] +1: *j* + *m* [+] *[ S]* *j* [(1)] + *m* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[S]* *j* [(1)] +1: *j* + *m* [+] *[ S]* *j* [(2)] + *m* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[ .]*
Thus, *T* 1 can be represented as *T* 1 = *T* 11 + *T* 12 + *T* 13, where


*T* 11 =

*T* 12 =

*T* 13 =

For the first term, using Lemma 10 we get


( *N* *−* 1) *m*
� (I *−* *α* **A** [¯] ) *[n][−][j][−][m]* *S* *j* [(2)] +1: *j* + *m* *[ε]* [(] *[Z]* *[j]* [)] *[,]*

*j* =1

( *N* *−* 1) *m*
� *S* *j* [(1)] + *m* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[S]* *j* [(1)] +1: *j* + *m* *[ε]* [(] *[Z]* *[j]* [)] *[,]*

*j* =1

( *N* *−* 1) *m*
� *S* *j* [(2)] + *m* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[ε]* [(] *[Z]* *[j]* [)] *[ .]*

*j* =1


E [1] *ξ* *[/p]* [ *∥T* 11 *∥* *[p]* ] *≤* *κ* [1] *Q* *[/]* [2]


( *N* *−* 1) *m*
� (1 *−* *αa* ) [(] *[n][−][j][−][m]* [)] *[/]* [2] E [1] *ξ* *[/p]* [ *∥S* *j* [(2)] +1: *j* + *m* *[ε]* [(] *[Z]* *[j]* [)] *[∥]* *[p]* []] (76)

*j* =1


( *N* *−* 1) *m*
*≤* *κ* [1] *Q* *[/]* [2] [(][D] [1] [ +][ D] [2] [)] *[t]* mix [3] *[/]* [2] *[p]* [2] *[∥][ε][∥]* *[∞]* *[m]* [3] *[/]* [2] � (1 *−* *αa* ) [(] *[n][−][j][−]* [1)] *[/]* [2] *≤* c *J,* 2 *m* [3] *[/]* [2] *t* [3] mix *[/]* [2] *[p]* [2] *[α]* *[−]* [1] *[,]*

*j* =1


where

For the second term, we have

E [1] *ξ* *[/p]* [ *∥T* 12 *∥* *[p]* ]


*κ* [1] *Q* *[/]* [2] [(][D] 1 [(1)] + D [(1)] 2 [)] *[∥][ε][∥]* *[∞]*
c *J,* 2 := *.* (77)
*a*

*j* + *m*
� E [1] *ξ* *[/p]* [ *∥S* *j* [(1)] + *m* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[j]* [+] *[m][−][k]* [+1] [ ˜] **[A]** [(] *[Z]* *[k]* [)(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[k][−][j][−]* [1] *[ε]* [(] *[Z]* *[j]* [)] *[∥]* *[p]* []]

*k* = *j* +1


*j* + *m*
� E [1] *[/p]* [ *∥v* *j,k* *∥* *[p]* E *[F]* *[j]* [+] *[m]* [ *∥S* *j* [(1)] + *m* +1: *n* *[v]* *[k]* *[/][∥][v]* *[j,k]* *[∥∥]* *[p]* []]] *[,]*

*k* = *j* +1


*≤*

*≤*


( *N* *−* 1) *m*
�

*j* =1

( *N* *−* 1) *m*
�

*j* =1


-----

where
*v* *j,k* = (I *−* *α* **A** [¯] ) *[j]* [+] *[m][−][k]* [+1] [ ˜] **A** ( *Z* *k* )(I *−* *α* **A** [¯] ) *[k][−][j][−]* [1] *ε* ( *Z* *j* ) *.*

Let


*j* + *m*

*B* 1 ( *α* ) = � ( *n −* *j −* *m* ) [1] *[/]* [2] (1 *−* *αa* ) [(] *[n][−][j][−][m][−]* [1)] *[/]* [2] E [1] *ξ* *[/p]* [ *∥v* *j,k* *∥* *[p]* ] *.*

*k* = *j* +1


Then, we have

( *N* *−* 1) *m*
*B* 1 ( *α* ) *≤* *κ* *Q* C **A** *∥ε∥* *∞* *m* (1 *−* *αa* ) *[m/]* [2] � ( *n −* *j −* *m* ) [1] *[/]* [2] (1 *−* *αa* ) [(] *[n][−][j][−][m][−]* [1)] *[/]* [2] *≤* 8 *[√]* *π* ( *αa* ) *[−]* [3] *[/]* [2]

*j* =1

Thus, using (Durmus et al. 2025, Lemma 5), we get


*j* + *m*
� E [1] *ξ* *[/p]* [ *∥v* *j,k* *∥* sup E *ξ* *′* [ *∥S* *j* [(1)] + *m* +1: *n* *[u][∥]* *[p]* []]] (78)

*k* = *j* +1 *u∈* S *[d]* [=1] *,ξ* *[′]* *∈P* (Z)


E [1] *ξ* *[/p]* [ *∥T* 12 *∥* *[p]* ] *≤*


( *N* *−* 1) *m*
�

*j* =1


where


*≤* 16 *κ* *Q* C **A** ( *t* mix *p* ) [1] *[/]* [2] *B* 1 ( *α* ) *≤* c *J,* 3 *m* ( *t* mix *p* ) [1] *[/]* [2] *α* *[−]* [3] *[/]* [2] *,*

128 *[√]* *πκ* [2] *Q* [C] **A** [2] *[∥][ε][∥]* *[∞]*
c *J,* 3 := *.* (79)
*a* [3] *[/]* [2]


To bound the third term we should switch to the extended space (Z [˜] N *,* *Z* [˜] N *,* P [˜] N ). From Lemma 6 it follows that E [1] *ξ* *[/p]* [ *∥T* 13 *∥* *[p]* ] =
E˜ [1] *ξ* *[/p]* [ *∥T* [˜] 13 *∥* *[p]* ] with


*T* ˜ 13 =


( *N* *−* 1) *m*
� *S* ˜ *j* [(2)] + *m* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[ε]* [( ˜] *[Z]* *[j]* [)] *[,]*

*j* =1


where *S* [˜] *j* [(2)] + *m* +1: *n* [is a counterpart of] *[ S]* *j* [(2)] + *m* +1: *n* [but defined on the extended space. Thus, we have]


*m*
� *S* ˜ ( [(2)] *s* +1) *m* + *j* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [(] *[ε]* [( ˜] *[Z]* *[sm]* [+] *[j]* [)] *[ −]* *[ε]* [( ˜] *[Z]* *sm* *[∗]* + *j* [)) = ˜] *[T]* [131] [+ ˜] *[T]* [132] *[.]*

*j* =1


*T* ˜ 13 =

+


*N* *−* 2
�

*s* =0

*N* *−* 2
�

*s* =0


*m*
� *S* ˜ ( [(2)] *s* +1) *m* + *j* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[ε]* [( ˜] *[Z]* *sm* *[∗]* + *j* [)]

*j* =1


Now, define the function *g* ( *z* ) : Z *→* R *[d]*, *g* ( *z* ) = (I *−* *α* **A** [¯] ) *[m]* *ε* ( *z* ). Using Proposition 10, we can bound this function by
*∥g∥* *∞* *≤* *κ* [1] *Q* *[/]* [2] [(1] *[ −]* *[αa]* [)] *[m/]* [2] *[∥][ε][∥]* *[∞]* [while] *[ π]* [(] *[g]* [) = 0][. Using Lemma 6 and (Durmus et al. 2025, Lemma 6) we can estimate][ ˜] *[T]* [131] [ as]
follows


1 */* 2
�


*N* *−* 2
�
� *s* =0


E˜ [1] *ξ* *[/p]* [ *∥T* [˜] 131 *∥* *[p]* ] *≤*

+


*m*
� 2 *p∥g∥* *∞*

*j* =1


*m*
�

*j* =1


� sup E˜ [2] *ξ* *[/p]* [ *∥S* [˜] ( [(2)] *s* +1) *m* + *j* +1: *n* *[u][∥]* *[p]* []]

*s* =0 *u∈* S *[d][−]* [1]


(80)


*N* *−* 2
� *∥ξ* Q *[sm]* [+] *[j]* *g∥* sup E˜ [1] *ξ* *[/p]* [ *∥S* [˜] ( [(2)] *s* +1) *m* + *j* +1: *n* *[u][∥]* *[p]* []] *[ .]*

*s* =0 *u∈* S *[d][−]* [1]


Further, using Lemma 10 and *∥ξ* Q *[sm]* [+] *[j]* *g∥≤* ∆(Q *[sm]* [+] *[j]* ) *∥g∥* *∞*, we get


*m*
�

*j* =1


*N* *−* 2
� *∥ξ* Q *[sm]* [+] *[j]* *g∥* sup E˜ [1] *ξ* *[/p]* [ *∥S* [˜] ( [(2)] *s* +1) *m* + *j* +1: *n* *[u][∥]* *[p]* []] (81)

*s* =0 *u∈* S *[d][−]* [1]


*≤* 2 *∥g∥* *∞* (D [(1)] 1 + D [(1)] 2 [)] *[t]* [3] mix *[/]* [2] *[p]* [2] [ sup] *{x* [3] *[/]* [2] (1 *−* *αa* ) *[x/]* [2] *}*
*x≥* 1

where we used that sup *x≥* 1 *{x* [3] *[/]* [2] (1 *−* *αa* ) *[x/]* [2] *} ≤* 3( *αa* ) *[−]* [3] *[/]* [2] and


+ *∞*
� ∆(Q *[ℓ]* ) *≤* c *J,* 4 *t* mix [5] *[/]* [2] *[p]* [2] [(1] *[ −]* *[αa]* [)] [(] *[m][−]* [1)] *[/]* [2] *[α]* *[−]* [3] *[/]* [2] *[,]*

*ℓ* =0


12 *κ* [1] *Q* *[/]* [2] [(][D] 1 [(1)] + D [(1)] 2 [)] *[∥][ε][∥]* *[∞]*
c *J,* 4 := *.* (82)
*a* [3] *[/]* [2]


-----

Denote

We can bound *B* 2 ( *α* ) as


*B* 2 ( *α* ) =


( *N* *−* 1) *m*
� ( *n −* *j −* *m* ) [2] log [2] ( *n −* *j −* *m* )(1 *−* *αa* ) *[n][−][j][−][m][−]* [1] *.*

*j* =1


+ *∞*
*B* 2 ( *α* ) *≤*
� 0


+ *∞* + *∞*

*t* [2] log [2] ( *t* ) *e* *[−][αat/]* [2] *dt ≤* 16( *αa* ) *[−]* [3] log [2] (2 */αa* )
0 � 0


+ *∞* + *∞*

*t* [2] *e* *[−][t]* *dt* + 16( *αa* ) *[−]* [3]
0 � 0


*t* [2] log [2] ( *t* ) *e* *[−][t]* *dt*
0


*≤* (32 log [2] (2 */αa* ) + 112)( *αa* ) *[−]* [3] *,*

For the first term of (80), using Jensen’s inequality and Lemma 10, we obtain


*≤* 2(D [(1)] 1 + D [(1)] 2 [)] *[t]* [3] mix *[/]* [2] *[p]* [3] *[m]* [1] *[/]* [2] *[∥][g][∥]* *[∞]* *[B]* 1 [1] *[/]* [2] ( *α* ) (83)

*≤* c *J,* 5 *t* [3] mix *[/]* [2] *[p]* [3] *[m]* [1] *[/]* [2] [(1] *[ −]* *[αa]* [)] [(] *[m][−]* [1)] *[/]* [2] *[α]* *[−]* [3] *[/]* [2] [(8 log(1] *[/αa]* [) + 17)] *[,]*


1 */* 2
�


*N* *−* 2
�
� *s* =0


2 *p∥g∥* *∞*


*m*
�

*j* =1


� sup E˜ [2] *ξ* *[/p]* [ *∥S* [˜] ( [(2)] *s* +1) *m* + *j* +1: *n* *[u][∥]* *[p]* []]

*s* =0 *u∈* S *[d][−]* [1]


where we set

2 *κ* [1] *Q* *[/]* [2] [(][D] 1 [(1)] + D [(1)] 2 [)] *[∥][ε][∥]* *[∞]*
c *J,* 5 = *,* (84)
*a* [3] *[/]* [2]

+ *∞*
combined with the fact that � 0 *t* [2] log [2] ( *t* ) *e* *[−][t]* *dt ≤* 7. Now we can bound *T* [˜] 132 . Set *V* *l* = *ε* ( *Z* [˜] *l* ) *−* *ε* ( *Z* [˜] *l* *[∗]* [)][ and] *[ F]* *l* *[∗]* =
*σ* ( *Z* [˜] *i* *,* *Z* [˜] *i* *[∗]* *[|]* [1] *[ ≤]* *[i][ ≤]* *[l]* [)][. For the term][ ˜] *[T]* [132] [, we have]


E˜ [1] *ξ* *[/p]* [ *∥T* [˜] 132 *∥* *[p]* ] *≤*

*≤*

*≤*


*N* *−* 2
�

*s* =0

*N* *−* 2
�

*s* =0

*N* *−* 2
�

*s* =0


*m*
� E˜ [1] *ξ* *[/p]* [ *∥S* [˜] ( [(2)] *s* +1) *m* + *j* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[V]* *[sm]* [+] *[j]* *[∥]* *[p]* []]

*j* =1


*m*
�


*m*

˜ ˜ ˜

� E [1] *ξ* *[/p]* [ *∥V* *sm* + *j* *∥* *[p]* sup E *ξ* *′* [ *∥S* ( [(2)] *s* +1) *m* + *j* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[u][∥]* *[p]* []]] *[,]*

*j* =1 *u∈* S *[d][−]* [1] *,ξ* *[′]* *∈P* (Z)


*m*
�


*m*
� E˜ [1] *ξ* *[/p]* [ *∥V* *sm* + *j* *∥* *[p]* E [˜] *ξF* *sm* *[∗]* + *j* [ *∥S* [˜] ( [(2)] *s* +1) *m* + *j* +1: *n* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[V]* *[sm]* [+] *[j]* *[/][∥][V]* *[sm]* [+] *[j]* *[∥∥]* *[p]* []]]

*j* =1


*m*
�


where *P* (Z) is the set of probability measure on (Z *, Z* ). Under A2 and **UGE** 1, we have *∥V* *sm* + *j* *∥≤* 2 *∥ε∥* *∞* I *{Z* [˜] *sm* + *j* *̸* = *Z* [˜] *sm* *[∗]* + *j* *[}]*
and P [˜] [ *Z* [˜] *sm* + *j* *̸* = *Z* [˜] *sm* *[∗]* + *j* []] *[ ≤]* [∆][(Q] *[m]* [)] *[ ≤]* [(1] *[/]* [4)] *[⌊][m/t]* [mix] *[⌋]* [. Denote]


*B* 3 ( *α* ) =

Then, as in case with *B* 2 ( *α* ), we have


( *N* *−* 1) *m*
� ( *n −* *j −* *m* ) log( *n −* *j −* *m* )(1 *−* *αa* ) [(] *[n][−][j][−][m][−]* [1)] *[/]* [2] *.*

*j* =1


+ *∞*
*B* 3 ( *α* ) *≤* *t* log( *t* ) *e* *[−][αat/]* [2] *dt ≤* ( *αa* ) *[−]* [2] (4 log(1 */αa* ) + 7)
� 0


Applying Lemma 10, we obtain

E˜ [1] *ξ* *[/p]* [ *∥T* [˜] 132 *∥* *[p]* ] *≤* 2 *∥ε∥* *∞* *t* [3] mix *[/]* [2] *[p]* [2] [(1] *[/]* [4)] [(1] *[/p]* [)] *[⌊][m/t]* [mix] *[⌋]* [(1] *[ −]* *[αa]* [)] *[m/]* [2] *[B]* [2] [(] *[α]* [)] (85)


*≤* c *J,* 6 *t* [3] mix *[/]* [2] *[p]* [2] [(1] *[/]* [4)] (1 */p* )� *t* mix *m*


� (1 *−* *αa* ) [(] *[m][−]* [1)] *[/]* [2] *α* *[−]* [2] (4 log(1 */αa* ) + 7) *,*


where

Finally, we set


1 + D [(1)] 2 [)] *[∥]* *[ε]* *[∥]* *[∞]*
c *J,* 6 := [2] [(] [D] [(1)] *.* (86)
*a* [2]


*.*
�


*m* = *t* mix


*p* lo g ( 1 */* *αa* )
� 2 log 2


With this choice of *m ≥* *t* mix, we have (1 */* 4) [(1] *[/p]* [)] *[⌊][m/t]* [mix] *[⌋]* *≤* ( *αa* ) [1] *[/]* [2] and
*m ≤* 2 *t* mix *p* log (1 */αa* ) */* (2 log 2). Thus, substituting such *m* into the (73), (76), (78), (83), (81), (85) we obtain the result.


-----

#### **D.1 Proof of Theorem 2**

We preface the proof of main result by auxiliary lemma and proposition.

**Lemma 7.** *Assume A1, A2, and* ***UGE*** *1. Let* 2 *≤* *p ≤* *q/* 2 *. Then, for any α ∈* (0; *α* *q,* [(M)] *∞* *[t]* *[−]* mix [1] [)] *[ with][ α]* *q,* [(M)] *∞* *[defined in]* [ (31)] *[,][ θ]* 0 *[∈]* [R] *[d]* *[,]*
*probability ξ on* (Z *, Z* ) *, and n ∈* N *, it holds*

E [1] *ξ* *[/p]* [ *∥θ* *n* [(] *[α]* [)] *−* *θ* *[⋆]* *∥* *[p]* ] *≤* *[√]* *κ* *Q* *e* [2] *d* [1] *[/q]* *ρ* *[n]* 1 *,α* *[∥][θ]* [0] *[−]* *[θ]* *[⋆]* *[∥]* [+][ D] [2] *[d]* [1] *[/q]* *[√][αapt]* [mix] *[∥][ε][∥]* *[∞]* *[,]*

*where* D 2 *and ρ* 1 *,α* *are defined as*


D 2 = D 1 (1 + 24 *√*

*Proof.* See (Durmus et al. 2025, Proposition 9).


2e [2] *[√]* *κ* *Q* C **A** *a* *[−]* [1] ) *,* *ρ* 1 *,α* = *e* *[−][αa/]* [12] *.*


**Proposition 9.** *Assume A1, A2 and* ***UGE*** *1. Fix* 2 *≤* *p ≤* *q/* 2 *, α ∈* (0 *, α* *q,* [(M)] *∞* *[t]* *[−]* mix [1] [)] *[ and probability distribution][ ξ][ on]* [ (][Z] *[,][ Z]* [)] *[.]*
*Then, we have*

E [1] *ξ* *[/p]* [ *∥H* *n* [(2] *[,α]* [)] *∥* *[p]* ] *≤* D *H* *d* [1] *[/q]* *t* [5] mix *[/]* [2] *[p]* [7] *[/]* [2] *[α]* [3] *[/]* [2] [ log] [3] *[/]* [2] [(1] *[/αa]* [)] *[,]*

*where*

D *H* = 384 *κ* [1] *Q* *[/]* [2] [C] **[A]** *[ a]* *[−]* [1] *[e]* [2] [D] *[J]* *[,]*

*and* D *J* *is defined in* (71) *.*

*Proof.* Unrolling the recursion (14), we get


*n*

*H* *n* [(2] *[,α]* [)] = *−α* � Γ [(] *l* +1: *[α]* [)] *n* **[A]** [˜] [(] *[Z]* *[l]* [)] *[J]* *l* [(2] *−* *[,α]* 1 [)]

*l* =1


Thus, using Minkowski’s and Holder inequalities, we have


E [1] *ξ* *[/p]* [ *∥H* *n* [(2] *[,α]* [)] *∥* *[p]* ] *≤* *α*


*n*
� E [1] *[/]* [2] *[p]* [ *∥* Γ [(] *l* +1: *[α]* [)] *n* **[A]** [˜] [(] *[Z]* *[l]* [)] *[∥]* [2] *[p]* []][E] [1] *ξ* *[/]* [2] *[p]* [ *∥J* *l* [(2] *−* *[,α]* 1 [)] *[∥]* [2] *[p]* []]

*l* =1


Using (Durmus et al. 2025, Proposition 7), we can bound the first factor as

E *ξ* [1] *[/]* [2] *[p]* [ *∥* Γ [(] *l* +1: *[α]* [)] *n* **[A]** [˜] [(] *[Z]* *[l]* [)] *[∥]* [2] *[p]* []] *[ ≤]* [2] *[√][κ]* *[Q]* [ C] **[A]** *[ e]* [2] *[d]* [1] *[/q]* *[e]* *[−][αa]* [(] *[n][−][l]* [)] *[/]* [12]

Combining the inequalities above and using Proposition 8, we obtain


E [1] *ξ* *[/p]* [ *∥H* *n* [(2] *[,α]* [)] *∥* *[p]* ] *≤* 16D *J* *κ* [1] *Q* *[/]* [2] [C] **[A]** *[ e]* [2] *[d]* [1] *[/q]* *[t]* mix [5] *[/]* [2] *[p]* [7] *[/]* [2] *[α]* [5] *[/]* [2] [ log] [3] *[/]* [2] [(1] *[/αa]* [)]

Finally, using that *e* *[−][x]* *≤* 1 *−* *x/* 2 for *x ∈* (0 *,* 1), we get the result.

Define the quantities


*n*
� *e* *[−][αal/]* [12]

*l* =1


D [(] 1 [RR][)] = C **A** (12D 2 *a* [1] *[/]* [2] + 3456D 1 c [(1)] *W,* 3 *[a]* *[−]* [1] *[/]* [2] [)e] [1] *[/p]* *[t]* mix [3] *[/]* [2] *[∥][ε][∥]* *[∞]* *[,]*

D [(] 2 [RR][)] = 2688 C **A** *κ* [1] *Q* *[/]* [2] *[a]* *[−]* [1] *[/]* [2] *[t]* [mix] *[∥][ε][∥]* *[∞]* *[,]*

D [(] 3 [RR][)] = C **A** (6D *J* + 3D *H* e [1] *[/p]* ) *t* [5] mix *[/]* [2] [+ 28] *[∥]* **[A]** [¯] *[∥∥]* **[A]** [¯] *[−]* [1] *[∥][t]* [5] mix *[/]* [2] *[∥][ε][∥]* *[∞]* *[,]*

D [(] 4 [RR][)] = 16D *J* *t* [5] mix *[/]* [2] *[,]* C Ros *,* p = 2(C [(M)] Ros *,* 1 [+ C] [(M)] Ros *,* 2 [)] *[t]* mix [3] *[/]* [4] [log] 2 [(2] *[p]* [)] *[,]*


and


*R* *n,p,α,t* [(][fl][)] mix [= C] [Ros] *[,]* [p] *[pn]* *[−]* [3] *[/]* [4] [ + (][D] [(] 1 [RR][)] *p* [3] *[/]* [2] ( *αn* ) *[−]* [1] *[/]* [2] [�]


log (1 */αa* ) + D [(] 2 [RR][)] *α* [1] *[/]* [2] ) *p* [3] *[/]* [2] *n* *[−]* [1] *[/]* [2] (87)


+ (D [(] 3 [RR][)] *α* + D [(] 4 [RR][)] *n* *[−]* [1] ) *p* [7] *[/]* [2] *α* [1] *[/]* [2] log [3] *[/]* [2] (1 */αa* ) *,*

*R* *n,p,α,t* [(][tr][)] mix [= 13(1 + C] **[A]** [)] *[κ]* [1] *Q* *[/]* [2] [e] [2+1] *[/p]* [(] *[αn]* [)] *[−]* [1] *[ .]*


-----

*Proof of Theorem 2.* We start with applying (27) to the decomposition (26). Setting *n* 0 = *n/* 2, we get


¯
**A** (¯ *θ* *n* [(][RR][)] *−* *θ* *[⋆]* ) = [4]


*αn* [4] [(] *[θ]* *n/* [(] *[α]* [)] 2 *[−]* *[θ]* *n* [(] *[α]* [)] [)] *[ −]* *αn* [1]


*n−* 1
� *ε* ( *Z* *t* +1 )

*t* = *n/* 2


*αn* [4] [(] *[θ]* *n/* [(] *[α]* [)] 2 *[−]* *[θ]* *n* [(] *[α]* [)] [)] *[ −]* *αn* [1] [(] *[θ]* *n/* [(2] *[α]* 2 [)] *[−]* *[θ]* *n* [(2] *[α]* [)] )

~~�~~ � ~~�~~ �


+ [2]



[2] *n* *−* [4]

*n* *[E]* [(][tr] *[,]* [2] *[α]* [)] *n*


*n* *−* *n*
*n* *[E]* [(][tr] *[,]* [2] *[α]* [)] *n* *[E]* [(][tr] *[,α]* [)]

� � ~~�~~ �


*−* [2]

*n*


*T* 1 [(2)]


*T* tr [(2)]


*n*


˜

� **A** ( *Z* *t* +1 ) *J* *t* [(] *[l,]* [2] *[α]* [)] *−* *n* [4]

*t* = *n/* 2


*n−* 1
�






+


1
�

*l* =0






 *n* 2




� **A** ˜ ( *Z* *t* +1 ) *J* *t* [(] *[l,α]* [)]

*t* = *n/* 2


*n−* 1
�


 *t* = *n/* 2 *t* = *n/* 2 

~~�~~ �� �


*T* [(2)]
*J,l*


*n*


+ [2]

*n*


˜

� **A** ( *Z* *t* +1 ) *H* *t* [(2] *[,]* [2] *[α]* [)] *−* *n* [4]

*t* = *n/* 2


*n−* 1
�


*.*


*n−* 1
� **A** ˜ ( *Z* *t* +1 ) *H* *t* [(2] *[,α]* [)]

*t* = *n/* 2


~~�~~ �� �

*T* *H* [(2)]

Now, we use Lemma 7 to bound the terms which correspond to the deviation of the last iterate. Hence, using Minkowski’s
inequality we can bound *T* 1 [(2)], as

E [1] *ξ* *[/p]* [ *∥T* 1 [(2)] *∥* *[p]* ] *≤* 10( *αn* ) *[−]* [1] *[√]* *κ* *Q* *e* [2] *d* [1] *[/q]* *e* *[−][αan/]* [24] *∥θ* 0 *−* *θ* *[⋆]* *∥* + 12D 2 *d* [1] *[/q]* ( *pt* mix *a* ) [1] *[/]* [2] *α* *[−]* [1] *[/]* [2] *n* *[−]* [1] *∥ε∥* *∞* *.*

To bound the transient terms we should use the exponential stability for the product of random matrices. That is, using (Durmus
et al. 2025, Proposition 7), we get

E [1] *ξ* *[/p]* [ *∥E* *n* [(][tr] *[,α]* [)] *∥* *[p]* ] *≤* ( *n/* 2) *[√]* *κ* *Q* *e* [2] *d* [1] *[/q]* C **A** *e* *[−][αan/]* [24] *∥θ* 0 *−* *θ* *[⋆]* *∥* *.*

Thus, we can bound *T* tr [(2)] as
E [1] *ξ* *[/p]* [ *∥T* tr [(2)] *[∥]* *[p]* []] *[ ≤]* [3] *[√][κ]* *Q* *[e]* [2] *[d]* [1] *[/q]* [ C] **A** *[e]* *[−][αan/]* [24] *[∥][θ]* 0 *[−]* *[θ]* *[⋆]* *[∥]* *[.]*

The leading term (2 */n* ) [�] *[n]* *t* = *[−]* *n/* [1] 2 *[ε]* [(] *[Z]* *[t]* [+1] [)][ is a linear statistic of UGE Markov chain. Thus, using Theorem 3, we get]


E [1] *ξ* *[/p]* [ *∥*


*n−* 1
� *ε* ( *Z* *t* +1 ) *∥* *[p]* ] *≤* C Rm *,* 1 *p* [1] *[/]* [2] *n* [1] *[/]* [2] *{* Tr Σ *ε* *}* [1] *[/]* [2] + C [(M)] Ros *,* 1 *[n]* [1] *[/]* [4] *[t]* mix [3] *[/]* [4] *[p]* [ log] 2 [(2] *[p]* [) + C] [(M)] Ros *,* 2 *[t]* [mix] *[p]* [ log] 2 [(2] *[p]* [)] *[ .]*

*t* = *n/* 2


Now, we can bound *T* *J,* [(2)] 1 [through] *[ J]* *t* [(2] *[,α]* [)] . Indeed, using the expansion (14), we have


*n−* 1

˜

� **A** ( *Z* *t* +1 ) *J* *t* [(1] *[,α]* [)] = *α* *[−]* [1] ( *J* *n/* [(2] *[,α]* 2 [)] *−* *J* *n* [(2] *[,α]* [)] ) *−*

*t* = *n/* 2


*n−* 1
� **A** ( *Z* *t* +1 ) *J* *t* [(2] *[,α]* [)] *.*

*t* = *n/* 2


The first term can be bounded directly using Proposition 8. Also, using Minkowski’s inequality, we can bound the second term,

as


E [1] *ξ* *[/p]* [ *∥*


*n−* 1
� **A** ( *Z* *t* +1 ) *J* *t* [(2] *[,α]* [)] *∥* *[p]* ] *≤* ( *n/* 2) C **A** sup E [1] *[/p]* [ *∥J* *t* [(2] *[,α]* [)] *∥* *[p]* ] *.*

*t* = *n/* 2 *n/* 2 *≤t≤n*


Hence, we get


E [1] *ξ* *[/p]* [ *∥*


*n−* 1
� **A** ˜ ( *Z* *t* +1 ) *J* *t* [(1] *[,α]* [)] *∥* *[p]* ] *≤* (2 *α* *[−]* [1] + ( *n/* 2) C **A** ) sup E [1] *ξ* *[/p]* [ *∥J* *t* [(2] *[,α]* [)] *∥* *[p]* ] *.*

*t* = *n/* 2 *n/* 2 *≤t≤n*


Thus, using Proposition 8, we can bound *T* *J,* [(2)] 1 [, as follows]

E [1] *ξ* *[/p]* [ *∥T* *J,* [(2)] 1 *[∥]* *[p]* []] *[ ≤]* [(2] *[/n]* [)(] *[α]* *[−]* [1] [ + (] *[n/]* [2) C] **[A]** [)] sup E [1] *ξ* *[/p]* [ *∥J* *t* [(2] *[,]* [2] *[α]* [)] *∥* *[p]* ] + (4 */n* )(2 *α* *[−]* [1] + ( *n/* 2) C **A** ) sup E [1] *ξ* *[/p]* [ *∥J* *t* [(2] *[,α]* [)] *∥* *[p]* ]
*n/* 2 *≤t≤n* *n/* 2 *≤t≤n*

*≤* (16( *αn* ) *[−]* [1] + 6 C **A** ) sup *ξ* [ *∥J* *t* [(2] *[,α]* [)] *∥* *[p]* ] *≤* (16( *αn* ) *[−]* [1] + 6 C **A** )D *J* *t* [5] mix *[/]* [2] *[p]* [7] *[/]* [2] *[α]* [3] *[/]* [2] [ log] [3] *[/]* [2] [(1] *[/αa]* [)] *[ .]*
*t∈* N *[∗]* [E] [1] *[/p]*


-----

Using the notation of Appendix C, we have the following expansion


*n−* 1

˜

� **A** ( *Z* *t* +1 ) *J* *t* [(0] *[,]* [2] *[α]* [)] *−* 2

*t* = *n/* 2


*n−* 1

˜

� **A** ( *Z* *t* +1 ) *J* *t* [(0] *[,α]* [)] =

*t* = *n/* 2


*n−* 1
�

*t* = *n/* 2


¯
*ψ* *t* [(2] *[α]* [)] *−* 2


*n−* 1
�

*t* = *n/* 2


*ψ* ¯ *t* [(] *[α]* [)] (88)


+


*n−* 1
�

*t* = *n/* 2


E *π* *J* [ **A** [˜] ( *Z* *t* +1 ) *J* *t* [(0] *[,]* [2] *[α]* [)] ] *−* 2E *π* *J* [ **A** [˜] ( *Z* *t* +1 ) *J* *t* [(0] *[,α]* [)] ] *.*
� �


To bound the last term, we apply Proposition 2, and get


*∥*


*n−* 1
�

*t* = *n/* 2


E *π* *J* [ **A** [˜] ( *Z* *t* +1 ) *J* *t* [(0] *[,]* [2] *[α]* [)] ] *−* 2E *π* *J* [ **A** [˜] ( *Z* *t* +1 ) *J* *t* [(0] *[,α]* [)] ] *∥*
� �


*≤* ( *n/* 2) *∥* 2 **A** [¯] *R* ( *α* ) *−* **A** [¯] *R* (2 *α* ) *∥≤* 14 *∥* **A** [¯] *∥∥* **A** [¯] *[−]* [1] *∥* C **A** *t* [2] mix *[nα]* [2] *[∥][ε][∥]* *[∞]* *[.]*

For the other terms, we apply Corollary 6, and obtain

( *n/* 2)E [1] *ξ* *[/p]* [ *∥T* *J,* [(2)] 0 *[∥]* *[p]* []] *[ ≤]* [1344 C] **[A]** *[ κ]* [1] *Q* *[/]* [2] *[p]* [3] *[/]* [2] *[t]* [mix] [(] *[αn]* [)] [1] *[/]* [2] *[a]* *[−]* [1] *[/]* [2] *[∥][ε][∥]* *[∞]*


+ 1728 C **A** D 1 c [(1)] *W,* 3 *[t]* mix [3] *[/]* [2] *[p]* [3] [(] *[αa]* [)] *[−]* [1] *[/]* [2] [�]

+ 14 *∥* **A** [¯] *∥∥* **A** [¯] *[−]* [1] *∥* C **A** *t* [2] mix *[nα]* [2] *[∥][ε][∥]* *[∞]* *[.]*

Now, to bound *T* *H* [(2)] we apply Minkowski’s inequality


log (1 */αa* ) *∥ε∥* *∞*


E [1] *ξ* *[/p]* [ *∥*


*n−* 1
� **A** ˜ ( *Z* *t* +1 ) *H* *t* [(2] *[,α]* [)] *∥* *[p]* ] *≤* ( *n/* 2) C **A** sup E [1] *ξ* *[/p]* [ *∥H* *t* [(2] *[,α]* [)] *∥* *[p]* ] *.*

*t* = *n/* 2 *n/* 2 *≤t≤n*

E [1] *ξ* *[/p]* [ *∥T* *H* [(2)] *[∥]* *[p]* []] *[ ≤]* [3 C] **[A]** [ sup] *ξ* [ *∥H* *t* [(2] *[,α]* [)] *∥* *[p]* ] *.*
*t∈* N *[∗]* [E] [1] *[/p]*


Using this bound, we get


Finally, we apply Proposition 9 and obtain the result (24).
### **E Technical lemmas**

Recall that *S* *ℓ* [(1)] +1: *ℓ* + *m* [is defined, for] *[ ℓ, m][ ∈]* [N] *[∗]* [, as]

*S* *ℓ* [(1)] +1: *ℓ* + *m* [=][ �] *[ℓ]* *k* [+] = *[m]* *ℓ* +1 **[B]** *[k]* [(] *[Z]* *[k]* [)] *[,]* [ with] **[ B]** *[k]* [(] *[z]* [) = (I] *[ −]* *[α]* [ ¯] **[A]** [)] *[ℓ]* [+] *[m][−][k]* [ ˜] **[A]** [(] *[z]* [)(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[k][−]* [1] *[−][ℓ]* *[.]*

**Lemma 8.** *Assume A1, A2 and* ***UGE*** *1. Then, for any p ≥* 2 *, α ∈* (0 *, α* *∞* ] *, and initial probability measure ξ on* (Z *, Z* ) *, it holds*
*that*
E [1] *ξ* *[/p]* [ *∥J* *n* [(1] *[,α]* [)] *∥* *[p]* ] *≤∥ε∥* *∞* ( *αat* mix )(D [(] *J,* *[M]* 1 [)] �log(1 */αa* ) *p* [2] + D [(] *J,* *[M]* 2 [)] [(] *[αat]* [mix] [)] [1] *[/]* [2] *[p]* [1] *[/]* [2] [)] *[ .]*


*Particularly, it holds that*
E [1] *ξ* *[/p]* [ *∥J* *n* [(1] *[,α]* [)] *∥* *[p]* ] *≤* (D [(M)] *J,* 1 [+][ D] [(M)] *J,* 2 [)] *[∥][ε][∥]* *[∞]* *[p]* [2] *[t]* mix [3] *[/]* [2] *[αa]* �


log (1 */αa* ) *.*


*Proof.* The precise constants and proof can be found in (Durmus et al. 2025, Proposition 10).

**Lemma 9.** *Assume A 1, A 2 and* ***UGE*** *1. Then, for any p, q ≥* 2 *, satisfying* 2 *≤* *p ≤* *q/* 2 *, α ∈* (0 *, α* *q,* [(M)] *∞* *[t]* *[−]* mix [1] []] *[, and initial]*
*probability measure ξ on* (Z *, Z* ) *, it holds that*


E [1] *ξ* *[/p]* [ *∥H* *n* [(1] *[,α]* [)] *∥* *[p]* ] *≤* *d* [1] *[/q]* *∥ε∥* *∞* ( *αat* mix )(D [(M)] *H,* 1 �


log(1 */αa* ) *p* [2] + D [(M)] *H,* 2 [(] *[αat]* [mix] [)] [1] *[/]* [2] *[p]* [1] *[/]* [2] [)] *[ .]*


*Proof.* The precise constants and proof can be found in (Durmus et al. 2025, Proposition 10).

**Lemma 10.** *Assume A1, A2 and* ***UGE*** *1. For any probability measure ξ ∈P* (Z) *, j, r ∈* N *and u ∈* S *[d][−]* [1] *, step size α ∈* (0 *, α* *∞* ) *,*
*we have*

sup *ξ* [ *∥S* *j* [(2)] +1: *j* + *r* *[u][∥]* *[p]* []] *[ ≤]* [(][D] 1 [(1)] *[p]* [ log (] *[r]* [) +][ D] [(2)] 2 [)] *[t]* [3] mix *[/]* [2] *[pr]* [(1] *[ −]* *[αa]* [)] [(] *[r][−]* [1)] *[/]* [2] *[,]*
*u∈* S *[d][−]* [1] [ E] [1] *[/p]*

*where*
D [(1)] 1 = *κ* [3] *Q* *[/]* [2] [(48] *[κ]* [1] *Q* *[/]* [2] + 1) C **A** [2] *[/]* [ log(2)] *[,]* D [(1)] 2 = *κ* *Q* (34 *κ* *Q* + 1) C **A** [2] *[.]*


-----

*Proof.* Firstly, define for any *k ∈{j* + 1 *, . . ., j* + *m −* 1 *}* the function *g* *k* ( *z* ) = **A** [˜] ( *z* )(I *−* *α* **A** [¯] ) *[k][−][j][−]* [1] *u*, which is bounded by
*∥g* *k* *∥* *∞* *≤* *[√]* *κ* *Q* C **A** (1 *−* *αa* ) [(] *[k][−][j][−]* [1)] *[/]* [2] . Then, following the definition (72) we get


*i*
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][i]* [ ˜] **A** ( *Z* *i* )(I *−* *α* **A** [¯] ) *[i][−][k]* *g* *k* ( *Z* *k* )

*k* = *j* +1


*S* *j* [(2)] +1: *j* + *r* [=]

=


*j* + *r*
�

*i* = *j* +1


*j* + *r*
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][k]* [ ˜] **A** ( *Z* *k* ) *g* *k* ( *Z* *k* ) +

*k* = *j* +1


*j* + *r*
�

*i* = *j* +2


*i−* 1
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][i]* [ ˜] **A** ( *Z* *i* )(I *−* *α* **A** [¯] ) *[i][−][k]* *g* *k* ( *Z* *k* ) = *T* 1 [(1)] + *T* 2 [(1)] *.*

*k* = *j* +1


The first term can be bounded directly as

E [1] *ξ* *[/p]* [ *∥T* 1 [(1)] *∥* *[p]* ] *≤* *κ* *Q* C [2] **A** *[r]* [(1] *[ −]* *[αa]* [)] [(] *[r][−]* [1)] *[/]* [2] *[ .]* (89)

For the second term we can use the Berbee’s lemma technique established in Lemma 6. Note that after switching the variables,
we get


*T* 2 [(1)] =

=


*j* + *r−* 1
�

*k* = *j* +1


*j* + *r−* 1
� *M* *k* +1 *g* *k* ( *Z* *k* ) =

*k* = *j* +1


*j* + *r*
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][i]* [ ˜] **A** ( *Z* *i* )(I *−* *α* **A** [¯] ) *[i][−][k]*
� *i* = *k* +1 �


*j* + *r*
�
� *i* = *k* +1


*g* *k* ( *Z* *k* ) (90)


*j* + *r−* 1
� *S* *k* [(1)] +1: *j* + *r* [(] *[I][ −]* *[α]* [ ¯] **[A]** [)] *[g]* *[k]* [(] *[Z]* *[k]* [)] *[,]*

*k* = *j* +1


where


*M* *k* +1 =


*j* + *r*
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][i]* [ ˜] **A** ( *Z* *i* )(I *−* *α* **A** [¯] ) *[i][−][k]* *.*

*i* = *k* +1


For any *m ≥* *t* mix we have the following decomposition

*S* *k* [(1)] +1: *j* + *r* [= (I] *[ −]* *[α]* [ ¯] **[A]** [)] *[j]* [+] *[r][−][m][−][k]* *[S]* *k* [(1)] +1: *k* + *m* [+] *[ S]* *k* [(1)] + *m* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* *[ .]*

Let *N* = *⌊* ( *r −* 1) */m⌋* . Substituting the above relation into (90), we get


( *N* *−* 1) *m*
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][m][−][k]* *S* *k* [(1)] +1: *k* + *m* [(] *[I][ −]* *[α]* [ ¯] **[A]** [)] *[g]* *[k]* [(] *[Z]* *[k]* [)]

*k* = *j* +1


*T* 2 [(1)] =

+


*j* + *r−* 1
� *S* *k* [(1)] +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[g]* *[k]* [(] *[Z]* *[k]* [) +]

*k* =( *N* *−* 1) *m* +1


( *N* *−* 1) *m*
� *S* *k* [(1)] + *m* +1: *j* + *r* [(] *[I][ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[g]* *[k]* [(] *[Z]* *[k]* [) =] *[ T]* [ (1)] 21 [+] *[ T]* [ (1)] 22 [+] *[ T]* [ (1)] 23 *[.]*

*k* = *j* +1


Using Minkowski’s inequality and (Durmus et al. 2025, Lemma 5), we can bound the first term as

E [1] *ξ* *[/p]* [ *∥T* 21 [(1)] *[∥]* *[p]* []] *[ ≤]* [16] *[κ]* [2] *Q* [C] [2] **A** *[mr]* [1] *[/]* [2] *[t]* [1] mix *[/]* [2] [(1] *[ −]* *[αa]* [)] *[r][−]* [1] *[ .]* (91)

For the second term, again we can use Minkowski’s inequality to get


E [1] *ξ* *[/p]* [ *∥T* 22 [(1)] *[∥]* *[p]* []] *[ ≤]*


( *N* *−* 1) *m*
�

*k* = *j* +1


*k* + *m*
� E [1] *[/p]* [ *∥* (I *−* *α* **A** [¯] ) *[k]* [+] *[m][−][i]* [ ˜] **A** ( *Z* *i* )(I *−* *α* **A** [¯] ) *[i][−][k]* *g* *k* ( *Z* *k* ) *∥* *[p]* ] (92)

*i* = *k* +1


*≤* *κ* [3] *Q* *[/]* [2] [C] **A** [2] *[mr]* [(1] *[ −]* *[αa]* [)] [(] *[r][−]* [1)] *[/]* [2] *[ .]*

For the third term we should use the Berbee lemma technique established in Lemma 6. Switching to the extended space
(Z [˜] N *,* *Z* [˜] N *,* P [˜] N ), we have E [1] *ξ* *[/p]* [ *∥T* 23 [(1)] *[∥]* *[p]* [] = ˜][E] [1] *ξ* *[/p]* [ *∥T* [˜] 23 [(1)] *[∥]* *[p]* []][, where]


*T* ˜ 23 [(1)] [=]


( *N* *−* 1) *m*
� *S* ˜ *k* [(1)] + *m* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[g]* *[k]* [( ˜] *[Z]* *[k]* [)] *[,]*

*k* = *j* +1


*S* ˜ *k* [(1)] + *m* +1: *j* + *r* [=]


*j* + *r*
� (I *−* *α* **A** [¯] ) *[j]* [+] *[r][−][i]* [ ˜] **A** ( *Z* [˜] *i* )(I *−* *α* **A** [¯] ) *[i][−][k][−][m][−]* [1] *.*

*i* = *k* + *m* +1


-----

Thus, we have


*j* + *m*
� *S* ˜ ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[g]* *[sm]* [+] *[k]* [( ˜] *[Z]* *sm* *[∗]* + *k* [)]

*k* = *j* +1


*j* + *m*
� *S* ˜ ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] [(] *[g]* *[sm]* [+] *[k]* [( ˜] *[Z]* *[sm]* [+] *[k]* [)] *[ −]* *[g]* *[sm]* [+] *[k]* [( ˜] *[Z]* *sm* *[∗]* + *k* [))]

*k* = *j* +1


*T* ˜ 23 [(1)] [=]

+


*N* *−* 2
�

*s* =0

*N* *−* 2
�

*s* =0


= *T* 231 [(1)] [+] *[ T]* [ (1)] 232 *[.]*

We start with bounding *T* 231 [(1)] [. Let]


1 */* 2
�


*I* 1 ( *α* ) =


*j* + *m*
� 2 *p*

*k* = *j* +1


*N* *−* 2
�
� *s* =0


˜

� (1 *−* *αa* ) *[sm]* [+] *[k][−][j][−]* [1] sup E [2] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* *[u]* *[′]* *[∥]* *[p]* []]

*s* =0 *u* *[′]* *∈* S *[d][−]* [1]


*.*


Applying (Durmus et al. 2025, Lemma 6), we obtain

E˜ [1] *ξ* *[/p]* [ *∥T* 231 [(1)] *[∥]* *[p]* []] *[ ≤]* *[κ]* *[Q]* [ C] **[A]** [(1] *[ −]* *[αa]* [)] [(] *[m]* [+1)] *[/]* [2] *[I]* [1] [(] *[α]* [)]


*j* + *m*
�

*k* = *j* +1


+


*N* *−* 2

˜

� *∥ξ{* Q *[sm]* [+] *[k]* (I *−* *α* **A** [¯] ) *[m]* [+1] *g* *sm* + *k* *}∥* sup E [1] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* *[u]* *[′]* *[∥]* *[p]* []] *[ .]*

*s* =0 *u* *[′]* *∈* S *[d][−]* [1]


*N* *−* 2
�


For the first term, using (Durmus et al. 2025, Lemma 5), we have


*j* + *m*
� 2 *p*

*k* = *j* +1


*N* *−* 2
�
� *s* =0


1 */* 2
�


˜

� (1 *−* *αa* ) *[sm]* [+] *[k][−][j][−]* [1] sup E [2] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* *[u]* *[′]* *[∥]* *[p]* []]

*s* =0 *u* *[′]* *∈* S *[d][−]* [1]


(93)








1 */* 2


*≤* 32 *κ* *Q* C **A** *t* [1] mix *[/]* [2] *[p]* [3] *[/]* [2] [(1] *[ −]* *[αa]* [)] [(] *[r][−][m][−]* [2)] *[/]* [2] *[m]* [1] *[/]* [2]








( *N* *−* 1) *m*
�


� ( *j* + *r −* *k* )

*k* = *j* +1


*≤* 32 *κ* *Q* C **A** *t* [1] mix *[/]* [2] *[p]* [3] *[/]* [2] *[m]* [1] *[/]* [2] *[r]* [(1] *[ −]* *[αa]* [)] [(] *[r][−][m][−]* [2)] *[/]* [2] *[ .]*

For the second term, we know that *π* ( *g* *sm* + *k* ) = 0, and from **UGE** 1 it follows that

*∥ξ* Q *[sm]* [+] *[k]* (I *−* *α* **A** [¯] ) *[m]* [+1] *g* *sm* + *k* *∥≤* *κ* [1] *Q* *[/]* [2] [(1] *[ −]* *[αa]* [)] [(] *[m]* [+1)] *[/]* [2] [∆][(Q] *[sm]* [+] *[k]* [)] *[∥][g]* *[sm]* [+] *[k]* *[∥]* *[∞]* *[,]*

and thus


*j* + *m*
�

*k* = *j* +1


*N* *−* 2

˜

� *∥ξ* Q *[sm]* [+] *[k]* (I *−* *α* **A** [¯] ) *[m]* [+1] *g* *sm* + *k* *∥* sup E [1] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* *[u]* *[′]* *[∥]* *[p]* []] (94)

*s* =0 *u* *[′]* *∈* S *[d][−]* [1]

*≤* 32 *κ* [2] *Q* [C] [2] **A** *[t]* [3] mix *[/]* [2] *[p]* [1] *[/]* [2] *[r]* [1] *[/]* [2] [(1] *[ −]* *[αa]* [)] [(] *[r][−]* [1)] *[/]* [2] *[,]*


where we used that


*j* + *m*
�

*k* = *j* +1

Combining (93) and (94), we get


*N* *−* 2
� ( *j* + *r −* ( *s* + 1) *m −* *k* ) [1] *[/]* [2] ∆(Q *[sm]* [+] *[k]* ) *≤* 2 *t* mix *r* [1] *[/]* [2] *.*

*s* =0


*N* *−* 2
�


E˜ [1] *ξ* *[/p]* [ *∥T* 231 [(1)] *[∥]* *[p]* []] *[ ≤]* [32] *[κ]* [2] *Q* [C] [2] **A** [(] *[pm]* [1] *[/]* [2] *[r]* [1] *[/]* [2] [ +] *[ t]* [mix] [)] *[t]* [1] mix *[/]* [2] *[p]* [1] *[/]* [2] *[r]* [1] *[/]* [2] [(1] *[ −]* *[αa]* [)] [(] *[r][−]* [1)] *[/]* [2] *[ .]* (95)

Now, to bound *T* 232 [(1)] [we set] *[ V]* *[l]* [ =] *[ g]* *[l]* [( ˜] *[Z]* *[l]* [)] *[ −]* *[g]* *[l]* [( ˜] *[Z]* *l* *[∗]* [)][ and][ ˜] *[F]* *l* *[∗]* [=] *[ σ]* [( ˜] *[Z]* *[i]* *[,]* [ ˜] *[Z]* *i* *[∗]* [:] *[ i][ ≤]* *[l]* [)][. Using Lemma 6, we get]

E˜ [1] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[V]* *[sm]* [+] *[k]* *[∥]* *[p]* []]

= E [˜] [1] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[V]* *[sm]* [+] *[k]* **[1]** *{Z* [˜] *sm* + *k* = *̸* *Z* [˜] *sm* *[∗]* + *k* *[}]* *[∥]* *[p]* []]

*≤* E [˜] [1] *ξ* *[/p]* � *∥V* *sm* + *k* *∥* *[p]* E [˜] *F* ˜ *sm* *[∗]* + *k* � *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[V]* *[sm]* [+] *[k]* *[/][∥][V]* *[sm]* [+] *[k]* *[∥∥]* *[p]* [��]


*≤* E [˜] [1] *[/p]*
*ξ*


˜

� *∥V* *sm* + *k* *∥* *[p]* *u* *[′]* *∈* S *[d][−]* sup [1] *,ξ* *[′]* *∈P* (Z) E *ξ* *′* � *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[u]* *[′]* *[∥]* *[p]* [��]


*,*


-----

where *P* (Z) is the set of probability measure on (Z *, Z* ). Let


*j* + *m*
� ( *j* + *r −* ( *s* + 1) *m −* *k* ) [1] *[/]* [2] (1 *−* *αa* ) [(] *[j]* [+] *[r][−][sm][−][k]* [)] *[/]* [2] *∥g* *sm* + *k* *∥* *∞* (∆(Q *[m]* )) [1] *[/p]* *.*

*k* = *j* +1


*I* 2 ( *α* ) =


*N* *−* 2
�

*s* =0


Noting that *∥V* *sm* + *k* *∥≤* 2 *∥g* *sm* + *k* *∥* *∞* **1** *{* ˜ *Z* *sm* + *k* = *̸* *Z* [˜] *sm* *[∗]* + *k* *[}]* [ and applying (Durmus et al. 2025, Lemma 5), we obtain]


*N* *−* 2
�

*s* =0


*j* + *m*
� E˜ [1] *ξ* *[/p]* [ *∥S* [˜] ( [(1)] *s* +1) *m* + *k* +1: *j* + *r* [(I] *[ −]* *[α]* [ ¯] **[A]** [)] *[m]* [+1] *[V]* *[sm]* [+] *[k]* *[∥]* *[p]* []] *[ ≤]* [2] *[κ]* [3] *Q* *[/]* [2] [C] **[A]** [(] *[t]* [mix] *[p]* [)] [1] *[/]* [2] *[I]* [2] [(] *[α]* [)] (96)

*k* = *j* +1

*≤* 2 *κ* [2] *Q* [C] [2] **A** [(] *[t]* [mix] *[p]* [)] [1] *[/]* [2] *[r]* [3] *[/]* [2] [(1] *[ −]* *[αa]* [)] [(] *[r][−]* [1)] *[/]* [2] [(1] *[/]* [4)] [(1] *[/p]* [)] *[⌊][m/t]* [mix] *[⌋]* *[.]*


Setting


*,*
�


*m* = *t* mix


*p* lo g ( *r* )
� 2 log (2)


we get (1 */* 4) [(1] *[/p]* [)] *[⌊][m/t]* [mix] *[⌋]* *≤* *r* *[−]* [1] *[/]* [2] and *m ≤* 2 *t* mix *p* log ( *r* ) */* (2 log(2)). Combining together (89), (91), (92), (95) and (96) the
result follows.

**Proposition 10.** *Assume that −* **A** [¯] *is Hurwitz. Then there exists a unique symmetric positive definite matrix Q satisfying the*
*Lyapunov equation* **A** [¯] *[⊤]* *Q* + *Q* **A** [¯] = I *. In addition, setting*

*a* = *∥Q∥* *[−]* [1] */* 2 *,* *and* *α* *∞* = (1 */* 2) *∥* **A** [¯] *∥* *[−]* *Q* [2] *[∥][Q][∥]* *[−]* [1] *[ ∧∥][Q][∥]* *[,]* (97)

*it holds for any α ∈* [0 *, α* *∞* ] *that ∥* I *−* *α* **A** [¯] *∥* [2] *Q* *[≤]* [1] *[ −]* *[aα][, and][ αa][ ≤]* [1] *[/]* [2] *[.]*

*Proof.* Proof of this result can be found in (Durmus et al. 2021, Proposition 1).

For a bounded function *f* : Z *→* R *[d]*, we define

*σ* *π* [2] [(] *[f]* [) = lim] *[n][→∞]* *[n]* *[−]* [1] [E][[] *[∥]* [�] *i* *[n]* =0 *[−]* [1] *[{][f]* [(] *[Z]* *[i]* [)] *[ −]* *[π]* [(] *[f]* [)] *[}∥]* [2] []] *[ .]* (98)

**Theorem 3.** *Assume* ***UGE*** *1. Then, for any measurable function f* : Z *→* R *[d]* *, ∥f* *∥* *∞* *≤* 1 *, p ≥* 2 *, and n ≥* 1 *, it holds*


E [1] *ξ* *[/p]* [ *∥* [�] *[n]* *i* =0 *[−]* [1] *[f]* [(] *[Z]* *[i]* [)] *[ −]* *[π]* [(] *[f]* [)] *[∥]* *[p]* []] *[ ≤]* [C] [Rm] *[,]* [1] *√* 2 *p* [1] *[/]* [2] *n* [1] *[/]* [2] *σ* *π* ( *f* )


+ C Ros *,* 1 *n* [1] *[/]* [4] *t* [3] mix *[/]* [4] *[p]* [ log] 2 [(2] *[p]* [) + C] [Ros] *[,]* [2] *[ t]* [mix] *[p]* [ log] 2 [(2] *[p]* [)] *[,]*

*where the constants* C Ros *,* 1 *,* C Ros *,* 2 *,* C Rm *,* 1 *can be found in (Durmus et al. 2025, Theorem 6) and σ* *π* [2] [(] *[f]* [)] *[ is defined in]* [ (98)] *[.]*

Below we establish the result similar to (Durmus et al. 2025, Lemma 9). But for our purpose we make it a bit sharper.

**Lemma 11.** *Assume* ***UGE*** *1. Let {g* *i* *}* *[n]* *i* =1 *[be a family of measurable functions from]* [ Z] *[ to]* [ R] *[d]* *[ such that][ c]* *[i]* [ =] *[ ∥][g]* *[i]* *[∥]* *[∞]* *[<][ ∞]* *[for any]*
*i ≥* 1 *and π* ( *g* *i* ) = 0 *for any i ∈{* 1 *, . . ., n}. Then, for any initial probability ξ on* (Z *, Z* ) *, n ∈* N *, t ≥* 0 *, it holds*


1 */* 2
� *√t* mix *.*


*n*
� *c* [2] *i*
� *i* =1


P *ξ*


*n*
*∥* � *i*
�


*n*

*−* *[t]* [2]
*≤* 2 exp
*i* =1 *[g]* *[i]* [(] *[Z]* *[i]* [)] *[∥≥]* *[t]* � � 2 *u*


2 *u* [2] *n*


*, where u* *n* = 8
�


*Proof.* The proof follows the lines of (Durmus et al. 2025, Lemma 9).
### **F Additional experiments**

In Figure 2, Figure 3a we compute E[ *∥θ* [¯] *n* *−* *θ* *[⋆]* + (1 */n* ) [�] *[n]* *k* =1 *[ε]* [(] *[Z]* *[k]* [)] *[∥]* [2] []][ and][ E][[] *[∥][θ]* [¯] *n* [(][RR][)] *−* *θ* *[⋆]* + (1 */n* ) [�] *[n]* *k* =1 *[ε]* [(] *[Z]* *[k]* [)] *[∥]* [2] []][ estimated]
by averaging over *N* traj trajectories. The results show that after subtracting the leading term, the remainder term is optimized
when *α ≍* *n* *[−]* [1] *[/]* [2], as predicted by Theorem 2. In contrast, PR-averaged iterates are optimized in the range *α ≍* *n* *[−]* [2] *[/]* [3], which is
consistent with the theory presented in (Durmus et al. 2025). Moreover, for *α ≍* *n* *[−]* [2] *[/]* [3], we note that the leading term in (25) is
( *αn* ) *[−]* [1] *[/]* [2] *n* *[−]* [1] *[/]* [2], and we observe this dependence on *α* in Figure 3a. Additionally, Figure 3b demonstrates that for *α ≍* *n* *[−]* [1] *[/]* [2],
the remainder term indeed has an order of *n* *[−]* [2] *[/]* [3], as predicted by Corollary 3.


-----

(a) *α* = *cn* *[−]* [2] *[/]* [3] (b) *α* = *cn* *[−]* [1] *[/]* [2]

Figure 2: Comparison of remainder errors for Polyak-Ruppert averaged and Richardson-Romberg iterates in two regimes. In
the first regime (a), the error attains its optimum for PR averaging, whereas for RR iterates it decays as predicted by Theorem 2.
Conversely, in the second regime (b), the optimum is achieved for RR iterates, which is also consistent with the theory.

(a) *α* = *cn* *[−]* [2] *[/]* [3] (b)

Figure 3: Subfigure (a): the MSE remainder term for the Richardson-Romberg iterates is well approximated by *rc* *[−]* [1] for some
constant *r >* 0, matching the leading term ( *αn* ) *[−]* [1] *n* *[−]* [1] in (25). Subfigure (b): rescaled by *n* [4] *[/]* [3] MSE remainder trajectories for
varying step sizes *α* . These plots cease decaying and stabilize, confirming the predicted order of the remainder term.


-----


