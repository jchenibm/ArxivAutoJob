{
  "title": "论 SFT 的泛化性：一种基于奖励修正的强化学习视角",
  "detailed_summary": "该论文提出了一种简单但理论上合理的改进监督微调 (SFT) 的方法，用于大型语言模型 (LLM)，旨在解决其与强化学习 (RL) 相比有限的泛化能力。通过数学分析，作者揭示了标准 SFT 梯度隐式地编码了一种有问题的奖励结构，这可能会严重限制模型的泛化能力。为了纠正这一点，论文提出了动态微调 (DFT)，通过使用token的概率动态地重新缩放目标函数，从而稳定每个token的梯度更新。这种修改显著优于多种具有挑战性的基准和基础模型上的标准 SFT，展示了大大提高的泛化能力。此外，该方法在离线 RL 设置中也显示出有竞争力的结果，提供了一种有效但更简单的替代方案。这项工作将理论洞察力和实际解决方案联系起来，大大提高了 SFT 的性能。",
  "background": "监督微调（SFT）已经成为一种标准的 LLM 后训练方法，用于将模型适配到新任务或增强现有能力。它的广泛应用主要归功于其易于实现和快速获得类似专家的行为。然而，尽管有这些优点，与强化学习（RL）方法相比，SFT 通常受到有限泛化的困扰。RL 利用显式奖励或验证信号，允许模型探索多样化的策略，从而实现更强的泛化。然而，RL 方法通常需要大量的计算资源，对超参数调整敏感，并且依赖于奖励信号的可用性，而这些条件在实践中并不总是可行的。即使 RL 可行，SFT 在快速获取 RL 可能难以独立发现的专家行为模式方面仍然具有优势。",
  "contributions": [
    "从理论上数学地将 LLM SFT 确立为策略梯度空间中的一种特殊 RL，并找出了 SFT 泛化能力有限的根本原因，并推导出了一种改进它的方法。",
    "提出了动态微调 (DFT) 方法，通过token概率动态地重新缩放 SFT 损失，有效中和了导致意外奖励结构和无界方差的逆概率加权。",
    "实验结果表明，DFT 方法只需一行代码的修改，就可以显著增强 LLM SFT 在各种任务和模型上的性能和泛化能力。",
    "DFT 在离线 RL 设置中也表现出色，优于其他离线 RL 方法，甚至在某些情况下优于在线 RL 方法，提供了一种更高效和可扩展的替代方案。",
    "分析了 DFT 对模型概率分布的影响，发现 DFT 在提高部分token概率的同时，会主动抑制另一些token的概率，这有助于模型更好地关注关键语义内容。"
  ],
  "problem": "论文旨在解决监督微调（SFT）在大型语言模型（LLM）中存在的泛化能力不足的问题，特别是与强化学习（RL）方法相比。SFT虽然易于实现并能快速模仿专家行为，但其泛化能力受限。论文试图通过理论分析揭示SFT泛化性差的根本原因，并提出一种简单有效的方法来改进SFT，使其在没有额外奖励信号或负样本的情况下也能实现更好的泛化性能。",
  "methods": [
    "**数学分析：** 将 SFT 梯度重写为策略梯度，通过重要性采样，揭示 SFT 可以被看作是一种带有隐式奖励的 on-policy-gradient 方法，但存在一个由重要性权重 1/πθ 引起的偏差。",
    "**动态重加权（Dynamic Reweighting）：** 为了解决 SFT 中奖励偏差问题，通过将奖励乘以一个由策略概率给出的校正逆比率，动态地对奖励进行重加权。这通过token概率动态地重新缩放 SFT 损失来实现。",
    "**动态微调（DFT）：** 通过对每个token概率进行动态调整，实现奖励修正和梯度稳定，从而提高模型的泛化能力。",
    "**Token级别的重要性采样：** 为了避免在整个轨迹上计算重要性权重时可能出现的数值不稳定问题，采用了token级别的重要性采样，如 PPO 中所采用的。"
  ],
  "experimental_design": {
    "datasets": [
      "NuminaMath CoT 数据集：包含约 860,000 个数学问题及相应解决方案，涵盖中国高中数学练习和美国及国际数学奥林匹克竞赛题。"
    ],
    "models": [
      "Qwen2.5-Math-1.5B",
      "Qwen2.5-Math-7B",
      "LLaMA-3.2-3B",
      "LLaMA-3.1-8B",
      "DeepSeekMath-7B-Base"
    ],
    "training_details": [
      "使用 AdamW 优化器，学习率为 5 × 10^-5（LLaMA-3.1-8B 除外，其学习率为 2 × 10^-5）。",
      "mini-batch 大小设置为 256，最大输入长度设置为 2048 个token。",
      "学习率采用余弦衰减schedule，预热比率为 0.1。",
      "包括了一种并行方法，即重要性加权 SFT (iw-SFT)，用于比较。所有训练设置均遵循原始论文中报告的设置，只是将训练 epoch 数设置为 1。"
    ],
    "evaluation_settings": [
      "在 Math500、Minerva Math、Olympiad Bench、AIME 2024 和 AMC 2023 等数学推理任务上进行评估。",
      "每个模型使用默认的聊天模板和思维链 (CoT) 提示来激发逐步推理。",
      "所有报告的结果代表 16 次解码运行的平均准确率，评估时温度为 1.0，最大生成长度为 4096 个token。"
    ]
  },
  "results": "实验结果表明，DFT在各种数学推理基准测试中始终显著优于标准SFT。例如，对于 Qwen2.5-Math-1.5B，DFT 相对于基础模型实现了 +15.66 个点的平均增益，这比 SFT 的 +2.09 个点的改进高出 5.9 倍以上。DFT 在具有挑战性的基准测试中表现出泛化性和鲁棒性，在标准 SFT 产生最小甚至负面影响的情况下尤其如此。例如，在 Olympiad Bench 上，SFT 降低了 Qwen2.5-Math-1.5B 的性能，将准确率从 15.88 降低到 12.63，而 DFT 将其提高到 27.08，比基础模型提高了 +11.20 个点。",
  "result_analysis": "DFT 通过动态重加权机制实现了更高效的梯度更新，从而更快地收敛到高质量的解决方案。它避免了标准 SFT 中常见的优化平台期或噪声区域，从而能够更有效地获取复杂的数学推理模式。此外，DFT在学习token概率分布方面表现出与标准SFT和其他RL方法不同的行为。SFT 倾向于一致地提高token概率，而DFT 表现出一种极化的效应，显著提高了部分token的概率，同时主动抑制了其他token的概率。这导致了一种双峰分布，更多的token占据了最高和最低的概率区间。",
  "conclusions": "该论文通过理论分析和实验验证表明，SFT梯度等价于一种带有隐式奖励的策略梯度更新，该隐式奖励与模型的置信度成反比。基于此，论文提出DFT方法，通过动态重加权SFT损失，稳定学习过程并促进更好的泛化。DFT在各种模型和具有挑战性的数学推理基准测试中持续且显著地优于标准SFT。",
  "limitations": "虽然我们的实验证明 DFT 在数学推理基准测试中取得了显着收益，但此评估仅限于以数学为中心的数据集和最多 70 亿个参数的模型。我们尚未评估其他任务领域（例如，代码生成、常识 QA）或更大的 LLM（例如，13 B+）的性能。此外，我们目前的研究仅限于纯文本场景。在未来的工作中，我们计划不仅将我们的研究扩展到更广泛的文本基准测试，并将 DFT 扩展到最先进的模型，而且还验证其在视觉语言任务上的有效性，以确认其在不同模态中的通用性。",
  "future_work": "未来的工作计划包括：扩展研究到更广泛的文本基准测试，并将 DFT 扩展到最先进的模型；验证其在视觉语言任务上的有效性，以确认其在不同模态中的通用性。",
  "applications": "该研究成果可以应用于各种需要大型语言模型进行微调的任务中，特别是在缺乏奖励信号或负样本的情况下。DFT 可以提高模型在数学推理、代码生成、常识问答等领域的性能，并减少对大量计算资源和超参数调整的依赖。该方法还可以作为一种更高效和可扩展的替代方案，用于传统的强化学习流程，尤其是在偏好监督可用但奖励建模或在线响应采样成本高昂或不切实际的领域。",
  "related_work": "该论文详细讨论了与SFT和RL相关的研究工作，包括混合方法，如InstructGPT、DPO等，以及将SFT和RL统一起来的理论研究。论文还对比了DFT和Focal Loss，指出了LLM时代下underfitting和overfitting问题的转变。",
  "github_links": [
    "https://github.com/yongliang-wu/DFT"
  ],
  "published": "2025-08-07T17:59:04+00:00"
}