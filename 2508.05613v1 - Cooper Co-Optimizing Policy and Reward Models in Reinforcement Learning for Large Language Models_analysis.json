{
  "title": "COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS",
  "detailed_summary": "这篇论文提出了Cooper，一个用于大型语言模型（LLMs）的强化学习（RL）框架，旨在解决现有基于模型和基于规则的奖励范式的局限性。基于规则的奖励缺乏鲁棒性，而基于模型的奖励容易受到奖励攻击。Cooper通过联合优化策略模型和奖励模型来应对这些问题，利用基于规则的奖励在识别正确响应时的高精度，并动态构建和选择正负样本对，以持续训练奖励模型。为了支持Cooper，论文还引入了一种混合标注策略，以高效准确地生成奖励模型的训练数据，并提出了一个基于参考的奖励建模范式，其中奖励模型以参考答案作为输入。实验结果表明，Cooper不仅减轻了奖励攻击，还提高了端到端RL的性能，例如在Qwen2.5-1.5B-Instruct上平均准确率提高了0.54%。",
  "background": "大型语言模型（LLMs）在推理任务中表现出了卓越的性能，强化学习（RL）已成为增强其推理能力的关键算法。然而，现有的基于模型和基于规则的奖励函数都存在固有的局限性。基于模型的奖励容易受到奖励攻击，模型可能会利用输出模式来欺骗奖励函数，从而获得高分，而不管输出的正确性。另一方面，基于规则的奖励函数往往依赖于手动制定的规则来解析和验证模型的输出，缺乏鲁棒性，容易产生误判，从而限制了模型的进一步优化。这项研究的动机是为了克服这些局限性，提出一种能够同时实现高鲁棒性和抗奖励攻击能力的RL框架。",
  "contributions": [
    "提出了一个新的奖励建模数据集，该数据集使用混合标注策略进行标注，结合了基于规则的验证和基于LLM的验证，实现了高效且可靠的正确性监督。在该数据集上训练的奖励模型在VerifyBench上实现了89.42%的准确率，超过了现有同等规模的奖励模型。",
    "基于基于规则的奖励在识别正确答案时的高精度，提出了Cooper，一个同时共同优化策略模型和奖励模型的强化学习框架。该框架缓解了在基于奖励模型的RL中常见的奖励攻击问题，并提高了整体训练性能。",
    "研究表明，在RL训练过程中动态调整奖励模型的参数可以有效缓解奖励攻击现象，为研究界如何更好地在强化学习中利用奖励模型提供了有价值的见解。",
    "提出了基于参考答案的奖励模型VerifyRM，通过引入参考答案作为奖励模型的输入，提高了奖励模型在推理任务中的准确性。",
    "提出了一个两阶段训练流程：策略模型优化和奖励模型优化，通过协同优化策略模型和奖励模型，提高了策略模型的推理能力。"
  ],
  "problem": "论文旨在解决以下问题：\n1.  现有基于模型的奖励函数容易受到奖励攻击，导致模型利用奖励函数的漏洞来获得高分，而忽略了实际的正确性。\n2.  现有基于规则的奖励函数缺乏鲁棒性，容易产生误判，从而限制了模型的进一步优化。\n3.  如何设计一种能够同时实现高鲁棒性和抗奖励攻击能力的RL框架。\n4.  如何构建一个高质量的奖励模型，用于指导RL训练，并避免对人工标注的依赖。",
  "methods": [
    "**VerifyRM训练方法:**\n   *   **数据准备:** 收集包含推理问题、参考答案和模型生成的补全的数据三元组。使用了7个常用的数学推理数据集，并使用11个主流LLM生成补全。采用混合标注策略，结合基于规则的验证器(Math-verify)和LLM-as-a-judge(Qwen3-4B)进行自动正确性标注。",
    "**VerifyRM的奖励模型训练:**\n   *   将奖励模型构建为一个文本分类器，并引入参考答案作为奖励模型的输入。使用二元交叉熵损失进行训练，目标函数为公式(1)和(2)。",
    "**Cooper强化学习框架:**\n   *   **阶段1: 策略模型优化:** 遵循GRPO范式，使用策略模型对每个训练样本采样一组响应，并使用参考答案感知的奖励模型对每个 rollout进行评估，根据组内归一化优势和KL正则化更新策略。奖励计算公式如公式(3)。",
    "**阶段2: 奖励模型优化:** 使用对比学习优化奖励模型，通过最大化正确响应和不正确响应之间的奖励差异来更新奖励模型参数。优化目标函数如公式(4)。\n   *   **正样本选择:**  利用基于规则的奖励的高精度，从rollout中选择被规则判断为正确的响应作为正样本，如公式(5)。\n   *   **负样本生成:** 使用辅助LLM，通过一个精心设计的提示，将正确的推理过程转化为最终产生错误答案的过程，从而生成负样本，如公式(6)。引入验证机制，确保生成的响应确实不正确。"
  ],
  "experimental_design": "实验设置包括：\n1.  **数据集:** DeepMath数据集（随机抽样10K个样本用于训练）。VerifyBench用于奖励模型评估。\n2.  **基线模型:** 使用Math-Verify作为奖励函数的模型，使用VerifyRM-1.5B作为奖励模型但不更新参数的模型。\n3.  **评估指标:** 在五个数学推理基准上评估模型：GSM8K, SVAMP, MATH500, OlympiadBenchEN (OB-EN), 和 Math Odyssey。评估指标为平均准确率。\n4.  **模型参数:** 使用Qwen2.5-1.5B-Instruct和Llama-3.2-1B-Instruct作为初始模型。 GRPO算法的全局批量大小设置为512，最大提示长度设置为1024，最大响应长度设置为3072。学习率设置为1e-6，KL惩罚系数设置为0.001。每个prompt生成16个rollout，模型训练10个epochs。",
  "results": "主要结果包括：\n1.  Cooper在Qwen2.5-1.5B-Instruct上实现了58.02%的平均准确率，优于基于规则的奖励(57.48%)和静态奖励模型(38.91%)。\n2.  静态奖励模型遭受了灾难性的失败，性能从54.93%下降到38.91%，下降了16%。\n3.  VerifyRM-1.5B在VerifyBench上实现了89.42%的准确率，超过了现有的9B参数xVerify模型。\n4.  实验结果表明，Cooper可以有效减轻奖励攻击，并提高整体训练性能。",
  "result_analysis": "结果分析表明：\n1.  训练动态显示，静态奖励模型在训练过程中会崩溃，而Cooper可以维持稳定的训练。\n2.  VerifyRM在训练过程中表现出很高的稳定性，验证了协同优化可以实现稳定训练。\n3.  消融实验表明，Cooper的优势主要来自于通过动态更新防止奖励攻击，而不是奖励粒度。\n4.   rule-based verifiers具有高精度但低召回率的特性，通过结合符号精度和神经灵活性，混合方法对于可靠的AI系统至关重要。",
  "conclusions": "论文的核心结论是：Cooper是一个有效的强化学习框架，它通过协同训练策略模型和奖励模型来缓解奖励攻击问题，并提高整体训练性能。通过结合基于规则的奖励的高精度和基于模型的奖励的鲁棒性，Cooper优于单独使用任何一种奖励类型。此外，基于参考答案的奖励模型VerifyRM在VerifyBench基准测试中优于现有的同等规模的模型。研究结果表明，在RL训练过程中动态更新奖励模型可以有效地对抗奖励攻击。",
  "limitations": "论文的局限性包括：\n1.  依赖于特定领域的验证工具，限制了其在没有明确正确性标准的任务中的泛化能力。\n2.  双重优化带来的计算开销可能会影响可扩展性。\n3.  依赖于辅助LLM生成负样本引入了外部依赖。",
  "future_work": "论文建议的未来研究方向包括：\n1.  探索自监督的对比样本生成方法。\n2.  将Cooper扩展到基于过程的奖励，以实现更密集的监督。\n3.  开发用于协同进化稳定性的理论框架。",
  "applications": "这项研究可能的实际应用场景包括：\n1.  可以应用于各种需要LLM进行推理的任务，例如数学问题解决、代码生成和常识推理。\n2.  可以用于训练更安全、更可靠的LLM，防止模型利用奖励函数的漏洞来获得高分。\n3.  可以用于提高LLM的整体性能，使其在各种任务中都能表现出色。",
  "related_work": "论文中提到了与本研究相关的重要文献，包括：\n1.  **Reinforcement Learning for Large Language Models:** 讨论了RL作为对齐LLM的基础方法。\n2.  **Reward Models for Reinforcement Learning:** 讨论了推理任务中奖励模型的应用。\n3.  **Reinforcement Learning with Verifiable Rewards:** 讨论了使用基于规则的验证函数生成奖励信号。",
  "github_links": [
    "https://github.com/zju-real/cooper",
    "https://github.com/huggingface/Math-Verify"
  ],
  "published": "2025-08-07T17:53:56+00:00"
}