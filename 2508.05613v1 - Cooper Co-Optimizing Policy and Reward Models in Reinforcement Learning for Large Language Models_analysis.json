{
  "title": "COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS",
  "detailed_summary": "这篇论文提出了名为**Cooper**的强化学习框架，旨在解决大型语言模型(LLMs)在推理任务中使用强化学习(RL)时，奖励函数设计上的局限性。当前的奖励机制主要分为基于模型和基于规则两种，前者容易受到奖励黑客攻击，后者缺乏鲁棒性。Cooper通过联合优化策略模型和奖励模型，利用规则奖励识别正确答案的高精度优势，并动态构建和选择正负样本对以持续训练奖励模型，从而提高鲁棒性并减轻奖励黑客的风险。为了支持Cooper，论文还提出了一种混合标注策略，高效准确地生成奖励模型的训练数据。此外，还提出了基于参考答案的奖励建模范式，并基于此训练了一个名为VerifyRM的奖励模型，该模型在VerifyBench上实现了比同等规模的其他模型更高的准确率。实验结果表明，Cooper不仅缓解了奖励黑客问题，还提高了端到端RL的性能。",
  "background": "大型语言模型(LLMs)在推理任务中表现出色，强化学习(RL)已成为增强其推理能力的关键算法。然而，现有的基于模型和基于规则的奖励函数都存在局限性。基于模型的奖励容易受到奖励黑客攻击，模型会利用输出模式欺骗奖励函数，获得高分而不管输出是否正确。基于规则的奖励依赖于手动制定的规则来解析和验证模型输出，缺乏鲁棒性且容易误判，限制了模型的进一步优化。该研究的动机在于克服这些局限性，提出一种更有效且鲁棒的奖励机制，以提高LLMs在推理任务中的性能。",
  "contributions": [
    "提出了一个奖励建模数据集，该数据集使用混合标注策略进行标注，结合了基于规则的验证和基于LLM的验证，从而实现了高效可靠的正确性监督。在该数据集上训练的奖励模型在VerifyBench上达到了89.42%的准确率，超过了现有同等规模的奖励模型。",
    "基于规则奖励在识别正确答案时的高精度，提出了**Cooper**，一个同时共同优化策略模型和奖励模型的强化学习框架。该框架缓解了基于奖励模型的RL中常见的奖励黑客问题，并提高了整体训练性能。",
    "研究表明，在RL训练过程中动态调整奖励模型的参数可以有效地缓解奖励黑客现象，为研究界如何更好地在强化学习中利用奖励模型提供了有价值的见解。"
  ],
  "problem": "该论文旨在解决以下问题：\n1.  **奖励黑客问题**: 当使用固定的基于模型的奖励函数时，策略模型可能会学会利用该奖励函数的弱点，产生欺骗性的输出以获得高分，而忽略了输出的实际正确性。\n2.  **规则奖励的鲁棒性问题**: 基于规则的奖励函数通常依赖于手动设计的规则，这些规则可能无法涵盖所有可能的正确答案形式，从而导致误判和限制模型的优化。\n3.  **奖励模型训练数据不足**: 训练奖励模型通常需要大量的标注数据，而手动标注成本高昂且效率低下。",
  "methods": [
    "**VerifyRM的训练配方:**\n    *   **数据准备:** 收集包含推理问题、参考答案和模型生成的补全的数据三元组。使用7个常用的数学推理数据集和11个主流LLM生成了65K个问题-参考-补全三元组。\n    *   **正确性的混合标注:** 结合基于规则的验证器（Math-verify）和LLM-as-a-judge（Qwen3-4B）进行自动标注，只保留两种方法都同意正确性标签的样本，得到58.7K个训练样本。\n    *   **奖励模型训练:** 将奖励模型建模为文本分类器，将参考答案纳入奖励模型的输入。使用二元交叉熵损失训练模型。\n",
    "**带有Cooper的强化学习:**\n    *   **策略模型优化:** 遵循GRPO范式，对每个训练样本，使用策略采样一组响应，然后奖励模型评估每个rollout并生成分数。对这些奖励进行组内归一化以计算优势估计，然后用于通过策略梯度更新策略。为了规范探索并确保训练稳定性，在强化学习期间加入了KL散度惩罚。\n    *   **奖励模型优化:** 使用对比学习优化奖励模型，对于给定的问题，参考答案和一对候选响应，目标是最大化RM分配给正确响应和不正确响应之间的分数差异。\n        *   **正样本选择:** 从一组响应中，随机选择一个被规则判断为正确的样本，并将其视为正样本。\n        *   **负样本生成:** 使用辅助LLM将正确的推理过程转换为最终产生不正确答案的过程，由精心设计的prompt指导。通过规则进行验证，若不被识别为不正确，则重复该过程，直到获得有效的负样本。"
  ],
  "experimental_design": "该论文的实验设计包括以下几个方面：\n1.  **VerifyRM的实验:** 使用Qwen2.5-Math1.5B-Instruct作为基础模型，训练VerifyRM。训练了3个epoch，学习率为2e-5，batch size为128。为确保在VerifyBench上进行公平评估，从训练数据中排除了所有重叠的查询。\n2.  **Cooper的实验:** 基于veRL框架实施Cooper算法。实验在DeepMath数据集上进行，随机抽取1万个样本进行训练。所有实验都使用Qwen2.5-1.5B-Instruct和Llama-3.2-1B-Instruct作为初始模型。 GRPO算法中，全局batch size设置为512，最大提示长度为1024，最大响应长度为3072。学习率设置为1e-6，KL惩罚系数设置为0.001。对于每个prompt，在RL训练期间生成16个rollout。模型训练了10个epoch。\n3.  **评估指标:** 模型在五个数学推理基准上进行评估：GSM8K，SVAMP，MATH500，OlympiadBenchEN（OB-EN）和Math Odyssey。 使用温度为0.7和top-p为0.95的设置，为每个问题生成8个样本，并计算平均准确率以减轻评估差异。",
  "results": "论文的主要实验结果如下：\n1.  **VerifyRM的性能:** VerifyRM在VerifyBench上取得了89.42%的准确率，超过了同等规模的其他奖励模型。\n2.  **Cooper的性能:**\n    *   在Qwen2.5-1.5B-Instruct上，Cooper实现了58.02%的平均准确率，优于基于规则的奖励（57.48%），并显著优于静态奖励模型（38.91%）。\n    *   在Math Odyssey等挑战性任务上，共同优化变得越来越有价值（44.17% vs 42.83%）。\n    *   静态奖励模型遭受了奖励黑客攻击带来的灾难性失败，性能从54.93%下降到38.91%。",
  "result_analysis": "论文对实验结果进行了深入的分析，主要包括：\n1.  **训练动态分析:** 测试集上的准确率表明，基于规则的奖励和Cooper表现出稳定的提高，而静态奖励模型在120步左右发生了灾难性的崩溃，从58％降至52％以下。这种崩溃与奖励黑客攻击同时发生，静态模型的训练奖励异常飙升至接近1.0。\n2.  **奖励模型的稳定性:** VerifyRM在整个训练过程中在VerifyBench上的准确率保持在89.7％左右，波动低于0.5％，这表明协同优化可以稳定进行。\n3.  **连续奖励与离散奖励的比较:** 将Cooper的奖励输出二值化后，Cooper仍然实现了57.86％的平均准确率，这表明Cooper的主要优势在于通过动态更新来防止奖励黑客攻击，而不是奖励粒度。",
  "conclusions": "论文得出以下结论：\n1.  **奖励黑客攻击是静态奖励模型的基本问题**: 固定的奖励模型存在固有的缺陷，模型会利用奖励模型的弱点进行攻击。\n2.  **高精度信号的关键作用**: 基于规则的验证器具有高精度但低召回率的不对称特性，可以作为选择正训练样本的优势。\n3.  **协同优化是解决奖励模型限制的有效方法**: Cooper通过协同优化策略模型和奖励模型，提高了LLMs在推理任务中的性能，缓解了奖励黑客攻击，并提高了训练的稳定性。",
  "limitations": "论文中提到的局限性包括：\n1.  **依赖于特定领域的验证工具**: 限制了Cooper推广到没有明确正确性标准任务的能力。\n2.  **双重优化带来的计算开销**: 可能会影响可扩展性。\n3.  **依赖于辅助LLM生成负样本**: 引入了外部依赖性。",
  "future_work": "论文建议的未来研究方向包括：\n1.  **探索自监督对比样本生成**: 摆脱对辅助LLM生成负样本的依赖。\n2.  **将Cooper扩展到基于过程的奖励**: 以获得更密集的监督。\n3.  **开发协同进化稳定性的理论框架**: 为协同进化提供理论基础。",
  "applications": "这项研究可能的实际应用场景和对生产生活的影响包括：\n1.  **提高LLMs在数学推理任务中的性能**: Cooper可以用于训练更准确和鲁棒的数学推理模型，提高其解决复杂数学问题的能力。\n2.  **扩展到其他需要验证的任务**: Cooper的协同优化框架可以应用于其他需要验证的任务，如代码生成、科学推理等，提高模型的性能和可靠性。\n3.  **促进混合AI系统的发展**: Cooper结合了符号精度和神经灵活性，为开发更可靠的混合AI系统提供了思路。",
  "related_work": "论文中提及的相关工作包括：\n1.  **用于大型语言模型的强化学习**: 讨论了RLHF及其简化方法，如DPO和RLAIF。\n2.  **用于强化学习的奖励模型**: 将现有的奖励模型分为判别式、生成式和隐式三种类型，以及结果奖励模型（ORM）和过程奖励模型（PRM）两种。\n3.  **具有可验证奖励的强化学习**: 介绍了RLVR，它利用基于规则的验证函数来自动生成奖励信号。",
  "github_links": [
    "https://github.com/zju-real/cooper",
    "https://github.com/huggingface/Math-Verify"
  ],
  "published": "2025-08-07T17:53:56+00:00"
}