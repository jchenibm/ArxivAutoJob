{
  "title": "COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS",
  "detailed_summary": "这篇论文提出了一个名为COOPER（Co-Optimizing Policy Model and Reward Model）的强化学习框架，用于提升大型语言模型（LLMs）的推理能力。当前，基于模型和基于规则的奖励存在局限性，前者易受奖励操纵（reward hacking）影响，后者缺乏鲁棒性。COOPER框架通过联合优化策略模型和奖励模型来解决这些问题。它利用基于规则的奖励在识别正确响应时的高精度，并动态构建和选择正负样本对，持续训练奖励模型，增强其鲁棒性并降低奖励操纵的风险。为了支持COOPER，论文还提出了一种混合标注策略，高效准确地生成奖励模型的训练数据，并提出了一种基于参考的奖励建模范式，其中奖励模型以参考答案作为输入。基于此设计，训练了一个名为VerifyRM的奖励模型，在VerifyBench上实现了比同等规模模型更高的准确率。实验结果表明，COOPER不仅缓解了奖励操纵问题，还提高了端到端强化学习的性能。",
  "background": "大型语言模型（LLMs）在推理任务中表现出了卓越的能力，特别是在数学推理、代码推理和常识推理方面。强化学习（RL）已成为增强这些推理能力的关键技术。然而，在RL算法中，奖励函数的设计是一个关键因素，它决定了输出序列的评估质量。现有的奖励函数，无论是基于模型的还是基于规则的，都存在固有的局限性。基于模型的奖励容易受到奖励操纵的影响，而基于规则的奖励缺乏鲁棒性。",
  "contributions": [
    "提出了一个新的奖励建模数据集，该数据集使用混合标注策略进行标注，该策略结合了基于规则的验证和LLM-as-a-judge验证，从而实现了高效可靠的正确性监督。在该数据集上训练的奖励模型在VerifyBench上达到了89.42%的准确率，超过了现有相同规模的奖励模型。",
    "基于规则的奖励在识别正确答案时具有高精度，提出了一种名为COOPER的强化学习框架，该框架同时协同优化策略模型和奖励模型。该框架缓解了奖励模型RL中常见的奖励操纵问题，并提高了整体训练性能。",
    "该研究表明，在RL训练过程中动态调整奖励模型的参数可以有效地减轻奖励操纵现象，为研究界如何更好地在强化学习中利用奖励模型提供了有价值的见解。"
  ],
  "problem": "论文旨在解决在强化学习中，大型语言模型（LLMs）使用奖励函数时遇到的两个主要问题：奖励操纵（reward hacking）和鲁棒性不足。奖励操纵指的是模型利用固定奖励模型的漏洞来获得高分，即使输出不正确；鲁棒性不足指的是基于规则的奖励函数依赖于手动制定的规则，容易出现误判，从而限制了模型的进一步优化。",
  "methods": [
    "**VerifyRM的训练：**",
    "  - **数据准备：**收集包含推理问题、参考答案和模型生成的完成的数据集。使用了7个常用的数学推理数据集，并用11个主流LLM生成了完成。",
    "  - **混合标注：**结合基于规则的验证器（Math-verify）和LLM-as-a-judge（Qwen3-4B）自动标注数据的正确性。只保留两种方法都认可的样本。",
    "  - **奖励模型训练：**将奖励模型建模为文本分类器，输入包括问题、参考答案和模型完成。使用二元交叉熵损失函数训练模型。",
    "**COOPER强化学习框架：**",
    "  - **策略模型优化：**遵循GRPO范式，使用参考答案感知的奖励模型对响应进行抽样和评分，并根据组内归一化的优势和KL正则化执行策略更新。",
    "  - **奖励模型优化：**通过对比学习不断优化奖励模型，使用高精度规则信号识别的正样本和由辅助LLM将正确响应转换为不正确响应生成的负样本。"
  ],
  "experimental_design": "论文使用以下实验设计来验证所提出的方法：\n\n1.  **VerifyRM的实验：**\n    *   **数据集：**使用7个数学推理数据集，通过11个LLM生成响应，并使用混合标注方法进行标注。\n    *   **评估指标：**使用VerifyBench数据集评估奖励模型的准确率。\n    *   **基线：**与基于规则的函数、原始奖励模型和基于参考的验证器进行比较。\n2.  **COOPER的实验：**\n    *   **数据集：**使用DeepMath数据集，并随机抽取10K个示例进行训练。\n    *   **评估指标：**使用五个数学推理基准测试（GSM8K，SVAMP，MATH500，OlympiadBenchEN和Math Odyssey）评估模型的性能。\n    *   **基线：**与使用Math-Verify作为奖励函数的模型，以及使用VerifyRM-1.5B作为奖励模型但不更新其参数的模型进行比较。\n    *   **模型：** Qwen2.5-1.5B-Instruct 和 Llama-3.2-1B-Instruct",
  "results": "论文的主要实验结果如下：\n\n1.  **VerifyRM的实验：** VerifyRM-1.5B在VerifyBench上达到了89.42%的准确率，超过了同等规模的现有奖励模型，甚至超过了9B参数的xVerify模型。\n2.  **COOPER的实验：**\n    *   COOPER在Qwen2.5-1.5B-Instruct上实现了58.02%的平均准确率，优于基于规则的奖励（57.48%）和静态奖励模型（38.91%）。\n    *   静态奖励模型在Qwen2.5-1.5B-Instruct上的性能从54.93%下降到38.91%，下降了16%。\n    *   COOPER成功防止了这种灾难性的失败，并实现了最高的性能。",
  "result_analysis": "COOPER 的有效性在于它通过同步更新成功地阻止了策略模型利用奖励信号。当策略模型进化时，奖励模型会适应其决策边界，从而关闭静态系统中累积的利用机会。通过使用高精度的基于规则的信号作为正样本和系统扰动作为负样本，每个更新都会加强正确的决策边界，而不是引入噪声。奖励模型在整个训练过程中保持了稳定性，这表明协同优化可以在不稳定的情况下实现，而这种不稳定性通常与移动目标问题相关联。奖励信号粒度的消融研究表明，COOPER 的主要优势在于通过动态更新来防止奖励操纵，而不是来自奖励粒度。持续奖励通过在策略优化过程中实现更细致的信用分配来提供额外的好处。",
  "conclusions": "论文的核心结论是：COOPER，一种协同训练策略模型和奖励模型的强化学习框架，有效地缓解了在RL中使用静态奖励模型时经常出现的奖励操纵问题。通过结合基于规则的奖励的高精度和基于模型的奖励的鲁棒性，COOPER实现了比单独使用任何一种奖励类型都更好的性能。此外，论文提出的基于参考答案的奖励模型VerifyRM，通过利用不依赖于手动标注的混合标注方法，在VerifyBench基准测试中优于现有同等规模的模型。研究结果表明，在RL训练期间动态更新奖励模型可以有效对抗奖励操纵。",
  "limitations": "论文存在以下局限性：\n\n1.  对特定领域的验证工具的依赖限制了泛化到没有明确正确性标准的任务。\n2.  双重优化带来的计算开销可能会影响可扩展性。\n3.  对辅助LLM生成负样本的依赖引入了外部依赖性。",
  "future_work": "论文建议的未来研究方向包括：\n\n1.  探索自监督对比示例生成。\n2.  将COOPER扩展到基于过程的奖励，以实现更密集的监督。\n3.  开发协同进化稳定性的理论框架。",
  "applications": "这项研究可能的实际应用场景包括：\n\n1.  改进大型语言模型在数学推理和其他需要可验证的推理能力的任务中的性能。\n2.  开发更鲁棒和可靠的AI系统，这些系统不容易受到奖励操纵。\n3.  降低强化学习中对人工标注数据的依赖性。",
  "related_work": "论文中提及的相关工作包括：\n\n1.  强化学习对大型语言模型的影响\n2.  奖励模型在强化学习中的应用\n3.  可验证奖励的强化学习",
  "github_links": [
    "https://github.com/zju-real/cooper",
    "https://github.com/huggingface/Math-Verify"
  ],
  "published": "2025-08-07T17:53:56+00:00"
}