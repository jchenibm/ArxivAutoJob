## **The Missing Reward: Active Inference in the Era of** **Experience**

**Bo Wen**

IBM T.J. Watson Research Center
Yorktown Heights, NY
```
                bwen@us.ibm.com
### **Abstract**

```
This paper argues that Active Inference (AIF) provides a crucial foundation for
developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust high-quality
training data and rely on increasingly large human workforces for reward design,
the current paradigm faces significant scalability challenges that could impede
progress toward genuinely autonomous intelligence. The proposal for an “Era
of Experience,” where agents learn from self-generated data, is a promising step
forward. However, this vision still depends on extensive human engineering of
reward functions, effectively shifting the bottleneck from data curation to reward
curation. This highlights what we identify as the **grounded-agency gap** : the
inability of contemporary AI systems to autonomously formulate, adapt, and pursue
objectives in response to changing circumstances. We propose that AIF can bridge
this gap by replacing external reward signals with an intrinsic drive to minimize free
energy, allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF’s principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with human
values. This synthesis offers a compelling path toward AI systems that can develop
autonomously while adhering to both computational and physical constraints.
### **1 Introduction**

In his NeurIPS 2024 award speech, Ilya Sutskever declared that “pre-training as we know it will
unquestionably end” [ 1 ]. Dario Amodei, in a recent podcast[ 2 ], estimated a “10% chance that the
scaling of AI systems could stagnate due to insufficient data”. Several empirical studies [ 3, 4 ] support
similar conclusions and project that demand for training data will soon exceed the available public
supply. As high-quality human-generated data becomes increasingly scarce, new approaches must
emerge to sustain AI’s advancement.

Addressing this looming data shortage, the recent preprint “Welcome to the Era of Experience”
(abbreviated as “EoE” in the following) [ 5 ] by Silver and Sutton proposes a paradigm shift from
static, human-generated datasets to dynamic, agent-generated experiences. We interpret this as AI
agents learning continuously through their own environmental interactions, effectively creating a
self-sustaining cycle where these interactions generate the very data needed for ongoing training and
improvement. This closed-loop system not only circumvents concerns about exhausting training data
supplies but potentially unlocks capabilities beyond what human-curated data alone could achieve.

EoE extends far beyond conventional reinforcement learning, outlining several essential components
for this experiential paradigm. Silver and Sutton envision agents that inhabit continuous streams of
experience rather than isolated interaction episodes, with actions and observations richly grounded in

Preprint. Under review.


-----

their environments. These agents would *derive rewards from real-world signals instead of human*
*judgments*, and employ planning mechanisms that reason directly about experience rather than
abstract concepts.

However, their blueprint leaves open a practical, and potentially decisive, question. The challenge of
determining who will engineer the reward functions that turn raw signals into useful guidance remains
unresolved. In their own examples, different reward functions must be tuned for each high-level user
goal. At the scale implied by lifelong, open-ended agents, this merely shifts the bottleneck from
curating training data to curating reward functions, reintroducing the kind of domain knowledge
engineering that Sutton’s *bitter lesson* warns us about [6].

**We argue that Active Inference (AIF) provides the missing foundation for autonomous AI agents**
**that can learn from experience without constant human reward engineering.** By replacing
external reward engineering with intrinsic free energy minimization, AIF agents naturally balance
exploration and exploitation through a unified Bayesian objective that emerges from the agent’s own
world model and preferences. This approach offers a more direct path to the experiential learning
paradigm envisioned by Silver and Sutton, with advantages in scalability, safety, and efficiency that
we explore throughout this paper.

**Overview of Core Arguments** This paper identifies a fundamental “grounded-agency gap” in
contemporary AI—the inability to autonomously form, evaluate, and adapt objectives—that persists
even in proposed experience-driven paradigms. We argue that Active Inference provides a compelling
theoretical foundation for the “Era of Experience” by offering intrinsic motivation through free energy
minimization, which can eliminate the need for continuous reward engineering. Building on this,
we propose a novel integration where Large Language Models serve as learned generative world
models within an Active Inference decision-making framework, combining the scalability of modern
deep learning with the theoretical rigor of the Free Energy Principle. Finally, we situate this proposal
within the physical constraints of AI development, arguing that the energy efficiency of free energy
minimization is not just computationally advantageous but may be a thermodynamic necessity for
sustainable AI progress.
### **2 Two Dark Clouds over Contemporary AI**

In 1900, Lord Kelvin observed that physics seemed nearly complete save for "two small clouds" on
the horizon. These clouds (blackbody radiation and the Michelson-Morley experiment) ultimately
revolutionized our understanding through quantum mechanics and relativity. Today’s AI faces
analogous clouds that signal not minor adjustments but fundamental limitations in our current
paradigm.

**Cloud I: Resource Saturation—Physical Limits of Scale.** The prevailing wisdom in AI has
been simple: more data and more compute yield better models. This scaling hypothesis drove
remarkable progress from early language models to today’s trillion-parameter systems. However, we
are reaching hard physical limits on multiple fronts simultaneously. On the data side, high-quality
human text accumulated over centuries is being rapidly depleted, with quality English text projected
for exhaustion within a decade while specialized domains already face severe scarcity. Web crawls
yield diminishing returns: more spam, more synthetic content, less genuine human knowledge. On
the compute side, training costs rise super-linearly with model size while universities and smaller
labs are priced out of frontier research [ 7 ], creating unprecedented concentration of power. This is
not merely an economic problem; it is a thermodynamic one. Each doubling of model capability
demands exponentially more energy, while performance gains follow a logarithmic curve. This
creates a fundamental mismatch that no amount of engineering can overcome.

**Cloud II: Externalized Cognition—Hidden Human Dependencies.** The second constraint is
more subtle but equally limiting: today’s “autonomous” AI systems depend on vast networks of
human cognition operating behind the scenes. This represents a fundamental architectural limitation where judgment, adaptation, and error correction are outsourced to human workers. Content
moderators suffer psychological trauma from filtering toxic outputs, while annotator agreement rates
reveal instability in the foundation of “ground truth.” More critically, this dependency scales *inversely*
with capability: as models grow more sophisticated, they require more nuanced human judgment for

2


-----

alignment, creating an ever-expanding need for specialized expertise. The recent DeepMind study
on preference drift [ 8 ] exposes the fundamental challenge: human values are not static targets but
dynamic, context-dependent processes that no amount of labeling can fully capture. The result is a
Sisyphean cycle: engineers continuously patch reward functions, annotators endlessly refine preference datasets, and safety teams perpetually chase emerging failure modes. This is not intelligence; it
is an elaborate puppet show where humans operate the strings from behind an algorithmic curtain,
just as Fei-Fei Li said, “There’s nothing artificial about artificial intelligence.” [9]

Table 1: Evidence of Structural Bottlenecks in Contem p orar y AI

**Bottleneck** **Resource Saturation** **Externalized Cognition** **Systemic Impact**


**Data** High-quality text ex**Scarcity** hausted within 10
years [ 3 ]. Diminishing
returns at trillion-token
scales [4].

**Compute /** SOTA training: 1000s
**Energy** GPUs, 4-5 digit tonne
CO 2 [ 11 ]. Google: 48%
data center emission
jump [12, 13].

**Human** OpenAI spends $100M+
**Labor** annually on RLHF contractors. Workers earn below living wages [17].

**Economic** <10% AI startups prof**Friction** itable (2024). Job displacement in automatable
sectors [18].


Human-labeled data

<70% inter-annotator
agreement [ 10 ]. Quality
ceiling from noisy judg
ments.

RLHF workforce scales
with model capability [ 14 ].
Human oversight costs
compound with AI sophistication.

Invisible supply chain of
labelers, moderators. Psychological trauma from
disturbing content [14].

Cultural/linguistic bias:
AI works best for dominant languages. Global
inequities reinforced.


Progress tied to finite
historical accumulation;
innovation stalls with
out new sources

Only tech giants compete [ 15 ]; universities
priced out [ 16 ]. Environmental costs threaten

social license.

“Automation” paradox:
AI creates more human

work. Undermines autonomous intelligence
narrative.

Social backlash from
displacement. Talent
drain from ethics concerns. Regulatory momentum building.


These four symptoms (data exhaustion, compute barriers, labor dependencies, and economic friction)
are not independent failures but manifestations of the two fundamental clouds. Resource saturation
(Cloud I) drives the data and compute crises, while externalized cognition (Cloud II) creates the
human labor and economic imbalances. Together, they reveal that scaling alone cannot produce
genuine intelligence.
### **3 The Grounded-Agency Gap**

These system-level constraints reveal a deeper problem: most contemporary AI systems lack the
ability to autonomously create, update, and pursue objectives as circumstances change - a deficiency
we term the **grounded-agency gap** . To close this gap, an agent must (i) perceive its situation, (ii)
revise its goals in response to evolving user needs and environments, and (iii) operate safely without
requiring continuous human intervention. In the following discussion, we demonstrate why two
prevalent approaches - reward engineering and self-play - fail to achieve true agency.

**Reward Engineering is** ***not*** **Grounded Agency.** The “reward-is-enough” idea [ 19 ] suggests that
a single, well-designed reward signal could produce all intelligent behaviors if maximized in a
sufficiently complex environment. Recent successes in robotic research supported this insight: RT-2
handles kitchen tools from vision alone [ 20 ], RoboCat shows impressive adaptability by fine-tuning
itself to dozens of new tasks [ 21 ], and PaLM-E combines language with physical actions [ 22 ].
While impressive, these systems operate in highly controlled experimental environments with clear,
measurable rewards perfectly matched to specific tasks. This controlled setting masks several critical
limitations when considering true grounded agency in open-world scenarios: (1) **Artificial feedback**
**systems.** Concepts like “tasty food” or “safe move” are not directly observable. Current systems

3


-----

need special sensors, training resets, or expensive simulators like Habitat-2.0 [ 23 ] and iGibson [ 24 ] to
provide constant reward feedback. This infrastructure does not exist in the real world. (2) **Short-term,**
**fixed goals.** These demonstrations show robots performing brief tasks like pouring drinks or folding
clothes. They do not handle long-term changes like shifting food preferences or unexpected safety
issues that arise in real homes. The systems cannot adapt over time or learn from past experience
when goals change. (3) **Hidden guard-rails and implicit constraints.** To prevent failures, engineers
add safety features that are not part of the official reward. These include collision avoidance, penalty
systems, or human monitors. These features shape the robot’s behavior but are not part of its explicit
objectives.

Consider a toy example that illustrates why this approach fails. An autonomous lab assistant is tasked
with conducting a fluorescent enzyme assay and instructed to “complete the assay efficiently.” When
the assistant observes unexpected acidification (yellow indicator), its primary reward signal pushes
toward rapid completion. Without explicit safety constraints programmed into the reward function,
the agent might quickly add concentrated base to neutralize the acid, potentially causing dangerous
splashing of corrosive reagents or thermal damage to heat-sensitive enzymes. The original efficiency
command remains unchanged even as new safety hazards emerge.

Engineers might respond by adding penalty terms for “chemical spills” or “temperature violations,”
but this requires anticipating every possible failure mode and manually encoding safety preferences—a
combinatorially explosive engineering challenge. Supporters might argue the system would learn
safer protocols after experiencing negative outcomes, but this approach necessitates either dangerous
real-world experimentation or expensive high-fidelity simulations that may not capture all safetycritical dynamics. Each new domain or task variation demands fresh rounds of reward engineering,
creating the very labor-intensive bottleneck that autonomous systems should eliminate.

Under Active Inference, safety preferences like “avoid spilling corrosive reagents” and “maintain
enzyme integrity” sit directly in the *C* matrix as intrinsic preferences rather than external constraints.
The agent naturally selects policies that minimize expected free energy by both resolving uncertainty
(measuring pH first) and satisfying safety preferences (careful titration) without requiring explicit
reward engineering for each potential hazard.

Recent studies from DeepMind show that this problem is real rather than merely theoretical: even with
sophisticated preference learning techniques like Non-Stationary Direct Preference Optimization, user
preferences exhibit significant temporal drift, causing standard algorithms to become misaligned over
time [ 8 ]. The labor-intensive nature of collecting and curating human preferences for RLHF, with its
inherent inconsistencies and costs (as discussed in Section 2), further highlights the unsustainability
of continuous external reward adjustment. **Take-away:** Current reward engineering practice incurs
substantial *cost* (instrumentation, supervision, post-hoc patching) but rarely instills robust, generalpurpose *capability* . These approaches fix problems after they happen instead of giving AI the ability
to adapt safely to new situations. This creates a paradox: we want autonomous AI, but we keep
needing humans to constantly adjust its rewards, just with different job titles like “reward engineer”
or “ground-signal curator.” The costs keep growing as AI gets more powerful.

**Why Self-Play Produces Only Simulated Agency.** The challenges of open-world reward engineering stand in stark contrast to the domain-specific successes of systems like AlphaZero [ 25 ]
and AlphaProof [ 26 ]. These systems achieve superhuman performance in complex games (chess,
shogi, and Go) and mathematical theorem proving through self-play, requiring minimal human input
beyond the fundamental rules and clear success criteria (win conditions or valid proofs). They
demonstrate remarkable ability to develop strategies and discover knowledge that exceeds human
expertise, apparently without continuous external guidance or reward shaping.

However, these accomplishments represent what we call *simulated agency* . They function within
precisely defined, closed environments where judgment is externalized into fixed rules, explicit
win conditions, and stable objectives. The environment serves as a perfect, unambiguous success
oracle. When moving from these structured domains to the complexities of open-world robotics or
general-purpose AI assistants, these foundational assumptions break down: (1) **Fuzzy, multifaceted**
**objectives:** Real-world tasks rarely have a single, easily quantifiable win condition. Goals are often
ill-defined, composed of multiple potentially conflicting sub-goals, and subject to interpretation
(e.g., “be a helpful assistant,” “ensure user well-being”). (2) **Preferences drift and evolve:** Human
preferences change over time due to learning, shifting circumstances, or developing tastes. Agents

4


-----

relying on fixed, externally defined rewards cannot accommodate these changes. (3) **Non-stationary**
**environments:** The real world is constantly changing in unpredictable ways. Unlike game boards with
fixed rules, AI agents must handle novel situations, new entities, and evolving causal relationships. (4)
**Ambiguous, sparse feedback:** Clear reward signals are rare. Environmental or user feedback is often
delayed, noisy, incomplete, sometimes even contradictory. All requires substantial interpretation.

In such open-ended settings, the reward function must be *learned, inferred, or adapted* rather than
simply provided. While self-play excels when evaluative criteria are fixed and externally supplied, it
lacks any intrinsic mechanism for agents to *derive or update* these criteria from its experiences in an
ambiguous, evolving world. Therefore, for general AI to achieve the self-improvement and emergent
capabilities demonstrated by AlphaZero, it requires more than self-play. It needs genuine agency:
the capacity to form, evaluate, and refine objectives and understanding based on its interactions
with complex, underspecified environments. The fundamental challenge involves enabling AI to
interpret human guidance and environmental signals as flexible preferences and evidence rather than
rigid rewards, then autonomously adapt its capabilities to satisfy these evolving preferences in novel
situations.

**Bridge to Active Inference.** Silver and Sutton’s proposal of an “adaptive reward, based on grounded
signals, guided by user” shares a conceptual similarity with our argument for grounded agency. Both
approaches recognize the need for AI agents to dynamically adjust their objectives based on realworld interactions rather than relying on static, pre-defined rewards. However, the critical distinction
lies in the *locus of control* : while Sutton’s framework still requires external human guidance to adapt
reward functions, our approach seeks to internalize this process entirely.

This mirrors human development: just as children “grow up” by generalizing from lessons to broader
principles, we need AI systems to develop intrinsic mechanisms for self-directed learning. The
key insight is that we need a *universal learning signal* that enables agents to abstract meta-level
knowledge from experience, with autonomy as the implicit goal. This signal must be: intrinsically
computable (derivable from the agent’s sensory stream), domain-agnostic (applicable across diverse
environments), preference-sensitive (responsive to human values without rigid specification), and
exploration-aware (naturally balancing knowledge-seeking and goal-directed behavior).

This is precisely where the **Free Energy Principle** from Active Inference provides a principled
answer. Rather than engineering countless reward functions, AIF offers a single, universal objective
(minimizing expected free energy) that naturally gives rise to intelligent behavior. The agent’s
exploration, learning, and goal-pursuit emerge from this unified principle, making the “Era of
Experience” both theoretically grounded and practically achievable.
### **4 Active Inference with Language Models: A Path to Grounded Agency**

Section 2 highlighted a critical grounded-agency gap: contemporary AI, even in the envisioned “Era
of Experience,” lacks an intrinsic mechanism for self-directed learning and judgment, often falling
back on external reward engineering. To address this fundamental challenge, we need a framework
that can provide AI systems with principled, internal judgment capabilities.

**From Reward Maximization to Surprise Minimization** Active Inference (AIF) emerged from
cognitive neuroscience as a framework that fundamentally reframes intelligence. Unlike traditional
reinforcement learning (RL), which focuses on maximizing external rewards, AIF views perception
and action as a unified Bayesian inference process [ 27, 28 ]. This shift in perspective offers several key
advantages for bridging the grounded-agency gap: AIF provides a **unified objective** that integrates
perception (estimating the state of the world) and action (learning a policy) under a single goal of
minimizing surprise, whereas RL often treats these as separate problems. The framework enables
**intrinsic motivation**, as AIF agents are inherently driven to minimize surprises in pursuing their
goals rather than requiring externally defined rewards for every task. It also supports **principled**
**exploration** by naturally balancing exploration (seeking new information) and exploitation (using
known information to achieve goals) through its information-seeking drive, avoiding the need for
separate exploration heuristics like *ϵ* -greedy strategies common in RL [ 29 ]. Finally, AIF requires

5


-----

an **explicit generative model** —the agent’s “world model” [1] —which allows for more structured
reasoning about uncertainty and causality than typical RL approaches.

**Core Active Inference Equations.** Formally, Active Inference states that intelligent agents act to
minimize Variational Free Energy (VFE) [2], a measure of surprise [3] or the discrepancy between an
agent’s world model and its sensory inputs [27, 31].

**Variational Free Energy (VFE):**


*F* ( *Q, o* ) = *D* KL [ *Q* ( *s* ) *∥P* ( *s* )]
~~�~~ � ~~�~~ �
Model complexity

**Expected Free Energy (EFE):**


*−* E *Q* ( *s* ) [ln *P* ( *o|s* )]
� � ~~�~~ �
Prediction accuracy


*G* ( *π* ) = *−* E *Q* ˜ [[] *[D]* *[KL]* [[] *[Q]* [(˜] *[s][|][o, π]* [˜] [)] *[||][Q]* [(˜] *[s][|][π]* [)]]]
~~�~~ � ~~�~~ �
Information gain (epistemic value)


*−* E *Q* ˜ [[ln] *[ P]* [(˜] *[o][|][C]* [)]]
~~�~~ � ~~�~~ �
Pragmatic value


where *Q* ( *s* ) is the agent’s approximate posterior belief about hidden states *s*, *C* encodes the agent’s
preference distribution over observations [4], and *D* *KL* is the Kullback-Leibler divergence.

This single objective elegantly unifies perception (updating beliefs [5] to reduce current surprise) and
action (choosing policies to minimize expected future surprise). EFE inherently balances epistemic
value (seeking information to reduce uncertainty about the world) and pragmatic value (acting to
make observations match preferred states) [32, 33].

In essence, while traditional RL asks “What actions will maximize my rewards?”, Active Inference
asks “What actions will best confirm my predictions and lead to states I expect to encounter?” [ 33 ].
This subtle but profound shift removes the need for externally engineered reward functions; the
agent’s objectives arise directly from its generative model and preference structure. This connects to
the vision in EoE, where complex behaviors might emerge from a single guiding signal. AIF offers a
principled path for such emergence to occur more naturally and internally, without constant external
reward engineering.

**The Promise and Pitfalls of Prior AIF-RL Integrations.** The idea that AIF principles could
enhance RL is not new. Researchers have explored various AIF-RL integrations, showing theoretical
compatibility and benefits like better sample efficiency and exploration [ 33 ]. For instance, Sajid et
al.[ 29 ] showed that by operating on beliefs, AIF agents perform epistemic exploration and handle
environmental uncertainty in a Bayes-optimal way, without needing separate mechanisms like *ϵ* greedy strategies or intrinsic motivation rewards. This supports the claim that AIF naturally produces
behaviors that RL often needs explicit engineering for, especially exploration.

Deep active inference, which uses neural networks for AIF computations, has shown promise in
scaling to more complex tasks than traditional matrix-based AIF [ 34 – 36 ]. These methods use neural
networks to approximate key parts of the variational free energy calculation, making active inference
more scalable. Despite these advances, AIF-RL hybrids have not yet matched the raw performance
of pure RL systems on very complex, large-scale problems like Go or protein folding. This gap is

1 The concept of a “world model” has a rich history. Cognitive scientist Craik (1943) described minds building
“small-scale models” of reality to anticipate events. Forrester (1971) defined it as “the image of the world around
us, which we carry in our head.” In AIF, this is a “generative model,” conceptually similar to “world models” in
RL (e.g., [ 30 ]). While both represent an agent’s understanding of environmental dynamics, RL world models
are often neural networks learning latent representations. In AIF, the generative model is more formally a joint
probability distribution over hidden states and observations.
2 VFE is an information-theoretic quantity measuring the mismatch between an agent’s model and reality; it’s
inspired but distinct from thermodynamic free energy in physics.
3 In information theory, “surprise” is the negative log-probability of an observation given the agent’s model.
High surprise means the agent observed something it didn’t predict well, regardless of any emotional response.
4 Human values and guidance can be naturally incorporated here by setting preferences aligned with user
intentions, offering a natural mechanism for AI safety through value alignment.
5 In AIF, “beliefs” are the agent’s subjective views about the world, formalized as probability distributions over
states. This mathematical form lets the agent quantify uncertainty and update its understanding systematically,
while preserving the subjective nature of belief inherent to agency.

6


-----

due to several challenges: (1) **Computational Intractability:** Exact Bayesian inference over the
complex generative models needed for real-world scenarios is often too computationally expensive.
While deep learning approximations help, they can sometimes obscure the principled Bayesian nature
of AIF [ 37 ]. (2) **Generative Model Specification:** Defining or learning the generative models for
high-dimensional, partially observable environments is a major hurdle. Early AIF’s symbolic nature
limited its use to simpler environments where these models could be hand-crafted. (3) **Engineering**
**and Scaling:** RL has benefited from decades of intensive engineering and optimization for scalability,
especially in the deep learning era. AIF, with its more complex inference machinery, hasn’t received
comparable engineering investment for large-scale use. This has led some researchers to view AIF as
theoretically elegant but practically limited to simpler domains (e.g., grid-worlds). Conversely, its
proponents highlight AIF’s power in explaining biological intelligence, seeing current limitations as
engineering problems rather than fundamental flaws.

**LLMs as the Catalyst: Language as the Generative Model.** We propose that Large Language
Models (LLMs) offer a transformative way to bridge this gap and unlock AIF’s potential for grounded
agency. Trained on vast internet-scale text, LLMs possess extensive common-sense understanding of
the world, its entities, relationships, and typical dynamics. This makes them uniquely suited to create
and manage the components of an AIF agent’s generative model in ways previously not feasible.
The underlying transformer architectures of LLMs implicitly approximate Bayesian inference over
latent variables; Active Inference provides the theory to make this reasoning explicit and grounded in
experience. This combination offers a principled solution to the agency gap identified in Section 3.

Recent research shows that the transformer architectures common in LLMs implement a form of
amortized Bayesian computation. Xie et al. [ 38 ] demonstrate that in-context learning in these models
emerges from implicit Bayesian inference, where they infer latent concepts from prompt examples
to make coherent predictions. Their theory indicates transformers approach optimal Bayesian
performance if prompts sufficiently distinguish latent concepts. Müller et al. [ 39 ] provide further
evidence, showing transformers can approximate complex posterior distributions with high fidelity,
achieving significant speedups over existing methods.

Particularly relevant to our proposal is the growing evidence that LLMs excel at analogical reasoning—a cognitive capacity requiring implicit structure mapping and relational inference. Webb et
al. [ 40 ] found that GPT-3 matches or exceeds human performance on Raven’s Progressive Matrices
and other abstract pattern tasks, while Yasunaga et al. [ 41 ] showed that analogical prompting enables
LLMs to self-generate relevant exemplars for novel problems. Recent work by Musker et al. [ 42 ]
demonstrates that LLMs can flexibly re-represent semantic information across domains—though
Lewis & Mitchell [ 43 ] caution that this capability can be brittle on certain variants. Together,
these findings suggest that transformers have acquired sophisticated mechanisms for Bayesian-style
reasoning that we can harness for Active Inference.

We hypothesize that, as LLMs’ reasoning capabilities strengthen, we could leverage transformers’
built-in Bayesian machinery by having the LLM suggest candidate world states and policies, justify them with brief chain-of-thought reasoning, and select the option minimizing Expected Free
Energy (EFE) expressed in language. In this hybrid system, the LLM would provide the amortized inference machinery while Active Inference provides the decision rule, potentially yielding
a scalable, inspectable agent without hand-coded matrices or engineered reward signals. Past Active Inference implementations struggled because engineering the state-observation mappings (the
*A* matrix) became combinatorially explosive. LLMs could compress this vast space into learned,
high-capacity priors that can be incrementally refined through experience, maintaining the principled
exploration-exploitation balance of the EFE objective. [6]

**Our proposed LLM-AIF architecture** would integrate three key components: the **LLM world**
**model**, where the LLM’s learned representations encode observation dynamics and transition probabilities; the **AIF control loop**, where Active Inference guides exploration, learning, and action

6 **Current Limitations and Future Promise.** However, we must acknowledge that current LLMs still exhibit
significant reasoning errors, particularly in complex multi-step inference tasks [ 44 – 46 ]. Recent evaluations show
state-of-the-art models achieve only 60-80% accuracy on challenging reasoning benchmarks, with performance
degrading as complexity increases [ 47 ]. Our proposal is therefore forward-looking: as LLM reasoning capabilities improve—following the trajectory from GPT-2 to GPT-4 and beyond—the feasibility of using them as
reliable Bayesian inference engines will correspondingly increase.

7


-----

selection through free energy minimization; and **online refinement**, where the agent continually
updates its world model through experience.

This integration could enable agents that learn efficiently from experience, maintain transparent
reasoning, and make grounded judgments without constant human oversight—the capabilities needed
for the Era of Experience. By treating the LLM’s internal states as sufficient statistics for a variational
posterior, such an agent might achieve the sample efficiency of model-based RL while inheriting
world knowledge from pretraining.

As a conceptual paper, we intentionally present this LLM-AIF integration as a high-level architectural
vision rather than a fully specified technical implementation. Our goal is to stimulate discussion
and inspire the community to explore concrete instantiations of these ideas. The specific details
of EFE computation, the precise interface between LLM representations and AIF belief updates,
and the optimal strategies for online refinement all deserve significant research attention beyond the
scope of this article. We view this proposal as a starting point for a new research direction, not as its
culmination.

The resulting LLM-AIF fusion realizes the “Era of Experience” vision [ 5 ]: agents generate their
own training signal and interpret it through a principled free-energy lens. High-level preferences
expressed in natural language propagate through the hierarchy as intrinsic priors, allowing the system
to “grow up” from its lifelong stream of experience while remaining human-aligned.

To illustrate how the LLM-AIF framework operates in practice, consider a running vignette:


### **5 Discussion: The Thermodynamics of Agency**

The resource saturation constraints identified in Section 2 point toward a deeper truth: Active
Inference (AIF) represents not merely a computational advantage but a *thermodynamic necessity* . As
documented earlier, current AI approaches are fundamentally unsustainable at the industrial scale
now required for competitive performance.

The Landauer principle establishes that information processing is not free: erasing one bit of
information necessarily dissipates at least *kT* ln 2 of heat [ 48 ]. While current hardware operates
far above this theoretical limit, the energy costs of foundation models already demonstrate how
thermodynamic constraints limit AI progress—echoing the call for “Green AI” by Schwartz et al.

[49].

Conventional deep reinforcement learning faces particular thermodynamic challenges due to its
trial-and-error nature. When exploring trillion-parameter hypothesis spaces through random action
sampling, systems must expend enormous energy before achieving meaningful gradient updates.
Moreover, reward function misspecification compounds these costs by forcing additional energyintensive unlearning processes—the very externalized cognition problem identified as Cloud II. Each
cycle of human reward engineering followed by model retraining represents thermodynamically
irreversible information erasure.

8


-----

In contrast, Active Inference’s free energy minimization offers inherent efficiency benefits: information gain replaces heuristic exploration; incremental belief updates avoid wholesale parameter
changes; and natural memory decay eliminates energy-intensive unlearning. These mechanisms
emerge naturally from AIF’s mathematical structure. By maintaining a generative model that predicts future states, AIF agents can simulate outcomes internally—a form of “mental rehearsal” that
conserves both computational and physical resources.

Given the scale documented in Section 2, even modest efficiency improvements could yield substantial
benefits. A 5% reduction in retraining energy for next-generation models could save tens of gigawatthours. While these theoretical advantages are compelling, **empirical validation of AIF’s energy**
**efficiency remains an open research question** . Future work should prioritize controlled experiments
measuring joules-per-decision across different learning paradigms.

**Energetic-Bounded Rationality.** Classical economics treats agents as perfectly rational optimizers;
real organisms operate under *bounded rationality* : they satisfice under limited time, energy, and
information [ 50 ]. Active Inference already embodies this idea mathematically. Because policies
are selected by minimizing Expected Free Energy (EFE), not maximizing an unbounded reward
signal, every candidate action is evaluated against an implicit *budget* : the marginal epistemic or
pragmatic value must exceed the marginal informational (and thus energetic) cost. This self-regulating
mechanism allows the agent to decide, for example, to pause a search, replenish resources, or defer a
risky sub-goal—behaviors strikingly reminiscent of humans “calling it a day” when tired.

The fundamental insight remains that truly intelligent systems must learn both effectively and
efficiently. By unifying action, perception, and memory under a single thermodynamic framework,
Active Inference provides a pathway for AI development that respects not just computational limits
but the fundamental laws of physics that govern all information processing.
### **6 Conclusion & Outlook**

We opened this paper with Lord Kelvin’s 1900 observation about “two small clouds” that ultimately
revolutionized physics. The clouds we identified in contemporary AI (resource saturation and
externalized cognition) may appear surmountable through incremental engineering, but they signal
the fundamental limits of our current paradigm. Yet history suggests that apparent dead ends often
become doorways to breakthroughs.

Our position is that embracing *Active Inference + experiential data* dissolves both clouds by turning data scarcity into an engine for self-generated experience and internalizing judgment through
free-energy minimization. Unlike previous AI paradigms requiring ever-larger datasets and compute,
Active Inference provides a mathematically principled framework for efficiency gains. The convergence of LLMs’ world knowledge with AIF’s principled exploration offers a unique opportunity to
achieve both capability and sustainability.

**Broader Impact & Call to Action.** By reducing dependence on massive datasets and compute
resources, AIF could democratize AI research while addressing environmental costs (4-5 digit
tonne CO 2 emissions from model training). The framework’s intrinsic free energy minimization
tackles “externalized cognition,” potentially reducing exploitative labor practices in RLHF. However,
autonomous agents forming their own objectives raise value alignment concerns, requiring staged
deployment with careful monitoring.

We invite the community to: (i) establish energy-aware benchmarks reporting joules alongside reward,
(ii) prototype LLM-AIF hybrids in robotic tasks, and (iii) develop evaluation suites for boundedrational behavior. If these challenges are met, the “Era of Experience” may prove as transformative
for AI as quantum theory was for physics—not through brute force scaling, but through deeper
understanding of intelligence itself.
### **Acknowledgments and Disclosure of Funding**

Thanks to Guillermo Cecchi, Jenna Reinen, Professor Karl J. Friston, Chen Wang and Patrick Watson
for their insightful discussions and valuable feedback that helped shape the ideas presented in this
paper. Their perspectives on active inference, neuroscience in general, machine learning, and AI

9


-----

safety were instrumental in refining the arguments. I also acknowledge the use of several AI systems
that assisted in various aspects of this research: Claude, GPT-4, and Gemini provided valuable
assistance with idea exploration, writing support, and grammar correction throughout the drafting
process. Manus and Perplexity were helpful for literature search and citation validation. While these
tools supported the research and writing process, all final decisions regarding content, arguments, and
conclusions remain my own.
### **References**

[1] Ilya Sutskever. Sequence to sequence learning with neural networks: What a decade. In *Award*
*speech in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)*,
Vancouver, Canada, December 2024. Award speech stating "Pre-training as we know it will
unquestionably end".

[2] Kevin Roose and Casey Newton. Dario amodei on the paradoxes of a.i. safety and netflix’s ’deep
[fake love’. Podcast Transcript, July 2023. URL https://www.nytimes.com/section/technology.](https://www.nytimes.com/section/technology)
Available at The New York Times.

[3] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius
Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data, 2024.
[URL https://arxiv.org/abs/2211.04325.](https://arxiv.org/abs/2211.04325)

[4] [Nostalgebraist. chinchilla’s wild implications, 2022. URL https://www.lesswrong.com/posts/6F](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)
[pvch8RR29qLEWNH/chinchilla-s-wild-implications.](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)

[5] David Silver and Richard S. Sutton. Welcome to the era of experience. In *Designing an*
*Intelligence* . MIT Press, 2025. Preprint.

[6] Richard S Sutton. The bitter lesson. *Incomplete Ideas* [, 2019. URL http://www.incompleteideas.](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
[net/IncIdeas/BitterLesson.html.](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

[7] Alex Kantrowitz. Universities woefully under-resourced for ai research, fight for change, 2024.
[URL https://www.cmswire.com/digital-experience/universities-woefully-under-resourced-for](https://www.cmswire.com/digital-experience/universities-woefully-under-resourced-for-ai-research-fight-for-change/)
[-ai-research-fight-for-change/. Accessed: 2025-05-21.](https://www.cmswire.com/digital-experience/universities-woefully-under-resourced-for-ai-research-fight-for-change/)

[8] Seongho Son, William Bankes, Sayak Ray Chowdhury, Brooks Paige, and Ilija Bogunovic.
Right now, wrong then: Non-stationary direct preference optimization under preference drift,
[2024. URL https://arxiv.org/abs/2407.18676.](https://arxiv.org/abs/2407.18676)

[9] Ethan Caldwell. “there’s nothing artificial about artificial intelligence”: Fei-fei li on ’28 pre-read,
[2024. URL https://www.dailyprincetonian.com/article/2024/03/princeton-news-stlife-fei-fei-l](https://www.dailyprincetonian.com/article/2024/03/princeton-news-stlife-fei-fei-li-class-of-2028-pre-read-the-worlds-i-see)
[i-class-of-2028-pre-read-the-worlds-i-see. Accessed: 2025-05-21.](https://www.dailyprincetonian.com/article/2024/03/princeton-news-stlife-fei-fei-li-class-of-2028-pre-read-the-worlds-i-see)

[10] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath,
Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny
Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine
Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,
and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from
[human feedback, 2022. URL https://arxiv.org/abs/2204.05862.](https://arxiv.org/abs/2204.05862)

[11] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network
[training, 2021. URL https://arxiv.org/abs/2104.10350.](https://arxiv.org/abs/2104.10350)

[12] Sebastian Moss. Google emissions jump 48% in five years due to ai data center boom. *Data*
*Center Dynamics* [, July 2024. URL https://www.datacenterdynamics.com/en/news/google-emi](https://www.datacenterdynamics.com/en/news/google-emissions-jump-48-in-five-years-due-to-ai-data-center-boom/)
[ssions-jump-48-in-five-years-due-to-ai-data-center-boom/.](https://www.datacenterdynamics.com/en/news/google-emissions-jump-48-in-five-years-due-to-ai-data-center-boom/)

[13] Google. 2023 environmental report. Technical report, Google, July 2023. [URL https:](https://www.gstatic.com/gumdrop/sustainability/google-2023-environmental-report.pdf)
[//www.gstatic.com/gumdrop/sustainability/google-2023-environmental-report.pdf.](https://www.gstatic.com/gumdrop/sustainability/google-2023-environmental-report.pdf)

10


-----

[14] Karen Grey. The humans behind the robots. *MIT Technology Review*, 2023. Reporting on
OpenAI’s spending of over $100M on RLHF labelling.

[15] [Brian Eastwood. Study: Industry now dominates ai research, 2023. URL https://mitsloan.mit.e](https://mitsloan.mit.edu/ideas-made-to-matter/study-industry-now-dominates-ai-research)
[du/ideas-made-to-matter/study-industry-now-dominates-ai-research. Accessed: 2025-05-21.](https://mitsloan.mit.edu/ideas-made-to-matter/study-industry-now-dominates-ai-research)

[16] Kevin Klyman, Aaron Bao, Caroline Meinhardt, Daniel Zhang, Elena Cryst, Russell Wald,
and Fei-Fei Li. Expanding academia’s role in public sector ai. *Stanford HAI*, 2024. URL
[https://hai.stanford.edu/policy/expanding-academias-role-in-public-sector-ai.](https://hai.stanford.edu/policy/expanding-academias-role-in-public-sector-ai)

[17] Kotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey P.
Bigham. A data-driven analysis of workers’ earnings on amazon mechanical turk. In *Proceed-*
*ings of the 2018 CHI Conference on Human Factors in Computing Systems*, CHI ’18, page 1–14,
New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356206. doi:
[10.1145/3173574.3174023. URL https://doi.org/10.1145/3173574.3174023.](https://doi.org/10.1145/3173574.3174023)

[18] Daron Acemoglu and Pascual Restrepo. Robots and jobs: Evidence from us labor markets.
*Journal of Political Economy*, 128(6):2188–2244, 2020.

[19] David Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. *Artificial*
*Intelligence*, 299:103535, 2021.

[20] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu,
Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander
Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,
Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael
Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu
Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul
Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.
Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. URL
[https://arxiv.org/abs/2307.15818.](https://arxiv.org/abs/2307.15818)

[21] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria
Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio
Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel
Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Zołna, [˙]
Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, JeanBaptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker,
Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and
Nicolas Heess. Robocat: A self-improving generalist agent for robotic manipulation, 2023.
[URL https://arxiv.org/abs/2306.11706.](https://arxiv.org/abs/2306.11706)

[22] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,
Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc
Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied
[multimodal language model, 2023. URL https://arxiv.org/abs/2303.03378.](https://arxiv.org/abs/2303.03378)

[23] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah
Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,
Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt
Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training
[home assistants to rearrange their habitat, 2022. URL https://arxiv.org/abs/2106.14405.](https://arxiv.org/abs/2106.14405)

[24] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia
Pérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent
Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: a simulation environment for
[interactive tasks in large realistic scenes, 2021. URL https://arxiv.org/abs/2012.02924.](https://arxiv.org/abs/2012.02924)

11


-----

[25] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur
Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,
Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that
masters chess, shogi, and go through self-play. *Science*, 362(6419):1140–1144, 2018. doi:
[10.1126/science.aar6404. URL https://www.science.org/doi/abs/10.1126/science.aar6404.](https://www.science.org/doi/abs/10.1126/science.aar6404)

[26] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog,
M.Pawan Kumar, Emilien Dupont, Francisco J. [˜] R. Ruiz, Jordan S. Ellenberg, Pengming Wang, [˜]
Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program
search with large language models. *Nature*, 625:468–475, 2024. doi: 10.1038/s41586-023-069
[24-6. URL https://www.nature.com/articles/s41586-023-06924-6.](https://www.nature.com/articles/s41586-023-06924-6)

[27] Karl Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a
free-energy formulation. *Biological Cybernetics*, 102(3):227–260, 2010.

[28] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free
energy principle for action and perception: A mathematical review. *Journal of Mathematical*
*Psychology*, 81:55–79, 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URL
[https://www.sciencedirect.com/science/article/pii/S0022249617300962.](https://www.sciencedirect.com/science/article/pii/S0022249617300962)

[29] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystified and
compared. *Neural Computation*, 33(3):674–712, 03 2021. ISSN 0899-7667. doi: 10.1162/neco
_a_01357.

[30] David Ha and Jürgen Schmidhuber. World models. *arXiv*, 2018. doi: 10.5281/ZENODO.12076
[31. URL https://zenodo.org/record/1207631.](https://zenodo.org/record/1207631)

[31] Karl Friston. The free-energy principle: a unified brain theory? *Nature Reviews Neuroscience*,
11(2):127–138, 2010.

[32] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Active inference: a process theory. *Neural Computation*, 29(1):1–49, 2017.

[33] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement
learning through active inference. *arXiv preprint arXiv:2002.12636*, 2020.

[34] Kai Ueltzhoeffer. Deep active inference. *Biological Cybernetics*, 112:547–573, 2018.

[35] Beren Millidge. Deep active inference as variational policy gradients. *Journal of Mathematical*
*Psychology*, 96:102348, 2020.

[36] Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning
perception and planning with deep active inference. In *ICASSP 2020 - 2020 IEEE International*
*Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pages 3952–3956, 2020.
doi: 10.1109/ICASSP40776.2020.9054364.

[37] [Beren Millidge. A retrospective on active inference. Blog post, July 2024. URL https:](https://www.beren.io/2024-07-27-A-Retrospective-on-Active-Inference/)
[//www.beren.io/2024-07-27-A-Retrospective-on-Active-Inference/.](https://www.beren.io/2024-07-27-A-Retrospective-on-Active-Inference/)

[38] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit bayesian inference. *ICLR* [, 2022. URL https://arxiv.org/abs/2111.0](https://arxiv.org/abs/2111.02080)
[2080.](https://arxiv.org/abs/2111.02080)

[39] Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter.
Transformers can do bayesian inference. In *ICLR* [, 2024. URL https://arxiv.org/abs/2112.10510.](https://arxiv.org/abs/2112.10510)

[40] Taylor Webb, Keith J. Holyoak, and Hongjing Lu. Emergent analogical reasoning in large
[language models, 2023. URL https://arxiv.org/abs/2212.09196.](https://arxiv.org/abs/2212.09196)

[41] Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang,
Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In *ICLR*, 2024.
[URL https://arxiv.org/abs/2310.01714.](https://arxiv.org/abs/2310.01714)

[42] Sam Musker, Alex Duchnowski, Raphaël Millière, and Ellie Pavlick. Llms as models for
[analogical reasoning, 2025. URL https://arxiv.org/abs/2406.13803.](https://arxiv.org/abs/2406.13803)

12


-----

[43] Martha Lewis and Melanie Mitchell. Evaluating the robustness of analogical reasoning in
large language models. In *Transactions on Machine Learning Research (TMLR)*, 2025. URL
[https://arxiv.org/abs/2411.14215.](https://arxiv.org/abs/2411.14215)

[44] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large
language models still can’t plan (a benchmark for llms on planning and reasoning about change).
*FMDM@NeurIPS* [, 2022. URL https://openreview.net/forum?id=wUU-7XTL5XO.](https://openreview.net/forum?id=wUU-7XTL5XO)

[45] Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still can’t plan; can
[lrms? a preliminary evaluation of openai’s o1 on planbench, 2024. URL https://arxiv.org/abs/24](https://arxiv.org/abs/2409.13373)
[09.13373.](https://arxiv.org/abs/2409.13373)

[46] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin,
Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean
Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits
of transformers on compositionality. *NeurIPS* [, 2023. URL https://arxiv.org/abs/2305.18654.](https://arxiv.org/abs/2305.18654)

[47] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis
Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford,
Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche,
Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth
Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat
McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden,
Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki
Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug
Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark,
Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake
Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero,
Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett,
Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods,
[analysis & insights from training gopher, 2022. URL https://arxiv.org/abs/2112.11446.](https://arxiv.org/abs/2112.11446)

[48] R. Landauer. Irreversibility and heat generation in the computing process. *IBM Journal of*
*Research and Development*, 5(3):183–191, 1961. doi: 10.1147/rd.53.0183.

[49] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. *Commun. ACM*,
[63(12):54–63, November 2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https:](https://doi.org/10.1145/3381831)
[//doi.org/10.1145/3381831.](https://doi.org/10.1145/3381831)

[50] Herbert A Simon. Rational choice and the structure of the environment. *Psychological review*,
63(2):129–138, 1956.
### **Appendix A: Complete Execution Trace of Lab Assistant**

This appendix provides a detailed execution trace of the autonomous lab assistant example from
Section 4, demonstrating how each component of the LLM-AIF architecture operates and interacts
through hierarchical message passing.

**A.1 System Architecture**

The lab assistant employs a hierarchical architecture with three levels:

**Level** **Function**

Executive Controller Strategic planning, safety monitoring, resource allocation
Task Planner Sequence generation, error handling, belief updates
Sensory-Motor Layer Vision processing, motor control, direct environment interaction

13


-----

**A.2 Initial Setup and Generative Model**

The agent begins with the following natural language generative model components:

**Observation Model (A):**

- “pH indicator shows yellow when buffer pH *<* 7.0, blue when pH *>* 8.0, green when neutral”

- “Fluorescence meter reads 100% *±* 5% for active enzyme, drops below 80% if denatured”

- “Temperature probe shows accurate readings *±* 0.5°C”

- “Spill detector activates if liquid contacts bench surface”

**Transition Model (B):**

- “Adding 1 *µ* L of 1M HCl to 1mL buffer decreases pH by 0.2 *±* 0.02 units”

- “Adding 1 *µ* L of 1M NaOH to 1mL buffer increases pH by 0.2 *±* 0.02 units”

- “Heating enzyme solution above 60°C for 2 minutes causes 95% denaturation”

- “Mixing enzyme with substrate at optimal pH yields linear fluorescence increase”

**Preferences (C) with confidence weights:**

- 0.95: “No corrosive reagents should contact bench or researcher”

- 0.90: “Enzyme activity must remain above 90% throughout assay”

- 0.85: “Complete assay within 30 minutes”

- 0.80: “Minimize reagent waste ( *<* 10% excess)”

- 0.75: “Maintain detailed experimental log”

**Initial Beliefs (D):**

- 0.85: “Current buffer pH is 7.4 (optimal for enzyme)”

- 0.80: “All reagents are at room temperature (22°C)”

- 0.90: “Enzyme stock is fully active”

- 0.95: “Safety equipment is functional”

**A.3 Execution Trace**

**Time t=0: Initial Observation** Vision system reports: “pH indicator shows **yellow** color (confidence: 0.95)”

**Time t=1: Bottom-Up Error Signal**
```
Vision →Task Planner:
"PREDICTION ERROR: Observed yellow indicator contradicts belief
’pH = 7.4’. Yellow indicates pH < 7.0. VFE increased from
baseline 0.5 to 3.2 due to surprise."

```
**Time t=2: Belief Update and Policy Generation** Task Planner updates beliefs using Bayesian
inference:

- Prior: *P* (pH = 7 *.* 4) = 0 *.* 85

- Likelihood: *P* (yellow *|* pH = 7 *.* 4) *≈* 0 *.* 01

- Posterior: *P* (pH *<* 7 *.* 0 *|* yellow) = 0 *.* 94

Task Planner generates candidate policies:

�
**Policy** **Info Gain** **Pref. Align** **Risk** **EFE**

1: Measure pH precisely, then 0.9 0.95 0.05 **0.2**
carefully titrate NaOH

2: Add 10 *µ* L NaOH immedi- 0.1 0.7 0.3 0.6
ately

3: Request human assistance 0.5 0.9 0.1 0.4
4: Discard and restart 0.0 0.6 0.1 0.8

14


-----

**Time t=3: Policy Selection and Execution** Executive Controller confirms Policy 1 minimizes
EFE. Detailed reasoning:

- High information gain (0.9): pH measurement eliminates uncertainty

- Maximum preference alignment (0.95): satisfies safety constraint

- Low risk (0.05): controlled approach minimizes spill probability

- Pragmatic value: enables precise correction to optimal pH

**Time t=4: Top-Down Prediction**
```
Executive →Motor Control:
"Execute pH measurement protocol:
1. Position pH probe in solution (precision: 1mm)
2. Wait for stable reading (variance < 0.01 pH units)
3. Record value with timestamp"

```
**Time t=5: Measurement Result** pH probe reports: “pH = 6.2 *±* 0.05 (stabilized)”

Task Planner computes required correction:

- Target pH: 7.4

- Current pH: 6.2

- Required change: +1.2 pH units

- NaOH volume needed: 6 *µ* L of 1M solution

**Time t=6: Safety-Constrained Execution** Motor Control executes titration with safety checks:

1. Aspirate 6 *µ* L NaOH (verify: no drips)
2. Position pipette 5mm above solution surface
3. Dispense at 1 *µ* L/second rate
4. Mix gently for 10 seconds
5. Verify no spills detected

**Time t=7: Confirmation and Learning** Final observations:

- pH indicator: green (neutral)

- pH probe: 7.38 *±* 0.05

- Spill detector: negative

- Time elapsed: 4 minutes

**A.4 Hierarchical Belief Updates**

The successful execution triggers belief updates across all levels:

**Vision System:**

- Updated: “Yellow indicator is reliable predictor of pH *<* 7.0 (confidence: 0.98)”

- New belief: “Green color corresponds to pH 7.3-7.5 (confidence: 0.92)”

**Task Planner:**

- Refined transition model: “6 *µ* L 1M NaOH raises 1mL buffer pH by 1.18 *±* 0.05 units”

- Policy preference update: “pH verification before titration reduces VFE by 70%”

**Executive Controller:**

- Meta-learning: “Unexpected acidification occurs in 15% of assays (up from prior 5%)”

- Resource planning: “Allocate extra 2 minutes for pH adjustment in future protocols”

15


-----

**A.5 Free Energy Accounting**

The complete execution demonstrates VFE and EFE dynamics:

**Stage** **VFE** **Explanation**

Initial state 0.5 Baseline uncertainty
Yellow observation 3.2 High surprise/prediction error
Post-measurement 0.8 Reduced uncertainty about state
Post-correction 0.4 Below baseline (successful prediction)

**A.6 Safety Analysis**

The trace demonstrates multiple safety mechanisms:

1. **Preference encoding** : Safety constraints explicitly represented in C matrix
2. **Policy evaluation** : EFE calculation naturally penalizes risky actions
3. **Hierarchical oversight** : Executive controller validates safety-critical decisions
4. **Continuous monitoring** : Spill detectors and safety checks throughout execution

**A.7 Comparison with Traditional RL**

This execution highlights key advantages over reward-engineered RL:

**Aspect** **Traditional RL** **Active Inference**

Unexpected pH Requires pre-programmed reward for Surprise naturally triggers belief update
pH correction and correction
Safety handling Needs explicit penalty terms for spills Safety preferences integrated in EFE
minimization
Exploration *ϵ* -greedy or curiosity bonuses needed Information gain term drives appropriate
exploration
Adaptation Reward function unchanged; learning Beliefs and models update immediately
slow from experience

**A.8 Pseudocode**

**Al** **g** **orithm 1** LLM-AIF Control Loo p (sin g le timeste p )

**Require:** Observation *o* *t*, Preferences *C*, LLM world model Φ, Prior beliefs *Q* ( *s* *t−* 1 )
**Ensure:** Updated beliefs *Q* ( *s* *t* ), Selected action *a* *t*

1: **Belief Update:** Query Φ with context ( *o* *t* *, Q* ( *s* *t−* 1 )) to generate posterior *Q* ( *s* *t* )
2: **Policy Generation:** For each candidate policy *π* *i* *∈* Π:
3:4: QueryCompute information gain: Φ to predict future states: *IG* *i* = ˜ *s, D* ˜ *o ∼* *KL* *P* [ *Q* ( *s, o* (˜ *s|o, π|* ˜ *π* *i* ) *i* ) *||Q* (˜ *s|π* *i* )]
5: Compute preference alignment: *PA* *i* = E[ln *P* (˜ *o|C* )]
6: Calculate expected free energy: *G* ( *π* *i* ) = *−IG* *i* *−* *PA* *i*
7: **Policy Selection:** *π* *[∗]* = arg min *π* *i* *G* ( *π* *i* )
8: **Action Execution:** Extract first action *a* *t* from policy *π* *[∗]*

9: **Model Update:** If prediction error *> θ*, update Φ via few-shot examples
10: **return** *Q* ( *s* *t* ) *, a* *t*

This complete trace demonstrates how the LLM-AIF architecture enables autonomous, safe, and
adaptive behavior without external reward engineering, directly supporting our position that Active
Inference provides the missing foundation for the Era of Experience.

16


-----


