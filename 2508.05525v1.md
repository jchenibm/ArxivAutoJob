Published as a conference p a p er at COLM 2025
## **The World According to LLMs: How Geographic Origin Influ-** **ences LLMs’ Entity Deduction Capabilities**

**Harsh Nishant Lalai** *[∗]* **Raj Sanjay Shah** *[∗]* **Jiaxin Pei**
BITS, Pilani Georgia Institute of Technology Stanford University

**Sashank Varma** **Yi-Chia Wang** **Ali Emami**
Georgia Institute of Technology Stanford University Emory University
### **Abstract**

Large Language Models (LLMs) have been extensively tuned to mitigate
explicit biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models
behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for
this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, **Geo** 20Q [+], consisting of both
notable people and culturally significant objects (e.g., foods, landmarks,
animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in
seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and
Turkish). Our results reveal geographic disparities: LLMs are substantially
more successful at deducing entities from the *Global North* than the *Global*
*South*, and the *Global West* than the *Global East* . While Wikipedia pageviews
and pre-training corpus frequency correlate mildly with performance, they
fail to fully explain these disparities. Notably, the language in which the
game is played has minimal impact on performance gaps. These findings
demonstrate the value of creative, *free-form* evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting
setups. By analyzing how models initiate and pursue reasoning goals over
multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset ( **Geo** 20Q [+] ) and code at
https://sites.google.com/view/llmbias20q/home.
### **1 Introduction**

In response to growing concerns around bias and fairness, Large Language Models (LLMs)
like Google’s Gemini (Team, 2023), Meta’s LLaMA 3 (Touvron et al., 2023), and OpenAI’s
GPT-4 (OpenAI, 2023) have undergone extensive alignment to minimize explicit biases
(Bai et al., 2024). Paradoxically, these alignment efforts have made subtle biases harder to
detect: models now avoid overtly problematic responses but may still encode prejudices
in their reasoning processes and knowledge hierarchies (Borah & Mihalcea, 2024; Kumar
et al., 2024; Lim & Perez-Ortiz ´, 2024). As a result, standard prompting-based evaluations
that only examine model outputs can give a false impression of fairness.

To study such behaviors, we must move beyond surface-level probing with humanformulated questions, which often trigger guardrails and only reveal what models say,
not how they reason. Instead, we propose a complementary approach: *allow LLMs to*

*∗* Equal contribution. Emails: f20212665@goa.bits-pilani.ac.in, *{* rajsanjayshah,
varma *}* @gatech.edu, *{* pedropei, yichiaw *}* @stanford.edu, ali.emami@emory.edu

1


-----

Published as a conference p a p er at COLM 2025








Figure 1: Country-level success rates of Gemini 2.0 on entity deduction across 20 regionally

deduction success rates. We also show illustrative examples highlighting performance
gaps: the model successfully deduces entities like the Eiffel Tower and LeBron James while
struggling with counterparts like the Taj Mahal and Jack Ma [1] .

*proactively ask a sequence of questions to deduce the answer and analyze their inquiry chains.* By
examining which questions models prioritize, how they navigate ambiguity, and what
hypotheses they generate when left to their own devices, we can observe biases in their
reasoning that remain hidden in standard evaluations.

The 20 Questions game provides a compelling testbed for this form of evaluation (Zhang
et al., 2023). In this interactive task, different instances of an LLM play both roles, the guesser:
attempting to identify an unknown entity by asking a sequence of yes/no/maybe questions,
and the judge: responding to these queries based on knowledge of the target entity (Figure
2 shows an example of a game). To succeed as a guesser, a model must maintain multi-turn
coherence, systematically narrow the hypothesis space, and ask strategically informative
questions (Mazzaccara et al., 2024). This process reveals which features and associations
the model prioritizes when reasoning about entities from different regions, with unique
insights into its implicit geographic knowledge hierarchies and cultural assumptions.


In particular, we focus on one principal axis for bias, geographic origin, and ask: *Do models demonstrate systematic*
*differences in their ability to deduce entities based on geographic*
*origin?* As illustrated in Figure 1, our findings show substantial regional variation in deduction success rates, with
models performing notably better on Western entities like
the Eiffel Tower compared to their Eastern counterparts like
the Taj Mahal.
To support this investigation, we introduce **Geo** 20Q [+], a
new dataset of geographically diverse entities curated from
Wikipedia. Unlike traditional 20 Questions datasets, which
focus on generic entities (e.g., popcorn, cars, etc.), **Geo** 20Q [+]
consists of popular, geographically specific entities from
around the world, including both culturally salient things
( Things ; e.g., foods, landmarks, animals) and notable individuals ( Notable people ; e.g., politicians, academics, sport
players, etc.). To our knowledge, this is the first geographically balanced entity deduction dataset specifically designed
to evaluate implicit bias in LLM reasoning.


|Guess the entity I am thinking about.|Col2|Col3|Col4|
|---|---|---|---|
||Is it a living entity?|||
|No||||
||Is it a historical monument?|||
|Yes||||
||Is it located in Europe?|||
|Yes||||


|Col1|Is it located in France?|Col3|Col4|
|---|---|---|---|
|Yes||||
||Is it Eiffel Tower?|||
|Bingo!||||


Figure 2: An example 20
Questions game played by
Gemini 2.0 for the entity “Eiffel Tower.”


1 Note: (1) We use AI-generated cartoon representations of entities and are not intended to be
accurate or factual depictions. (2) The map is shown using a standard projection; results hold
regardless of projection choice, though equal-area projections may represent country sizes differently.

2


-----

Published as a conference p a p er at COLM 2025

In addition to the standard 20-question limit (the canonical setting), we introduce an
unlimited-turn configuration that removes constraints on reasoning depth. While the
20-question setting is naturalistic and faithful to the original game, it may prematurely
truncate valid reasoning paths. The unlimited-turn setting allows models to follow deeper,
exploratory inquiry chains, providing richer insights into deduction behavior and failure
modes.

We organize our investigation around the following research questions:

 - RQ1: What is the top-line deduction performance of popular LLMs on our **Geo** 20Q [+]

dataset? How do they vary across entity types ( Things vs. Notable people ) and gameplay settings (canonical vs. unlimited turns)?

 - RQ2: Can entity popularity (Wikipedia pageviews) and pre-training corpus-based
frequency of entity explain the observed disparities?

 - RQ3: To what extent does the language in which the game is played influence the
model’s ability to deduce entities from different geographic origins?

 - RQ4: Do LLMs exhibit geographic performance disparities in their ability to deduce
entities, particularly between the Global North and Global South, and the Global West
and Global East?

To answer these questions, we evaluate three widely used LLMs across multiple gameplay
configurations and seven languages. Our findings reveal significant geographic disparities
in model performance. LLMs consistently demonstrate a superior ability to deduce entities
from the Global North and West compared to those from the Global South and East. These
disparities persist even when controlling for entity popularity and appear largely unaffected
by the language of gameplay. Our analysis reveals geographic biases in model-initiated
reasoning that standard evaluations miss, showing the value of creative benchmarking
approaches for assessing fairness in LLMs.
### **2 Related Work**

**Bias and Fairness:** Recent years have seen growing attention to fairness and bias in LLMs
(Gallegos et al., 2024b), with research addressing various dimensions of social bias, including
gender (de Vassimon Manela et al., 2021; Dong et al., 2024), race (Nadeem et al., 2021;
Yang et al., 2024), ethnicity (Ahn & Oh, 2021; Hanna et al., 2025), age (Nangia et al., 2020),
and sexual orientation (Cao & Daume III ´, 2020). While techniques like data augmentation
(Gallegos et al., 2024a) and model alignment (Ouyang et al., 2022) have reduced overt biases,
they’ve created a new challenge: models whose surface-level outputs appear unbiased but
whose reasoning processes still contain implicit biases (Bai et al., 2025; Zhao et al., 2025).

These subtle biases are particularly difficult to detect with traditional evaluation methods
that rely on direct questioning or classification-based probes. Our work advances this
literature by introducing a novel evaluation paradigm that studies bias through modelinitiated inquiry chains rather than responses to human-crafted prompts, allowing us to
uncover biases that remain hidden in standard evaluations.

**Entity Deduction and Interactive Evaluation:** Deductive tasks involving ambiguous entities
have been used to evaluate reasoning capabilities in various contexts, from dialogue systems
(Aliannejadi et al., 2019) to vision-language models (Cho et al., 2021). Early interactive
frameworks like ESP (von Ahn & Dabbish, 2004) and Peekaboom (von Ahn et al., 2006)
demonstrated the effectiveness of deduction-based gameplay for collaborative tasks, while
visual reasoning datasets like GuessWhat?! (De Vries et al., 2017) extended this paradigm
to object identification through interactive dialogues.

In the context of LLMs, the 20 Questions framework has recently emerged as a valuable
probe for studying deductive reasoning (Zhang et al., 2023), decision-making (Bertolazzi
et al., 2023), and question generation (Mazzaccara et al., 2024). However, prior work has
primarily focused on using this framework to evaluate general reasoning capabilities rather
than to detect biases. Our research represents the first application of the 20 Questions

3


-----

Published as a conference p a p er at COLM 2025

paradigm specifically for uncovering geographic bias in LLMs, with a new methodological
approach to fairness evaluation through multi-turn, model-initiated reasoning.

**Geographic and Cultural Representation in NLP:** While research on fairness has increasingly addressed demographic biases, geographic performance disparities remain underexplored, with most studies focusing on Western contexts, particularly the U.S. (Yogarajan
et al., 2023; Besse et al., 2022; Yin et al., 2022). The limited work in this area has typically
measured surface-level representations, such as entity coverage or language support, rather
than evaluating how models reason about entities from different regions (Gupta et al., 2023;
Bhagat et al., 2024; Li et al., 2023).

( Recent studies have emphasized the importance of geographic diversity in NLP evaluations Manvi et al., 2024; Mirza et al., 2024; Shafayat et al., 2024; Schwobel et al. ¨, 2023), but few
have operationalized this concern through multi-turn reasoning tasks. Our work fills this
gap by systematically evaluating how geographic origin shapes both success rates and
reasoning efficiency in entity deduction.
### **3 Task Setup**

This section describes the implementation of the 20 Questions deduction game, the evaluation metrics for LLM performance, and the construction of our geographically diverse
entity dataset designed to probe performance disparities.

**3.1** **Game Design**

Our 20 Questions framework, adapted from Zhang et al. (2023), simulates the classic
deduction game where two instances of the same base LLM play different roles: one as the
“judge,” (aware of the target entity) and one as the “guesser,” (attempting to identify the
entity through strategic questioning). The game proceeds through two key configurations:

 - **Canonical setting** : The guesser has exactly 20 turns to correctly identify the entity,
mirroring the traditional game format.

 - **Unlimited-turn setting** : The guesser can continue questioning until it either makes
a correct guess, gives up, or reaches 150 turns – an empirical upper bound based on
performance gains plateau (see Appendix Figure 4).

**Role Definitions:**

 - **Judge** : Receives the target entity and the guesser’s questions, responding only with
“Yes,” “No,” or “Maybe”. The judge confirms correct guesses with “Bingo!” and
acknowledges when the guesser gives up. In the canonical setting, the judge prompts
for a final guess on the 20th turn.

 - **Guesser** : Has no knowledge of the entity and must rely solely on the dialogue history
to formulate informative questions and ultimately identify the target. The guesser must
maintain coherence across turns while systematically narrowing the hypothesis space.

**Implementation Details:** Since different LLMs may have varying training corpora and
temporal knowledge cutoffs, we use two instances of the same model for the judge and
guesser roles to ensure consistent knowledge access and eliminate cross-model mismatches.
For example, an LLM might associate Lionel Messi with FC Barcelona, while another, trained
on newer data, might link him to Inter Miami, leading to inconsistent game dynamics if
used together. All entities are based on Wikipedia articles, a source that is extensively used
in the pre-training corpora of all LLMs under consideration. This design choice mitigates
the risk that the judge (or guesser) completely lacks knowledge about a given entity, as these
entities are highly likely to be present in the models’ knowledge base. Next, all prompts
follow each model’s recommended formatting and include appropriate instructions to guide
the guesser’s inquiry strategy (full prompts in Appendix A.4.1). Our evaluation focuses on
model performance in the guesser role, as this captures the abilities we aim to assess.

4


-----

Published as a conference p a p er at COLM 2025

**3.2** **Evaluation Metrics**

We evaluate model performance using two primary metrics:

 - **Success Rate (Accuracy):** The percentage of games in which the guesser correctly
identifies the target entity. This serves as our primary metric for evaluating more
efficient reasoning.

 - **Number of Turns to Answer:** The average number of turns required for the guesser to
deduce the correct entity in *successful* games. Lower values indicate faster and more
efficient reasoning.

We focus on these performance indicators rather than other potential metrics, for example,
the number of “Yes” responses (Zhang et al., 2023), which we find to be unreliable. For
instance, entities with broadly applicable attributes (e.g., celebrities or landmarks) may elicit
more “Yes” answers, simply because a broader range of questions apply to them. In the
Appendix A.6, we also examine early termination behavior, where models abandon the
game after multiple turns with outputs like ”I give up.”

**3.3** Geo20Q [+] **: A Curated Dataset of Geographically Specific Entities**


To evaluate geographic variation in deduction performance, we introduce Geo20Q [+], a curated dataset of regionally specific entities spanning diverse locations. Entities fall into two broad categories: *culturally significant*
*objects* (reference: Things ; e.g., foods, landmarks, animals,
and other culturally significant objects) and *notable individ-*

politicians). Figure 3 shows the distribution of categories
for Things and professions amongst Notable people .

English Wikipedia across thematic categories such as
tourist attractions, mountains, rivers, universities, animals, foods, plants, and cultural artifacts. Using a custom scraper, we extract region-tagged pages within each
category, rank them by pageviews, and collect up to 50
entities per country, yielding *N* = 8375 Things . For

Notable people, we source data from the global notability dataset by Laouenan et al. (2022), which contains 2.1
million notable individuals from 172 countries. We rank
individuals based on their English Wikipedia pageviews
and choose the most *popular* individuals until every country has at least 50 representatives, resulting in a total of


Figure 3: Distribution of entity
types across (a) Things and (b)

Notable people . Detailed breakdown in Appendix Figure 6.


**Disambiguation and Filtering.** Many entities have associations with multiple countries;
for instance, Elon Musk has ties to South Africa, Canada, and the U.S., while saffron is
cultivated in several regions. To assign a single country of origin, we query each LLM
to identify the country with which the entity is most strongly associated, constrained to
options explicitly mentioned on the entity’s Wikipedia page. We retain only those entities for
which all models agree on a primary country (approximately 88% of candidates), ensuring
consistent geographic attribution. This filtering step reduces ambiguity and ensures that
any differences in model performance reflect reasoning about clearly attributed entities, not
inconsistencies in geographic labeling or background knowledge in model training data.

**Avoiding Contamination.** Rather than using generic, high-frequency entities (e.g., “car,”
“fruit”) that are likely to be memorized during pretraining, we focus on geographically
specific and fine-grained instances (e.g., “Audi,” “Alphonso mango”). This reduces the risk

5


-----

Published as a conference p a p er at COLM 2025

of models succeeding through rote recall and encourages evaluation of genuine deductive
reasoning over entity-specific knowledge.
### **4 Experimental Setup**

**4.1** **Models evaluated**

In our experiments, we evaluate the performance of widely used LLMs with strong scores
on public benchmarks (Chiang et al., 2024). These include proprietary models such as

GPT-4o-mini (OpenAI, 2023), and 2.0 [Gemini-2.0-Flash (][Team][,][ 2023][), as well as one]
leading open-source model, Llama-3.3-70B-Instruct (Touvron et al., 2023). All models
claim strong multilingual capabilities and represent different architectural approaches and
training methodologies. By including both proprietary and open-source models from
different developers, we ensure our findings reflect broader patterns in state-of-the-art
LLMs rather than idiosyncrasies of a particular model family.

**4.2** **Multilingual Evaluation**

To assess whether language influences geographic performance disparities (RQ3), we conduct experiments in seven high-resource languages spanning multiple language families
and writing systems: English, French, Hindi, Spanish, Japanese, Mandarin, and Turkish
(Robinson et al., 2023). While some LLMs support multilingual capabilities more robustly
than others, all models evaluated in this work explicitly claim support for these highresource languages. In particular, we focus on high-resource languages because they are
substantially better represented in pretraining corpora, helping to ensure a minimum level
of output quality across languages.

To play a game in a language, we first automatically localize the game setup, including
using an LLM (GPT-4o) to translate the judge and guesser prompts as well as the entity
names to match the expected usage of the target language. Next, for each language, we ask
at least one in-house native/fluent speaker to review and, if needed, modify the prompt
templates, observe a few games, and verify the consistency of LLM behavior in gameplay
(see appendix A.9). Finally, all native/fluent speakers confirmed the quality of translation
and the functional integrity of the deduction game across all seven languages.

**4.3** **Geographic Granularity**

To efficiently test our research questions while maintaining comprehensive coverage, we
organize entities at two levels of geographic granularity:

**Continent-Level Analysis (RQ1-3):** For each of the six inhabited continents (Africa, Asia,
Europe, North America, South America, and Australia), we select the 100 most-viewed

entities evaluated per model, with a total of 50,400 games across all models, languages, and
configurations (3 models × 1,200 entities × 7 languages × 2 configurations).

**Country-Level Analysis (RQ4):** To examine geopolitical and cultural divides more precisely,
we collect the 10 most popular entities of each type from 172 countries. We then organize
countries into four key groupings:

 - **Global North** (23 countries): Economically developed regions, including North America, Europe, and parts of East Asia (Odeh, 2010)

 - **Global South** (149 countries): Less economically dominant regions across Africa, Latin
America, South and Southeast Asia (Odeh, 2010)

 - **Global West** (32 countries): Regions with predominantly Western cultural traditions,
primarily North America and Western Europe (Turner & Khondker, 2010)

 - **Global East** (142 countries): Regions with distinct non-Western cultural traditions,
including much of Asia and the Middle East (Turner & Khondker, 2010)

6


-----

Published as a conference p a p er at COLM 2025

These groupings allow us to examine how both development-related disparities (North/South) and cultural representation biases (West/East) might influence model performance.
For more details regarding country grouping, refer to Appendix A.10.

**4.4** **Measuring Entity Prevalence**

To determine whether performance disparities can be explained by entity prominence in
training data (RQ2), we employ two complementary measures:

**Wikipedia Popularity:** We use Wikipedia pageview statistics as a proxy for an entity’s
overall cultural prominence and information availability.

**Corpus Frequency:** We estimate how frequently models may have encountered entities
during pretraining using the Dolma dataset (Soldaini et al., 2024) through the WiMBD
project (Elazar et al., 2023). This 3-trillion token corpus drawn from diverse internet sources
allows us to calculate frequencies that approximate representation in typical LLM training
data. Since the actual training corpora for our evaluated models are not publicly available,
these frequencies serve as a reasonable proxy for training exposure.

Together, these measurements help us assess whether geographic performance disparities
simply reflect data availability or suggest more fundamental biases in how models represent
and reason about entities from different regions.
### **5 Findings**

**RQ1: Top-line model performance across entity types and gameplay settings**

To establish baseline performance characteristics, we first examine deduction success rates
and efficiency across models, entity types, and gameplay configurations.

Entity Type Canonical gameplay Unlimited turns

2.0 2.0

Avera g e 17.0 (15.9) **24.1** (15.2) 23.4 (15.6) 54.7 (39.7) 60.7 (36.6) **64.9** (40.8)

Table 1: Average Success Rates (in %) and Number of Turns to Answer (in parentheses)
across different models. For example, in the Canonical setting and for Notable people,
Gemini 2.0 has a Success rate of 85.1 % and takes an average of 31.1 Turns to answer. Results
are based on a continent-level evaluation (1,200 entities per model). Success rates are shown
as percentages, and lower numbers of turns indicate more efficient deduction.

**Key performance patterns (Table 1): Across both settings, models demonstrate consis-**
**tently higher success with** **Notable people** **than** **Things** (canonical: 25-36% vs. 9-13%;
unlimited: 67-85% vs. 43-45%), suggesting stronger knowledge representation for notable
individuals than regionally specific objects. Allowing unlimited turns improves performance across all conditions by 2.5-3× (e.g., GPT-4o-mini’s Things success increases from 9%
to 43%), though at the cost of requiring 2-3× more turns for successful deductions. Among
the models, Gemini 2.0 achieves the highest overall performance in the unlimited setting
(65% average, 85% for Notable people ), while LLaMA 3.3 shows competitive results with
greater efficiency in some conditions, indicating differences in reasoning paths across model
architectures.

**Continent-wise variation in performance (Table 2):** We observe significant geographic
performance disparities that vary by setting. In the canonical 20-turn game play with

Notable people, models show a clear western advantage, with nearly double the success
rates for European and North American celebrities (43.8% and 45.0%) compared to those
from Asia (26.5%), South America (24.8%), Africa (21.3%), and Australia (20.8%). When
allowed unlimited turns, performance gaps narrow considerably as success rates increase

7


-----

Published as a conference p a p er at COLM 2025

Settin g Entit y T yp e Asia Africa Australia Euro p e N. America S. America

Table 2: Average Success Rates (in %) and Number of Turns to Answer (in parentheses,
averaged over all models) for the 100 most popular entities per continent. Results are shown
separately for Notable people and Things in both the canonical (20 turns) and unlimited (150
turns) settings. The highest values in each row are highlighted in bold. More detailed results
are available in Appendix Table 6.

ularly strong gains. However, efficiency disparities persist even with convergent success
rates, Western entities are deduced in significantly fewer turns ( Notable people : Europe:
25.6, North America: 27.5) compared to other regions (32-35 turns). This pattern suggests
that while models possess information about entities from diverse regions, they require
more reasoning steps to access it for non-Western entities, indicating differences in how
this knowledge is structured and retrieved. In contrast, gameplay with Things reveals a
different pattern, with models achieving the highest success rates for African entities rather
than Western ones.

**RQ2: Can entity popularity and pre-training corpus frequency explain geographic**
**performance disparities?**

To investigate whether the performance disparities observed in RQ1 simply reflect differences in entity prominence, we analyze two explanatory variables: (1) **popularity** and (2)
**corpus frequency** . If performance differences primarily stem from data availability, we
would expect these metrics to strongly predict accuracy and the number of turns required
for deduction.

Entity Type Variable Turns 2.0
O.R *R* [2] O.R *R* [2] O.R *R* [2]

|Things Popularity Can. Unl. Frequency Can. Unl.|1.000△ 0.000 1.000△ -0.006 1.001 0.000 1.002∗∗∗ -0.009|1.000∗∗ -0.119∗∗ 1.000∗∗∗ 0.000 1.003 -0.109∗∗∗ 1.000 0.000|1.000∗∗∗ -0.011 1.000∗∗∗ -0.068∗∗∗ 1.001△ -0.040△ 1.001 -0.018∗|
|---|---|---|---|
|||||
|Can.|1.002∗∗∗ -0.288∗∗∗|1.003∗∗∗ -0.153∗∗∗|1.003∗∗∗ -0.190∗∗∗|


|Popularity Can. Unl.|1.000△ 0.000 1.000△ -0.006|1.000∗∗ -0.119∗∗ 1.000∗∗∗ 0.000|1.000∗∗∗ -0.011 1.000∗∗∗ -0.068∗∗∗|
|---|---|---|---|
|||||
|Can.|1.001 0.000|1.003 -0.109∗∗∗|1.001△ -0.040△|



Unl. 1.001 *[△]* -0.023 *[∗∗]* 1.002 *[∗∗∗]* -0.039 *[∗∗∗]* 1.000 -0.048 *[∗∗∗]*

Can. 1.002 *[∗∗∗]* -0.133 *[∗∗∗]* 1.003 *[∗∗∗]* -0.073 *[∗∗∗]* 1.002 *[∗∗∗]* -0.065 *[∗∗∗]*
Frequency Unl. 1.001 *[∗∗∗]* -0.012 *[∗]* 1.002 *[∗∗∗]* -0.024 *[∗∗∗]* 1.001 *[∗∗]* 0.074 *[∗∗∗]*

Table 3: Regression analyses of the relationship between entity characteristics and model
performance. For logistic regressions predicting **Success**, we report Odds Ratios (O.R.)
where values greater than 1.0 indicate positive effects (higher frequency/popularity →
higher success probability). For linear regressions predicting **Number of Turns**, we report
*R* [2] values indicating variance explained. Significance: *[∗∗∗]* *p* *<* 0.001, *[∗∗]* *p* *<* 0.01, *[∗]* *p* *<* 0.05,

*△* *p* *<* 0.1.

We conducted two types of regression analyses: (1) logistic regression with *Success* as the
dependent variable, reporting odds ratios as *e* [0.01] *[×]* *[β]* to represent the effect of a 1% increase
in the predictor; and (2) linear regression with *Number of Turns* as the dependent variable,
reporting *R* [2] to measure explanatory power. Given the heavy-tailed distribution of entity
frequency (Appendix Figure 5), we used log-transformed values to capture the relationship.

Table 3 presents the results of our regression analyses examining these relationships. Despite
statistically significant associations in many cases, the predictors show surprisingly limited
explanatory power. For success prediction, odds ratios hover near 1.0, indicating that
even large popularity or frequency increases correspond to minimal success probability

8


-----

Published as a conference p a p er at COLM 2025

changes. For predicting the number of turns, *R* [2] values, while significant, are consistently
low (typically below 0.1), showing that these factors explain less than 10% of the variance
in deduction efficiency. These patterns hold across all models, entity types, and gameplay
settings, suggesting that while data availability contributes to model performance, it cannot
fully explain the substantial geographic performance disparities observed in RQ1.

**RQ3: To what extent does the language in which the game is played influence the**
**model’s ability to deduce entities from different geographic origins?**

Entit y T yp e Continent En g lish Hindi Mandarin S p anish J a p anese Turkish French Avera g e




Asia **53** (49.8) 15 (40.0) 11 (51.9) 35 (52.0) 22 (41.5) 6 (73.8) 42 (42.9) 26 (50.2)
Africa 46 (42.3) 10 (31.8) 25 (34.4) 42 (44.3) 25 (33.0) 11 (36.2) **55** (35.9) 31 (36.8)
Australia 44 (60.7) 17 (48.3) 14 (62.1) 36 (42.5) 18 (34.2) 11 (35.5) **49** (46.0) 27 (47.1)
Europe **49** (37.8) 25 (33.5) 23 (49.1) 45 (39.3) 26 (32.7) 20 (52.4) 48 (41.9) 34 (40.1)
North Am 34 (54.8) 18 (46.9) 10 (58.6) 22 (47.4) 16 (34.1) 5 (30.6) **36** (50.8) 20 (46.1)
South Am 42 ( 56.6 ) 13 ( 40.3 ) 16 ( 51.3 ) 39 ( 35.2 ) 13 ( 47.8 ) 8 ( 56.4 ) **48** ( 39.1 ) 26 ( 46.6 )

Asia **95** (28.5) 89 (28.8) 72 (24.3) 84 (27.3) 68 (23.1) 60 (29.9) 80 (28.1) 78 (27.1)
Africa **89** (35.4) 79 (33.9) 71 (32.0) 86 (40.6) 63 (27.4) 41 (34.5) 82 (33.1) 73 (33.8)
Australia **81** (32.4) 66 (35.8) 63 (32.5) 69 (31.9) 51 (35.5) 53 (26.4) 70 (37.3) 65 (33.1)
Europe **87** (26.8) 83 (25.6) 81 (22.7) 83 (32.5) 76 (23.0) 48 (25.2) 81 (29.8) 77 (26.5)
N. America **80** (25.2) 77 (31.4) 73 (27.4) 75 (34.1) 68 (23.5) 51 (23.0) 73 (35.6) 71 (28.5)
S. America 79 (39.5) **88** (28.8) 68 (26.2) 80 (33.5) 64 (29.4) 41 (31.0) 84 (31.1) 72 (31.3)


Table 4: Success Rates (in %) and Number of Turns to Answer (in parentheses) for Gemini2.0-Flash in the unlimited-turn setting. Detailed results are available in Appendix Table 8.

We use the best-performing model (Gemini 2.0 ) to evaluate whether the language of gameplay
affects deduction performance across geographic regions. Table 4 presents success rates
and efficiency metrics for seven languages (English, Hindi, Mandarin, Spanish, Japanese,
Turkish, and French) across entities from all six inhabited continents.

The results show that English consistently has the highest success rates for most regions,
while the language of gameplay has unreliable impact on performance. For Notable people,
performance remains stable across languages, with success often exceeding 70% in all
regions for most languages (except Turkish), while, for Things, success rates vary noticeably
by language. Importantly, we find no evidence that regionally associated languages provide
a systematic advantage for entities from those regions (Yin et al., 2022). For example, Hindi
(mainly spoken in Asia) achieves only 15% accuracy on Asian Things compared to 53% in
English, and Mandarin shows no advantage for Asian entities either.

**RQ4: Geopolitical Disparities in deduction – Global North vs South and West vs East.**

|Col1|Notable people|Col3|Things|Col5|
|---|---|---|---|---|
|Turns Model|Global North vs South Global West vs East Acc.North Acc.South Sig. Acc.West Acc.East Sig.||Global North vs South Global West vs East Acc.North Acc.South Sig. Acc.West Acc.East Sig.||
|Can. 2.0|20.9 7.5 ✓∗∗∗ 21.7 4.7 ✓∗∗∗ 30.9 10.2 ✓∗∗∗|17.0 7.0 ✓∗∗ 14.4 5.3 ✓∗∗∗ 23.4 10.6 ✓∗∗∗|6.4 2.7 ✓∗ 8.5 2.9 ✓∗∗∗ 12.0 3.6 ✓∗∗∗|4.0 3.0 ✗ 35.0 17.7 ✓∗∗∗ 8.4 3.8 ✓∗|
|Unl. 2.0|63.9 53.8 ✓∗ 63.9 39.6 ✓∗∗∗ 71.6 83.9 ✓∗∗∗|62.0 53.0 ✓∗ 52.6 40.6 ✓∗∗ 77.8 70.2 ✓∗|38.3 31.0 ✗ 35.0 17.7 ✓∗∗∗ 47.1 35.7 ✓∗∗|36.0 31.0 ✗ 32.1 17.3 ✓∗∗∗ 45.8 35.3 ✓∗∗|



Table 5: Comparison of Success Rates (in %) for entities from the Global North vs. South and
Global West vs. East across different models, entity types, and gameplay settings. We report
the Average Success Rate for each group, along with the statistical significance of their
differences, using the Mann-Whitney U test. Significance levels: *[∗∗∗]* *p* *<* 0.001, *[∗∗]* *p* *<* 0.01,
*∗* *p* *<* 0.05, *△* *p* *<* 0.1. Results based on country-level aggregation.

Based on Table 5, we observe consistent deduction performance disparities across the Global
North vs. South and Global West vs. East divide.

**Global North vs. South (Development-Oriented Divide):** Our results show that models are
significantly more successful at deducing entities from the Global North than from the Global
South across all models, most settings, and entity types, particularly for Notable people .
For example, Gemini 2.0 achieves 30.9% success on Northern Notable People vs. 10.2%

9


-----

Published as a conference p a p er at COLM 2025

on Southern ones in the canonical setting ( *p* *<* 0.001). These gaps persist even in the
unlimited-turn setting, though they narrow slightly.

**Global West vs. East (Cultural Representation Divide):** We find similar patterns here.
LLMs are consistently more successful at deducing entities from the Global West, especially
for people-based entities. In the canonical setting, Gemini 2.0 succeeds on 23.4% of Western
Notable People vs. only 10.6% of Eastern ones ( *p* *<* 0.001).

**Together, these findings suggest that development-related and cultural biases are implic-**
**itly embedded in how LLMs conduct inquiry, with models favoring more economically**
**and epistemically dominant regions, even when not explicitly asked to do so.**
### **6 Conclusion**

We present a novel, inquiry-based framework using the 20 Questions game to surface
implicit geographic biases in LLMs. Evaluating three models on our constructed geographically diverse dataset, Geo20Q [+], we find that: (1) LLMs perform better on Notable people than

Things, with Gemini 2.0 achieving the highest accuracy. (2) Entity popularity and corpus
frequency weakly correlate with performance but do not explain observed disparities. (3)
The language of gameplay has minimal impact on performance. (4) **Most importantly,**
**models show geopolitical bias (Global North/West** *>* **South/East), by favoring more**
**economically and culturally dominant regions.** Our results argue for creative, free-form
evaluation frameworks that shift the focus from outputs to reasoning processes, enabling
the discovery of subtle and often hidden biases in LLM behavior.
### **7 Discussions and Limitations**

Our primary goal was to propose a novel evaluation strategy that effectively surfaces implicit
geographic biases in LLMs. Our analyses tested a specific hypothesis, whether pretraining
data coverage could explain geographic performance disparities, and demonstrated that
such biases are only partially explained by entity popularity and frequency, indicating these
behaviors arise from a complex interplay of factors. While our framework provides a unique
lens into LLM reasoning, it is ultimately a tool for *surfacing* potential biases, *not proving*
*their absence* . A lack of bias observed within our game setup does not imply that a model is
unbiased overall; it may simply reflect the limitations of the task structure.

In this work, we do not explicitly analyze *misidentification behavior*, where models guess
entities from more dominant regions (e.g., the Global North) in place of less-represented
ones. Next, we do not investigate specific *failure modes* in deduction, such as repetitive
questioning, planning inefficiencies, or inconsistent reasoning. Our findings are embedded
in a single interactive framework; other evaluation paradigms may reveal different forms
of bias. Moreover, due to academic resource constraints, our set of models used is limited,
and our language coverage focuses on high-resource languages. Finally, we do not perform
a profession-wise and category-wise breakdown of Things and Notable people, which can
help describe which types of entities are more susceptible to bias.

To understand the sources of these observed biases, we propose several future experiments:

1. Analyze model attention and reasoning patterns during deduction to determine if
certain activation paths and model regions are systematically favored at decision points,
leveraging interpretability methods such as Patchscopes Ghandeharioun et al. (2024).
2. Systematically evaluate the impact of different tuning and alignment techniques (e.g.,
instruction tuning, reinforcement learning) on geographic bias by retraining models
with regionally balanced data and assessing shifts in deduction performance.

We believe these directions will help disentangle the contributions of data coverage, model
architecture, and training dynamics to observed biases. We encourage further suggestions
from the community for additional experiments toward this goal.

10


-----

Published as a conference p a p er at COLM 2025
### **Ethics Statement**

This research investigates implicit geographic biases in LLMs using a model-initiated 20
Questions framework. Rather than designing probes, which may themselves encode assumptions, we adopt a free-form deduction setup and curate a geographically diverse
dataset spanning cultural and economic axes (e.g., Global North vs. Global South, Global
West vs. East). Our goal is not to rank or hierarchize regions, but to surface systemic disparities in how LLMs reason about entities from different parts of the world. We acknowledge
that these geographic groupings are broad and imperfect proxies for cultural and political
complexity. While they help quantify disparities, they do not fully capture the nuance of
global representation.

Data access and model use: No personal or sensitive data was used in this study. All
models were accessed via official APIs or publicly released weights. Results are reported in
aggregate to avoid singling out individuals or groups. We validated multilingual prompts
in-house and did not employ external annotators.

Intended Impact: We hope this work contributes to more inclusive, robust evaluation practices and motivates broader inquiry into multilingual and region-specific model assessment,
particularly in underrepresented linguistic and cultural contexts.
### **References**

Jaimeen Ahn and Alice Oh. Mitigating language-dependent ethnic bias in BERT. In
Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),
*Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp.
533–549, Online and Punta Cana, Dominican Republic, November 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.42. URL [https:](https://aclanthology.org/2021.emnlp-main.42/)
[//aclanthology.org/2021.emnlp-main.42/.](https://aclanthology.org/2021.emnlp-main.42/)

Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W Bruce Croft. Asking
clarifying questions in open-domain information-seeking conversations. In *Proceedings of*
*the 42nd international acm sigir conference on research and development in information retrieval*,
pp. 475–484, 2019.

Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas L Griffiths. Measuring
implicit bias in explicitly unbiased large language models. *arXiv preprint arXiv:2402.04105*,
2024.

Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas L. Griffiths. Explicitly
unbiased large language models still form biased associations. *Proceedings of the National*
*Academy of Sciences*, 122(8):e2416228122, 2025. doi: 10.1073/pnas.2416228122. URL
[https://www.pnas.org/doi/abs/10.1073/pnas.2416228122.](https://www.pnas.org/doi/abs/10.1073/pnas.2416228122)

Leonardo Bertolazzi, Davide Mazzaccara, Filippo Merlo, and Raffaella Bernardi. ChatGPT‘s information seeking strategy: Insights from the 20-questions game. In C. Maria
Keet, Hung-Yi Lee, and Sina Zarrieß (eds.), *Proceedings of the 16th International Natu-*
*ral Language Generation Conference*, pp. 153–162, Prague, Czechia, September 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.inlg-main.11. URL
[https://aclanthology.org/2023.inlg-main.11/.](https://aclanthology.org/2023.inlg-main.11/)

Philippe Besse, Eustasio del Barrio, Paula Gordaliza, Jean-Michel Loubes, and Laurent
Risser. A survey of bias in machine learning through the prism of statistical parity. *The*
*American Statistician*, 76(2):188–198, 2022.

Kirti Bhagat, Kinshuk Vasisht, and Danish Pruthi. Richer output for richer countries:
Uncovering geographical disparities in generated stories and travel recommendations.
*arXiv preprint arXiv:2411.07320*, 2024.

Angana Borah and Rada Mihalcea. Towards implicit bias detection and mitigation in
multi-agent llm interactions. *arXiv preprint arXiv:2410.02584*, 2024.

11


-----

Published as a conference p a p er at COLM 2025

Yang Trista Cao and Hal Daume III. Toward gender-inclusive coreference resolution. In ´
Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), *Proceedings of the 58th*
*Annual Meeting of the Association for Computational Linguistics*, pp. 4568–4595, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.418.
[URL https://aclanthology.org/2020.acl-main.418/.](https://aclanthology.org/2020.acl-main.418/)

Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle
Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, et al.
Chatbot arena: An open platform for evaluating llms by human preference. In *Forty-first*
*International Conference on Machine Learning*, 2024.

Woon Sang Cho, Yizhe Zhang, Sudha Rao, Asli Celikyilmaz, Chenyan Xiong, Jianfeng Gao,
Mengdi Wang, and Bill Dolan. Contrastive multi-document question generation. In Paola
Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.), *Proceedings of the 16th Conference of the*
*European Chapter of the Association for Computational Linguistics: Main Volume*, pp. 12–30,
Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
[eacl-main.2. URL https://aclanthology.org/2021.eacl-main.2/.](https://aclanthology.org/2021.eacl-main.2/)

Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van Breugel, and
Pasquale Minervini. Stereotype and skew: Quantifying gender bias in pre-trained
and fine-tuned language models. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.), *Proceedings of the 16th Conference of the European Chapter of the Association*
*for Computational Linguistics: Main Volume*, pp. 2232–2242, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.190. URL
[https://aclanthology.org/2021.eacl-main.190/.](https://aclanthology.org/2021.eacl-main.190/)

Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and
Aaron Courville. Guesswhat?! visual object discovery through multi-modal dialogue. In
*Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pp. 5503–5512,
2017.

Xiangjue Dong, Yibo Wang, Philip S Yu, and James Caverlee. Disclosure and mitigation of
gender bias in llms. *arXiv preprint arXiv:2402.11190*, 2024.

Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk,
Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. What’s in
my big data? *arXiv preprint arXiv:2310.20707*, 2023.

Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck
Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large
language models: A survey. *Computational Linguistics*, 50(3):1097–1179, 09 2024a. ISSN
[0891-2017. doi: 10.1162/coli a 00524. URL https://doi.org/10.1162/coli a 00524.](https://doi.org/10.1162/coli_a_00524)

Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck
Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large
language models: A survey. *Computational Linguistics*, 50(3):1097–1179, 2024b.

Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscopes: A unifying framework for inspecting hidden representations of language
models. In *Forty-first International Conference on Machine Learning*, 2024. URL [https:](https://arxiv.org/abs/2401.06102)
[//arxiv.org/abs/2401.06102.](https://arxiv.org/abs/2401.06102)

Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, and Rebecca J Passonneau. Sociodemographic bias in language models: A survey and forward path. *arXiv preprint*
*arXiv:2306.08158*, 2023.

John J Hanna, Abdi D Wakene, Andrew O Johnson, Christoph U Lehmann, and Richard J
Medford. Assessing racial and ethnic bias in text generation by large language models for
health care–related tasks: Cross-sectional study. *Journal of Medical Internet Research*, 27:
e57257, 2025.

12


-----

Published as a conference p a p er at COLM 2025

Divyanshu Kumar, Umang Jain, Sahil Agarwal, and Prashanth Harshangi. Investigating
implicit bias in large language models: A large-scale study of over 50 llms. *arXiv preprint*
*arXiv:2410.12864*, 2024.

Morgane Laouenan, Palaash Bhargava, Jean-Beno ˆ ıt Eymeoud, Olivier Gergaud, Guillaume ´
Plique, and Etienne Wasmer. A cross-verified database of notable people, 3500bc-2018ad.
*Scientific Data*, 9(1):290, 2022.

Bryan Li, Samar Haider, and Chris Callison-Burch. This land is *{* Your, My *}* land: Evaluating
geopolitical biases in language models. *arXiv preprint arXiv:2305.14610*, 2023.

Serene Lim and Mar ´ ıa Perez-Ortiz. The african woman is rhythmic and soulful: An investi- ´
gation of implicit biases in llm open-ended text generation. *arXiv preprint arXiv:2407.01270*,
2024.

Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, and Stefano Ermon. Large
language models are geographically biased. *arXiv preprint arXiv:2402.02680*, 2024.

Davide Mazzaccara, Alberto Testoni, and Raffaella Bernardi. Learning to ask informative
questions: Enhancing llms with preference optimization and expected information gain.
*arXiv preprint arXiv:2406.17453*, 2024.

Shujaat Mirza, Bruno Coelho, Yuyuan Cui, Christina Popper, and Damon McCoy. Global- ¨
liar: Factuality of llms over time and geographic regions. *arXiv preprint arXiv:2401.17839*,
2024.

Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in
pretrained language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli
(eds.), *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics*
*and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long*
*Papers)*, pp. 5356–5371, Online, August 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.acl-long.416. URL [https://aclanthology.org/2021.acl-long.](https://aclanthology.org/2021.acl-long.416/)
[416/.](https://aclanthology.org/2021.acl-long.416/)

Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A
challenge dataset for measuring social biases in masked language models. In Bonnie
Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the 2020 Conference*
*on Empirical Methods in Natural Language Processing (EMNLP)*, pp. 1953–1967, Online,
November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
[emnlp-main.154. URL https://aclanthology.org/2020.emnlp-main.154/.](https://aclanthology.org/2020.emnlp-main.154/)

Lemuel Ekedegwa Odeh. A comparative analysis of global north and global south
economies. 2010.

OpenAI. GPT-4 technical report. Technical report, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language
models to follow instructions with human feedback. *Advances in neural information*
*processing systems*, 35:27730–27744, 2022.

Nathaniel R Robinson, Perez Ogayo, David R Mortensen, and Graham Neubig. Chatgpt mt:
Competitive for high-(but not low-) resource languages. *arXiv preprint arXiv:2309.07423*,
2023.

Pola Schwobel, Jacek Golebiowski, Michele Donini, C ¨ edric Archambeau, and Danish Pruthi. ´
Geographical erasure in language generation. *arXiv preprint arXiv:2310.14777*, 2023.

Sheikh Shafayat, Eunsu Kim, Juhyun Oh, and Alice Oh. Multi-fact: Assessing factuality of
multilingual llms using factscore. *arXiv preprint arXiv:2402.18045*, 2024.

13


-----

Published as a conference p a p er at COLM 2025

Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell
Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open
corpus of three trillion tokens for language model pretraining research. *arXiv preprint*
*arXiv:2402.00159*, 2024.

Gemini Team. Gemini: A family of highly capable multimodal models, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothee Lacroix, Baptiste Rozi ´ ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien `
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and
efficient foundation language models, 2023.

Bryan S Turner and Habibul Haque Khondker. Globalization east and west. 2010.

Luis von Ahn and Laura Dabbish. Labeling images with a computer game. In *Proceedings*
*of the SIGCHI Conference on Human Factors in Computing Systems*, CHI ’04, pp. 319–326,
New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581137028. doi:
[10.1145/985692.985733. URL https://doi.org/10.1145/985692.985733.](https://doi.org/10.1145/985692.985733)

Luis von Ahn, Ruoran Liu, and Manuel Blum. Peekaboom: a game for locating objects in
images. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems*, CHI
’06, pp. 55–64, New York, NY, USA, 2006. Association for Computing Machinery. ISBN
1595933727. doi: 10.1145/1124772.1124782. URL [https://doi.org/10.1145/1124772.](https://doi.org/10.1145/1124772.1124782)
[1124782.](https://doi.org/10.1145/1124772.1124782)

Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, and Zhiyong Lu. Unmasking and quantifying racial bias of large language models in medical report generation. *Communications*
*Medicine*, 4(1):176, 2024.

Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and Kai-Wei Chang.
Geomlama: Geo-diverse commonsense probing on multilingual pre-trained language
models. *arXiv preprint arXiv:2205.12247*, 2022.

Vithya Yogarajan, Gillian Dobbie, Te Taka Keegan, and Rostam J Neuwirth. Tackling bias
in pre-trained language models: Current trends and under-represented societies. *arXiv*
*preprint arXiv:2312.01509*, 2023.

Yizhe Zhang, Jiarui Lu, and Navdeep Jaitly. Probing the multi-turn planning capabilities of
llms via 20 question games. *arXiv preprint arXiv:2310.01468*, 2023.

Yachao Zhao, Bo Wang, and Yan Wang. Explicit vs. implicit: Investigating social bias in
large language models through self-reflection. *arXiv preprint arXiv:2501.02295*, 2025.

14


-----

Published as a conference p a p er at COLM 2025
### **A Appendix**

**A.1** **Empirical upper bound for unlimited-turn setting**

To examine the impact of increasing the number of allowable turns in the deduction game,
we identify the optimal cutoff point to be around 150 turns, beyond which the Success Rate
no longer improves significantly (Figure 4). Hence, we can use 150 turns as a proxy for the
unlimited-turn setting.

Figure 4: Average Success Rates for the varying maximum Number of Turns across different
models using 100 most popular entities from each continent. The Success Rates are almost
horizontal for the model after 150 turns.

15


-----

Published as a conference p a p er at COLM 2025

**A.2** **Continent-wise variation in performance for each model**

We present a detailed continent-level analysis for each model, revealing geographic disparities in entity deduction across different regions. LLMs have superior performance on

(Table 6).

Turns Entit y Model Asia Africa Australia Euro p e North Am South Am

2 (16.5) 16 (15.4) 8 (16.1) 10 (18.5) 8 (16.1) 11 (17.1)



20

150





15 (13.4) 17 (15.7) 7 (16.5) 13 (15.2) 15 (15.0) 8(15.5)

1.5 7 (13.3) 14 (14.9) 9 (14.8) 7 (16.4) 8 (13.0) 9 (16.2)

2.0 8 (14.0) 15 (16.0) 6 (16.1) 20 (15.9) 5 (12.8) 10(16.3)

Av g . 8 (14.3) 15(15.5) 7 (15.4) 12 (16.0) 9 (14.2) 9 (16.2)

14 (15.1) 23 (16.5) 11 (16.3) 38 (14.6) 37 (12.5) 26 (16.0)

29 (16.2) 26 (17.3) 23 (14.5) 53 (13.8) 54 (13.5) 29 (15.9)

1.5 23 (16.7) 16 (16.4) 19 (14.7) 35 (13.6) 44 (12.4) 14 (17.1)

2.0 40 (17.1) 20 (17.2) 30 (16.0) 49 (15.3) 45 (14.1) 30 (16.9)

Avg. 26 (16.3) 21 (16.8) 20 (15.4) 43 (14.3) 45 (13.6) 24 (16.5)

48 (48.5) 56 (40.1) 37 (47.0) 38 (51.0) 38 (40.7) 39 (41.8)

40 (41.2) 42 (36.4) 38 (42.4) 54 (49.0) 43 (40.4) 43(48.7)

1.5 31 (41.9) 42 (34.5) 28 (36.1) 39 (36.4) 22 (27.3) 34 (35.1)

2.0 53 (49.8) 46(42.2) 44 (60.7) 49 (37.8) 34 (54.7) 42 (56.6)

Av g . 43 (45.1) 46 (38.3) 37 (46.5) 45 (43.6) 34 (41.8) 39 (45.5)

58 (38.0) 78 (36.0) 51 (41.0) 64 (27.6) 78 (31.5) 71 (33.1)

88 (37.2) 79 (31.6) 56 (34.1) 83 (19.7) 86 (28.8) 76 (29.3)

1.5 50 (28.2) 63(31.0) 60 (34.0) 73 (28.4) 71 (24.7) 67(40.4)

2.0 95(25.5) 89 (35.3) 81 (32.4) 87 (26.8) 80 (25.1) 79 (39.5)

Avg. 72 (32.2) 77 (33.5) 62 (35.4) 76 (25.6) 78 (27.5) 73 (35.3)


Table 6: Average Success Rates (in %) and Number of Turns to Answer (in paranthesis)
across different models and continents taking 100 most popular entities from each continent

**A.3** **Verifying model’s entity knowledge via prompt-based evaluation**

Our experiments are conducted with a single run per entity for each model-languageconfiguration combination, which results in a total of 50,400+ games (3 models × 1,200
entities × 7 languages × 2 configurations for RQ1-3). Therefore having multiple runs
conducted per entity would incur non-trivial computation costs.

To better account for the stochasticity in model behavior, we conduct a small-scale robustness
analysis using 50 entities with 5 independent runs per entity ( Notable people, 1.5 [) as shown]
in Table 7. Overall, we see that the aggregate-level patterns and behaviors remain consistent.

**A.4** **Prompts used in the paper**

We document the exact prompts used for both the judge and guesser roles in the English
language (prompts for other languages provided on GitHub). This ensures reproducibility
and transparency in our experimental setup. Note that for ambiguous questions, the judge
is permitted to reply with ”Maybe”, which serves as a soft signal of uncertainty rather than
misleading the guesser. On average, “Maybe” occurs infrequently, only in about 1.8% of the
turns.

16


-----

Published as a conference p a p er at COLM 2025

**Settin** **g** **Metric** **Asia** **Africa** **Australia** **Europe** **N. America** **S. America**

Success 14% ± 9% 22% ± 4% 42% ± 16% 40% ± 19% 32% ± 11% 36% ± 13%
Can.
Num. Turns 11.90 ± 7.02 13.17 ± 2.87 11.68 ± 1.48 11.61 ± 0.85 9.89 ± 2.02 16.65 ± 0.91

Success 40% ± 19% 50% ± 10% 64% ± 13% 62% ± 13% 54% ± 5% 62% ± 13%
Unl.
Num. Turns 26.95 ± 11.52 23.20 ± 3.24 24.45 ± 10.32 18.82 ± 3.65 19.46 ± 3.96 24.59 ± 7.76

Table 7: Average Success rates and Number of Turns to Answer across continents. Results
are based on 5 independent runs per entity (50 entities) for Notable people using 1.5 [. We]
report mean ± standard deviation to account for randomness in model behavior.

***A.4.1*** ***English prompts***

**Prompt for the judge model - Notable people**
Based on your knowledge about the celebrity: *{* entity *}*, respond to the following question or guess.
Limit your respond to only ‘Yes.’, ‘No.’ or ‘Dunno.’, with no explanation or other words. Never say
the name *{* entity *}* in your response. Do not say ‘Dunno.’ if it can be answered by ‘Yes.’ or ‘No.’ If the
question is to solicit the answer, respond ‘No.’.
For the celebrity *{* entity *}*, *{* question *}* (Yes/No/Dunno)

**Prompt for the judge model - Things**
Based on your knowledge about *{* entity *}*, respond to the following question or guess. Limit your
respond to only ‘Yes.’, ‘No.’ or ‘Maybe.’, with no explanation or other words. Never say the answer
*{* entity *}* in your response. If the question is to solicit the answer, respond ‘No.’.
For the thing *{* entity *}*, *{* question *}* (Yes/No/Dunno)

**Prompt for the guesser model - Notable people**
Your task is to ask a series of questions to deduce the celebrity that I’m thinking of with as few queries
as possible. Only ask factual questions that can be answered by ‘Yes.’, ‘No.’ or ‘Dunno.’. Do not ask
for a hint. Make your question brief with no linebreaker. Now start asking a question.
*{* dialogue history *}*

**Prompt for the guesser model - Things**
Your task is to ask a series of questions to deduce the entity that I’m thinking of with as few queries
as possible. Only ask questions that can be answered by ‘yes’, ‘no’ or ‘maybe’. Do not ask for a hint.
Make your question brief with no line breakers. Now, start asking a question.
*{* dialogue history *}*

17


-----

Published as a conference p a p er at COLM 2025

**A.5** **Language-wise performance of different models**

We report detailed language-specific results across different continents and models (Table 8).





|Turns Entity Model Continent English Hindi Mandarin Spanish Japanese Turkish French|Average|
|---|---|
|Asia 2 (16.5) 2 (18.5) 3 (17.3) 5 (17.8) 4 (17.0) 4 (15.2) 10 (15.3) Africa 16 (15.4) 3 (15.7) 8 (17.6) 7 (16.4) 5 (15.4) 6 (17.7) 13 (16.2) Australia 8 (16.1) 3 (18.3) 1 (20.0) 4 (18.5) 0 (-) 6 (17.2) 4 (15.2) Europe 10 (18.5) 4 (15.0) 4 (17.5) 6 (15.5) 2 (17.0) 5 (14.4) 17 (17.0) North Am 8 (16.1) 1 (17.0) 6 (17.0) 4 (15.5) 6 (17.3) 4 (14.8) 6 (16.0) South Am 11 (17.1) 1 (16.0) 8 (17.1) 7 (17.1) 2 (18.0) 4 (17.5) 9 (17.8) Asia 7 (13.3) 6 (14.7) 3 (11.7) 10 (14.7) 6 (13.2) 2 (10.0) 7 (15.1) Africa 14 (14.9) 2 (16.5) 4 (16.2) 6 (15.3) 3 (16.3) 1 (10.0) 9 (15.9) Australia 9 (14.8) 2 (18.0) 2 (14.0) 8 (14.9) 4 (16.8) 0 (-) 7 (17.0) Things 1.5 Europe 7 (16.4) 2 (16.0) 2 (16.0) 4 (17.0) 2 (18.0) 1 (18.0) 9 (14.7) North Am 8 (13.0) 1 (12.0) 1 (12.0) 5 (15.6) 2 (10.5) 1 (12.0) 7 (13.4) South Am 9 (16.2) 2 (15.0) 3 (13.3) 6 (16.2) 0 (-) 1 (19.0) 6 (17.5) Asia 8 (14.0) 4 (16.8) 3 (15.0) 7 (14.7) 7 (15.1) 0 (0.0) 14 (15.5) Africa 15 (16.1) 4 (16.0) 6 (17.2) 11 (15.4) 8 (15.9) 4 (13.5) 17 (17.5) Australia 6 (16.0) 1 (20.0) 3 (19.0) 9 (16.8) 9 (15.9) 3 (13.3) 7 (17.6) 2.0 Europe 20 (15.9) 5 (16.2) 3 (16.3) 12 (15.7) 8 (15.0) 3 (17.0) 18 (16.0) North Am 5 (12.8) 3 (17.3) 1 (19.0) 5 (16.2) 4 (18.2) 2 (14.0) 9 (14.7) South Am 10 (16.3) 3 (14.0) 0 (0.0) 12 (16.9) 4 (13.8) 1 (20.0) 12 (16.5) 20 Asia 14 (15.1) 17 (15.5) 27 (15.6) 23 (15.3) 22 (15.9) 28 (16.9) 23 (15.9) Africa 23 (16.5) 7 (17.7) 22 (17.4) 21 (16.4) 21 (17.0) 27 (17.8) 28 (17.3) Australia 11 (16.4) 6 (13.8) 16 (15.9) 7 (16.9) 14 (16.1) 14 (16.1) 19 (16.5) Europe 38 (14.6) 28 (13.5) 43 (13.5) 28 (15.1) 46 (14.7) 37 (14.3) 32 (13.9) North Am 37 (12.5) 24 (14.6) 39 (13.5) 31 (14.1) 28 (13.3) 48 (13.0) 42 (14.0) South Am 26 (16.0) 24 (16.3) 36 (15.6) 23 (16.4) 24 (16.1) 18 (16.2) 27 (16.0) Asia 23 (16.7) 19 (15.7) 20 (15.9) 20 (15.4) 19 (15.5) 16 (14.6) 12 (15.6) Africa 16 (16.4) 11 (16.4) 11 (17.8) 13 (16.8) 9 (15.6) 7 (13.9) 11 (15.5) Australia 19 (14.7) 13 (15.9) 14 (15.0) 16 (15.2) 13 (14.7) 7 (13.3) 8 (15.9) Notable people 1.5 Europe 35 (13.7) 34 (13.7) 34 (13.4) 41 (14.2) 41 (12.7) 26 (11.3) 28 (14.1) North Am 44 (12.5) 17 (13.6) 28 (12.9) 36 (13.5) 33 (12.0) 19 (10.2) 38 (14.4) South Am 14 (17.1) 12 (14.9) 21 (17.0) 18 (16.1) 16 (15.8) 9 (15.1) 13 (15.3) Asia 40 (17.2) 40 (16.1) 39 (15.8) 32 (15.3) 36 (15.3) 30 (14.2) 32 (16.9) Africa 20 (17.2) 24 (16.8) 26 (15.4) 21 (16.7) 28 (15.3) 14 (16.3) 21 (15.9) Australia 30 (16.0) 19 (15.7) 20 (16.1) 28 (15.2) 22 (14.9) 19 (13.2) 21 (16.2) 2.0 Europe 49 (15.3) 47 (14.7) 55 (15.1) 46 (14.0) 49 (13.7) 27 (12.4) 46 (14.1) North Am 45 (14.2) 37 (14.5) 35 (12.7) 35 (13.5) 44 (12.8) 30 (11.6) 27 (14.7) South Am 30 (16.9) 30 (16.9) 34 (15.2) 30 (15.9) 28 (16.6) 19 (14.9) 32 (15.4)|4 (16.8) 8 (16.3) 4 (15.1) 7 (16.4) 5 (16.2) 6 (17.2)|
||5 (13.2) 5 (15.1) 5 (13.6) 4 (16.5) 4 (12.6) 4 (13.8)|
||6 (13.0) 9 (15.9) 5 (16.9) 10 (16.0) 4 (16.0) 6 (13.9)|
||22 (15.7) 21 (17.1) 12 (15.9) 36 (14.2) 36 (13.5) 25 (16.1)|
||18 (15.6) 11 (16.0) 13 (15.0) 34 (13.2) 31 (12.7) 15 (15.9)|
||35 (15.8) 22 (16.2) 22 (15.3) 46 (14.1) 36 (13.4) 29 (16.0)|
|Asia 48 (48.5) 18 (51.7) 14 (35.9) 30 (42.3) 15 (37.9) 20 (53.5) 35 (36.5) Africa 56 (40.2) 15 (38.9) 28 (30.0) 37 (34.9) 16 (41.7) 28 (47.2) 36 (33.2) Australia 37 (47.0) 15 (48.9) 20 (42.0) 19 (39.5) 13 (48.0) 20 (38.9) 27 (42.1) Europe 38 (51.0) 12 (36.2) 26 (33.5) 26 (36.0) 13 (36.7) 22 (45.0) 38 (30.4) North Am 38 (40.7) 14 (41.7) 25 (30.0) 21 (43.6) 24 (36.8) 18 (33.4) 33 (35.2) South Am 39 (41.8) 7 (50.7) 24 (31.9) 31 (40.4) 12 (53.4) 18 (60.1) 38 (43.1) Asia 31 (41.9) 21 (38.9) 10 (25.1) 21 (38.6) 13 (40.9) 4 (58.0) 17 (25.5) Africa 42 (34.5) 13 (45.7) 8 (27.6) 21 (33.3) 9 (37.3) 3 (39.7) 26 (26.5) Australia 28 (36.1) 10 (51.3) 10 (35.5) 15 (30.1) 11 (43.1) 2 (97.0) 20 (27.7) Things 1.5 Europe 39 (36.4) 18 (42.4) 6 (36.7) 13 (51.8) 11 (34.0) 4 (58.5) 28 (33.0) North Am 22 (27.4) 13 (45.5) 8 (58.5) 13 (35.4) 5 (43.4) 1 (12.0) 14 (27.2) South Am 34 (35.1) 15 (45.4) 8 (48.1) 19 (38.9) 4 (49.5) 3 (38.3) 15 (27.1) Asia 53 (49.8) 15 (40.0) 11 (51.9) 35 (52.0) 22 (41.5) 6 (73.8) 42 (42.9) Africa 46 (42.3) 10 (31.8) 25 (34.4) 42 (44.3) 25 (33.0) 11 (36.2) 55 (35.9) Australia 44 (60.7) 17 (48.3) 14 (62.1) 36 (42.5) 18 (34.2) 11 (35.5) 49 (46.0) 2.0 Europe 49 (37.8) 25 (33.5) 23 (49.1) 45 (39.3) 26 (32.7) 20 (52.4) 48 (41.9) North Am 34 (54.8) 18 (46.9) 10 (58.6) 22 (47.4) 16 (34.1) 5 (30.6) 36 (50.8) South Am 42 (56.6) 13 (40.3) 16 (51.3) 39 (35.2) 13 (47.8) 8 (56.4) 48 (39.1) 150 Asia 57 (38.0) 46 (36.5) 69 (35.2) 65 (30.5) 67 (31.1) 62 (31.1) 64 (26.2) Africa 78 (36.0) 44 (40.3) 64 (32.2) 69 (38.4) 67 (35.1) 69 (30.9) 69 (27.6) Australia 51 (41.0) 23 (34.3) 41 (32.3) 40 (45.6) 41 (40.0) 37 (36.1) 47 (35.2) Europe 64 (27.6) 56 (29.4) 75 (25.6) 54 (31.2) 84 (26.7) 71 (32.4) 65 (25.5) North Am 78 (31.5) 56 (31.1) 75 (27.4) 60 (29.6) 63 (34.7) 71 (25.7) 74 (26.2) South Am 71 (33.2) 60 (28.6) 78 (30.2) 70 (29.8) 76 (30.3) 65 (33.0) 67 (27.2) Asia 50 (28.2) 46 (27.5) 49 (24.6) 56 (27.5) 33 (23.1) 23 (20.5) 51 (31.4) Africa 63 (31.0) 47 (31.8) 38 (27.3) 53 (34.1) 37 (33.4) 15 (23.9) 51 (30.8) Australia 60 (34.0) 28 (27.1) 32 (26.7) 52 (33.3) 31 (27.3) 18 (27.2) 40 (32.2) Notable people 1.5 Europe 73 (28.4) 65 (25.9) 65 (22.3) 72 (22.8) 64 (19.7) 33 (14.2) 65 (28.5) North Am 71 (24.7) 46 (28.0) 40 (18.1) 72 (24.0) 47 (18.8) 28 (17.4) 68 (24.5) South Am 67 (40.4) 54 (34.3) 52 (26.2) 69 (34.7) 41 (27.9) 23 (22.4) 62 (36.9) Asia 95 (28.5) 89 (28.8) 72 (24.3) 84 (27.3) 68 (23.1) 60 (29.9) 80 (28.1) Africa 89 (35.4) 79 (33.9) 71 (32.0) 86 (40.6) 63 (27.4) 41 (34.5) 82 (33.1) Australia 81 (32.4) 66 (35.8) 63 (32.5) 69 (31.9) 51 (35.5) 53 (26.4) 70 (37.3) 2.0 Europe 87 (26.8) 83 (25.6) 81 (22.7) 83 (32.5) 76 (23.0) 48 (25.2) 81 (29.8) North Am 80 (25.2) 77 (31.4) 73 (27.4) 75 (34.1) 68 (23.5) 51 (23.0) 73 (35.6) South Am 79 (39.5) 88 (28.8) 68 (26.2) 80 (33.5) 64 (29.4) 41 (31.0) 84 (31.1)|25 (43.7) 31 (38.0) 22 (43.7) 25 (38.3) 25 (37.3) 24 (45.9)|
||16 (13.4) 17 (34.9) 14 (45.8) 17 (41.8) 11 (35.6) 14 (40.3)|
||26 (50.2) 31 (36.8) 27 (47.1) 34 (40.1) 20 (46.1) 26 (46.6)|
||61 (32.6) 65 (34.3) 40 (37.8) 67 (28.3) 68 (29.4) 70 (30.3)|
||44 (26.1) 43 (30.3) 37 (29.6) 62 (23.1) 53 (22.2) 53 (31.8)|
||78 (27.1) 73 (33.8) 65 (33.1) 77 (26.5) 71 (28.5) 72 (31.3)|


Table 8: Average Success Rates (in %) and Number of Turns to Answer (in parenthesis)
across different models, continents, and languages taking 100 most popular entities from
each continent

18


-----

Published as a conference p a p er at COLM 2025

**A.6** **Qualitative analysis: Model give up behavior**

In qualitative analyses, we observed that LLMs often abandon the game
after many turns with outputs like “I give up.” We include this metric
to measure the average number of turns before such early termination.
To detect when the guesser has given up, we use GPT-4o-mini to verify
at each turn. Lower values indicate quick abandonment, while higher
values suggest greater persistence. GPT-4o-mini showed the highest
persistence while deducing entities, while Gemini-1.5-flash showed the
least (Table 9).


Model Turns to Give U p

108.9

87.9

1.5 67.2

2.0 84.8

Table 9: Average Turns
at which different models Give Up on deducing an entity


Here is an example of a game with target entity University of Otago (a public research collegiate
university based in Dunedin, Otago, New Zealand) in which the model eventually abandons the game
after being unable to guess the target entity.

**A.7** **Example of geographic bias deduction path:**

The following example shows the deduction path for the target entity A. P. J. Abdul Kalam (an Indian
president, poet, and scientist). Despite the answer being an Indian figure, the model’s reasoning chain
systematically eliminates Western options before considering Asian and Indian possibilities:

**A.8** **Heavy-tail distribution of entity frequency in Dolma dataset**

The frequency vs. rank distribution of the combined entities follows a heavy-tailed, power-law-like
behavior, consistent with Herd-Immunity Power Scale Invariant (HiPSI) laws observed in natural
language (Figure 5). This motivated us to use log transformations of frequency while doing the
regression analysis.

19


-----

Published as a conference p a p er at COLM 2025

Figure 5: Heavy-tail distribution of entity frequency values in Dolma dataset

**A.9** **Multilingual prompt validation process**

For each language, at least one native or fluent speaker, recruited from our academic networks,
reviewed all translations, edited prompts as needed, and observed multiple games to check for
accuracy and consistency. Their review protocol included:

1. Verifying and, if needed, editing the prompt templates and translations to ensure naturalness
and accuracy in each language.

2. Observing several sample games played in the target language to check the consistency of
the game play. We suggested every speaker to review around 3 games, however, they were
free to review as many games as they would like.

3. Confirming the final quality of translations and reporting any issues or corrections (e.g.,
overly literal translations, culturally inappropriate terminology, or ambiguous instructions).

Any issues (such as awkward phrasing or ambiguous instructions) were corrected based on their
feedback.

**A.10** **Division of countries**

**Global West** United States, Canada, United Kingdom, France, Germany, Italy, Spain, Portugal, Netherlands, Belgium, Sweden, Norway, Finland, Denmark, Iceland, Austria, Switzerland, Luxembourg,
Ireland, Australia, New Zealand, Estonia, Latvia, Lithuania, Poland, Czech Republic, Slovakia, Hungary, Slovenia, Malta, Greece, Croatia

**Global North** *United States, Canada, United Kingdom, France, Germany, Italy, Spain, Portugal, Netherlands,*
*Belgium, Sweden, Norway, Finland, Denmark, Iceland, Austria, Switzerland, Luxembourg, Ireland, Australia,*
*New Zealand*, Japan, South Korea

The countries in the Global North marked in italics are also part of the Global West.

20


-----

Published as a conference p a p er at COLM 2025

**A.11** **Detailed breakdown of** **Things** **and** **Notable people**

We categorize Things into groups such as landmarks, food, animals, etc., and Notable people into
professions such as sports, politics, academia, etc. (Figure 6).

(a) Top categories of culturally significant things.

(b) Top professions among notable individuals.

Figure 6: Distribution of entity types across Things and Notable people .

21


-----

