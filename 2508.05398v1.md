# **On the Reliability of Sampling Strategies in O!line** **Recommender Evaluation**

## Rodrygo L. T. Santos
#### Universidade Federal de Minas Gerais Belo Horizonte, MG, Brazil rodrygo@dcc.ufmg.br

## Bruno L. Pereira
#### Universidade Federal de Minas Gerais Belo Horizonte, MG, Brazil brunolaporais@dcc.ufmg.br
### **Abstract**

## Alan Said
#### University of Gothenburg Gothenburg, Sweden alansaid@acm.org


O!ine evaluation plays a central role in benchmarking recommender systems when online testing is impractical or risky. However, it is susceptible to two key sources of bias: exposure bias,
where users only interact with items they are shown, and sampling
bias, introduced when evaluation is performed on a subset of logged
items rather than the full catalog. While prior work has proposed
methods to mitigate sampling bias, these are typically assessed
on "xed logged datasets rather than for their ability to support
reliable model comparisons under varying exposure conditions or
relative to true user preferences. In this paper, we investigate how
di#erent combinations of logging and sampling choices a#ect the
reliability of o!ine evaluation. Using a fully observed dataset as
ground truth, we systematically simulate diverse exposure biases
and assess the reliability of common sampling strategies along four
dimensions: sampling resolution (recommender model separability),
"delity (agreement with full evaluation), robustness (stability under
exposure bias), and predictive power (alignment with ground truth).
Our "ndings highlight when and how sampling distorts evaluation
outcomes and o#er practical guidance for selecting strategies that
yield faithful and robust o!ine comparisons.
### **CCS Concepts**

- **Information systems** â†’ **Recommender systems** ; *Evaluation*
*of* *retrieval* *results* ; *Content* *ranking* ; *Personalization* .
### **Keywords**

o!ine evaluation, target item sampling, global metrics, exposure
bias, negative sampling

**ACM** **Reference** **Format:**

Bruno L. Pereira, Alan Said, and Rodrygo L. T. Santos. 2025. On the Reliability
of Sampling Strategies in O!ine Recommender Evaluation. In *Proceedings*
*of* *the* *Nineteenth* *ACM* *Conference* *on* *Recommender* *Systems* *(RecSys* *â€™25),*
*September* *22â€“26,* *2025,* *Prague,* *Czech* *Republic.* ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3705328.3748086

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full
citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s).
*RecSys* *â€™25,* *September* *22â€“26,* *2025,* *Prague,* *Czech* *Republic*
Â© 2025 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-1364-4/2025/09
https://doi.org/10.1145/3705328.3748086

### **1 Introduction**

In an ideal world, recommender systems would be evaluated by
observing how users respond to fully ranked lists of items, ensuring
that every item has a fair chance of being chosen [ 15, 23 ]. While
this is rarely achievable in practice, online A/B testing allows for
more controlled exposure, where interaction data more reliably
re$ects user preferences over the presented items, and exposure
bias can be actively managed by the experimenter. However, such
tests are costly and risky: researchers may lack access to live tra%c,
and practitioners may hesitate to expose users to untested models.
As a result, o!ine evaluation remains a prevalent practice, despite
its well-documented limitations [7, 22].
A well-known issue in o!ine evaluation is selection bias, also
referred to as logging bias [ 48 ]. Since users interact only with items
they were shown, the resulting data is often extremely sparse [ 46 ]
and *missing-not-at-random* (MNAR) [ 35 ]. This skews evaluation in
favor of models that replicate past exposure patterns rather than
uncovering genuine preferences [ 39, 42 ]. While various techniques
have been proposed to mitigate thisâ€”such as inverse propensity
scoring [ 45 ] and counterfactual estimators [ 17 ]â€”this paper focuses
on a second, often overlooked source of distortion: *sampling bias* .
Due to the computational cost incurred by ranking over large
item catalogs, most o!ine evaluation pipelines sample a small set of
negatives for each positive instance when computing top- *ğ¿* metrics.
The way these negatives are sampledâ€”whether uniformly [ 10, 24 ],
by popularity [ 6, 11 ], by positivity [ 39 ], or using debiasing strategies like weighted sampling (WTD) [ 5 ] or Skew [ 21, 30 ]â€”can substantially impact evaluation outcomes [ 8, 25, 27, 33 ]. However,
prior work typically assesses sampling strategies using "xed logged
datasets, without accounting for di#erent sources or intensities of
exposure bias [ 15, 21, 51 ]. Moreover, most studies take the logged
data itself as ground truth, leaving open the question of how well
sampled evaluation outcomes re$ect actual user preferences.
To systematically examine these e#ects, we distinguish between
the data we wish we hadâ€”fully observed user preferencesâ€”and the
data actually available for evaluation, which is shaped by logging
and sampling decisions. Inspired by past research, we investigate
the reliability of existing sampling strategies across a range of sample sizes [ 15, 21, 51 ]. Unlike prior studies on sampling strategies,
we conduct experiments under multiple simulated exposure bias
conditions and provide the "rst systematic analysis of how these
sampling strategies performâ€”not just against biased logged data,
but also in comparison to ground-truth user preferences. By disentangling the e#ects of exposure and sampling bias, this work
establishes a more principled foundation for o!ine evaluation and
o#ers practical guidance on selecting sampling strategies in the
presence of biased data. Our contributions are as follows:


-----

RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic Pereira et al.



  - We formalize four dimensions of sampling reliability: res
olution (model separability), "delity (agreement with full
evaluation), robustness (stability under bias), and predictive
power (alignment with ground truth).

  - We propose an evaluation framework based on a fully ob
served dataset, enabling a controlled assessment of sampled
evaluation under both unbiased and biased exposure.

  - We conduct a comprehensive empirical analysis of widely

used sampling strategies across multiple logging con"gurations, assessing their reliability in ranking-based evaluation.

In the remainder of this paper, Section 2 reviews related work on
exposure bias, sampling strategies, and the reliability of evaluation
protocols. Section 3 introduces our conceptual framework for sampling reliability assessment. Section 4 presents our experimental
setup, including how we simulate exposure bias ahead of sampling.
Section 5 reports our results, structured around four dimensions
of evaluation reliability. Finally, Section 6 concludes with practical
implications and directions for future work.
### **2 Related Work**

A rich body of research has examined the challenges of o!ine
recommender evaluation, particularly in the presence of biases
introduced by system-driven exposure and incomplete observations.
In this section, we review related work in three areas: methods for
mitigating exposure bias in logged data, approaches to addressing
sampling bias in evaluation protocols, and recent e#orts focused
on assessing the reliability of evaluation procedures.
### **2.1 Exposure Bias in Logged Data**

User interaction data in recommender systems is inherently biased
by the systemâ€™s exposure policies. Users can only interact with
items they are shown, which means that logged interactions are
confounded by both user preferences and the systemâ€™s exposure
decisions [ 48 ]. This phenomenonâ€”known as *exposure bias* â€”renders
the logged data MNAR and undermines the validity of both learning
and evaluation [ 9 ]. Prior work has characterized the nature of
exposure bias [ 4 ] and proposed several mitigation strategies. These
include inverse propensity weighting and its variants [ 41, 45, 47 ],
counterfactual learning approaches [ 12, 18, 20, 29, 31 ], and selfsupervised methods for estimating propensities [ 32 ]. Some studies
adopt doubly robust estimation to combine reward modeling with
exposure modeling for improved accuracy [13].

While much of this literature focuses on learning unbiased recommenders, a growing number of works examine how exposure
bias a#ects evaluation. Wang et al . [51] and Liu et al . [34] show that
biased logging policies can signi"cantly distort o!ine evaluation
metrics and relative model rankings, leading to potentially misleading conclusions. Other studies have examined the implications for
fairness [ 1, 19 ], highlighting how skewed exposure can amplify disparities across users or item groups. Several works propose adjusted
evaluation procedures that assume missing-at-random (MAR) exposure and seek to correct standard metrics accordingly [ 35, 51 ].
Finally, randomized exposure datasets such as Yahoo R3 [ 35 ] and
KuaiRand [ 16 ] provide empirical testbeds for enabling more principled evaluation, helping to isolate the e#ects of speci"c exposure
policies from underlying user preferences.

### **2.2 Sampling Bias in O!line Evaluation**

O!ine evaluation is also shaped by sampling candidate items used
to approximate top- *ğ¿* metrics. Because scoring all items is computationally expensive, evaluations typically sample a subset of
non-positive items for each user. This introduces *sampling bias*,
which can signi"cantly a#ect evaluation metric values and, in turn,
alter the relative ranking of models by over- or underestimating
their true recommendation performance [8, 21, 25, 27].

The choice of sampling strategy matters. Sampling uniformly or
by popularity [ 27 ], positivity [ 33 ], or exposure [ 6 ] can each lead to
di#erent evaluation outcomes. Some methods aim to approximate
the full ranking using weighted correction techniques [ 6, 30 ], or
adaptively adjust the number of items sampled per user [ 28 ]. An
alternative to sampling unexposed items as synthetic negatives is
to evaluate over impressionsâ€”items that were actually exposed to
the user, regardless of interaction. This naturally avoids unexposed
false negatives and grounds evaluation in observed choice sets [ 37 ].
Impression-based evaluation has been explored in datasets like ContentWise [ 38 ], MIND [ 52 ], and FINN.no [ 14 ]â€”but such datasets are
rare, and the reliability of impression-based metrics under di#erent
exposure biases remains underexplored. In our study, we extend
this line of work by explicitly assessing how di#erent strategies for
sampling exposure data a#ect evaluation outcomes.
### **2.3 Reliability of Evaluation Protocols**

A third line of work examines the broader question of whether of$ine evaluation protocols reliably re$ect true model performanceâ€”a
concern known as *meta-evaluation* . This perspective emphasizes
that even well-intentioned evaluation strategies may yield misleading conclusions if confounding factors are unaccounted for.

In information retrieval, Sanderson et al . [43] introduced the
concept of *predictive power*, de"ned as the degree to which partial judgments (e.g. sampled relevance) recover system rankings
derived from complete judgments. In recommendation, Krichene
and Rendle [25] showed that commonly used sampled metrics like
recall@ *ğ¿* can be unfaithful estimators of ranking performance when
evaluation is conducted over sampled candidates. Liu et al . [34]
applied item response theory to model the latent abilities of recommender algorithms and the di%culty of recommendation tasks,
revealing that high scores on rank-based metrics often fail to re$ect
genuine improvements in modeling user preferences.

Other studies have focused on the evaluation sensitivity to design choices. Carraro and Bridge [6] and CaÃ±amares and Castells

[8] conducted empirical evaluations of how sampling strategies
a#ect metric consistency. Ihemelandu and Ekstrand [21] provided
a detailed framework for analyzing candidate set sampling under known exposure. Surveys such as Castells and Mo#at [7] and
Zangerle and Bauer [53] have emphasized the fragility of current
evaluation practices, highlighting issues such as test set leakage,
missing negative feedback, and the lack of robustness across tasks.

Most prior studies on sampling reliability use "xed exposure
logs and compare sampled evaluation against full evaluation over
those logs [ 25, 34 ]. Our work expands on this by systematically
varying both the exposure policy and the sampling strategy, and
evaluating reliability across four dimensions: *resolution*, the capacity
to distinguish between competing models; *!delity*, the agreement


-----

On the Reliability of Sampling Strategies in O!line Recommender Evaluation RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic


between sampled and full evaluations; *robustness*, the stability of
results under varying bias conditions; and *predictive power*, the
ability to recover ground-truth model rankings.
### **3 Conceptual Framework**

To facilitate systematic analysis, we develop a conceptual framework that separates o!ine evaluation into three sequential stages:
user preferences, item exposure, and evaluation sampling. This strati"ed approach allows us to formally characterize the discrepancy
between ideal evaluation based on complete preference information
and practical evaluation constrained by exposure bias and sampling
limitations. Let *ğ‘€* be the set of users and *ğ‘* the set of items. We

de"ne three artifacts central to our evaluation design:

*Ground-Truth Preferences (* *ğ‘‚* *).* We assume a fully observed useritem preference matrix *ğ‘‚* â†‘ R [|] *[ğ¿]* [|â†“|] *[ğ‘€]* [|], in which each entry indicates
the extent to which an item is relevant to a user. This matrix represents an idealized view of user preferencesâ€”every user has an
opinion about every item. While such complete data is seldom available in practice, *ğ‘‚* serves as a notional ground truth against which
the reliability of evaluation pipelines can be measured.

*Logged Interactions (* *ğ‘ƒ* *).* In real-world systems, users are exposed
to only a subset of items. The logged matrix *ğ‘ƒ* is a partially observed
version of *ğ‘‚*, derived by applying an exposure policy. It contains
interactions only for items shown to each user, while unexposed
entries remain missing. Thus, *ğ‘ƒ* re$ects both user preferences and
exposure bias introduced by the systemâ€™s logging mechanism. Without loss of generality, we assume a typical implicit feedback scenario, where observed interactions are interpreted as either positive
(indicating meaningful engagement) or not.

*Sampled Interactions (* *ğ‘‚* *ğ‘* *and* *ğ‘ƒ* *ğ‘* *).* To operationalize top- *ğ¿* evaluation, it is common to construct evaluation sets by pairing each
userâ€™s known positives with a sampled subset of non-positivesâ€”
that is, items that were either exposed but not interacted with, or
not exposed at all. This yields a typical sampled o!ine evaluation
scenario, *ğ‘ƒ* *ğ‘*, where sampling is applied to the partially observed
logged matrix *ğ‘ƒ* . To isolate the impact of sampling from exposure
e#ects, we also consider a sibling scenario, *ğ‘‚* *ğ‘*, where sampling is
applied to the fully observed ground-truth matrix *ğ‘‚* .

This conceptual framework centers *ğ‘ƒ* *ğ‘* as the primary evaluation
artifact and supports a systematic analysis of multiple sources of
distortion. We begin by assessing how well *ğ‘ƒ* *ğ‘* distinguishes among
competing recommender models, re$ecting evaluation resolution
as an intrinsic property of the sampling protocol itself. Next, comparing *ğ‘ƒ* *ğ‘* to *ğ‘ƒ* (or *ğ‘‚* *ğ‘* to *ğ‘‚* ) isolates the e#ect of sampling under
"xed exposure. In turn, comparing *ğ‘ƒ* *ğ‘* to *ğ‘‚* *ğ‘* captures the impact
of exposure bias under "xed sampling. Finally, comparing *ğ‘ƒ* *ğ‘* to *ğ‘‚*
reveals the combined distortion introduced by both logging and
sampling, and ultimately, the extent to which sampled evaluation
over biased data re$ects true user preferences.
### **4 Experimental Setup**

Our experiments are designed to evaluate the reliability of samplingbased o!ine evaluation across four complementary dimensions.
Taking *ğ‘ƒ* *ğ‘* as our main object of studyâ€”the artifact produced by


applying a sampling strategy to logged dataâ€”we assess the quality
of the resulting evaluation along the following axes:

*Q1.* *Resolution: Can the sampler distinguish between models?* We

ask whether sampling provides enough resolution to differentiate between recommender systems, measured by the
number of ties in model rankings produced under *ğ‘ƒ* *ğ‘* .
*Q2.* *Fidelity: Does the sampler preserve full evaluation outcomes?*

We compare sampled evaluations ( *ğ‘ƒ* *ğ‘* and *ğ‘‚* *ğ‘* ) to their full
counterparts ( *ğ‘ƒ* and *ğ‘‚* ) to assess whether sampling introduces distortions, even when the data source is "xed.
*Q3.* *Robustness: Is the sampler stable under bias?* We apply the

same sampling strategy to both unbiased ground-truth data
( *ğ‘‚* ) and biased logged data ( *ğ‘ƒ* ), and assess whether the eval
uation outcomes remain consistent.

*Q4.* *Predictive Power: Does the sampler help us recover the truth?*

We examine the extent to which evaluations based on a

biased sample ( *ğ‘ƒ* *ğ‘* ) yield the same model rankings we would
obtain from full evaluation on ground-truth preferences ( *ğ‘‚* ).

The remainder of this section describes how we instantiate the

key elements of our framework introduced in Section 3â€”groundtruth preferences ( *ğ‘‚* ), logged observations ( *ğ‘ƒ* ), and sampled evaluations ( *ğ‘‚* *ğ‘*, *ğ‘ƒ* *ğ‘* )â€”and how we vary logger policies, sampling strategies,
and recommender models to answer these questions. All experiments were conducted by extending the Microsoft Recommenders
framework (v1.2.0). [1] Experiments were run on a Linux workstation
with Ubuntu 22.04.4 LTS and Python 3.10.16, equipped with an
IntelÂ® XeonÂ® Silver 4314 CPU (16 cores, 32 threads, base clock
2.4 GHz), 512 GB DDR4 RAM, and a single NVIDIAÂ® A100 GPU.
To ensure reproducibility, all code is publicly available. [2]
### **4.1 Dataset and Ground Truth Construction**

Our experiments use the KuaiRec dataset [ 15 ], [3] a large-scale collection of user interactions with short-form videos. As illustrated in

Fig. 1, interactions are treated as implicit feedback, with binary relevance labels derived from user engagement behavior. Speci"cally,
following Gao et al . [15], we de"ne positive feedback as cases where
the cumulative watch time of a video exceeds twice its duration.

The dataset is partitioned into training and test splits following the
original authorsâ€™ methodology. Summary statistics for both splits
are provided in Table 1. Notably, the test split contains complete
interaction histories (i.e. nearly 100% density), allowing us to construct a fully-observed binary relevance matrix that serves as our
ground-truth preference matrix *ğ‘‚* . To the best of our knowledge,
KuaiRec is the only publicly available dataset with full exposure
logs, enabling the controlled simulations central to our study.

**Table 1: KuaiRec dataset statistics.**

**#Users** **#Items** **#Interact** [â†”] **#Interact** [â†—] **Density**

*Train* 7,176 10,728 936,390 11,594,416 16.3%

*Test* 1,411 3,327 217,175 4,459,395 99.6%

1 https://github.com/recommenders-team/recommenders
2 https://github.com/LatinUFMG/recommenders-sampling
3 https://kuairec.com/


-----

RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic Pereira et al.













### **4.2 Logger Simulation**


**Figure 1: The experimental setup using the KuaiRec dataset.**

(8) **WTDH@** *ğ‘…* **:** heuristic version of WTD with uniform expo

Following Gao et al . [15], to obtain partially observed data *ğ‘ƒ*, we
simulate exposure using three logging strategies applied to *ğ‘‚* :

  - **Uniform:** items are exposed to users uniformly at random,

reproducing a baseline, unbiased exposure scenario.

  - **Popularity-biased:** items are exposed with probabilities

proportional to their global popularity in *ğ‘‚* .

  - **Positivity-biased:** items are exposed with probabilities pro
portional to their positive feedback count in *ğ‘‚* .

These logging strategies are inspired by prior work on modeling
and analyzing exposure bias in recommendation [ 21, 30 ]. For each
logger, we retain a subset of each userâ€™s interactions from *ğ‘‚* according to target sparsity levels [4] â€” 0% *,* 10% *,* 30% *,* 50% *,* 70% *,* 85% *,* 90% *,* 95% â€”
while ensuring at least one positive item per test user. The resulting
matrix *ğ‘ƒ* captures logged interactions shaped by exposure bias.
### **4.3 Sampling Strategies**

From each simulated logged interactions matrix *ğ‘ƒ*, we construct
multiple sampled evaluation matrices *ğ‘ƒ* *ğ‘* by retaining all known
positives and appending a set of non-positive items per test user,
according to one of the following sampling strategies:

(1) **Full:** uses all available non-positives (no sampling).
(2) **Exposed:** uses all *exposed* non-positives for each user.
(3) **Random@** *ğ‘„* **:** samples *ğ‘„* non-positives uniformly at random,

where *ğ‘„* is the number of exposed non-positives per user.
(4) **Random@** *ğ‘…* **:** samples *ğ‘…* non-positives uniformly at random.
(5) **Popularity@n:** samples *ğ‘…* non-positive items, where selec
tion probabilities are determined by applying Zipfâ€™s law [ 36 ]
based on each itemâ€™s popularity rank [15].
(6) **Positivity@** *ğ‘…* **:** samples *ğ‘…* non-positive items, with selection

probabilities weighted by their estimated relevance scores,
also following Zipfâ€™s law [15, 36].
(7) **WTD@** *ğ‘…* **:** samples *ğ‘…* non-positives weighted based on em
pirical exposure distributions [5].

4 A random 10% of *ğ‘‚* is held out prior to logging and consistently excluded from all
logger simulations. This partition is used exclusively to support samplers that require
unbiased data for propensity estimation [ 5 ], as discussed in Section 4.3. Sparsity levels
here refer to the remainder of *ğ‘‚* after this holdout.


sure assumptions.
(9) **Skew@** *ğ‘…* **:** samples *ğ‘…* non-positives, with selection probabili
ties de"ned by empirical popularity distributions [21, 30].

For samplers (4)â€“(9), we vary the number of non-positives sampled, *ğ‘…* â†‘{ 1 *,* 2 *,* 5 *,* 10 *,* 20 *,* 50 *,* 100 *,* 200 *,* 500 *,* 1000 }, to assess its impact
on the outcome of the evaluation process. Combined with the 24
loggers introduced in Section 4.2 (3 logging strategies â†“ 8 sparsity
levels), these 63 samplersâ€”comprising 3 "xed samplers and 6 parametric ones, each evaluated at 10 sample sizesâ€”yield a total of 1,512
evaluation scenarios used in our analysis.
### **4.4 Recommender Models**

To assess sampling reliability across diverse recommendation techniques, we implement and evaluate the following models, covering
distinct algorithmic families:

  - **ALS** [ 49 ]: Alternating Least Squares is a matrix factorization

technique that models user-item interactions through latent
factors. It alternately optimizes user and item embeddings
to minimize squared error, and is well-suited for implicit
feedback.

  - **BPR** [ 40 ]: Bayesian Personalized Ranking is a pairwise rank
ing method that optimizes for item ranking by learning to
prefer observed interactions over unobserved ones, using a
Bayesian framework.

  - **LightFM** [ 26 ]: A hybrid model combining collaborative and

content-based signals. We train two variants: one using collaborative signals only, and another leveraging metadata
features. Its $exibility makes it e#ective in both sparse and
cold-start settings.

  - **SAR-Cosine** [ 44 ]: A lightweight item-based collaborative

"ltering approach that computes item-item similarities using
cosine similarity.

  - **SAR-Jaccard** [ 44 ]: A variant of SAR-Cosine that uses the Jac
card index for similarity computation, making it particularly
suitable for binary interaction data.

  - **Popularity** : A non-personalized baseline that ranks items

by global popularity, independent of the target user.

  - **Random** : A naive baseline that ranks items randomly.


-----

On the Reliability of Sampling Strategies in O!line Recommender Evaluation RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic


We optimized hyperparameters for each model using random
search [ 2 ] with the Hyperopt library [5] and parallelized evaluations
via Ray Tune. [6] Each search ran for 128 iterations, with candidate
con"gurations evaluated through 5-fold cross-validation on random
splits of the training set. Full details on search ranges and selected
values are available in the public code repository.
### **4.5 (Meta-)Evaluation Metrics**

Each of the seven models described in Section 4.4 is evaluated on

the same sampled matrix *ğ‘ƒ* *ğ‘*, for a total of 1,512 such matrices, as
detailed in Section 4.3. For each case, we compute precision, recall,
and nDCG at various ranking cuto#s (@5, @10, @50, @100). While
we report results primarily for nDCG@100 due to space constraints,
this deeper cuto# was deliberately chosen to better assess the complete ranking behavior of sampling strategies. Although users typically interact with only the top few items, evaluation sampling
a#ects the entire ranked list. Corroborating the "ndings of Valcarce
et al . [50], our analyses showed that nDCG@100 provides greater
resolution between sampling strategies than conventional shorter
cuto#s, while still re$ecting the same directional patterns observed
across all metrics and cuto#s. Based on these evaluation results, we
assess the reliability of each *ğ‘ƒ* *ğ‘* construction protocol using two
meta-evaluation metrics:

  - **Tie rate:** the fraction of tied recommender model pairs in

each individual evaluation scenario [8]; used to address *Q1* .

  - **Kendallâ€™s** *ğ‘†* **:** the degree of ranking agreement across pairs

of evaluation scenarios; used to address *Q2* â€“ *Q4* .
### **5 Experimental Results**

We now present "ndings from our experimental study, organized
around four key dimensions of evaluation reliability. We begin
with resolution ( *Q1* )â€”whether di#erent sampling strategies can
e#ectively separate recommender models based on their performance. We then analyze "delity ( *Q2* )â€”how closely each strategy
reproduces model rankings obtained under full evaluationâ€”and
robustness ( *Q3* )â€”the stability of these rankings under varying exposure biases. Finally, we address predictive power ( *Q4* )â€”the extent
to which evaluation over biased and sampled data recovers the
ground-truth ranking de"ned by fully observed preferences. For
all experiments, con"dence intervals (95%, from 1,000 bootstrap
resamples) were computed but most are too small to be visible.
### **5.1 Q1: Resolution**

In this section, we address *Q1* by assessing the extent to which
di#erent sampling strategies provide su%cient resolution to distinguish between recommender models. We quantify resolution
using the tie rate [ 3, 8 ], de"ned as the fraction of recommender
model pairs that achieve identical nDCG@100 scores for a given
user. While the tie rate serves as a proxy for discriminative power, it
fundamentally di#ers from interpretations rooted in statistical signi"cance testing. This measure is particularly valuable in scenarios
where traditional statistical tests may be underpoweredâ€”such as in
cases involving very small or very large sample sizes [ 8 ]. Lower tie

5 https://github.com/hyperopt/hyperopt
6 https://docs.ray.io/en/latest/tune/index.html


rates indicate greater ability to resolve di#erences in model qualityâ€”
hence greater resolution. For each con"guration, we compute tie
rates at the user level and average them across users.

Fig. 2 presents the average tie rate (log scale on the *ğ‘‡* -axis) as
a function of sample size *ğ‘…* (on the *ğ‘ˆ* -axis), for various sampling
strategies (shown as plot lines). Horizontal lines represent "xed-size
samplers (Full, Exposed, and Random@ *ğ‘„* ), which do not vary with
*ğ‘…* . Each subplot corresponds to a unique combination of logging
strategy (rows: Uniform, Popularity-biased, Positivity-biased) and
sparsity level (columns, from 0% to 95%).

From Fig. 2, we "rst note that, in line with previous research [ 8 ],
tie rates are particularly high in too small or too large samples, when
recommenders are more likely to tie at either very high or very low
nDCG values, respectively. However, logger sparsity also plays a
critical role: under low sparsity levels (e.g., 0â€“50%), most samplers
achieve low tie rates with small to moderate sample sizes, indicating good resolution. As sparsity increasesâ€”particularly at 90% and
95%â€”tie rates remain substantially higher, even for large *ğ‘…*, due to
the loss of signal in the logged data. The choice of sampler becomes
especially important under these challenging conditions. Samplers
like Skew, Popularity, and Positivity consistently yield lower tie
rates, e#ectively preserving meaningful distinctions between models even under high sparsity and biased exposure. On the other
hand, the Full sampler, which includes all items, often yields lower
resolution in distinguishing among models across most settingsâ€”
particularly as sparsity increases. Exposed and Random@ *ğ‘„* serve as
useful baselines, assuming access to full exposure or a "xed sample
size based on exposed items, and show improved performance in
high-sparsity scenarios. Overall, samplers with small to moderate
size *ğ‘…* continue to deliver robust results.

Recalling *Q1*, not all samplers are equally capable of distinguishing models. The ability to recover meaningful model rankings depends on both the sample size and the interaction between the
sampler and the exposure bias introduced during logging. Carefully
designed samplers that account for item exposure and produce
a small to moderate sample size, such as Skew, Popularity, and
Positivity, exhibit stronger resolution, particularly under realistic
constraints of logging bias and high sparsity.
### **5.2 Q2: Fidelity**

To address *Q2*, we examine the "delity of each sampling strategy, i.e.
how closely the model rankings obtained from a sampled evaluation
( *ğ‘ƒ* *ğ‘* ) align with those derived from the full reference set ( *ğ‘ƒ* ). We
measure "delity using Kendallâ€™s *ğ‘†*, a rank correlation coe%cient that
ranges from 0 (no agreement) to 1 (perfect agreement). For each user,
we compute Kendallâ€™s *ğ‘†* between the rankings of recommenders
under the sampled and reference evaluations, and report the average
across users as an overall measure of ranking agreement.

Fig. 3 presents these results. Each subplot shows average Kendallâ€™s
*ğ‘†* (on the *ğ‘‡* -axis) as a function of sample size *ğ‘…* (on the *ğ‘ˆ* -axis), across
di#erent sampling strategies (plot lines). Horizontal lines indicate
"xed-size samplers (Full, Exposed, and Random@ *ğ‘„* ), which do not
vary with *ğ‘…* . Subplots are arranged by logging strategy (rows) and
sparsity level (columns). In the "rst column (0% sparsity), where the
logged matrix *ğ‘ƒ* is identical to the ground-truth preference matrix
*ğ‘‚*, the comparison is e#ectively between *ğ‘‚* *ğ‘* and *ğ‘‚* .


-----

RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic Pereira et al.

**Figure 2: Average tie rate across test users (log scale,** *ğ‘‡* **-axis) as a function of sample size** *ğ‘…* **(** *ğ‘ˆ* **-axis) for di!erent sampling**
**strategies (plot lines), shown across logging strategies (rows) and sparsity levels (columns). A tie occurs when two recommenders**
**achieve the same nDCG@100 for a given user. Lower tie rates indicate greater resolution. Horizontal lines represent "xed-size**
**samplers (Full, Exposed, Random@** *ğ‘„* **). Best viewed in color.**

**Figure 3: Average Kendallâ€™s** *ğ‘†* **correlation between model rankings under sampled evaluation (** *ğ‘ƒ* *ğ‘* **) and their corresponding**
**reference rankings (** *ğ‘ƒ* **), shown as a function of sample size** *ğ‘…* **(** *ğ‘ˆ* **-axis) for di!erent sampling strategies (plot lines), across logging**
**strategies (rows) and sparsity levels (columns). The** *ğ‘‡* **-axis ranges from 0 (no agreement) to 1 (perfect agreement), with higher**
**values indicating greater "delity. Horizontal lines indicate "xed-size samplers (Full, Exposed, Random@** *ğ‘„* **). In the "rst column**
**(0% sparsity), where** *ğ‘ƒ* = *ğ‘‚* **, the comparison is between** *ğ‘‚* *ğ‘* **and** *ğ‘‚* **. Best viewed in color.**


As expected, "delity improves with sample size: Kendallâ€™s *ğ‘†*
rises quickly as *ğ‘…* increases, particularly for samplers like WTD
and WTDH, which incorporate exposure or relevance signals, and
Random, which assumes a uniform distribution. These methods
achieve near-maximum "delity at moderate sparsity (10%â€“50%)


with *ğ‘…* â†˜ 200. Skew exhibited similar performance under low sparsity conditions; however, its e#ectiveness declines more rapidly as
sparsity increases, mainly under the positivity-biased logger. In contrast, Popularity and Positivity converge later, especially under high
sparsity. Notably, "delity depends not just on sample size, but also
on which items are sampled. Although Exposed and Random@ *ğ‘„*


-----

On the Reliability of Sampling Strategies in O!line Recommender Evaluation RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic

**Figure 4: Average Kendallâ€™s** *ğ‘†* **correlation between model rankings under sampled evaluation from biased (** *ğ‘ƒ* *ğ‘* **) and unbiased**
**(** *ğ‘‚* *ğ‘* **) data, with both constructed using the same sampler, shown as a function of sample size** *ğ‘…* **(** *ğ‘ˆ* **-axis) for di!erent sampling**
**strategies (plot lines), across logging strategies (rows) and sparsity levels (columns). The** *ğ‘‡* **-axis ranges from 0 (no agreement) to**
**1 (perfect agreement), with higher values indicating greater robustness to exposure bias. Horizontal lines represent "xed-size**
**samplers (Full, Exposed, Random@** *ğ‘„* **). Best viewed in color.**


yield similar resultsâ€”since both use the same number of samples
per userâ€”Exposed tends to produce lower Kendallâ€™s *ğ‘†*, likely due to
the presence of logger bias and increased sparsity. The 0% sparsity
case, where *ğ‘ƒ* = *ğ‘‚*, further con"rms that sampling alone can distort
rankings, even in the absence of exposure bias.

Recalling *Q2*, our results suggest that "delity is not guaranteed by
sample size alone; rather, it hinges on how well a sampling strategy
captures the structure and information present in the original data.
Even under ideal conditionsâ€”such as 0% sparsity, where the full
preference matrix is availableâ€”samplers di#er markedly in their
ability to preserve model rankings. This highlights that the sampling
process introduces its own source of distortion, independent of
exposure bias. Consequently, designing sampling protocols that
align more closely with user experience can be just as important as
ensuring su%cient evaluation coverage.
### **5.3 Q3: Robustness**

To address *Q3*, we evaluate the robustness of each sampling strategyâ€”
that is, how sensitive model rankings are to exposure bias in the
logged data. We assess this by comparing *ğ‘ƒ* *ğ‘*, sampled from the biased matrix *ğ‘ƒ*, against *ğ‘‚* *ğ‘*, sampled from the fully observed groundtruth matrix *ğ‘‚*, using the same sampler in both cases. Robustness
is measured by Kendallâ€™s *ğ‘†*, computed per user between rankings
under *ğ‘ƒ* *ğ‘* and *ğ‘‚* *ğ‘*, and averaged across users. Higher values indicate
more consistent evaluations regardless of exposure bias.

In Fig. 4, each subplot shows average Kendallâ€™s *ğ‘†* (on the *ğ‘‡* -axis) as
a function of sample size *ğ‘…* (on the *ğ‘ˆ* -axis), across di#erent sampling
strategies (plot lines). Horizontal lines represent "xed-size samplers
(Full, Exposed, Random@ *ğ‘„* ). Rows correspond to logger types and
columns to sparsity levels. Since *ğ‘‚* *ğ‘* is "xed for a given sampler


and sparsity level, all observed variation in Kendallâ€™s *ğ‘†* re$ects how
sampling interacts with bias in the logged data *ğ‘ƒ* .

From Fig. 4, we "rst note that, in the absence of exposure bias
("rst column, 0% sparsity), the comparison between *ğ‘ƒ* *ğ‘* and *ğ‘‚* *ğ‘* reduces to a test of internal consistency under repeated draws from
the same fully observed population ( *ğ‘ƒ* = *ğ‘‚* ), resulting in $at lines
across *ğ‘…* . As in earlier sections, increasing *ğ‘…* generally improves robustness when bias is present, but the extent and consistency of this
improvement vary across samplers and logging conditions. WTD,
and WTDH retain high agreement between *ğ‘ƒ* *ğ‘* and *ğ‘‚* *ğ‘*, especially
with the Uniform logger, suggesting they better mitigate the distortive e#ects of exposure bias in scenarios with MAR data available
and low sparsity. In contrast, Popularity and Positivity show limited
gains, particularly under high sparsity. Full isolates the e#ect of
exposure bias by avoiding any sampling, and performs better in the
majority of cases, indicating that the choice of sampler signi"cantly
impacts evaluation results. Exposed and Random@ *ğ‘„* show similar
performance under low sparsity; however, as sparsity increases,
Random@ *ğ‘„* consistently outperforms Exposed, reinforcing once
again that Exposed is highly a#ected by logger bias.

Recalling *Q3*, the results in this section show that most sampling
strategies are indeed a#ected by exposure biasâ€”but to varying degrees. While some exhibit strong robustness to bias, others diverge
signi"cantly from their unbiased counterparts, especially in sparse
or skewed settings. These "ndings highlight that samplers cannot
be evaluated in isolation from the data they operate on: robustness
depends not just on the strategy, but on how that strategy interacts
with the structure and bias of the logged data.


-----

RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic Pereira et al.

**Figure 5: Average Kendallâ€™s** *ğ‘†* **correlation between model rankings under sampled evaluation from biased data (** *ğ‘ƒ* *ğ‘* **) and the**
**ground-truth rankings from fully observed preferences (** *ğ‘‚* **), shown as a function of sample size** *ğ‘…* **(** *ğ‘ˆ* **-axis) for di!erent sampling**
**strategies (plot lines), across logging strategies (rows) and sparsity levels (columns). The** *ğ‘‡* **-axis ranges from 0 (no agreement) to**
**1 (perfect agreement), with higher values indicating greater predictive power. Horizontal lines represent "xed-size samplers**
**(Full, Exposed, Random@** *ğ‘„* **). Best viewed in color.**

### **5.4 Q4: Predictive Power**

In the previous sections, *Q1* assessed the intrinsic resolution capacity of each sampling strategy, while *Q2* and *Q3* focused on internal
agreementâ€”either under full data or matched samplers. We now
turn to *Q4*, which evaluates the predictive power of di#erent samplers: the extent to which evaluation over biased and sampled data
( *ğ‘ƒ* *ğ‘* ) reliably recovers the true model ranking induced by groundtruth user preferences ( *ğ‘‚* ). Predictive power is measured using
Kendallâ€™s *ğ‘†*, computed per user between model rankings from *ğ‘ƒ* *ğ‘*
and *ğ‘‚*, and averaged across users.

As expected, predictive power improves with larger *ğ‘…*, but this
improvement is uneven and strongly dependent on both the sampler and the logging con"guration. Skew, WTD, and WTDH again
stand out, achieving strong alignment with *ğ‘‚* even under moderate to high sparsity, outperforming Full in higher sparsity. These
methods are more resilient to compounded distortions from logging and sampling, often reaching Kendallâ€™s *ğ‘†* above 0.7 at *ğ‘…* â†˜ 200
in the 10%â€“50% sparsity range. Random yielded results comparable to those of weighted methods, highlighting its potential as a
simple, generalizable, and easy-to-apply strategy. In contrast, Popularity and Positivity samplers exhibit much lower predictive power,
especially in biased or sparse regimes. Once again, Exposed and
Random@ *ğ‘„* achieve similar results, reinforcing the notion that Exposed is a#ected by logger bias, particularly under high sparsity.

Recalling *Q4*, our results show that sampled evaluation over biased data can approximate ground-truth rankings, but only under
the right conditions. Samplers like Random, WTD, WTDH, and
Skew achieve high predictive power with su%cient *ğ‘…*, even under
moderate sparsity. In contrast, Popularity and Random@ *ğ‘„* struggle,
particularly in sparse or biased settings. Overall, predictive power


under sampled evaluation is attainable, but requires sampling strategies that are both bias-aware and evaluation-e%cient.
### **6 Conclusions**

This study introduced a framework for assessing the reliability of
sampling-based o!ine evaluation of recommender systems. Unlike
prior work that targets either exposure debiasing or sampler design
in isolation, our contribution lies in disentangling and analyzing
their interaction. Through a meta-evaluation protocol applied to
biased and unbiased data under varying sparsity and sample sizes,
we assess sampling resolution, "delity, robustness, and predictive
power. Leveraging a fully observed test set, we derive ground-truth
recommender model rankings to systematically quantify distortions
introduced by both logging and sampling.

Our "ndings show that sampling strategies vary in their ability
to distinguish models ( *Q1* ), preserve full evaluation rankings ( *Q2* ),
remain stable under exposure bias ( *Q3* ), and recover ground-truth
rankings ( *Q4* ). Strong performance depends not only on sample size,
but also on sample composition and alignment with the exposure
process. Bias-aware strategies such as WTD, WTDH, and Skew
consistently outperform naive baselines, while Exposed emerges
as a surprisingly strong contender under realistic constraints.

No single sampler excels across all criteria, requiring trade-o#s:
high-resolution methods may falter under bias, while robust strategies might miss "ne-grained di#erences. Moreover, larger samples
do not ensure better evaluations if item selection is biased or un
informative. These results highlight the importance of carefully
designing sampling strategies, especially in sparse or biased settings, by leveraging known propensities and accounting for the
interaction between logging, sparsity, and sampling.


-----

On the Reliability of Sampling Strategies in O!line Recommender Evaluation RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic


A broader challenge going forward is the limited availability of
fully observed datasets like the one used in this study, which are
essential for assessing the generalizability of our "ndings across
di#erent domains. While such data is uncommon, it enables a level
of controlled evaluation crucial for understanding the e#ects of
exposure and sampling bias. Applying our framework to domains
where dense user-item feedback can be collectedâ€”or approximatedâ€”
would help validate the generality of our "ndings. Future work may
also extend this analysis to a wider range of models, metrics, and
evaluation objectives, including fairness and user trust. An especially promising direction is the development of learned sampling
strategies that adaptively select evaluation candidates based on
exposure patterns, model uncertainty, or past evaluation outcomes.
### **Acknowledgments**

This research was partially funded by the authorsâ€™ individual grants
from CNPq and FAPEMIG.
### **References**

[1] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad

Mobasher. 2019. The Unfairness of Popularity Bias in Recommendation.
arXiv:1907.13286 [cs.IR] https://arxiv.org/abs/1907.13286

[2] James Bergstra, Daniel Yamins, and David Cox. 2013. Making a Science of Model

Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision
Architectures. In *Proceedings of the 30th International Conference on Machine*
*Learning (Proceedings of Machine Learning Research, Vol. 28)*, Sanjoy Dasgupta
and David McAllester (Eds.). PMLR, Atlanta, Georgia, USA, 115â€“123. https:
//proceedings.mlr.press/v28/bergstra13.html

[3] Chris Buckley and Ellen M. Voorhees. 2000. Evaluating evaluation measure

stability. In *Proceedings of the 23rd Annual International ACM SIGIR Conference*
*on Research and Development in Information Retrieval* (Athens, Greece) *(SIGIR*
*â€™00)* . Association for Computing Machinery, New York, NY, USA, 33â€“40. https:
//doi.org/10.1145/345508.345543

[4] RocÃ­o CaÃ±amares and Pablo Castells. 2018. Should I Follow the Crowd? A Proba
bilistic Analysis of the E#ectiveness of Popularity in Recommender Systems. In
*The 41st International ACM SIGIR Conference on Research & Development in Infor-*
*mation Retrieval* (Ann Arbor, MI, USA) *(SIGIR â€™18)* . Association for Computing Machinery, New York, NY, USA, 415â€“424. https://doi.org/10.1145/3209978.3210014

[5] Diego Carraro and Derek Bridge. 2020. Debiased o!ine evaluation of recommender systems: a weighted-sampling approach. In *Proceedings of the 35th*
*Annual ACM Symposium on Applied Computing* (Brno, Czech Republic) *(SAC*
*â€™20)* . Association for Computing Machinery, New York, NY, USA, 1435â€“1442.

https://doi.org/10.1145/3341105.3375759

[6] Diego Carraro and Derek Bridge. 2022. A sampling approach to Debiasing the

o!ine evaluation of recommender systems. *J. Intell. Inf. Syst.* 58, 2 (2022), 311â€“336.
https://doi.org/10.1007/s10844-021-00651-y

[7] Pablo Castells and Alistair Mo#at. 2022. O!ine Recommender System Evaluation:

Challenges and New Directions. *AI Magazine* 43, 2 (Jun. 2022), 225â€“238. https:
//doi.org/10.1002/aaai.12051

[8] RocÃ­o CaÃ±amares and Pablo Castells. 2020. On Target Item Sampling in O!ine

Recommender System Evaluation. In *Proceedings of the 14th ACM Conference on*
*Recommender Systems (RecSys â€™20)* . Association for Computing Machinery, New
York, NY, USA, 259â€“268. https://doi.org/10.1145/3383313.3412259

[9] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He.

2023. Bias and Debias in Recommender System: A Survey and Future Directions.
*ACM Trans. Inf. Syst.* 41, 3, Article 67 (Feb. 2023), 39 pages. https://doi.org/10.
1145/3564284

[10] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. 2010. Performance of

recommender algorithms on top-n recommendation tasks. In *Proceedings of the*
*Fourth ACM Conference on Recommender Systems* (Barcelona, Spain) *(RecSys*
*â€™10)* . Association for Computing Machinery, New York, NY, USA, 39â€“46. https:
//doi.org/10.1145/1864708.1864721

[11] Alexander Dallmann, Daniel Zoller, and Andreas Hotho. 2021. A Case Study

on Sampling Strategies for Evaluating Neural Sequential Item Recommendation
Models. In *Proceedings of the 15th ACM Conference on Recommender Systems*
(Amsterdam, Netherlands) *(RecSys â€™21)* . Association for Computing Machinery,
New York, NY, USA, 505â€“514. https://doi.org/10.1145/3460231.3475943

[12] Zhenhua Dong, Hong Zhu, Pengxiang Cheng, Xinhua Feng, Guohao Cai, Xi
uqiang He, Jun Xu, and Jirong Wen. 2020. Counterfactual learning for recommender system. In *Proceedings of the 14th ACM Conference on Recommender*
*Systems* (Virtual Event, Brazil) *(RecSys â€™20)* . Association for Computing Machinery, New York, NY, USA, 568â€“569. https://doi.org/10.1145/3383313.3411552



[13] Miroslav DudÃ­k, Dumitru Erhan, John Langford, and Lihong Li. 2014. Doubly

Robust Policy Evaluation and Optimization. *Statist. Sci.* 29, 4 (2014), 485â€“511.
http://www.jstor.org/stable/43288496

[14] Simen Eide, David S. Leslie, and Arnoldo Frigessi. 2022. Dynamic slate recom
mendation with gated recurrent units and Thompson sampling. *Data Min. Knowl.*
*Discov.* 36, 5 (sep 2022), 1756â€“1786. https://doi.org/10.1007/s10618-022-00849-w

[15] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang,

Xiangnan He, Jiaxin Mao, and Tat-Seng Chua. 2022. KuaiRec: A Fully-Observed
Dataset and Insights for Evaluating Recommender Systems. In *Proceedings of*
*the 31st ACM International Conference on Information & Knowledge Management*
(Atlanta, GA, USA) *(CIKM â€™22)* . 540â€“550. https://doi.org/10.1145/3511808.3557220

[16] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,

Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos. In *Proceedings of the 31st*
*ACM International Conference on Information & Knowledge Management (CIKM*
*â€™22)* . ACM, 3953â€“3957. https://doi.org/10.1145/3511808.3557624

[17] Alexandre Gilotte, ClÃ©ment CalauzÃ¨nes, Thomas Nedelec, Alexandre Abraham,

and Simon DollÃ©. 2018. O!ine A/B Testing for Recommender Systems. In *Proceed-*
*ings of the Eleventh ACM International Conference on Web Search and Data Mining*
(Marina Del Rey, CA, USA) *(WSDM â€™18)* . Association for Computing Machinery,
New York, NY, USA, 198â€“206. https://doi.org/10.1145/3159652.3159687

[18] Shantanu Gupta, Hao Wang, Zachary Lipton, and Yuyang Wang. 2021. Cor
recting Exposure Bias for Link Recommendation. In *Proceedings of the 38th*
*International Conference on Machine Learning (Proceedings of Machine Learn-*
*ing Research, Vol. 139)*, Marina Meila and Tong Zhang (Eds.). PMLR, 3953â€“3963.
https://proceedings.mlr.press/v139/gupta21c.html

[19] Maria Heuss, Fatemeh Sarvi, and Maarten de Rijke. 2022. Fairness of Exposure in

Light of Incomplete Exposure Estimation. In *Proceedings of the 45th International*
*ACM SIGIR Conference on Research and Development in Information Retrieval*
(Madrid, Spain) *(SIGIR â€™22)* . Association for Computing Machinery, New York,
NY, USA, 759â€“769. https://doi.org/10.1145/3477495.3531977

[20] Jin Huang, Harrie Oosterhuis, Masoud Mansoury, Herke van Hoof, and Maarten

de Rijke. 2024. Going Beyond Popularity and Positivity Bias: Correcting for Multifactorial Bias in Recommender Systems. In *Proceedings of the 47th International*
*ACM SIGIR Conference on Research and Development in Information Retrieval*
(Washington DC, USA) *(SIGIR â€™24)* . Association for Computing Machinery, New
York, NY, USA, 416â€“426. https://doi.org/10.1145/3626772.3657749

[21] Ngozi Ihemelandu and Michael D. Ekstrand. 2023. Candidate Set Sampling for

Evaluating Top-N Recommendation. In *2023 IEEE/WIC International Conference*
*on Web Intelligence and Intelligent Agent Technology (WI-IAT)* . 88â€“94. https:
//doi.org/10.1109/WI-IAT59888.2023.00018

[22] Olivier Jeunen. 2019. Revisiting o!ine evaluation for implicit-feedback recom
mender systems. In *Proceedings of the 13th ACM Conference on Recommender*
*Systems* (Copenhagen, Denmark) *(RecSys â€™19)* . Association for Computing Machinery, New York, NY, USA, 596â€“600. https://doi.org/10.1145/3298689.3347069

[23] Bart P. Knijnenburg and Martijn C. Willemsen. 2015. *Evaluating Recommender*

*Systems with User Experiments* . Springer US, Boston, MA, 309â€“352. https:
//doi.org/10.1007/978-1-4899-7637-6_9

[24] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted

collaborative "ltering model. In *Proceedings of the 14th ACM SIGKDD International*
*Conference on Knowledge Discovery and Data Mining* (Las Vegas, Nevada, USA)
*(KDD â€™08)* . Association for Computing Machinery, New York, NY, USA, 426â€“434.
https://doi.org/10.1145/1401890.1401944

[25] Walid Krichene and Ste#en Rendle. 2020. On Sampled Metrics for Item Rec
ommendation. In *Proceedings of the 26th ACM SIGKDD International Conference*
*on Knowledge Discovery & Data Mining (KDD â€™20)* . Association for Computing
Machinery, New York, NY, USA, 1748â€“1757. https://doi.org/10.1145/3394486.
3403226

[26] Maciej Kula. 2015. Metadata Embeddings for User and Item Cold-start Recom
mendations. In *Proceedings of the 2nd Workshop on New Trends on Content-Based*
*Recommender Systems co-located with 9th ACM Conference on Recommender Sys-*
*tems (RecSys 2015), Vienna, Austria, September 16-20, 2015. (CEUR Workshop Pro-*
*ceedings, Vol. 1448)*, Toine Bogers and Marijn Koolen (Eds.). CEUR-WS.org, 14â€“21.
http://ceur-ws.org/Vol-1448/paper4.pdf

[27] Dong Li, Ruoming Jin, Jing Gao, and Zhi Liu. 2020. On Sampling Top-K Rec
ommendation Evaluation. In *Proceedings of the 26th ACM SIGKDD International*
*Conference on Knowledge Discovery & Data Mining* (Virtual Event, CA, USA)
*(KDD â€™20)* . Association for Computing Machinery, New York, NY, USA, 2114â€“2124.
https://doi.org/10.1145/3394486.3403262

[28] Dong Li, Ruoming Jin, Zhenming Liu, Bin Ren, Jing Gao, and Zhi Liu. 2024. On

Item-Sampling Evaluation for Recommender System. *ACM Trans. Recomm. Syst.*
2, 1, Article 7 (March 2024), 36 pages. https://doi.org/10.1145/3629171

[29] Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, and Xiao
Hua Zhou. 2024. Debiased Recommendation with Noisy Feedback. In *Proceedings*
*of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*
(Barcelona, Spain) *(KDD â€™24)* . Association for Computing Machinery, New York,
NY, USA, 1576â€“1586. https://doi.org/10.1145/3637528.3671915


-----

RecSys â€™25, September 22â€“26, 2025, Prague, Czech Republic Pereira et al.



[30] Dawen Liang, Laurent Charlin, and David M Blei. 2016. Causal inference for

recommendation. In *Causation: Foundation to Application, Workshop at UAI. AUAI*,
Vol. 6. 108.

[31] Dawen Liang, Laurent Charlin, James McInerney, and David M. Blei. 2016. Mod
eling User Exposure in Recommendation. In *Proceedings of the 25th International*
*Conference on World Wide Web* (MontrÃ©al, QuÃ©bec, Canada) *(WWW â€™16)* . International World Wide Web Conferences Steering Committee, Republic and Canton
of Geneva, CHE, 951â€“961. https://doi.org/10.1145/2872427.2883090

[32] Haochen Liu, Da Tang, Ji Yang, Xiangyu Zhao, Hui Liu, Jiliang Tang, and Youlong

Cheng. 2022. Rating Distribution Calibration for Selection Bias Mitigation in
Recommendations. In *Proceedings of the ACM Web Conference 2022* (Virtual Event,
Lyon, France) *(WWW â€™22)* . Association for Computing Machinery, New York,
NY, USA, 2048â€“2057. https://doi.org/10.1145/3485447.3512078

[33] Yang Liu, Alan Medlar, and Dorota Glowacka. 2023. On the Consistency, Discrim
inative Power and Robustness of Sampled Metrics in O!ine Top-N Recommender
System Evaluation. In *Proceedings of the 17th ACM Conference on Recommender*
*Systems* (Singapore, Singapore) *(RecSys â€™23)* . Association for Computing Machinery, New York, NY, USA, 1152â€“1157. https://doi.org/10.1145/3604915.3610651

[34] Yang Liu, Alan Medlar, and Dorota Glowacka. 2023. What We Evaluate When

We Evaluate Recommender Systems: Understanding Recommender Systemsâ€™
Performance using Item Response Theory. In *Proceedings of the 17th ACM*
*Conference on Recommender Systems* (Singapore, Singapore) *(RecSys â€™23)* . Association for Computing Machinery, New York, NY, USA, 658â€“670. https:
//doi.org/10.1145/3604915.3608809

[35] Benjamin M. Marlin, Richard S. Zemel, Sam Roweis, and Malcolm Slaney. 2007.

Collaborative "ltering and the missing at random assumption. In *Proceedings of*
*the Twenty-Third Conference on Uncertainty in Arti!cial Intelligence* (Vancouver,
BC, Canada) *(UAIâ€™07)* . AUAI Press, Arlington, Virginia, USA, 267â€“275.

[36] M. E. J. Newman. 2005. Power laws, Pareto distributions and Zipfâ€™s law. *Contem-*

*porary Physics* 46, 5 (2005), 323â€“351. https://doi.org/10.1080/00107510500052444
arXiv:https://doi.org/10.1080/00107510500052444

[37] Fernando Benjamin Perez Maurera, Maurizio Ferrari Dacrema, and Paolo Cre
monesi. 2022. Towards the Evaluation of Recommender Systems with Impressions.
In *Proceedings of the 16th ACM Conference on Recommender Systems* (Seattle, WA,
USA) *(RecSys â€™22)* . Association for Computing Machinery, New York, NY, USA,
610â€“615. https://doi.org/10.1145/3523227.3551483

[38] Fernando B. PÃ©rez Maurera, Maurizio Ferrari Dacrema, Lorenzo Saule, Mario

Scriminaci, and Paolo Cremonesi. 2020. ContentWise Impressions: An Industrial Dataset with Impressions Included. In *Proceedings of the 29th ACM Inter-*
*national Conference on Information & Knowledge Management* (Virtual Event,
Ireland) *(CIKM â€™20)* . Association for Computing Machinery, New York, NY, USA,
3093â€“3100. https://doi.org/10.1145/3340531.3412774

[39] Bruno Pradel, Nicolas Usunier, and Patrick Gallinari. 2012. Ranking with non
random missing ratings: in$uence of popularity and positivity on evaluation
metrics. In *Proceedings of the Sixth ACM Conference on Recommender Systems*
(Dublin, Ireland) *(RecSys â€™12)* . Association for Computing Machinery, New York,
NY, USA, 147â€“154. https://doi.org/10.1145/2365952.2365982

[40] Ste#en Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt
Thieme. 2012. BPR: Bayesian Personalized Ranking from Implicit Feedback.

arXiv:1205.2618 [cs.IR] https://arxiv.org/abs/1205.2618

[41] Paul R. Rosenbaum and Donald B. Rubin. 1984. Reducing Bias in Observational

Studies Using Subclassi"cation on the Propensity Score. *J. Amer. Statist. Assoc.*
79, 387 (1984), 516â€“524. http://www.jstor.org/stable/2288398



[42] Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata.

2020. Unbiased Recommender Learning from Missing-Not-At-Random Implicit
Feedback. In *Proceedings of the 13th International Conference on Web Search and*
*Data Mining* (Houston, TX, USA) *(WSDM â€™20)* . Association for Computing Machinery, New York, NY, USA, 501â€“509. https://doi.org/10.1145/3336191.3371783

[43] Mark Sanderson, Monica Lestari Paramita, Paul Clough, and Evangelos Kanoulas.

2010. Do user preferences and evaluation measures line up?. In *Proceedings of the*
*33rd International ACM SIGIR Conference on Research and Development in Informa-*
*tion Retrieval* (Geneva, Switzerland) *(SIGIR â€™10)* . Association for Computing Machinery, New York, NY, USA, 555â€“562. https://doi.org/10.1145/1835449.1835542

[44] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item
based collaborative "ltering recommendation algorithms. In *Proceedings of the*
*10th International Conference on World Wide Web* (Hong Kong, Hong Kong)
*(WWW â€™01)* . Association for Computing Machinery, New York, NY, USA, 285â€“295.
https://doi.org/10.1145/371920.372071

[45] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and

Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning
and Evaluation. In *Proceedings of The 33rd International Conference on Machine*
*Learning (Proceedings of Machine Learning Research, Vol. 48)*, Maria Florina Balcan
and Kilian Q. Weinberger (Eds.). PMLR, New York, New York, USA, 1670â€“1679.
https://proceedings.mlr.press/v48/schnabel16.html

[46] Monika Singh. 2020. Scalability and sparsity issues in recommender datasets: a

survey. *Knowl. Inf. Syst.* 62, 1 (Jan. 2020), 1â€“43. https://doi.org/10.1007/s10115018-1254-2

[47] Harald Steck. 2010. Training and testing of recommender systems on data

missing not at random. In *Proceedings of the 16th ACM SIGKDD International*
*Conference on Knowledge Discovery and Data Mining* (Washington, DC, USA)
*(KDD â€™10)* . Association for Computing Machinery, New York, NY, USA, 713â€“722.
https://doi.org/10.1145/1835804.1835895

[48] Harald Steck. 2013. Evaluation of recommendations: rating-prediction and rank
ing. In *Proceedings of the 7th ACM Conference on Recommender Systems* (Hong
Kong, China) *(RecSys â€™13)* . Association for Computing Machinery, New York, NY,
USA, 213â€“220. https://doi.org/10.1145/2507157.2507160

[49] GÃ¡bor TakÃ¡cs and Domonkos Tikk. 2012. Alternating least squares for personal
ized ranking. In *Proceedings of the Sixth ACM Conference on Recommender Systems*
(Dublin, Ireland) *(RecSys â€™12)* . Association for Computing Machinery, New York,
NY, USA, 83â€“90. https://doi.org/10.1145/2365952.2365972

[50] Daniel Valcarce, Alejandro BellogÃ­n, Javier Parapar, and Pablo Castells. 2018. On

the robustness and discriminative power of information retrieval metrics for topN recommendation. In *Proceedings of the 12th ACM Conference on Recommender*
*Systems* (Vancouver, British Columbia, Canada) *(RecSys â€™18)* . Association for
Computing Machinery, New York, NY, USA, 260â€“268. https://doi.org/10.1145/
3240323.3240347

[51] Chengbing Wang, Wentao Shi, Jizhi Zhang, Wenjie Wang, Hang Pan, and Fuli

Feng. 2024. Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing Recommendation. arXiv:2409.04810 [cs.IR] https://arxiv.org/abs/2409.04810

[52] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian,

Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. 2020. MIND: A
Large-scale Dataset for News Recommendation. In *Proceedings of the 58th Annual*
*Meeting of the Association for Computational Linguistics*, Dan Jurafsky, Joyce
Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational
Linguistics, Online, 3597â€“3606. https://doi.org/10.18653/v1/2020.acl-main.331

[53] Eva Zangerle and Christine Bauer. 2022. Evaluating Recommender Systems:

Survey and Framework. *ACM Comput. Surv.* 55, 8, Article 170 (Dec. 2022), 38 pages.
https://doi.org/10.1145/3556536


-----

