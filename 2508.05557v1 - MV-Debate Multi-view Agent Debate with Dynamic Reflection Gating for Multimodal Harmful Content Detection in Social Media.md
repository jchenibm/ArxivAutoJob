## **MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for** **Multimodal Harmful Content Detection in Social Media**
### Rui Lu [1,2], Jinhe Bi [3], Yunpu Ma [3,4], Feng Xiao [5], Yuntao Du [1], Yijun Tian [6]

1 Shandong University
2 Ping An Technology
3 Ludwig Maximilian University of Munich
4 Munich Center for Machine Learning
5 Computility Lab, Beijing EB Technology Co.LTD
6 AWS AI
ruilu42@outlook.com, bijinhe@outlook.com, yunpu.ma@ifi.lmu.de, fengx@ebtech.com, yuntaodu@sdu.edu.cn,
yijunt@amazon.com,


**Abstract**

Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape
nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions,
rapid cultural shifts, and subtle pragmatic cues. To address
these challenges, we propose MV-Debate, a multi-view agent
debate framework with dynamic reflection gating for unified
multimodal harmful content detection. MV-Debate assembles
four complementary debate agents, a surface analyst, a deep
reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through
iterative debate and reflection, the agents refine responses under a ∆-gain criterion, ensuring both accuracy and efficiency.
Experiments on three benchmark datasets demonstrate that
MV-Debate significantly outperforms strong single-model and
existing multi-agent debate baselines. This work highlights
the promise of multi-agent debate in advancing reliable social
intent detection in safety-critical online contexts.
### **Introduction**

The rapid growth of social media platforms as multimodal
communication channels—integrating images, short videos,
emojis, and stylized texts—has significantly increased the
complexity and ambiguity of online messages, posing critical challenges for effective multimodal harmful content detection. For example, multimodal ambiguity occurs when a
seemingly neutral caption paired with an ironic image or exaggerated visual cues expresses hidden ridicule, undetectable
from text alone; similarly, memes or edited videos frequently
amplify emotional or persuasive meanings beyond their literal content, complicating harmful intent detection. Accurately identifying the underlying *social intent*, whether a
post *ridicules* (sarcasm), *vilifies* (hate speech), or *misleads*
(misinformation), is thus critical not only for content moderation and community safety, but also for opinion mining,
public-discourse analysis, and manipulation-campaign detection. The challenge is heightened by the creative ways
users blend modalities, often relying on cultural references,

Copyright © 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.


irony, or ambiguity to veil harmful messages. Consequently,
effective detection requires integrating linguistic cues, visual semantics, and contextual knowledge to uncover the
true communicative intent of multi-modal content, ensuring

                   reliable performance in safety critical applications across
increasingly complex online environments.
Yet such intent remains challenging to discern because
cues are often (i) *cross* *-* *modal* : an image can invert or reinforce a caption’s meaning; (ii) *context* *-* *dependent* : memes,
slang, and cultural references evolve rapidly; and (iii) *subtle*
*or sparsely distributed* : harmful intents may be concealed
subtly within predominantly benign content. Empirical studies confirm these obstacles: state-of-the-art multimodal classifiers struggle with culturally grounded irony (Lu et al. 2025),
while single-stream text-only models like BERT variants
become brittle when visual evidence contradicts textual sentiment (Hee, Chong, and Lee 2023).
Recently, with the development of LLM agents, multiagent-based frameworks have achieved remarkable progress
in many fields (Guo et al. 2024; Chan et al. 2023). Among
them, multi-agent debate (Chan et al. 2023; Madaan et al.
2023) is an effective manner of utilizing the debate among
multiple agents to promote the reasoning performance, which
can compensate for individual blind spots. Representative
methods include opinion-holding (Estornell and Liu 2024)
and free-form method (Chan et al. 2023), where the former
assigns a predefined opinion (e.g., true for sarcasm task),
while the latter performs free-form prediction. Multi-agent
debate mechanisms have improved performance on complex reasoning tasks, such as zero-shot stance detection and
value-sensitive decision-making, by encouraging agents to
expose, defend, and contest alternative interpretations (Du
et al. 2023a; Zhang et al. 2024). Although achieving significant performance, few work has explore multi-agent debate
for multimodal harmful content detection.

Current multi-agent debate-based systems typically have
the following shortcomings for this scenario: (i) In multiagent debate settings, existing methods often employ identical LLM prompts across roles, resulting in similar reasoning
patterns and repeated errors due to pre-training biases. (ii)
The General MAD strategy is designed for general question


-----

Previous Method Our Method

Figure 1: Comparison between existing MAD and our method.


answering tasks, which often overlooks task-specific design
and leads to sub-optimal performance. (iii) Existing methods
focus on a *single pragmatic category*, which needs multiple
different methods for different tasks.

To address these gaps, we propose **MV-Debate**, a
**M** ulti- **V** iew agent **Debate** framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate reformulates three historically siloed
tasks—sarcasm (Benchekroun et al. 2022), hate speech (Kiela
et al. 2020), and misinformation detection (Shu et al. 2020),
under a broader **Harmful Content Detection** objective. Inspired by the intuition that diverse ways of thinking (Liu
et al. 2025) can promote mutual inspiration, during the debate, our architecture assembles four different view-based
vision–language agents, namely *Surface Analyst*, *Deep Rea-*
*soner*, *Modality Contraster*, and *Social Contextualist* . Each
agent adopts a different reasoning perspective and combines
insights from the perspective of others. By comparing these
perspectives and extracting valuable insights, agents refine
their answers to reach the correct result.

Specifically, in the first round, given an image and text pair,
each agent generates an individual response with a unique
given prompting. Then, a judge agent is introduced to score
different competing responses, with the better response leading to a higher score. Next, to better stimulate the potential
of each agent, a reflection agent proposes to reflect the topscoring agents. The new response is adopted only when a
measurable ∆ **-gain criterion** (e.g., ∆ *≥* 0 *.* 1 in judge scoring) is met, thus improving the reflection quality. At the
second round, each agent reviews and critiques the highest
score response selected from the last round, incorporating
this feedback to produce its own response. Then, we perform
a similar process as the first round. This whole debate process is repeated over several rounds. MV-Debate promotes
diversity by introducing diverse views, leading to more robust
solutions.

We conduct extensive experiments on three public benchmarks from three tasks. The Experimental results on these
benchmarks demonstrate that: (i) MV-Debate consistently
outperforms strong single-model and existing multi-agent
debate baselines across all three intent types; (ii) MV-Debate
with heterogeneous agents achieves better results than that
of homogeneous agents; (iii) Large model size and more de

bate rounds often lead to better performance, while they also
require more cost and time.
To sum up, the contributions of this work are as follows:

- We introduce MV-Debate, a multi-agent debate framework
that guides agents to employ diverse reasoning views for
multi-modal harmful content detection in social media.

- We design four view-specific debate agents with a dynamic
reflection gating mechanism to improve the performance.

- We empirically validate the effectiveness of the proposed
method on multiple multi-modal harmful content benchmarks.
### **Related Work**
#### **Harmful Content Detection in Social Media**

Early multimodal studies addressed sarcasm, hate speech,
and misinformation as *separate* problems, each with its own
dataset and model design. Recent years have shifted toward
LMMs and agentic frameworks.
(i) **Sarcasm** : Tang et al. (Tang et al. 2024) adapt LMMs
via visual instruction templates with in-context demonstrations. S³ Agent (Wang et al. 2024) integrates multiple
LMMs from semantic, superficial, and sentiment views, while
Commander-GPT (Zhang et al. 2025) decomposes sarcasm
into six subtasks and aggregates rationales through a central
“commander.”

(ii) **Hate speech** : Van & Wu (Van and Wu 2023) show
that prompting LLaVA and GPT - 4V with crafted instructions
achieves strong zero-shot hateful-meme detection. Yamagishi (Yamagishi 2024) finds simple prompts outperform complex ones for event detection. Lin (Lin et al. 2024a) offers an
explainable method by reasoning over conflicting harmless
and harmful rationales.

(iii) **Misinformation** : To tackle scarce and noisy supervision, LVLM4FV (Tahmasebi, Muller-Budack, and Ewerth ¨
2024) combines GPT-ranked evidence retrieval with an InstructBLIP verifier, while SNIFFER (Qi et al. 2024) employs
two-stage instruction tuning with entity extraction and imagebased web search. LEMMA (Xuan et al. 2024) enhances
reasoning via multi-query retrieval and distillation. Cekinel
et al. (Cekinel, Karagoz, and oltekin 2025) probe VLM embeddings with a lightweight classifier.


-----

                          Toward unification of these tasks, MM SOC (Jin et al.
2024) integrates ten tasks, including sarcasm, hate, and misinformation, revealing LMMs’ fragility in socially nuanced
harmful content.
#### **Multi-Agent Debate**

Multi - agent debate was first shown to improve factual accu
        racy in open domain QA (Du et al. 2023a) and later operationalised through open - source frameworks such as A UTO G EN (Wu et al. 2023). Following works studied MAD from
different perspectives. Some assign different agents to play
different roles (Wang et al. 2023). There are also other methods improving MAD through embeddings (Pham et al. 2023).
R E C ONCILE arranges a round - table “conference” among
LLMs to reach consensus (Chen, Saha, and Bansal 2024),
while CAF - I tailors role - specialised agents to irony detection (Liu, Zhou, and Hu 2025). These successes confirm that
agent heterogeneity and structured interaction benefit hard
reasoning tasks.
However, most of them debate with a single thinking which
may lead to similar output patterns. Instead, we propose MVDebate to encourage different agents to think with distinct
reasoning views, which can prompt mutual inspiration. Similarly, (Gao et al. 2024a) dynamically selects the most suitable
reasoning method to solve the problem.
#### **Large Multimodal Model**

Large Multimodal Models (LMMs) have achieved progress
in integrating vision and language, enabling cross-modal understanding and reasoning. A typical LMM comprises three
components: a language encoder, a vision encoder, and a
cross-modal interaction module (Caffagni et al. 2024; Balauca et al. 2025; Bi et al. 2025; Jinhe et al. 2025b,a). The
cross-modality module bridges the two, allowing effective
processing of visual inputs.
Building on this architecture, models such as Qwen2.5VL (Bai et al. 2025), InternVL2.5 (Chen et al. 2024), and
LLaVA series (Liu et al. 2023; Li et al. 2025) adopt different
design choices and training strategies. These advances have
significantly improved vision-language alignment, yielding
strong performance across multimodal benchmarks (Kil et al.
2024; Huang and Zhang 2024). Additionally, closed-source
models such as GPT-4v, GPT-4o (Hurst et al. 2024), Gemini(Comanici et al. 2025), and Claude-Sonnet have demonstrated excellent results in diverse multimodal tasks. Besides,
agent-based method (Gao et al. 2024b; Fan et al. 2024; Wang
et al. 2025) also achieved remarkable progress.
#### **Reflection in LMMs**

Prompt - level *self* *-* *reflection* has proven to boost test - time reasoning. S ELF - R EFINE (Madaan et al. 2023) lets a model
iteratively critique and rewrite its own answer, improving
seven diverse tasks without extra training. Renze & Guven (Renze and Guven 2024) systematically evaluate eight
reflection variants and report significant gains across public question banks. Beyond prompting, Bansal (Bensal
et al. 2025) introduces R EFLECT –R ETRY –R EWARD, a
reinforcement - learning framework that rewards tokens produced during successful reflections. Taken together, these


studies confirm that reflection is powerful but costly when applied indiscriminately, underscoring the importance of *gated*
or selective reflection strategies.
### **Methodology**

This section describes our proposed reflection-gated multiagent debate framework ( **MV-Debate** ) for multimodal harmful content detection in social media.
#### **Problem Formulation**

Given a multimodal social-media post composed of text *x* *[text]*
and associated visual content *x* [img], the goal is to predict its
underlying *social intent* label *y ∈Y*, where *Y* = *{Y es, No}*
indicates whether there are sarcasm, hate content, or misinformation. The objective is to maximize predictive accuracy.
#### **System Architecture**

Inspired the intuition that diverse reasoning methods could
lead to better cooperation results in multi-agent debates. This
method could avoid the analogous phenomenon in existing
LMMs, that MAD with a fixed prompting strategy may always produce similar answers to a problem. Thus, we argue the importance of utilizing different reasoning views
in debate to promote diverse thinking and propose a multiview-based debate method for multimodal harmful content
detection. We employ a variety of prompting reasoning techniques to produce distinct modes of thought, which do not
need training or fine-tuning. We endeavor to design reasoning methods with significant divergence to avoid the issue of
similar reasoning processes.
Specially, the MV-Debate system consists of four types of
specialized debate agents, alongside three additional control
agents. Each debate agent is required to answer the question
with the corresponding assigned view, and the control agent
aims to score and reflect the reasoning path, and eventually,
make a final prediction.

1. **Specialized Debate Agents:**

  - **Surface Analyst agent (SA)** : This agent focuses exclusively on explicit textual and visual cues to detect.

  - **Deep Reasoner agent (DR)** : This agent uncovers implicit meanings and hidden intents to detect.

  - **Modality Contrast agent (MC)** : This agent assesses
alignment or contradictions between textual and visual
modalities to detect.

  - **Social Contextualist agent (SC)** : This agent leverages
external cultural and social-contextual knowledge to
detect.
2. **Judge Agent** : This agent evaluates arguments generated
by the debate agents. It assigns scores based on logical
coherence, consistency, and plausibility, where a better
response would get a higher score.
3. **Reflection Agent** : This agent generates structured feedback highlighting logical flaws and improvement suggestions.
4. **Summary Agent** : This agent aggregates the debate history and delivers the final prediction.


-----

Al g orithm 1: Reflection-Gated Multi-View Debate

**Require:** Input: ( *x* [text] *, x* [img] ), Max rounds *R*, Reflection
threshold *τ*, Top- *k*
**Ensure:** Predicted social intent label: ˆ *y*

1: Initialize debate history: *H ←* ∅
2: **for** *t* = 1 to *R* **do**
3: **Agent responses generation (parallel)**
4: **for** each agent *i ∈{* SA, DR, MC, SC *}* in parallel **do**
5: **r** *i,t* *←* *a.* G ENERATE ( *x* [text] *, x* [img] *, H* )
6: **end for**
7: **if** C ONSENSUS ( *{* **r** *i,t* *}* ) **then**
8: **return** S UMMARY ( *H ∪{* **r** *i,t* *}* )
9: **end if**
10: **s** *i,t* *←* J UDGE ( *{* **r** *i,t* *}* )
11: Update best response: *i* *[∗]* *←* arg max *i* *s* *i,t*
12: Append to history: *H ←* *H ∪{* ( **r** *i* *∗* *,t* *, s* *i* *∗* *,t* ) *}*
13: Calculate reflection gain:
14: ∆ *←* C OMPUTE D ELTA ( *x* [text] *, x* [img] *, k* )
15: **if** ∆ *≥* *τ* **then**
16: *ϕ* *t* *←* R EFLECT ( *H* )
17: Append reflection feedback:
18: *H ←* *H ∪{ϕ* *t* *}*
19: **end if**

20: **end for**
21: **return** S UMMARY ( *H* )
#### **Multi-View Debate**

**Initial Response Generation** At the first round of debate,
given an image-text pair, each specialized debate agent generates its response *r* *i,* 1, where the subscript “1” means the first
round, guided by the corresponding task view prompt *p* *i* :

*r* *i,* 1 = *M* *i* ( *x* *[text]* *, x* *[img]* *|h* *i* *, p* *i* ) *, i* = 1 *,* 2 *, ...,* 4 *.* (1)

where *h* *i* is the history messages for *i* -th agent and is initialized as an empty list. *r* *i,* 1 is output as a structured JSON
object comprising a binary decision (‘YES‘ or ‘NO‘) and a
brief reasoning. Their role-specific prompts strictly enforce
distinct analytical perspectives, ensuring diversity and complementarity in the overall reasoning process.
Consequently, the judge agent collects the solving processes and answers to the questions of all debate agents, and
it assigns a score *s* *i,* 1 for each agent’s response, with a better
response leading to a higher score.

**Top-** *k* ∆ **-Reflection Gating** As the initial response from
these agents may contain incorrect information, following
previous work, we introduce a reflection mechanism to selfimprove the response quality.
To reduce computational overhead, we introduce a Top- *k*
∆ -reflection gating strategy. At each round, the reflection
agent would receive all the debate agents’ responses and
check the reasoning process of each agent. Then, it would
point out the reasoning error and provide a revision suggestion. Next, the top *k* highest responses scored by the
judge agent are selected. Then, each selected original debate
agent would generate a new response ˆ *r* *i,* 1 with the query instance, initial response, and revision suggestions. After that,
the judge agent would rescore the new response, denoted as
*s* ˆ *i,* 1 .


Then, we estimate the expected utility of reflection by
comparing the scores of the agents with and without reflection feedback. Formally, reflection gain ∆ *i,* 1 is calculated as
follows:


∆ *i,* 1 = [1]

*k*


� (ˆ *s* *i,* 1 *−* *s* *i,* 1 ) (2)

*i∈* Top *k*


Reflection is only triggered when ∆ *i,* 1 surpasses a predefined
threshold *τ*, i.e., ∆ *i,* 1 *≥* *τ* . Otherwise, we would use the
original response.
In our experiments, we empirically set *k* = 2 and *τ* = 0 *.* 1
to achieve efficiency improvements. As it could reduce redundant reflection calls by over 60% compared with reflecting
all debate agents, while maintaining or improving accuracy
compared to the unconditional reflection baseline.

**History Update** After reflection, if the newer response is
not adopted, the judge agent would collect highest highestscoring response, and append it to the history. Otherwise,
we will additionally append the reasoning error and revision
suggestions *ϕ* 1 into the history.

**Debate Loop** Starting from the second round, the bestscoring response from the last round, including both the reasoning processes and the answers, is incorporated into each
agent’s history *h* *i* . In the following round, each agent leverages these reasoning traces and solutions as additional input,
selectively extracting useful information from the diverse
perspectives to refine its own answer. This iterative debate
process follows the same process as described above, until
either the maximum number of rounds *N* is reached or the
agents converge on the same judgment. In our experiments,
we set *N* = 3 . Finally, at the end of the debate, the summary
agent would aggregate the debate history and deliver the final
prediction ˆ *y* . Algorithm 1 summarizes the iterative debate
and reflection procedure.
#### **Discussion**

The proposed reflection-gated multi-view debate framework
(MV-Debate) offers three main advantages for multimodal
harmful content detection. First, assigning specialized roles to
debate agents enforces diverse reasoning perspectives. Unlike
single-view prompting, this design combines surface-level,
deep semantic, cross-modal, and social-cultural analyses, reducing the risk of missing implicit or context-specific harmful

cues.

Second, the Top- *k* ∆ -reflection gating mechanism enhances reliability while maintaining efficiency. By adaptively
triggering reflection only when substantial improvement is
expected, the framework avoids redundant computation yet
achieves accuracy comparable to or better than unconditional
reflection. This is relevant for real-world deployment where
scalability and cost-efficiency are critical.
Third, the iterative debate loop encourages cumulative
reasoning. Agents refine their predictions by integrating highquality responses and structured feedback into their histories,
promoting both inter-agent diversity and intra-agent improvement while mitigating repeated errors.


-----

Table 1: The comparison results on three multimodal harmful content detection datasets.



|Method|Model|MMSD HatefulMeMe GossipCop Avg|Col4|
|---|---|---|---|
|||Acc F1 Acc F1 Acc F1|Acc F1|
|Closed-source GPT o4-mini 77.5 78.0 70.8 63.7 72.8 66.9 73.7 69.5 GPT 4o 78.5 75.4 75.2 71.5 73.4 75.8 75.7 74.2 Gemini-Flash-2.5 73.9 80.1 77.4 67.5 76.5 72.7 75.9 73.4 Claude-4-Sonnet 82.5 84.9 72.2 64.2 76.6 73.9 77.1 74.3 Single Model Open-source InternVL3-14B 74.5 78.6 68.2 64.1 72.2 68.7 71.6 70.5 Gemma-3-12B 68.5 70.9 67.2 67.8 74.8 68.2 70.2 69.0 Qwen2.5-VL-7B-Instruct 56.1 54.7 59.1 42.3 68.8 63.7 61.3 53.6 LLaMA-4-Maverick-17B 75.4 77.4 67.8 65.5 72.4 64.2 71.9 69.0||||
|||74.5 78.6 68.2 64.1 72.2 68.7 68.5 70.9 67.2 67.8 74.8 68.2 56.1 54.7 59.1 42.3 68.8 63.7 75.4 77.4 67.8 65.5 72.4 64.2||
|Multi-Agent Debate (Heterogeneous)|MAD DMAD ChatEval DebUnc|78.6 77.0 69.1 66.5 75.0 70.3 81.1 81.8 72.5 69.2 77.1 72.0 81.9 88.5 71.3 68.3 77.6 72.9 79.6 72.1 68.6 63.2 73.8 69.6|74.2 71.3 76.9 74.4 77.0 76.6 74.0 68.3|
|Open-source Qwen2.5-VL-7B-instruct 65.7 62.3 61.7 61.5 71.1 65.6 66.2 63.1 InternVL3-14B 81.4 75.5 72.5 72.5 74.4 62.3 76.1 70.1 Ours LLaMA-4-Maverick-17B 82.1 83.5 74.4 76.1 77.6 64.7 78.0 74.8 (Homogeneous) Gemma-3-12B 80.4 79.1 68.3 67.5 78.1 69.8 75.6 72.1 Closed-source Claude-4-Sonnet 90.2 86.1 80.4 70.5 78.3 69.2 82.9 75.3||||
|||90.2 86.1 80.4 70.5 78.3 69.2||
|Ours (Heterogeneous)|Ours (Open-source) Ours (Closed-source)|86.1 82.5 76.0 64.5 79.4 72.3 92.3 93.1 80.8 70.9 81.7 70.1|80.5 73.1 84.9 78.0|

### **Experiment**
#### **Setup**

**Datasets** Following previous work (Lin et al. 2024b; Liang
et al. 2022), in this section, we conduct comprehensive
experiments on three widely-used multimodal social context datasets, including the MMSD dataset (Benchekroun
et al. 2022) for the sarcasm detection task, the HatefulMeMe
dataset (Kiela et al. 2020) for the hate speech task, and the
GossipCop dataset (Shu et al. 2020) for the misinformation
detection task. As our method and baseline multi-agent debate methods need many tokens for a given instance, following previous works (Du et al. 2023a; Liu et al. 2025). We
do not use the whole dataset and instead randomly select a
subset for evaluation. The number of MMSD, HatefulMeMe,
and GossipCop dataset is all 500.

**Baseline Methods** To show the effectiveness of the proposed method, we compare MV-Debate with several types of
methods.

The first type of method is the state-of-the-art large multimodal models. For these models, we perform zero-shot
prediction with the corresponding task prompt. The selected
models includes closed-source models: GPT 4o (Hurst et al.
2024), GPT o4-mini (OpenAI 2025), Gemini-Flash-2.5 (Comanici et al. 2025), Claude-4-Sonnet. Besides, we also
select some representative open-source models: Qwen2.5

VL (Bai et al. 2025), InternVL3 (Chen et al. 2024), LLaMA4-Maverick, and Gemma-3 (Team et al. 2025).
The second type of method is existing general multiagent debate methods, including MAD (Du et al. 2023b),
DMAD (Liu et al. 2025), ChatEval (Chan et al. 2023), and
DebUnc (Yoffe, Amayuelas, and Wang 2024). It is noted that
these methods are proposed based on LLM and replace the
corresponding debate agent with LMMs.
The third type of method is our proposed method and its
variants. We implement our methods with both homogeneous
and heterogeneous agent scenarios, where the former means
that all the debate agent adopts the same LMMs, and the
latter adopt different LMMs as the debate agents. For these
two scenarios, we test the model on both open-source and
closed-source LMMs.

**Implementation Details** We implement our method based
on PyTorch and Huggingface Transformer for the experiments. As for evaluation, we adopt the accuracy and F1 score
as metrics, and all the reported metrics were computed by
scikit-learn. We utilize closed-source LMMs as our control
agents (Judge Agent, Reflection Agent, and Summary Agent)
in MV-Debate, including Claude-4-Sonnet, GPT o4-mini, and
GPT 4o, respectively. We set the temperature to 0 and greedysearch to ensure reproducibility. For the Specialized Debate
Agents, we use both closed-source and open-source LMMs
in our experiments. We use the same model (e.g., LLaMA-4

-----

Maverick-17B) for the multi-view specialized agents in the
homogeneous experiments. For the heterogeneous settings,
we use four unique LMMs as our multi-view specialized
agents. We leveraged API interfaces to invoke closed-source
LMMs and implemented an asynchronous strategy to execute
specialized debate agents in parallel, significantly enhancing
runtime efficiency. The experimental hyperparameters in the
code fall into three main categories: (i) Randomness: We set
the random seed to 42 in all experiments. (ii) Debate process
control: we set max rounds *N* = 3, reflection-gain threshold
*τ* = 0 *.* 1, and *k* = *⌊* *[L]* 2 *[⌋]* [(where] *[ L]* [ represents the number of]

multi-view specialized agents) as the top- *k* agents selected
for the computation of reflection gain. (iii) API scheduling:
we set max retries *p* = 5 times and retry delay *q* = 3 seconds
for all the closed-source LMM agents in our experiments.
#### **Main Results**

The comparison with baseline methods is shown in Table 1.
Based on the results, we have the following findings.
**Single-model baselines.** Closed-source models generally outperform open-source counterparts. Claude-4-Sonnet
achieves the best overall performance among single models,
while GPT 4o and GPT o4-mini trail slightly behind. In contrast, open-source models show a notable performance gap,
with Qwen2.5-VL variants performing poorly. These results
indicate the difficulty of applying off-the-shelf open-source
LMMs to harmful content detection.

**Existing multi-agent debate baselines.** We implement the
existing multi-agent debate baseline in an heterogeneous setting with SOTA open-source LMMs ( including Qwen2.5-VL7B-instruct, InternVL3-14B, LLaMA-4-Maverick-17B, and
Gemma-3-12B). The results show that existing multi-agent
debate frameworks (e.g., DMAD, ChatEval) demonstrate
clear advantages over single models. ChatEval, for example, achieves 76.6% of F1 score, confirming that multi-agent
collaboration with a debate manner improves robustness.
**Our homogeneous MV-Debate.** In a homogeneous scenario, we implement MV-Debate with four kinds of opensource LMMs. The results show that our homogeneous framework consistently improves over its base models. For instance,
the accuracy of Gemma-3-12B increases from 70.2 (single)
to 75.6 with MV-Debate, while InternVL3-14B improves
from 71.6 to 76.1. LLaMA-4-Maverick-17B achieves the best
open-source results with an accuracy of 78.0. These gains
validate the effectiveness of enforcing multi-view reasoning
and reflection even without heterogeneous agents.
**Our heterogeneous MV-Debate.** We implement MVDebate in a heterogeneous scenario with both open- and
closed-source LMMs, and our heterogeneous framework
achieves the highest performance. Besides, the open-source
variant yields an accuracy of 80.5, which outperforms existing multi-agent debate methods. The closed-source variant
reaches an accuracy of 84.9, surpassing all baselines. This
demonstrates that reflection-gated multi-view debate establishes new state-of-the-art results. Besides, the heterogeneous
agent achieves better results than a homogenous agent under open-source models, which implies that different models
could also lead to diverse thinking.
In summary, MV-Debate effectively integrates diverse rea

Table 2: Ablation study about the debate agent.

|Settings|LLaMA-4-Maverick-17B Claude-4-Sonnet|Col3|
|---|---|---|
|Acc F1 Acc F1 Ours 82.1 83.5 90.2 86.1 w/o SA 80.4 87.2 87.6 85.1 w/o DR 78.4 69.8 86.3 84.4 w/o MC 75.7 72.5 85.7 83.5 w/o SC 77.5 74.5 86.1 82.5 Zero-shot 75.4 77.4 82.5 84.9|Acc F1|Acc F1|
||82.1 83.5 80.4 87.2 78.4 69.8 75.7 72.5 77.5 74.5 75.4 77.4|90.2 86.1 87.6 85.1 86.3 84.4 85.7 83.5 86.1 82.5 82.5 84.9|



soning views with adaptive reflection, achieving superior
accuracy and efficiency. The results highlight its promise as
a scalable and reliable framework for multimodal harmful

content detection.
#### **Insightful Analysis**

**Ablation study of debate agents** We conduct an ablation
study to evaluate the contribution of each specialized debate
agent in the homogeneous scenario, as the heterogeneous
scenario would couple the effect of different LMMs. We
evaluate four agents: Surface Analyst (SA), Deep Reasoner
(DR), Modality Contraster (MC), and Social Contextualist
(SC). The results are shown in Table 2.
For LLaMA-4-Maverick-17B, removing any agent leads to
a performance drop compared with the full MV-Debate. The
most significant decline occurs when excluding the Modality
Contrast. This confirms the critical role of assessing consistency and contradictions between modalities. Removing the
deep reasoner also causes a large drop, highlighting the importance of capturing implicit meanings and hidden harmful
intents that are often overlooked by surface-level cues. Excluding the Social Contextualist results in a moderate decline,
suggesting that external sociocultural knowledge is essential
to interpret nuanced harmful signals. In contrast, removing
the Surface Analyst causes only a minor drop, as explicit
cues may be partly covered by other agents. Besides, a similar trend is observed for Claude-4-Sonnet. These consistent
patterns across both open- and closed-source models validate
the necessity of combining diverse reasoning views.

**Ablation Study of reflection mechanism** Table 3 demonstrates the impact of incorporating reflection in our method in
both homogeneous and heterogeneous scenarios. Across all
models, reflection consistently improves both accuracy and
F1. For example, for Claude-4-Sonet, the accuracy increases
from 85.1 to 90.2 (+5.3), highlighting its effectiveness in
correcting reasoning errors and enhancing the detection of
implicit harmful cues. In summary, reflection plays a pivotal role in maximizing the effectiveness of multi-agent debate. By selectively guiding agents to revise their reasoning,
it stabilizes outputs and improves consistency, making the
framework more reliable and scalable for real-world harmful

content detection tasks.

**Ablation Study of the best history** During the debate, we
would select the best-scoring response of the debate agents
instead of that of all agents. To show the effectiveness, we
compare these two settings in both homogeneous and heterogeneous scenarios. The results of the ablation study of the


-----

Table 3: Ablation study about reflection. ”homo” and ”hete”
mean homogeneous and heterogeneous, respectively.

w/o Reflection with Reflection



|Acc F1 Acc F1 Ours LLaMA-4-maverick-17B 80.4 78.2 82.1 83.5 (homo) Claude-4-Sonnet 85.1 82.3 90.2 86.1 Ours Ours(Open-source) 84.3 79.5 86.1 82.5 (hete) Ours(Closed-source) 88.2 87.5 92.3 93.1|Col2|Acc F1|Acc F1|
|---|---|---|---|
||LLaMA-4-maverick-17B Claude-4-Sonnet|80.4 78.2 85.1 82.3|82.1 83.5 90.2 86.1|
||Ours(Open-source) Ours(Closed-source)|84.3 79.5 88.2 87.5|86.1 82.5 92.3 93.1|


best history are shown in Table 4. Our method with LLaMA4-Maverick-17B shows notable sensitivity to data quality,
with its accuracy improving from 70.1 (”All History”) to 82.1
(”Best History”), suggesting it benefits substantially from
filtered or higher-quality data. A consistent trend across all
models is the superior performance in the ”Best History” setting compared to ”All History,” highlighting the importance
of data quality and curation. This effect is most pronounced
for LLaMA-4-Maverick, suggesting it is particularly vulnerable to noisy or suboptimal data.

Table 4: Ablation study about history. ”homo” and ”hete”
mean homogeneous and heterogeneous, respectively.

All History Best History



|Acc F1 Acc F1 Ours LLaMA-4-Maverick(17B) 70.1 62.8 82.1 83.5 (homo) Claude-4-Sonnet 80.4 78.6 90.2 86.1 Ours Ours (Open-source) 72.0 63.5 86.1 82.5 (hete) Ours (Closed-source) 82.2 80.1 92.3 93.1|Col2|Acc F1|Acc F1|
|---|---|---|---|
||LLaMA-4-Maverick(17B) Claude-4-Sonnet|70.1 62.8 80.4 78.6|82.1 83.5 90.2 86.1|
||Ours (Open-source) Ours (Closed-source)|72.0 63.5 82.2 80.1|86.1 82.5 92.3 93.1|


**Ablation about debate round** The effect of varying the
number of debate rounds from 1 to 4 in a homogeneous scenario based on LLaMA-4-maverick-17B is shown in Figure 2.
We observe that increasing the debate rounds generally enhances performance. Specifically, accuracy rises from 76.6
at one round to 82.1 at four rounds. Notably, the transition
from one to three rounds yields the most substantial gains.
The fourth round brings marginal improvements, suggesting
that iterative debate allows agents to progressively refine their
reasoning by integrating complementary perspectives, though
performance gains tend to saturate after three rounds. These
results highlight the effectiveness of multi-round debate in
enhancing both robustness and precision, while indicating a
practical trade-off between performance gains and additional
computational overhead. Thus, considering both efficiency
and effectiveness, set the debate round to be 3 in our experi
ments.

**Ablation about model size** The results of the impact of
model size on sarcasm and hate detection in a homogeneous
scenario on Qwen2.5-VL series models are shown in Table 3. As the parameter scale increases from 7B to 72B, the
accuracy consistently improves across tasks. For sarcasm
detection, accuracy rises from 66% to 81%, indicating a substantial gain of over 15 percentage points. Similarly, for hate
detection, accuracy increases from 62% to 79%. These results suggest that larger models possess a stronger capacity
for capturing subtle multimodal cues and complex pragmatic
signals, leading to more accurate social intent classification.


Figure 2: Ablation about debate round.

Figure 3: Ablation about model size on Qwen2.5-VL.
### **Conclusion**

In this work, we introduced MV-Debate, a novel multi-view
debate framework for multimodal harmful content detection
on social media. By orchestrating four view-specific agents
with complementary reasoning strategies and a dynamic reflection gating mechanism, MV-Debate effectively integrates
cross-modal evidence and contextual cues to identify complex social intents such as sarcasm, hate speech, and misinformation. Extensive experiments across multiple benchmarks
confirm its superior accuracy, efficiency, and interpretability
compared with strong baselines. Beyond performance gains,
MV-Debate also generates transparent debate transcripts, supporting model debugging, auditing, and user trust. Looking
forward, our framework provides a foundation for extending multi-agent debate approaches to broader safety-critical
multimodal reasoning tasks.
As for the limitation, the framework’s performance depends on the underlying LMMs, which may inherit biases
or struggle with culturally nuanced content such as sarcasm.
The current design also fixes the number of reasoning views,
which may not always balance accuracy and efficiency.


-----

### **References**

Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang,
K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl
technical report. *arXiv preprint arXiv:2502.13923* .

Balauca, A.-A.; Garai, S.; Balauca, S.; Shetty, R. U.; Agrawal,
N.; Shah, D. S.; Fu, Y.; Wang, X.; Toutanova, K.; Paudel,
D. P.; and Gool, L. V. 2025. Understanding the World’s
Museums through Vision-Language Reasoning. In *ICCV* .

Benchekroun, M.; Istrate, D.; Zalc, V.; and Lenne, D. 2022.
Mmsd: A Multi-modal Dataset for Real-time, Continuous
Stress Detection from Physiological Signals. In *HEALTHINF*,
240–248.

Bensal, S.; Jamil, U.; Bryant, C.; Russak, M.; Kamble, K.;
Mozolevskyi, D.; Ali, M.; and AlShikh, W. 2025. Reflect,
Retry, Reward: Self-Improving LLMs via Reinforcement
Learning. *arXiv preprint arXiv:2505.24726* .

Bi, J.; Wang, Y.; Chen, H.; Xiao, X.; Hecker, A.; Tresp,
V.; and Ma, Y. 2025. LLaVA Steering: Visual Instruction
Tuning with 500x Fewer Parameters through Modality Linear
Representation-Steering. In *Proceedings of the 63rd Annual*
*Meeting of the Association for Computational Linguistics*,
15230–15250.

Caffagni, D.; Cocchi, F.; Barsellotti, L.; Moratelli, N.; Sarto,
S.; Baraldi, L.; Cornia, M.; and Cucchiara, R. 2024. The
revolution of multimodal large language models: a survey.
*ACL* .

Cekinel, R. F.; Karagoz, P.; and oltekin. 2025. Multimodal
Fact-Checking with Vision Language Models: A Probing
Classifier Based Solution with Embedding Strategies. In
*Proceedings of COLING* .

Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang,
S.; Fu, J.; and Liu, Z. 2023. Chateval: Towards better llmbased evaluators through multi-agent debate. *arXiv preprint*
*arXiv:2308.07201* .

Chen, J.; Saha, S.; and Bansal, M. 2024. ReConcile:
Round - Table Conference Improves Reasoning via Consensus
among Diverse LLMs. In *Proc. ACL 2024*, 7066–7085.

Chen, Z.; Wang, W.; Cao, Y.; Liu, Y.; Gao, Z.; Cui, E.; Zhu, J.;
Ye, S.; Tian, H.; Liu, Z.; et al. 2024. Expanding performance
boundaries of open-source multimodal models with model,
data, and test-time scaling. *arXiv preprint arXiv:2412.05271* .

Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.;
Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang,
D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context,
and next generation agentic capabilities. *arXiv preprint*
*arXiv:2507.06261* .

Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J.; and Igor Mordatch. 2023a. Improving Factuality and Reasoning in Language Models through Multiagent Debate. *arXiv preprint*,
arXiv:2305.14325.

Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch,
I. 2023b. Improving factuality and reasoning in language
models through multiagent debate. In *Forty-first Interna-*
*tional Conference on Machine Learning* .


Estornell, A.; and Liu, Y. 2024. Multi-LLM debate: Framework, principals, and interventions. *Advances in Neural*
*Information Processing Systems*, 37: 28938–28964.

Fan, Y.; Ma, X.; Wu, R.; Du, Y.; Li, J.; Gao, Z.; and Li,
Q. 2024. Videoagent: A memory-augmented multimodal
agent for video understanding. In *European Conference on*
*Computer Vision*, 75–92. Springer.

Gao, P.; Xie, A.; Mao, S.; Wu, W.; Xia, Y.; Mi, H.; and Wei,
F. 2024a. Meta reasoning for large language models. *arXiv*
*preprint arXiv:2406.11698* .

Gao, Z.; Du, Y.; Zhang, X.; Ma, X.; Han, W.; Zhu, S.-C.;
and Li, Q. 2024b. Clova: A closed-loop visual assistant with
tool usage and update. In *Proceedings of the IEEE/CVF con-*
*ference on computer vision and pattern recognition*, 13258–
13268.

Guo, T.; Chen, X.; Wang, Y.; Chang, R.; Pei, S.; Chawla,
N. V.; Wiest, O.; and Zhang, X. 2024. Large language model
based multi-agents: A survey of progress and challenges.
*arXiv preprint arXiv:2402.01680* .

Hee, M.; Chong, W.; and Lee, R. 2023. Decoding the Underlying Meaning of Multimodal Hateful Memes. In *Proceed-*
*ings of the 32nd International Joint Conference on Artificial*
*Intelligence (IJCAI 2023)*, 5995–6002.

Huang, J.; and Zhang, J. 2024. A survey on evaluation
of multimodal large language models. *arXiv preprint*
*arXiv:2408.15769* .

Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh,
A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. *arXiv preprint*
*arXiv:2410.21276* .

Jin, Y.; Choi, M.; Verma, G.; Wang, J.; and Kumar, S. 2024.
MM-SOC: Benchmarking Multimodal Large Language Models in Social Media Platforms. In *Findings of ACL 2024*,
6192–6210.

Jinhe, B.; Wang, Y.; Yan, D.; Xiao, X.; Hecker, A.; Tresp,
V.; and Ma, Y. 2025a. Prism: Self-pruning intrinsic selection
method for training-free multimodal data selection. *arXiv*
*preprint arXiv:2502.12119* .

Jinhe, B.; Yan, D.; Wang, Y.; Huang, W.; Chen, H.; Wan, G.;
Ye, M.; Xiao, X.; Schuetze, H.; Tresp, V.; et al. 2025b. CoTKinetics: A Theoretical Modeling Assessing LRM Reasoning
Process. *arXiv preprint arXiv:2505.13408* .

Kiela, D.; Firooz, H.; Mohan, A.; Goswami, V.; Singh, A.;
Ringshia, P.; and Testuggine, D. 2020. The hateful memes
challenge: Detecting hate speech in multimodal memes. *Ad-*
*vances in neural information processing systems*, 33: 2611–
2624.

Kil, J.; Mai, Z.; Lee, J.; Chowdhury, A.; Wang, Z.; Cheng,
K.; Wang, L.; Liu, Y.; and Chao, W.-L. H. 2024. Mllmcompbench: A comparative reasoning benchmark for multimodal llms. *Advances in Neural Information Processing*
*Systems*, 37: 28798–28827.

Li, B.; Zhang, Y.; Guo, D.; Zhang, R.; Li, F.; Zhang, H.;
Zhang, K.; Zhang, P.; Li, Y.; Liu, Z.; et al. 2025. Llavaonevision: Easy visual task transfer. *Transactions on Machine*
*Learning Research* .


-----

Liang, B.; Lou, C.; Li, X.; Yang, M.; Gui, L.; He, Y.; Pei, W.;
and Xu, R. 2022. Multi-modal sarcasm detection via crossmodal graph convolutional network. In *Proceedings of the*
*60th Annual Meeting of the Association for Computational*
*Linguistics (Volume 1: Long Papers)*, volume 1, 1767–1777.
Association for Computational Linguistics.

Lin, H.; Luo, Z.; Gao, W.; Ma, J.; Wang, B.; and Yang,
R. 2024a. Towards Explainable Harmful Meme Detection
through Multimodal Debate between Large Language Models. *Proceedings of the ACM Web Conference 2024* .

Lin, H.; Luo, Z.; Gao, W.; Ma, J.; Wang, B.; and Yang, R.
2024b. Towards explainable harmful meme detection through
multimodal debate between large language models. In *Pro-*
*ceedings of the ACM Web Conference 2024*, 2359–2370.

Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual instruction
tuning. *Advances in neural information processing systems*,
36: 34892–34916.

Liu, Y.; Cao, J.; Li, Z.; He, R.; and Tan, T. 2025. Breaking
mental set to improve reasoning through diverse multi-agent
debate. In *The Thirteenth International Conference on Learn-*
*ing Representations* .

Liu, Z.; Zhou, Z.; and Hu, M. 2025. CAF-I: A Collaborative
Multi - Agent Framework for Enhanced Irony Detection with
Large Language Models. *arXiv preprint*, arXiv:2506.08430.

Lu, M.; Dong, Z.; Guo, Z.; Zhang, X.; Lu, X.; Wang, T.; and
Zhang, L. 2025. A Multi-Modal Sarcasm Detection Model
Based on Cue Learning. *Scientific Reports*, 15(10261). Openaccess article.

Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;
and *et al.* 2023. Self - Refine: Iterative Refinement with
Self-Feedback. *arXiv preprint*, arXiv:2303.17651.

OpenAI. 2025. Introducing OpenAI o3 and o4-mini. https:
//openai.com/index/introducing-o3-and-o4-mini. Accessed
2 Aug 2025.

Pham, C.; Liu, B.; Yang, Y.; Chen, Z.; Liu, T.; Yuan, J.;
Plummer, B. A.; Wang, Z.; and Yang, H. 2023. Let models
speak ciphers: Multiagent debate through embeddings. *arXiv*
*preprint arXiv:2310.06272* .

Qi, P.; Yan, Z.; Hsu, W.; and Lee, M. L. 2024. SNIFFER:
Multimodal Large Language Model for Explainable Out-ofContext Misinformation Detection. In *Proceedings of CVPR* .

                        Renze, M.; and Guven, E. 2024. Self Reflection in LLM
Agents: Effects on Problem - Solving Performance. *Proceed-*
*ings of FLLM 2024* .

Shu, K.; Mahudeswaran, D.; Wang, S.; Lee, D.; and Liu, H.
2020. Fakenewsnet: A data repository with news content,
social context, and spatiotemporal information for studying
fake news on social media. *Big data*, 8(3): 171–188.

Tahmasebi, S.; Muller-Budack, E.; and Ewerth, R. 2024. ¨
Multimodal Misinformation Detection using Large VisionLanguage Models. In *Proceedings of CIKM* .

Tang, B.; Lin, B.; Yan, H.; and Li, S. 2024. Leveraging Generative Large Language Models with Visual Instruction and
Demonstration Retrieval for Multimodal Sarcasm Detection.
In *Proceedings of the 2024 Conference of the North American*
*Chapter of the Association for Computational Linguistics:*


*Human Language Technologies*, 1732–1742. Mexico City,
Mexico: Association for Computational Linguistics.

Team, G.; Kamath, A.; Ferret, J.; Pathak, S.; Vieillard, N.;
Merhej, R.; Perrin, S.; Matejovicova, T.; Rame, A.; Rivi ´ ere, `
M.; et al. 2025. Gemma 3 technical report. *arXiv preprint*
*arXiv:2503.19786* .

Van, M.-H.; and Wu, X. 2023. Detecting and Correcting Hate
Speech in Multimodal Memes with Large Visual Language
Model. In *arXiv preprint arXiv:2311.06737* .

Wang, H.; Ni, Z.; Zhang, S.; Lu, S.; Hu, S.; He, Z.; Hu,
C.; Lin, J.; Guo, Y.; Du, Y.; et al. 2025. RepoMaster:
Autonomous Exploration and Understanding of GitHub
Repositories for Complex Task Solving. *arXiv preprint*
*arXiv:2505.21577* .

Wang, P.; Zhang, Y.; Fei, H.; Chen, Q.; and Qin, L. 2024. S3
Agent: Unlocking the Power of VLLM for Zero-Shot MultiModal Sarcasm Detection. *ACM Transactions on Multimedia*
*Computing, Communications, and Applications* .

Wang, Z.; Mao, S.; Wu, W.; Ge, T.; Wei, F.; and Ji, H. 2023.
Unleashing the emergent cognitive synergy in large language
models: A task-solving agent through multi-persona selfcollaboration. *arXiv preprint arXiv:2307.05300* .

Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Li, B.; and
*et al.* 2023. AutoGen: Enabling Next - Gen LLM Applications via Multi - Agent Conversation. *arXiv preprint*,
arXiv:2308.08155.

Xuan, K.; Yi, L.; Yang, F.; Wu, R.; Fung, Y. R.; and Ji, H.
2024. LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation.
*arXiv preprint arXiv:2402.11943* .

Yamagishi, Y. 2024. Simpler Prompts, Better Results: Enhancing Zero-shot Detection with a Large Multimodal Model.
In *Proceedings of CASE 2024* .

Yoffe, L.; Amayuelas, A.; and Wang, W. Y. 2024. DebUnc:
Improving Large Language Model Agent Communication
With Uncertainty Metrics. *arXiv preprint arXiv:2407.06426* .
Zhang, Y.; Sun, R.; Chen, Y.; Pfister, T.; Zhang, R.; and
Arik, S. 2024. Chain of Agents: Large Language Models
Collaborating on Long - Context Tasks. In *Proceedings of the*
*38th Conference on Neural Information Processing Systems*
*(NeurIPS 2024)* .

Zhang, Y.; Zou, C.; Wang, B.; and Qin, J. 2025. CommanderGPT: Fully Unleashing the Sarcasm Detection Capability
of Multi-Modal Large Language Models. *arXiv preprint*
*arXiv:2503.18681* .


-----


