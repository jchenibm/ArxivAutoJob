{
  "title": "OMNI EAR: BENCHMARKING AGENT REASONING IN EMBODIED TASKS",
  "detailed_summary": "该论文提出了一个名为**OmniEAR**的综合框架，用于评估大型语言模型在具身任务中进行推理的能力，尤其是在物理互动、工具使用和多智能体协作方面。与现有基准测试不同，OmniEAR要求智能体动态地获取能力，并根据任务需求自主地确定协作策略。该框架使用基于文本的环境表示，对家庭和工业领域跨越1500个场景的连续物理属性和复杂的空间关系进行建模。系统的评估揭示了模型在面对约束条件时的严重性能下降：虽然在明确的指令下成功率达到85-96%，但在工具推理方面的性能下降到56-85%，在隐式协作方面的性能下降到63-85%，而复合任务的失败率超过50%。令人惊讶的是，完整的环境信息反而降低了协作性能，这表明模型无法过滤与任务相关的约束。微调可以显著提高单智能体任务的性能（0.6%到76.3%），但对多智能体任务的提升却微乎其微（1.5%到5.5%），揭示了底层架构的局限性。这些发现表明，具身推理提出了与当前模型所能解决的根本不同的挑战，OmniEAR成为了评估和推进具身人工智能系统的严格基准。",
  "background": "大型语言模型在复杂的推理任务中取得了显著的成功，但它们在具身环境中的推理能力仍然知之甚少。在具身任务中，智能体必须理解物体属性如何影响可能采取的行动，识别自身能力不足以完成任务的情况，并确定何时需要协作。这些推理能力与抽象的问题解决根本不同，因为它们需要理解支配现实世界互动的物理原理。现有的评估方法未能捕捉到这种具身推理的复杂性。现有的基准测试通过离散状态（如门的打开/关闭或物体的拾取/放置）对环境进行建模，忽略了诸如重量、温度或材料成分等决定行动可行性的连续属性。工具使用评估通常提供固定的动作集，错过了智能体应该如何推理能力差距。多智能体基准测试依赖于明确的协作指令或效率指标，而不是检查智能体是否能够识别任务何时超出个人能力。",
  "contributions": [
    "提出了OmniEAR框架，它通过需要智能体理解物理属性如何决定动作、能力和协调需求的场景来评估具身推理，解决了当前评估方法中的根本差距。",
    "开发了EAR-Bench，一个包含1500个场景的基准测试，具有连续的物理属性和动态能力，由EAR-Sim和一个自动生成流程支持。",
    "提供了经验证据，表明当前的语言模型缺乏核心的具身推理能力，当从显式指令转向具身推理时，性能下降超过60%，揭示了推进具身AI的关键要求。"
  ],
  "problem": "该论文旨在解决现有具身AI基准测试的局限性，这些测试通常未能充分捕捉到真实世界具身推理的复杂性。具体来说，现有基准测试通常：\n\n*   使用离散状态而非连续属性来模拟物理环境。\n*   提供固定的工具集，无法评估智能体动态获取工具的能力。\n*   依赖于明确的协作指令，未能测试智能体自主决定何时需要协作的能力。\n\n因此，该论文要解决的主要问题是：如何创建一个更全面、更真实的基准测试，以评估智能体在具身环境中进行物理推理、工具使用和多智能体协作的能力。",
  "methods": [
    "**环境表示：** 使用有向图G来表示具身环境，图中包含空间节点（房间和区域）、对象节点（可交互项目）和智能体节点。每个节点维护一个属性字典A，存储连续的物理属性，如重量、温度、材料成分和几何尺寸。边集E编码空间关系，包括静态的包含关系和动态的邻近关系。",
    "**任务形式化：** 每个评估任务定义为一个元组T，包括初始环境状态Sinit、自然语言指令I、通过逻辑谓词定义的目标状态Ggoal和参与的智能体集Atask。评估目标是评估智能体是否能生成一个动作序列Π，将环境从Sinit转换到满足Ggoal中所有谓词的最终状态Sfinal。",
    "**分层任务分类：** 任务沿着两个正交维度组织：智能体配置（单智能体 vs. 多智能体）和认知复杂性（L1: 基础，L2: 中级，L3: 高级）。",
    "**EAR-Sim环境模拟器：** 使用基于文本的环境建模来实现大规模的高效模拟。图结构G维护空间关系，避免了昂贵的碰撞检测。状态更新采用增量方法，动作只修改直接受影响的节点和边。",
    "**动态能力管理：** 代理动作分为基本动作和工具相关动作。当代理获得一个工具时，系统动态地将相关能力绑定到代理的动作集合，从而能够对代理通过使用工具扩展其能力进行现实建模。",
    "**涌现协作：** 当代理尝试对超出其能力的物体执行动作时，系统启用协作请求机制。",
    "**自动基准生成：** 结合LLM与基于规则的验证生成场景，包括场景生成，任务生成，评估逻辑提取以及专家轨迹生成。"
  ],
  "experimental_design": "实验设计主要包括以下几个方面：\n\n*   **模型选择：** 选择了9个代表性的模型，包括闭源模型（GPT-4o、Gemini-2.5-Flash）、开源基础模型（Deepseek-V3、Qwen2.5系列、Llama3.1-8B）和推理专用模型（Deepseek-R1、QwQ-32B）。\n*   **评估协议：** 所有模型都接受相同的评估，以确保公平比较。采用部分可观察性，即智能体必须探索环境才能发现物体的位置和属性。每个模型完成2800个测试场景，涵盖七个任务类别，并进行三次独立运行以确保统计可靠性。对所有模型采用标准化的提示、环境描述和动作词汇表，并根据上下文动态启用工具相关的动作。\n*   **微调配置：** 为了评估监督学习是否能解决推理的局限性，对Qwen2.5-3B在专家轨迹上进行了微调。从具有完全环境访问权限的Qwen2.5-72B收集了1942个成功的演示，并过滤了最佳的动作序列。使用标准的因果语言建模目标来训练模型。",
  "results": "主要实验结果如下：\n\n*   在直接命令任务上的成功率从85.2-96.6%下降到复合协作任务上的32.0-48.5%。\n*   工具使用需要识别上下文中的能力差距，属性推理需要将语言与物理属性联系起来。两者都涉及从环境约束中推断需求，而不是遵循明确的指令。\n*   具有物理约束推理的任务表现出更复杂的关系。工具的使用表现出类似的陡峭缩放，这表明维护用于能力获取的多步计划与模型容量密切相关。\n*   属性推理和复合推理较早趋于平稳，在超过72B参数后收益递减。这种差异化的缩放表明，原始参数计数可以更好地执行和计划，但并不一定提高对物理属性的理解。\n*   推理专用模型在显式逻辑规划方面表现出色，但在将抽象属性与物理环境联系起来时遇到了困难。\n*   Qwen2.5-3B在单智能体任务上的模仿学习表现出明显改进（从0.6%到76.3%），但在多智能体任务上的表现仍然可以忽略不计（从1.5%到5.5%），这意味着学习到的行为无法推广到需要自主评估物理约束和协调需求的场景中。",
  "result_analysis": "对结果的深入分析表明，模型在执行直接命令等简单任务时，性能会随着模型规模的增加而显著提高。然而，对于需要物理约束推理的任务，例如工具使用和属性推理，性能的提升速度较慢，并且在达到一定规模后会趋于饱和。这表明，简单地增加模型规模并不能有效地提高模型在具身环境中进行推理的能力。此外，推理专用模型虽然在逻辑规划方面表现出色，但在将抽象概念与具体的物理属性联系起来时表现不佳，这表明当前的架构缺乏必要的机制来实现自主的具身决策。最后，微调虽然可以提高单智能体任务的性能，但对多智能体任务的提升效果有限，这表明协作推理需要超越当前训练方法的架构能力。",
  "conclusions": "该论文的主要结论是：\n\n*   当前的语言模型在从显式指令转向基于约束的推理时，性能会显著下降，在工具使用和协调任务中，性能从85%以上降至65%以下。\n*   维持多步计划需要关键的参数阈值。\n*   环境信息对协调产生矛盾的影响。\n*   微调无法解决多智能体推理的差距。\n*   具身推理需要与当前语言模型不同的计算机制。",
  "limitations": "本文的局限性在于：\n\n*   该框架采用基于文本的环境，抽象掉了连续控制、感觉运动反馈和物理具身系统中存在的实时约束。\n*   虽然这种抽象可以进行系统的评估，但它可能无法捕捉到具身智能的方方面面。\n*   已识别的架构需求需要在连续控制设置中进行验证。\n*   未来的工作应该调查这些组件如何与感觉运动处理集成，并检查观察到的计算瓶颈是否仍然存在于物理接地的系统中。\n*   此外，探索混合符号-神经架构，可以明确地推理物理定律，同时保持学习的灵活性，代表了一个有希望的方向。",
  "future_work": "未来的工作应该调查具身智能体框架组件如何与感觉运动处理集成，并检查观察到的计算瓶颈是否仍然存在于物理接地的系统中。此外，探索混合符号-神经架构，可以明确地推理物理定律，同时保持学习的灵活性，代表了一个有希望的方向。",
  "applications": "这项研究的实际应用包括：\n\n*   开发更智能、更自主的机器人，可以在复杂和动态的环境中执行任务。\n*   改进虚拟助手和游戏AI，使其能够更好地理解和响应用户的指令。\n*   创建更逼真和身临其境的虚拟现实和增强现实体验。\n*   设计更有效的训练和模拟系统，用于机器人和人类。",
  "related_work": "文中提到了以下相关工作：\n\n*   ALFRED (Shridhar et al., 2020)\n*   BEHAVIOR-1K (Li et al., 2024a)\n*   RoCo (Mandi et al., 2024)\n*   PARTNR (Chang et al., 2024)\n*   TDW-MAT (Zhang et al., 2024)\n*   EmbodiedBench (Yang et al., 2025)",
  "github_links": [
    "https://github.com/ZJU-REAL/OmniEmbodied"
  ],
  "published": "2025-08-07T17:54:15+00:00"
}