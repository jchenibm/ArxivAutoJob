{
  "title": "OmniEAR：在具身任务中评估智能体推理能力",
  "detailed_summary": "本文提出了OmniEAR，一个综合性的框架，用于评估大型语言模型在具身任务中关于物理交互、工具使用和多智能体协作的推理能力。与现有基准测试不同，OmniEAR要求智能体动态地获取能力，并根据任务需求自主决定协作策略。该框架通过基于文本的环境表示，对跨越家庭和工业领域的1500个场景中的连续物理属性和复杂空间关系进行建模。系统的评估揭示了当模型必须从约束中进行推理时，性能会严重下降。精调可以显著提高单智能体任务的性能，但对多智能体任务的提升效果甚微，这暴露了基础架构的局限性。这些发现表明，具身推理提出了与当前模型所能解决的根本不同的挑战，从而将OmniEAR确立为评估和推进具身AI系统的严格基准。",
  "background": "大型语言模型在抽象推理任务中取得了显著成功，但它们在具身环境中的推理能力仍然知之甚少。在具身任务中，智能体必须理解物体属性如何影响可能的动作，识别何时自身能力不足以完成任务，并确定何时需要协作。这些推理能力与抽象问题解决根本不同，因为它们需要理解支配现实世界交互的物理原理。目前的评估方法未能捕捉到这种具身推理的复杂性。现有的基准测试通过离散状态（如门的打开/关闭或物体的拾取/放置）对环境进行建模，忽略了连续属性，例如重量、温度或材料组成，这些属性决定了动作的可行性。工具使用评估通常提供固定的动作集，忽略了智能体应如何推理能力差距。多智能体基准测试依赖于明确的协作指令或效率指标，而不是检查智能体是否可以识别任务何时超出个人能力。因此，需要一个能够评估智能体基于任务需求进行能力获取和协调需求的推理能力的框架。",
  "contributions": [
    "提出了OmniEAR框架，该框架通过要求智能体理解物理属性如何决定动作、能力和协调需求，从而评估具身推理，解决了当前评估方法中的根本差距。",
    "开发了EAR-Bench，一个包含1500个具有连续物理属性和动态能力的场景的基准测试，由EAR-Sim和自动生成流水线支持。",
    "提供了经验证据，表明当前的语言模型缺乏核心的具身推理能力，从显式指令转向具身推理时，性能下降超过60%，揭示了推进具身AI的关键需求。"
  ],
  "problem": "论文旨在解决当前具身AI评估基准的局限性，这些基准通常无法充分捕捉智能体在现实世界环境中推理的复杂性。具体来说，现有基准往往使用离散状态表示环境，忽略了连续的物理属性，并且依赖于静态的工具集和明确的协作指令，从而无法评估智能体基于物理约束和任务需求自主地进行能力获取和协作决策的能力。因此，论文旨在创建一个更全面的评估框架，能够更真实地反映具身智能体在复杂环境中的推理挑战。",
  "methods": [
    "**环境表示：** 使用有向图 Gt = (Vt, Et, At) 对具身环境进行建模，其中 Vt 包含空间节点、对象节点和智能体节点，Et 编码空间关系，At 存储连续物理属性。",
    "**任务形式化：** 每个评估任务定义为一个元组 T = (Sinit, I, Ggoal, Atask)，其中 Sinit 指定初始环境状态，I 提供自然语言指令，Ggoal 通过逻辑谓词定义成功条件，Atask 标识参与智能体。",
    "**动态能力管理：** 通过动态工具-能力绑定系统，智能体的动作分为基本动作和工具相关动作，工具对象维护一个能力属性，指定其启用的动作。当智能体抓住工具时，系统动态地将相关能力绑定到智能体的动作集。",
    "**突发协作：** 当智能体尝试对属性超出个人能力的对象执行动作时，系统启用协作请求机制。",
    "**自动基准生成：** 通过四阶段流程，结合LLM和基于规则的验证，生成场景、任务、评估逻辑和专家轨迹。"
  ],
  "experimental_design": "论文通过EAR-Bench对LLM进行系统评估。选择了GPT-4o、Gemini-2.5-Flash、Deepseek-V3、Qwen2.5系列和Llama3.1-8B等九个代表性模型，涵盖了三种架构范式。采用部分可观测性，即智能体需要探索环境以发现物体的位置和属性。在七个任务类别中完成了2,800个测试场景，每个模型独立运行三次以确保统计可靠性。通过专家轨迹微调了Qwen2.5-3B，并使用相同的推理参数对所有模型进行了评估，以确保公平比较。",
  "results": "实验结果表明，当模型必须从物理约束中动态获取工具和确定协作需求时，性能会显著下降。在直接指令任务上，模型成功率达到85.2-96.6%，但在复杂的协作任务上，成功率降至32.0-48.5%。工具使用任务的成功率为73.4-85.8%，属性推理任务为41.9-77.8%。与GPT4o相比，Deepseek-R1等推理专用模型在Compound Collaboration上的性能更高，这表明这些模型擅长逻辑规划，但在物理上下文中对抽象属性进行推理方面存在困难。此外，增加世界知识图谱有助于提高工具使用和属性推理任务的性能，但会降低隐式协作任务的性能。微调可以显著提高单智能体任务的性能，但对多智能体任务的提升效果甚微。",
  "result_analysis": "实验结果分析表明，在模型必须从物理约束中推理能力需求以及确定协作需求时，性能会显著下降。较小的模型无法维持对工具和协作进行多步推理所需的规划状态。推理模型擅长逻辑规划，但难以在具体的物理属性中找到抽象概念的基础。微调可以提高单智能体的性能，但是这些增益不能转移到多智能体场景中，这意味着协作推理需要超出当前训练方法之外的架构能力。完整的环境信息会损害协调性能，这表明模型无法过滤与任务相关的约束。",
  "conclusions": "OmniEAR基准测试表明，当前语言模型在具身推理方面存在显著局限性，尤其是在需要从物理约束中进行推理的情况下。模型在工具使用和协作任务中的性能从85%以上降至65%以下。同时，结果揭示了维持多步计划的关键参数阈值、环境信息对协作的悖论性影响，以及微调无法解决多智能体推理差距的问题。这些结果表明，具身推理需要与当前语言模型不同的计算机制。",
  "limitations": "本文提出的基于文本的框架抽象了连续控制、传感器反馈和物理具身系统中存在的实时约束。虽然这种抽象可以进行系统评估，但可能无法捕捉到具身智能的所有方面。因此，在连续控制设置中验证所确定的体系结构需求是必要的。此外，探索能够显式推理物理定律同时保持学习灵活性的混合符号-神经体系结构代表了一个有希望的方向。",
  "future_work": "未来的工作应该调查这些组件如何与传感器运动处理集成，并检查观察到的计算瓶颈是否在物理接地的系统中持续存在。此外，探索能够显式推理物理定律同时保持学习灵活性的混合符号-神经体系结构代表了一个有希望的方向。",
  "applications": "这项研究为具身人工智能的发展提供了重要的基准和评估工具，可以用于开发和评估在现实世界中与环境交互的智能体，例如家庭机器人、工业自动化系统和医疗辅助设备。通过更好地理解和解决具身推理的挑战，可以提高这些智能体在复杂环境中的自主性和适应性，从而改善生产效率和生活质量。",
  "related_work": "论文详细讨论了与本研究相关的工作，包括ALFRED、BEHAVIOR-1K、RoCo、PARTNR、TDW-MAT和EmbodiedBench等。这些基准测试在任务覆盖范围、物理状态表示、工具使用评估和多智能体协作等方面与OmniEAR进行了比较，突出了OmniEAR在连续属性推理、动态工具-能力绑定和隐式协作方面的独特贡献。",
  "github_links": [
    "https://github.com/ZJU-REAL/OmniEmbodied"
  ],
  "published": "2025-08-07T17:54:15+00:00"
}