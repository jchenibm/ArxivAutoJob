Pre p rint
## C OOPER : C O -O PTIMIZING P OLICY AND R EWARD M ODELS IN R EINFORCEMENT L EARNING FOR L ARGE L ANGUAGE M ODELS

**Haitao Hong** [1] *[,][∗]* **,** **Yuchen Yan** [1] *[,][∗]* **,** **Xingyu Wu** [1] **,** **Guiyang Hou** [1] **,** **Wenqi Zhang** [1]

**Weiming Lu** [1] **Yongliang Shen** [1] *[,][†]* **Jun Xiao** [1]

1 Zhejiang University
*{* haitaohong, yanyuchen, syl *}* @zju.edu.cn

� GitHub: [https://github.com/zju-real/cooper](https://github.com/zju-real/cooper)
� Project: [https://zju-real.github.io/cooper](https://zju-real.github.io/cooper)
#### A BSTRACT

Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However,
both approaches suffer from limitations: rule-based rewards lack robustness,
while model-based rewards are vulnerable to reward hacking. To address
these issues, we propose **Cooper** (Co     - o ptimizing P olicy Mod e l and R eward
Model), a RL framework that jointly optimizes both the policy model and the
reward model. Cooper leverages the high precision of rule-based rewards when
identifying correct responses, and dynamically constructs and selects positivenegative sample pairs for continued training the reward model. This design
enhances robustness and mitigates the risk of reward hacking. To further support
Cooper, we introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer
as input. Based on this design, we train a reward model named VerifyRM,
which achieves higher accuracy on VerifyBench compared to other models of
the same size. We conduct reinforcement learning using both VerifyRM and
Cooper. Our experiments show that Cooper not only alleviates reward hacking
but also improves end-to-end RL performance, for instance, achieving a 0.54%
gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate
that dynamically updating reward model is an effective way to combat reward
hacking, providing a reference for better integrating reward models into RL.
#### 1 I NTRODUCTION

*“He who teaches, who learns.”*

— Confucius

Large language models (LLMs) have demonstrated remarkable capabilities in reasoning (Kumar
et al., 2025; OpenAI et al., 2024), particularly in mathematical reasoning (Lewkowycz et al., 2022;
Shao et al., 2024; Ying et al., 2024; Yang et al., 2024b), code reasoning (Roziere et al., 2023; Zhu
et al., 2024; Hui et al., 2024), and commonsense reasoning (Wang et al., 2024), often achieving
or even surpassing human-level performance. Recent advances demonstrate that reinforcement
learning (RL) has become a pivotal technique for enhancing these reasoning capabilities (Xu et al.,

  - The first two authors have equal contributions.

  - Corresponding author.

1


-----

Pre p rint






|Ref Ans Question Policy RM|Col2|
|---|---|
|Response||
|Pos Neg||



**Robust**

**Hacking**
**Resist**

Figure 1: Model-based rewards are generally robust to variations in model outputs, but they are
susceptible to being hacked by the policy model. In contrast, rule-based rewards are less prone to
hacking but often lack robustness. We introduce **Cooper**, a reinforcement learning framework that
achieves both high robustness and resistance to reward hacking. In this figure, the black arrows
indicate the rollout process, the blue arrows represent the reward assignment process, and the brown
arrows denote the update process for the reward model.

2025). By generating multiple solution trajectories and optimizing the model to align with highquality responses, RL enables LLMs to achieve performance that often matches or exceeds human
expertise (Havrilla et al., 2024).

In RL algorithms, one critical factor affecting performance is the design of the reward function, as
it determines the quality of evaluation for output sequences. In the early stages of RL for LLMs,
human preference data was typically used to train a reward model that assigns rewards based on
the input question and model generations (Ouyang et al., 2022; Bai et al., 2022). This paradigm,
known as reinforcement learning from human feedback (RLHF), has been widely adopted (Havrilla
et al., 2024; Chen et al., 2025b). Since the introduction of reasoning models like OpenAI’s o1 (Jaech
et al., 2024), DeepSeek-R1 (Guo et al., 2025a) and Kimi-k1.5 (Team et al., 2025), RL for LLMs has
shifted focus towards verifiable tasks, where rule-based reward functions are commonly employed
to assign scores, thereby improving the reliability of the scoring system.

However, both model-based and rule-based reward functions have inherent limitations. Model-based
rewards, which rely on dynamic calculations based on the model’s parameters, are prone to reward
hacking (Gao et al., 2023). Specifically, when a fixed reward model is used, the model may exploit
output patterns that deceive the reward function, thereby obtaining high scores regardless of the
correctness of the output. This phenomenon can lead to catastrophic failures in the later stages
of training. On the other hand, rule-based reward functions often rely on manually crafted rules to
parse and verify the model’s output (Gandenberger & Kydl´ıˇcek, 2024; Gao et al., 2024). This method
lacks robustness and is susceptible to misjudgment, which constrains the further optimization of the
model (Christiano et al., 2017).

In our preliminary experiments, we observed that rule-based reward functions exhibit high precision
in identifying correct samples and high recall in detecting incorrect ones. That is, samples judged as
correct by rule-based reward functions are usually indeed correct, whereas those judged as incorrect
may still be correct in reality. This phenomenon arises because rule-based functions are typically
handcrafted and lack robustness in answer extraction. As a result, they often fail to accurately extract
and match answers, especially when facing diverse output formats generated by different models.

In this paper, we propose **Cooper**, a novel reinforcement learning framework that enables
synchronized co-optimization of the policy model and the reward model. Cooper introduces a twostage training pipeline: **(1) Policy model optimization** : Policy model optimization follows the

2


-----

Pre p rint

GRPO paradigm, involving sampling and scoring responses with a reference-aware reward model,
and performing policy updates based on within-group normalized advantages and KL regularization;
**(2) Reward model optimization** : Reward model optimization continuously refines the reward
model via contrastive learning, using positive samples identified by high-precision rule-based signals
and negative samples generated by transforming correct responses into incorrect ones with an
assistant LLM.

To support cooper framework, we first focus on training an accurate and robust reference-based
reward model. We construct a large-scale dataset using responses generated from diverse LLMs
across multiple high-quality math reasoning datasets. A hybrid annotation strategy is applied using
both rule-based verifier tools (e.g., Math-Verify) and LLM-based verifiers, allowing for automatic
correctness labeling at scale. Using this labeled data, we train a reward model **VerifyRM** that scores
responses, with the query and a reference answer as its input.

We then integrate this reward model into the Cooper pipeline and validate its effectiveness through
comprehensive experiments. Our results demonstrate that applying this co-optimization framework
significantly improves the policy model’s reasoning ability. Our experiments show that models
trained with Cooper outperform both rule-based and fixed-reward-model RL baselines across several
challenging math reasoning benchmarks.

The main contributions are summarized as follows:

    - We introduce a reward modeling dataset, which is labeled using a hybrid annotation strategy
that combines rule-based verification and LLM-as-a-judge verification, enabling efficient
and reliable correctness supervision. The reward model trained on our constructed dataset
achieves an accuracy of 89.42% on VerifyBench, surpassing existing reward models of the
same scale.

   - Based on the high precision of rule-based rewards in identifying correct answers, we
propose **Cooper**, a reinforcement learning framework that co-optimizes the policy model
and the reward model simultaneously. This framework mitigates the problem of reward
hacking commonly observed in reward-model-based RL and enhances overall training
performance.

   - Our work demonstrates that dynamically adjusting the parameters of the reward model
during the RL training process can effectively mitigate the phenomenon of reward hacking,
providing valuable insights for the research community on how to better utilize reward
models in reinforcement learning.
#### 2 R ELATED W ORKS

**Reinforcement Learning for Large Language Models.** Reinforcement Learning (RL) has
emerged as a foundational method for aligning large language models (Christiano et al., 2017;
Ziegler et al., 2020; Kaufmann et al., 2023). Early work such as InstructGPT (Ouyang et al.,
2022) demonstrated that fine-tuning LLMs with a reward model trained on human preference
data can significantly improve response helpfulness and alignment. However, RLHF is often
computationally expensive, costly, and reliant on large-scale human annotations. To mitigate
these issues, recent methods have proposed simplifying the RLHF process. Direct Preference
Optimization (DPO) (Rafailov et al., 2023) reformulates the RLHF objective as a contrastive loss
over preference pairs, eliminating the need for reward model training and sampling-based updates.
Reinforcement Learning with AI Feedback (RLAIF) (Bai et al., 2022) proposes replacing human
preference data with AI-generated feedback guided by predefined principles instead of humans,
significantly lowering the annotation cost and improving scalability.

**Reward Models for Reinforcement Learning.** Reward Models (RMs) has been widely used
in reasoning tasks (Zhong et al., 2025) for reinforcement learning (Ouyang et al., 2022; Lambert
et al., 2024b; Guo et al., 2025b) and verification-guided inference (Guo et al., 2025b; Li et al.,
2023; Zhang et al., 2025; Yu et al., 2024). According to reward modeling mechanisms, Existing
RMs broadly fall into three types: (1) discriminative reward models (Cai et al., 2024; Zang et al.,
2025), typically implemented as classifiers over response sequences, assigning binary or fine-grained
preference scores; (2) generative reward models (Liu et al., 2025; Alexandru et al., 2025; Hong

3


-----

Pre p rint

et al., 2025), which generate textual feedback or critiques before producing a scalar reward; and (3)
implicit reward models (Lambert et al., 2024a), often optimized via DPO (Rafailov et al., 2023),
where model likelihoods are interpreted as reward signals. Orthogonally, reward models can be
categorized into outcome reward models (ORMs) (Liu et al., 2024a; Cobbe et al., 2021), which
assign scalar feedback to final outputs, and process reward models (PRMs) (Setlur et al., 2025),
which evaluate intermediate reasoning steps to provide denser and more interpretable supervision.
Our VerifyLM belongs to the discriminative RM and ORM.

**Reinforcement Learning with Verifiable Rewards.** As an alternative to learned reward models,
Reinforcement Learning with Verifiable Rewards (RLVR) (Guo et al., 2025a; Lambert et al., 2024a;
Yue et al., 2025) leverages rule-based verification functions—such as exact answer matching or
logical consistency checks—to generate reward signals automatically. Notably, DeepSeek-R1 (Guo
et al., 2025a) achieves strong reasoning performance through a multi-stage pipeline combining
supervised pretraining with Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Cooper
draws inspiration from the recent advancements in enhancing reasoning through RLVR, screening
out the correct responses of the policy model via symbolic verification, and thereby constructing
preference data to update the reward model.
#### 3 M ETHODS

Our methods consist of two main components. The first part proposes a pipeline for constructing a
reference-based reward model **VerifyRM**, which includes data collection and annotation strategies,
as well as the training procedure for the reward model. The second part presents **Cooper**, a
reinforcement learning algorithm that co - o ptimizes both the p olicy mod e l and the r eward model.
In this framework, the RM trained in the first stage guides the policy model’s updates within Cooper
while being updated itself concurrently.

3.1 T RAINING R ECIPE OF V ERIFY RM

Most existing reward models score the input-output pairs of large language models (LLMs)
directly (Zhong et al., 2025). However, in reasoning tasks, there typically exists a reference
answer. Yan et al. (2025) have demonstrated the importance of reference answers for model-based
verification. Therefore, we propose a method for constructing reference-based reward models to
improve the accuracy of reward models in reasoning tasks.

3.1.1 D ATA P REPARATION

To train VerifyLM, the required data format consists of a reasoning problem, its corresponding
reference answer, a model-generated completion, and a label indicating whether the completion is

correct.

**Problem-reference-completion triples collection.** We collected 7 commonly used mathematical
reasoning datasets, each containing math problems and their corresponding reference answers.
Using 11 mainstream LLMs, we generated completions for these math problems, with each model
providing one completion per problem. During sampling, we set the temperature to 0.7 and top p
to 0.95. In total, we collected 65K problem-reference-completion triples. Details of the datasets,
LLMs, and their licenses are provided in the Appendix A and B.

**Hybrid labeling for correctness.** In prior works, researchers have relied heavily on manual
annotation to determine the correctness of model completions (Chen et al., 2025a). We observe that
current LLMs have already demonstrated strong capabilities in evaluating the correctness of completions against reference answers (e.g., Qwen3-4B achieves 94.17% accuracy on VerifyBench) (Yang
et al., 2025). Motivated by this, we propose an automated hybrid labeling approach that combines
a rule-based verifier and an LLM-as-a-judge. Specifically, we use Math-verify (Gandenberger &
Kydl´ıˇcek, 2024) as the rule-based verifier and Qwen3-4B (in non-thinking mode) (Yang et al., 2025)
as the LLM-as-a-judge. We only retain samples for which both methods agree on the correctness
label, resulting in a dataset of 58.7K examples for training VerifyRM. Detailed statistics are provided
in the Appendix D.

4


-----

Pre p rint












Figure 2: An overview of the Cooper training framework. Each training step in Cooper consists of
two stages: policy model optimization (blue area) and reward model optimization (green area).

3.1.2 R EWARD M ODEL T RAINING

In this paper, following the approach of Cai et al. (2024), the reward model is formulated as a
text classifier. However, we incorporate the reference answer into the input of the reward model.
The specific input template is provided in the appendix C.3. Inspired by Zang et al. (2025), we
initialize our model from an already aligned LLM, replacing its original language modeling head
with a newly initialized score head. The model is then trained using binary cross-entropy loss. The
objective function can be formally written as:

L( *θ* ) = E [ *{q,r,c,y}∼D* ] BCE( *σ* ( *M* *θ* ( *q, r, c* )) *, y* ) (1)

ˆ
BCE(ˆ *y, y* ) = *−y ∗* log ˆ *y −* (1 *−* *y* ) log(1 *−* *y* ) (2)

where *q* denotes the question, *r* denotes the reference answer, *c* denotes the model’s completion, *y*
indicates the correctness label, and *D* represents the training dataset. The sigmoid function *σ* is used
to map the logits output by the model *M* *θ* into the range [0 *,* 1], which is then used to compute the
binary cross-entropy loss.

3.2 R EINFORCEMENT L EARNING WITH C OOPER

We propose **Cooper**, a reinforcement learning framework that co-optimizes policy and reward
models. Cooper enables simultaneous tuning of the policy model and reward model in a single
training step. We present Cooper in Figure 2 and Algorithm 1.

**Stage 1: Policy model optimization.** Following the GRPO (Shao et al., 2024) paradigm, our
policy model optimization proceeds as follows. For each training sample *q*, we sample a set of
responses *{o* 1 *, o* 2 *, . . ., o* *n* *}* using the policy *π* *θ* . The reward model then evaluates each rollout,
producing scores *{r* 1 *, r* 2 *, . . ., r* *n* *}* . We normalize these rewards across the group to compute
advantage estimates *{A* 1 *, A* 2 *, . . ., A* *n* *}*, which are subsequently used to update the policy via
policy gradient. To regularize exploration and ensure training stability, a KL divergence penalty
is incorporated during reinforcement learning.

However, unlike previous RL methods based on reward models, we incorporate a reference answer,
denoted as *a*, into the scoring process of the reward model. Consequently, the reward *r* can be
computed as:
*r* *i* = *R* *φ* ( *q, a, o* *i* ) (3)

The computation of the remaining variables and the optimization of the policy model follow the
same methodology as proposed in Shao et al. (2024).

5


-----

Pre p rint

**Al** **g** **orithm 1 Co-O** **p** **timizin** **g** **Polic** **y** **and Reward Models (Coo** **p** **er)**

**Input:** initial policy model *π* *θ* init, reward models *R* *φ* init, training data D; hyperparameters *ε, β*
**Output:** *π* *θ* *[∗]* **[and]** *[ R]* *φ* *[∗]*
policy model *π* *θ* *←* *π* *θ* init reward model *R* *φ* *←* *R* *φ* init
**for** iteration *i* = 1 to *I* **do**

reference model *π* ref *←* *π* *θ*
Sample a batch D *b* *⊂* D
**for** each question and answer ( *q, a* ) *∈* D *b* **do**

Generate *G* rollouts: *{o* 1 *, . . ., o* *G* *}* using *π* *θ*
Compute rewards by running *r* *φ* on each output *o* *i* : *r* *j* = *σ* ( *R* *φ* ( *q, o* *j* *, a* ))
Compute *A* [ˆ] *i,t* for the *t* -th token of *o* *i* through group relative advantage estimation:

ˆ *[,]* *[ r]* *[G]* *[}]* [)]
*A* *i,t* = *[r]* *[i]* *[ −]* [mean ] [(] *[{]* *[r]* [1] *[,]* *[ r]* [2] *[,]* *[ · · ·]* *.*

std ( *{r* 1 *, r* 2 *, · · ·, r* *G* *}* )

Select a positive response *o* pos *∈{o* 1 *, . . ., o* *G* *}* using rule-based reward function:

*o* pos = *o ∼{o* *i* *|* Rule( *a, o* *i* ) = 1 *}*

Generate a negative response *o* neg from an assistant LLM:

*o* neg = *M* ( *p, o* pos )

**end for**
Update the policy model *π* *θ* by maximizing the GRPO objective: *▷* **Stage 1: Policy model optimization**


1
J GRPO ( *θ* ) = E *q ∼* *P* ( *Q* ) *, {o* *i* *}* *[G]* *i* =1 *[∼]* *[π]* *[θ]* old [(] *[O][ |][ q]* [)]
� � *G*


*G*
�

*i* =1


1

*|o* *i* *|*


*|o* *i* *|*
�


*i* =1 *t* =1

*π* *θ* ( *o* *i* *,* *t* *|* *q,* *o* *i* *,* *<t* ) ˆ *π* *θ* ( *o* *i* *,* *t* *|* *q,* *o* *i* *,* *<t* )

min *A* *i,t* *,* clip
� � *π* *θ* old ( *o* *i,t* *| q, o* *i,<t* ) � *π* *θ* old ( *o* *i,t* *| q, o* *i,<t*


*π* *θ* ( *o* *i* *,* *t* *|* *q,* *o* *i* *,* *<t* ) ˆ *π* *θ* ( *o* *i* *,* *t* *|* *q,* *o* *i* *,* *<t* )
*A* *i,t* *,* clip
� *π* *θ* old ( *o* *i,t* *| q, o* *i,<t* ) � *π* *θ* old ( *o* *i,t* *| q, o* *i,<t*


*π* *θ* ( *o* *i* *,* *t* *|* *q,* *o* *i* *,* *<t* ) ˆ

*A* *i,t*
*π* *θ* old ( *o* *i,t* *| q, o* *i,<t* ) *[, ε]* �


�


*−* *β* D KL [ *π* *θ* *∥* *π* ref ]
�


Update the reward model *R* *φ* by minimizing the loss: *▷* **Stage 2: Reward model optimization**

L RM = *−* E [ *{q,a,o* pos *,o* neg *}∼* D *b* ] *logσ* ( *R* *φ* ( *q, a, o* pos ) *−* *R* *φ* ( *q, a, o* neg ))

**end for**
**return** *π* *θ* *[∗]* [and] *[ R]* *φ* *[∗]*

**Stage 2: Reward model optimization.** Cooper introduces a new step into the existing GRPO
pipeline: the optimization of the reward model. This is designed to ensure that the RM’s parameters
are continuously updated during the RL process, thereby reducing the risk of the policy model
exploiting specific vulnerabilities in the RM (i.e., reward hacking) and ultimately maintaining the
stability of training.

Specifically, following the approach of Stiennon et al. (2020), we optimize the reward model using
contrastive learning. Given a question *q*, a reference answer *a*, and a pair of candidate responses *o* pos
and *o* neg to *q*, where *o* pos is a correct response and *o* neg is incorrect, the objective is to maximize the
score difference assigned by the RM between *o* pos and *o* neg . The optimization could be represented

as:

L RM = *−* E [ *{q,a,o* pos *,o* neg *}∼D* ] *logσ* ( *R* *φ* ( *q, a, o* pos ) *−* *R* *φ* ( *q, a, o* neg )) (4)

To obtain a set of positive and negative samples *o* pos *, o* neg for a given question *q* and its reference
answer *a*, we perform the following operations respectively:

**Positive sample selection.** Based on our preliminary observations of rule-based rewards, we found
that such rules tend to exhibit high precision but low recall in identifying correct responses. In other
words, responses classified as correct by the rule are highly likely to be truly correct. Therefore, for
a single rollout that yields a set of responses *{o* 1 *, o* 2 *, ..., o* *n* *}*, we randomly select one response that
is judged as correct by the rule and treat it as a positive sample:

*o* pos = *o ∼{o* *i* *|* Rule( *a, o* *i* ) = 1 *}* (5)

6


-----

Pre p rint

**Negative sample generation.** We propose a simple method for generating negative samples.
Specifically, we utilize an assistant LLM *M* to transform a correct reasoning process into one
that ultimately yields an incorrect answer, guided by a carefully designed prompt *p* (shown in
Appendix C.2). To ensure the generated response is indeed incorrect, we incorporate a verification
mechanism. Leveraging the high precision of a rule-based reward system, we pass the generated
reasoning process through the rule-reward to verify its correctness. If it is not identified as incorrect
by the rule-reward, the process is repeated until a valid negative sample is obtained:

*o* neg = *M* ( *p, o* pos ) (6)

We would like to mention that if no valid pair is constructed during our data construction process,
we add a loss mask to skip the optimization for that sample.
#### 4 E XPERIMENTS

4.1 P RELIMINARY E XPERIMENT

To validate our hypothesis that rule-based verifiers exhibit high precision despite low recall, we
analyzed the verification patterns of Math-Verify (Gandenberger & Kydl´ıˇcek, 2024) and Qwen34B (Yang et al., 2025) on VerifyBench (Yan et al., 2025). Table 1 reveals clear asymmetry: MathVerify achieves 96% precision (345/360) when identifying correct responses but only 63% recall
(345/549), while Qwen3-4B shows balanced performance with 90% precision and 99% recall. This
reflects Math-Verify’s conservative parsing, which only accepts responses with clearly extractable
answers in expected formats, rejecting many correct solutions with non-standard presentations.

This finding directly motivates Cooper’s de
**VerifyBench** **Math-Verify** **Qwen3-4B**

sign. The near-perfect precision of rulebased verification when it succeeds provides **(Math Reasoning)** Pred = 1 Pred = 0 Pred = 1 Pred = 0
highly reliable positive signals for training. Label = 1 345 204 543 6
By using Math-Verify to select positive ex- Label = 0 15 534 58 491
amples for reward model updates, we lever
Table 1: Confusion matrices for rule-based (Math
age its precision while avoiding its recall

Verify) and model-based (Qwen3-4B) verifiers on

limitations. Meanwhile, the reward model

VerifyBench.

handles the broader distribution of responses
during policy optimization. This complementary approach, combining rule-based precision for
reward updates with model-based flexibility for policy scoring, forms the foundation of our cooptimization framework.


**VerifyBench** **Math-Verify** **Qwen3-4B**


**(Math Reasoning)** Pred = 1 Pred = 0 Pred = 1 Pred = 0


Label = 1 345 204 543 6

Label = 0 15 534 58 491

Table 1: Confusion matrices for rule-based (MathVerify) and model-based (Qwen3-4B) verifiers on
VerifyBench.


4.2 E XPERIMENTS FOR V ERIFY RM

We trained VerifyRM following the methodology in Section 3.1, using Qwen2.5-Math1.5B-Instruct (Yang et al., 2024b) as the base
model. Training was conducted for 3 epochs
with a learning rate of 2e-5 and batch size of
128. To ensure fair evaluation on VerifyBench,
we excluded all overlapping queries from our
training data.

Table 2 compares VerifyRM against three categories of baselines: rule-based functions,
vanilla reward models, and reference-based
verifiers. The results demonstrate clear performance stratification. Vanilla reward models without reference answers perform poorly
(47-52% accuracy), confirming that standard
preference-based rewards lack the precision
needed for mathematical verification. Rulebased Math-Verify achieves 79.93%, validating
its utility but also highlighting its brittleness.


xVerify-0.5B-I 70.68
xVerify-3B-Ia 82.23
xVerify-8B-I 84.38
xVerify-9B-C 84.23
**VerifyRM-1.5B (ours)** **89.42**

Table 2: Reward model accuracy on VerifyBench.

7


**Method** **VerifyBench-Math**



Math-Verify 79.93



FsfairX-LLaMA3-RM-v0.1 49.53
Skywork-Reward-V2-Llama-3.2-1B 47.23
Skywork-Reward-V2-Llama-3.2-3B 52.63
Skywork-Reward-V2-Llama-3.1-8B 52.06
Llama-3.1-Tulu-3-8B-RM 51.56



-----

Pre p rint

Table 3: RL performance with different reward types across mathematical benchmarks.

Among model-based verifiers, performance scales with model size, yet our VerifyRM-1.5B achieves
the highest accuracy at 89.42%, outperforming even the 9B parameter xVerify model. This superior
performance with fewer parameters validates two key design choices: incorporating reference
answers provides crucial context for verification, and our hybrid annotation strategy creates higherquality training data than existing approaches. The strong performance of VerifyRM establishes the
reliable reward signal necessary for Cooper’s co-optimization framework.

4.3 E XPERIMENTS FOR C OOPER

**Setup.** We implemented the Cooper algorithm based on the veRL (Sheng et al., 2024) framework.
The experiments were conducted on the DeepMath (He et al., 2025) dataset. Due to resource
constraints, we randomly sampled 10K examples from the original dataset for training. All
experiments used Qwen2.5-1.5B-Instruct (Qwen et al., 2025) and Llama-3.2-1B-Instruct (Grattafiori
et al., 2024) as the initial model. To avoid introducing additional knowledge, the assistant model in
Cooper was also instantiated with the same model. In the GRPO algorithm, we set the global batch
size to 512, the maximum prompt length to 1024, and the maximum response length to 3072. The
learning rate was set to 1e-6, and the KL penalty coefficient was set to 0.001. For each prompt, we
generated 16 rollouts during RL training. The models are trained with 10 epochs.

**Evaluation.** We evaluate the model on five mathematical reasoning benchmarks: GSM8K (Cobbe
et al., 2021), SVAMP (Patel et al., 2021), MATH500 (Lightman et al., 2023), OlympiadBenchEN (OB-EN) (He et al., 2024), and Math Odyssey (Fang et al., 2024). Among them, GSM8K,
MATH500, and SVAMP represent elementary-level mathematical problems, while OB-EN and
Math Odyssey are competition-level tasks. During RL training, we periodically assess model
performance. For all evaluations, we use a temperature of 0.7 and top-p of 0.95, generating 8
samples per problem and computing the average accuracy to mitigate evaluation variance.

**Baselines.** Since Cooper integrates the advantages of both rule-based reward functions and reward
models, our baselines include: (1) a model using Math-Verify as the reward function, and (2) a
model using VerifyRM-1.5B as the reward model without updating its parameters during training.

4.3.1 M AIN R ESULTS

**Cooper achieves superior performance across diverse benchmarks.** Table 3 demonstrates
Cooper’s effectiveness: on Qwen2.5-1.5B-Instruct, Cooper achieves 58.02% average accuracy,
outperforming rule-based rewards (57.48%) while dramatically surpassing the collapsed static
reward model (38.91%). The improvements are consistent across both base models and particularly pronounced on challenging tasks like Math Odyssey (44.17% vs 42.83%), suggesting cooptimization becomes increasingly valuable for complex reasoning.

**Static reward models suffer catastrophic failure from reward hacking.** The most striking
finding is the severe degradation of static reward models: performance drops from 54.93% to
38.91% on Qwen2.5-1.5B-Instruct, a 16% relative decrease. This collapse, consistent across both
architectures, empirically validates that reward hacking is a fundamental failure mode in RL for
LLMs. Cooper not only prevents this catastrophic failure but achieves the highest performance,

8


-----

Pre p rint

60

58

56

54

52



|Rule Rew Coo|ard Model per|Reward H|acking!|
|---|---|---|---|
|||||


0 50 100 150
Training Steps

(b) Train set reward during RL training.



1.0

0.5


50


|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||Bas|eline|||
||Rul|e|Reward|Hacking!|
|Rew Coo|Rew Coo|ard Model per|||
|0 50 100 150 Training Steps|||||


(a) Test set accuracy (%) across MATH500.


Figure 3: Training dynamics across RL training steps of Cooper.

confirming that synchronized co-optimization successfully addresses the exploitation vulnerability
inherent in fixed reward functions.
#### 5 A NALYSIS

To understand the mechanisms underlying Cooper’s effectiveness, we conduct a comprehensive
analysis examining three key aspects: the training dynamics that reveal how Cooper prevents reward
hacking, the stability of the co-optimized reward model, and the impact of reward signal granularity
on performance.

5.1 T RAINING D YNAMIC

To understand how Cooper prevents reward hacking, we examine the training dynamics in Figures 3a
and 3b. The test accuracy on MATH500 (Figure 3a) reveals a critical divergence: while rulebased rewards and Cooper show steady improvement, the static reward model catastrophically
collapses around step 120, dropping from 58% to below 52%. This collapse coincides with
reward hacking visible in Figuree 3b, where the static model’s training rewards unnaturally spike
to near 1.0, indicating the policy has discovered exploits in the fixed reward function. In contrast,
Cooper maintains realistic reward levels around 0.5 throughout training while achieving the highest
final accuracy (58.05%). This demonstrates that synchronized updates successfully prevent the
policy from gaming the reward signal, as the policy evolves, the reward model adapts its decision
boundaries, closing exploitation opportunities that would otherwise accumulate in a static system.

5.2 S TABILITY OF R EWARD M ODEL THROUGHOUT T RAINING


A potential concern with Cooper is whether
continuous updates might destabilize the reward
model. Figure 4 tracks VerifyRM’s accuracy
on VerifyBench throughout training, showing
remarkable stability around 89.7% with fluctuations below 0.5%. This stability emerges from
our careful update mechanism: by using highprecision rule-based signals for positive examples
and systematic perturbations for negatives, each
update reinforces correct decision boundaries
rather than introducing noise. The consistent
performance confirms that co-optimization can
be implemented without the instability typically

9


90.0

89.5

89.0


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
||||ard Model er|
|||Rew Coop|ard Model er|
|||||


0 50 100 150 200
Training Steps


Figure 4: Accuracy of RM across training steps.


-----

Pre p rint

associated with moving target problems, validating that our contrastive learning approach maintains
verification quality while adapting to new policy distributions.

5.3 A BLATION ON C ONTINUOUS VERSUS D ISCRETE R EWARDS


To ensure Cooper’s improvements stem from
the co-optimization framework rather than differences in reward signal granularity, we con- / 74.10 54.63 54.93
duct an ablation study comparing continuous Rule 76.44 57.55 57.48
and discrete reward implementations. As Cooper **77.02** **58.05** **58.02**
shown in Table 4, when we binarize Cooper’s Cooper (discrete) 76.53 57.15 57.86
reward outputs to match the rule-based base- Table 4: Ablation on continuous vs. discrete
line’s format (1 for scores *>* 0.5, 0 otherwise), rewards.
Cooper still achieves 57.86% average accuracy,
maintaining most of its advantage over both the baseline (54.93%) and rule-based rewards (57.48%).
This result provides two key insights: first, the primary advantage of Cooper stems from preventing
reward hacking through dynamic updates rather than from reward granularity; second, continuous
rewards do provide additional benefits by enabling more nuanced credit assignment during policy
optimization. These findings confirm that Cooper’s co-optimization framework addresses a
fundamental limitation in current RL approaches for LLMs, remaining effective across different
reward signal designs.



Table 4: Ablation on continuous vs. discrete

rewards.

#### 6 D ISCUSSION

**Implications for reinforcement learning in LLMs.** Cooper reveals that reward hacking is not a
hyperparameter issue but a fundamental problem with static reward models. The 16% performance
collapse with fixed rewards demonstrates that treating reward models as dynamic components is
essential for stable RL. This principle extends beyond mathematical reasoning, any domain with
partial verification capabilities could benefit from synchronized optimization. By shifting from an
adversarial dynamic to a co-evolutionary framework, Cooper suggests that much of RL’s perceived
instability may stem from reward exploitation rather than optimization challenges.

**The critical role of high-precision signals.** Cooper’s success relies on an underappreciated
property of rule-based verifiers: their asymmetric performance with high precision (96%) but low
recall (63%). This pattern, common in structured domains, becomes a strength when used to
select positive training examples. By combining symbolic precision with neural flexibility, Cooper
demonstrates that hybrid approaches may be essential for reliable AI systems. The key insight is
transforming verification limitations into training advantages through careful system design.

**Limitations and future directions.** Three main limitations constrain Cooper’s current implementation: (1) dependency on domain-specific verification tools limits generalization to tasks
without clear correctness criteria; (2) computational overhead from dual optimization may affect
scalability; (3) reliance on an assistant LLM for negative sample generation introduces external
dependencies. Future work should explore self-supervised contrastive example generation, extend
Cooper to process-based rewards for denser supervision, and develop theoretical frameworks for coevolutionary stability. Despite these limitations, Cooper establishes synchronized optimization as a
promising direction for addressing fundamental challenges in reinforcement learning for LLMs.
#### 7 C ONCLUSION

In this paper, we introduce **Cooper**, a reinforcement learning (RL) framework that co-trains the
policy model and the reward model. Cooper combines the high precision of rule-based rewards with
the robustness of model-based rewards, effectively mitigating the issue of reward hacking that often
arises when using a static reward model in RL. Compared to using either type of reward in isolation,
Cooper achieves significantly better performance. In addition, we propose a reference-answer-based
reward model named VerifyRM. By leveraging a hybrid annotation method that does not rely on
manual labeling, VerifyRM outperforms existing models of the same scale on the VerifyBench

10


-----

Pre p rint

benchmark. Our results demonstrate that dynamically updating the reward model during RL training
is effective in countering reward hacking. Nevertheless, our work has room for improvement. One
important direction is exploring how to update the reward model without depending on external
LLMs. In future work, we aim to further pursue this line of research to develop more accurate and
robust RL training paradigms.
#### R EFERENCES

Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, et al. Atla selene
mini: A general purpose evaluation model. *CoRR*, abs/2501.17195, 2025. doi: 10.48550/ARXIV.
[2501.17195. URL https://doi.org/10.48550/arXiv.2501.17195.](https://doi.org/10.48550/arXiv.2501.17195)

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, et al. Training a helpful
and harmless assistant with reinforcement learning from human feedback. *arXiv preprint*
*arXiv:2204.05862*, 2022.

Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, et al. Internlm2 technical report.
*CoRR* [, abs/2403.17297, 2024. doi: 10.48550/ARXIV.2403.17297. URL https://doi.org/](https://doi.org/10.48550/arXiv.2403.17297)
[10.48550/arXiv.2403.17297.](https://doi.org/10.48550/arXiv.2403.17297)

Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, et al. xverify: Efficient answer
verifier for reasoning model evaluations. *arXiv preprint arXiv:2504.10481*, 2025a.

Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, et al. An empirical study on
eliciting and improving r1-like reasoning models. *CoRR*, abs/2503.04548, March 2025b. URL
[https://doi.org/10.48550/arXiv.2503.04548.](https://doi.org/10.48550/arXiv.2503.04548)

Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
(eds.), *Advances in Neural Information Processing Systems 30: Annual Conference on Neural*
*Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pp.
[4299–4307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)
[d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*,
2021.

Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking
mathematical problem-solving skills in large language models using odyssey math data. *arXiv*
*preprint arXiv:2406.18321*, 2024.

Greg Gandenberger and Hynek Kydl´ıˇcek. Math-verify: A robust mathematical expression evaluation
[system designed for assessing large language model outputs in mathematical tasks. https:](https://github.com/huggingface/Math-Verify)
[//github.com/huggingface/Math-Verify, 2024. GitHub repository.](https://github.com/huggingface/Math-Verify)

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In
*International Conference on Machine Learning*, pp. 10835–10866. PMLR, 2023.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, et al. The language model
[evaluation harness, 07 2024. URL https://zenodo.org/records/12608602.](https://zenodo.org/records/12608602)

Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, et al. Chatglm: A family of large
language models from glm-130b to glm-4 all tools, 2024.

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, et al.
[The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.](https://arxiv.org/abs/2407.21783)

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, et al. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning. *arXiv preprint*
*arXiv:2501.12948*, 2025a.

11


-----

Pre p rint

Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, et al. Reward reasoning model, 2025b.
[URL https://arxiv.org/abs/2505.14674.](https://arxiv.org/abs/2505.14674)

Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu,
et al. Teaching large language models to reason with reinforcement learning. *arXiv preprint*
*arXiv:2403.04642*, 2024.

Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, et al. Olympiadbench:
A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific
problems. *arXiv preprint arXiv:2402.14008*, 2024.

Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian
Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi,
and Dong Yu. Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable
mathematical dataset for advancing reasoning, 2025. [URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.11456)
[2504.11456.](https://arxiv.org/abs/2504.11456)

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. *NeurIPS*,
2021.

Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, et al. Think-rm: Enabling
long-horizon reasoning in generative reward models, 2025. [URL https://arxiv.org/](https://arxiv.org/abs/2505.16265)
[abs/2505.16265.](https://arxiv.org/abs/2505.16265)

Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, et al. Qwen2. 5-coder technical
report. *arXiv preprint arXiv:2409.12186*, 2024.

Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, et al. Openai o1
system card. *arXiv preprint arXiv:2412.16720*, 2024.

Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H¨ullermeier. A survey of reinforcement
learning from human feedback. *CoRR*, abs/2312.14925, 2023. doi: 10.48550/ARXIV.2312.
[14925. URL https://doi.org/10.48550/arXiv.2312.14925.](https://doi.org/10.48550/arXiv.2312.14925)

Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal,
Mubarak Shah, et al. Llm post-training: A deep dive into reasoning large language models. *arXiv*
*preprint arXiv:2502.21321*, 2025.

Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, et al. T¨ulu 3:
Pushing frontiers in open language model post-training. *CoRR*, abs/2411.15124, 2024a. doi: 10.
[48550/ARXIV.2411.15124. URL https://doi.org/10.48550/arXiv.2411.15124.](https://doi.org/10.48550/arXiv.2411.15124)

Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, et al.
Rewardbench: Evaluating reward models for language modeling, 2024b. [URL https://](https://arxiv.org/abs/2403.13787)
[arxiv.org/abs/2403.13787.](https://arxiv.org/abs/2403.13787)

Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, et al.
Solving quantitative reasoning problems with language models. *Advances in neural information*
*processing systems*, 35:3843–3857, 2022.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, et al. Making language models better
reasoners with step-aware verifier. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki
(eds.), *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*
*(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pp. 5315–5333.
Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.291. URL
[https://doi.org/10.18653/v1/2023.acl-long.291.](https://doi.org/10.18653/v1/2023.acl-long.291)

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, et al. Let’s verify
step by step, 2023.

Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, et al. Skywork-reward: Bag of tricks
for reward modeling in llms. *CoRR*, abs/2410.18451, 2024a. doi: 10.48550/ARXIV.2410.18451.
[URL https://doi.org/10.48550/arXiv.2410.18451.](https://doi.org/10.48550/arXiv.2410.18451)

12


-----

Pre p rint

Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, et al. Are your llms capable of
stable reasoning? *arXiv preprint arXiv:2412.13147*, 2024b.

Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, et al. Inference-time scaling for
generalist reward modeling. *CoRR*, abs/2504.02495, 2025. doi: 10.48550/ARXIV.2504.02495.
[URL https://doi.org/10.48550/arXiv.2504.02495.](https://doi.org/10.48550/arXiv.2504.02495)

OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, and othrts. Gpt-4o system
[card, 2024. URL https://arxiv.org/abs/2410.21276.](https://arxiv.org/abs/2410.21276)

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, et al. Training language
models to follow instructions with human feedback. *Advances in neural information processing*
*systems*, 35:27730–27744, 2022.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple
math word problems? In *Proceedings of the 2021 Conference of the North American Chapter*
*of the Association for Computational Linguistics: Human Language Technologies*, pp. 2080–
2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
[naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.](https://aclanthology.org/2021.naacl-main.168)

Qwen, :, An Yang, Baosong Yang, Beichen Zhang, et al. Qwen2.5 technical report, 2025. URL

[https://arxiv.org/abs/2412.15115.](https://arxiv.org/abs/2412.15115)

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine
(eds.), *Advances in Neural Information Processing Systems 36: Annual Conference on Neural*
*Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -*
*16, 2023* [, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/](http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)
[a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html.](http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, et al. Code llama: Open
foundation models for code. *arXiv preprint arXiv:2308.12950*, 2023.

Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, et al. Rewarding
progress: Scaling automated process verifiers for LLM reasoning. In *The Thirteenth International*
*Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025* . OpenRe[view.net, 2025. URL https://openreview.net/forum?id=A6Y7AqlzLW.](https://openreview.net/forum?id=A6Y7AqlzLW)

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. Deepseekmath: Pushing
the limits of mathematical reasoning in open language models. *arXiv preprint arXiv:2402.03300*,
2024.

Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, et al. Hybridflow: A flexible
and efficient rlhf framework. *arXiv preprint arXiv: 2409.19256*, 2024.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, et al. Learning to summarize
with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin
(eds.), *Advances in Neural Information Processing Systems*, volume 33, pp. 3008–3021. Curran
Associates, Inc., 2020. [URL https://proceedings.neurips.cc/paper_files/](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)
[paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)

Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, et al. Gemma
[2: Improving open language models at a practical size, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2408.00118)
[abs/2408.00118.](https://arxiv.org/abs/2408.00118)

Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, et al. Kimi k1.5: Scaling
[reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599.](https://arxiv.org/abs/2501.12599)

Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding, et al. Candle: iterative
conceptualization and instantiation distillation from large language models for commonsense
reasoning. *arXiv preprint arXiv:2401.07286*, 2024.

13


-----

Pre p rint

Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, et al. Towards large
reasoning models: A survey of reinforced reasoning with large language models. *arXiv preprint*
*arXiv:2501.09686*, 2025.

Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, et al. Verifybench: Benchmarking
reference-based reward systems for large language models, 2025. [URL https://arxiv.](https://arxiv.org/abs/2505.15801)
[org/abs/2505.15801.](https://arxiv.org/abs/2505.15801)

An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, et al. Qwen2 technical report. *arXiv*
*preprint arXiv:2407.10671*, 2024a.

An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. *arXiv preprint arXiv:2409.12122*,
2024b.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. Qwen3 technical report,
[2025. URL https://arxiv.org/abs/2505.09388.](https://arxiv.org/abs/2505.09388)

Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, et al. Internlm-math: Open
math large language models toward verifiable reasoning. *arXiv preprint arXiv:2402.06332*, 2024.

Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning
in mathematical reasoning. In Kevin Duh, Helena G´omez-Adorno, and Steven Bethard (eds.),
*Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico,*
*June 16-21, 2024*, pp. 858–875. Association for Computational Linguistics, 2024. doi: 10.
[18653/V1/2024.FINDINGS-NAACL.55. URL https://doi.org/10.18653/v1/2024.](https://doi.org/10.18653/v1/2024.findings-naacl.55)
[findings-naacl.55.](https://doi.org/10.18653/v1/2024.findings-naacl.55)

Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, et al. Does reinforcement learning
really incentivize reasoning capacity in llms beyond the base model? *CoRR*, abs/2504.13837,
[2025. doi: 10.48550/ARXIV.2504.13837. URL https://doi.org/10.48550/arXiv.](https://doi.org/10.48550/arXiv.2504.13837)
[2504.13837.](https://doi.org/10.48550/arXiv.2504.13837)

Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, et al. Internlm-xcomposer2.5reward: A simple yet effective multi-modal reward model. In Wanxiang Che, Joyce Nabende,
Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), *Findings of the Association for*
*Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025*, pp. 6547–
[6563. Association for Computational Linguistics, 2025. URL https://aclanthology.](https://aclanthology.org/2025.findings-acl.340/)
[org/2025.findings-acl.340/.](https://aclanthology.org/2025.findings-acl.340/)

Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, et al. R1-reward: Training multimodal
reward model through stable reinforcement learning, 2025. [URL https://arxiv.org/](https://arxiv.org/abs/2505.02835)
[abs/2505.02835.](https://arxiv.org/abs/2505.02835)

Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, et al. A comprehensive survey of
[reward models: Taxonomy, applications, challenges, and future, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.12328)
[org/abs/2504.12328.](https://arxiv.org/abs/2504.12328)

Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, et al. Deepseek-coder-v2: Breaking
the barrier of closed-source models in code intelligence. *arXiv preprint arXiv:2406.11931*, 2024.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, et al. Fine-tuning
[language models from human preferences, 2020. URL https://arxiv.org/abs/1909.](https://arxiv.org/abs/1909.08593)
[08593.](https://arxiv.org/abs/1909.08593)

14


-----

Pre p rint
#### A D ATA S OURCE

To construct a high-quality dataset for training VerifyRM, we carefully curated mathematical
problems from diverse sources that represent different difficulty levels and problem types. Table 5
presents the seven datasets used in our data collection pipeline, totaling 5,917 unique mathematical
problems.

**Dataset** **License** **Sample Count**

MATH(Hendrycks et al., 2021) MIT 2000
OlympiadBench(He et al., 2024) Apache-2.0 1177
AIME 2024 MIT 120

AIME 2025 MIT 120

AMC23 / 160
LiveMathBench(Liu et al., 2024b) CC-BY-4.0 340
GSM8K(Cobbe et al., 2021) MIT 2000

Table 5: Number of samples used in constructing problem-reference-completion triples.

Our dataset selection strategy ensures comprehensive coverage across mathematical domains.
GSM8K and MATH provide elementary to undergraduate-level problems, while OlympiadBench,
AIME and AMC23 contribute competition-level challenges. LiveMathBench adds recently created
problems to avoid data contamination issues. Each dataset includes both the problem statement and
a verified reference answer, which serves as the ground truth for our hybrid annotation process. All
data usage strictly complies with the licensing terms specified by the original sources.
#### B LLM U SAGE

To generate diverse model completions for our dataset, we employed 11 different large language
models spanning various architectures and parameter scales. This diversity is crucial for training a
robust reward model that can generalize across different reasoning styles and output formats. Table 6
details the models used and their contribution to our final dataset of 65,087 problem-completion
pairs.

**Series** **Model** **Sample Count**

ChatGLM ChatGLM3-6B(GLM et al., 2024) 5917

Gemma 2 Gemma-2-2B-it(Team et al., 2024) 5917
Gemma-2-9B-it(Team et al., 2024) 5917

GLM-4 GLM-4-9B-Chat(GLM et al., 2024) 5917

InternLM 2.5 InternLM2.5-7B-Chat(Cai et al., 2024) 5917

Qwen2 Qwen2-1.5B-Instruct(Yang et al., 2024a) 5917
Qwen2-7B-Instruct(Yang et al., 2024a) 5917

LLaMA 3.1 LLaMA-3.1-8B-Instruct(Grattafiori et al., 2024) 5917

Qwen2.5 Qwen2.5-7B-Instruct(Qwen et al., 2025) 5917
Qwen2.5-14B-Instruct(Qwen et al., 2025) 5917

Qwen2.5-Math Qwen2.5-Math-1.5B-Instruct(Yang et al., 2024b) 5917

Table 6: Number of samples generated by LLMs. Each generated one completion per problem.

Each model generated responses using consistent sampling parameters (temperature=0.7,
top p=0.95) to balance diversity with coherence. The model selection includes both general-purpose
instruction-tuned models (e.g., LLaMA-3.1, Qwen2.5) and specialized mathematical reasoning
models (e.g., Qwen2.5-Math). This mix ensures our reward model encounters both typical and
specialized reasoning patterns during training, improving its robustness in practical applications.

15


-----

Pre p rint





Figure 5: Prompt template for LLM-as-a-judge used in hybrid annotation.
#### C P ROMPT T EMPLATES

Our system employs three carefully designed prompt templates for different components of the
pipeline. Each template was iteratively refined to maximize performance while maintaining clarity
and consistency.

C.1 P ROMPT T EMPLATE FOR LLM- AS - A - JUDGE

For the hybrid annotation process, we utilize Qwen3-4B (Yang et al., 2025) as an LLM judge to
assess completion correctness. Figure 5 shows our prompt template, which explicitly provides the
problem, reference answer, and model completion. The prompt instructs the model to compare
the final answers while being lenient about minor formatting differences, focusing on mathematical
equivalence rather than syntactic matching. This design enables the LLM to handle diverse solution
formats while maintaining accuracy in correctness judgments.

C.2 P ROMPT TEMPLATE FOR GENERATING NEGATIVE RESPONSE

A key innovation in Cooper is the dynamic generation of negative examples for reward model
updates. Figure 6 presents our prompt for transforming correct solutions into plausible but incorrect
ones. The prompt specifically instructs the assistant LLM to maintain the reasoning structure
while introducing errors in calculations or logic. This approach ensures that negative examples
resemble actual model errors rather than random corruptions, improving the reward model’s ability
to distinguish subtle incorrectness patterns during contrastive learning.

C.3 P ROMPT T EMPLATE FOR V ERIFY RM

Figure 7 shows the input format for our reference-based reward model. Unlike traditional reward
models that only consider the query and response, VerifyRM incorporates the reference answer as
additional context. This three-part input structure (problem, reference, completion) enables more

16


-----

Pre p rint





Figure 6: Prompt template for generating negative response.




Figure 7: Prompt template for VerifyRM showing the problem-reference-completion triple format.

accurate verification by providing the expected solution approach and final answer, allowing the
model to perform comparative analysis between the completion and reference.
#### D D ETAILS OF H YBRID A NNOTATION

Our hybrid annotation strategy combines Math-Verify (Gandenberger & Kydl´ıˇcek, 2024) and
Qwen3-4B (Yang et al., 2025) to leverage their complementary strengths. Starting with 65,087
generated completions, we applied both methods independently and selected only samples where
they agreed on the correctness label. Table 7 presents the detailed results.

Qwen3-4B Predicted Correct Qwen3-4B Predicted Incorrect

Math-Verify Predicted Correct **32,119 (Correct)** 466
Math-Verify Predicted Incorrect 5,883 **26,619 (Incorrect)**

Table 7: Hybrid annotation results. Bold entries indicate the 58,738 samples where both methods
agree, which we selected for training VerifyRM.

The high agreement rate (87.2%) validates our approach. The 466 disagreements where Math-Verify
predicts correct but Qwen3-4B predicts incorrect likely represent formatting ambiguities, while the
5,883 opposite cases may include valid solutions with non-standard presentations. By selecting only
consensus samples, we create a high-confidence training set that inherits the precision of rule-based
verification and the flexibility of model-based judgment, trading data quantity for quality to ensure
robust reward model training.

17


-----


