## **X-VFL: A New Vertical Federated Learning Framework with Cross** **Completion and Decision Subspace Alignment**

### Qinghua Yao [∗] Singapore Management University & University of Pennsylvania

### Xiangrui Xu [∗] Singapore Management University & Beijing Jiaotong University

### Zhize Li [†] Singapore Management University August 7, 2025

**Abstract**

Vertical Federated Learning (VFL) enables collaborative learning by integrating disjoint feature subsets from multiple clients/parties. However, VFL typically faces two key challenges: i)
the requirement for perfectly aligned data samples across all clients (missing features are not
allowed); ii) the requirement for joint collaborative inference/prediction involving all clients (it
does not support locally independent inference on a single client). To address these challenges,
we propose X-VFL, a new VFL framework designed to deal with the non-aligned data samples
with (partially) missing features and to support locally independent inference of new data samples for each client. In particular, we design two novel modules in X-VFL: *Cross Completion*
(XCom) and *Decision Subspace Alignment* (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other clients. DS-Align
aligns local features with completed and global features across all clients within the decision
subspace, thus enabling locally independent inference at each client. Moreover, we provide convergence theorems for different algorithms used in training X-VFL, showing an *O* (1 */√T* ) con
vergence rate for SGD-type algorithms and an *O* (1 */T* ) rate for PAGE-type algorithms, where
*T* denotes the number of training update steps. Extensive experiments on real-world datasets
demonstrate that X-VFL significantly outperforms existing methods, e.g., achieving a 15% improvement in accuracy on the image `CIFAR-10` dataset and a 43% improvement on the medical
`MIMIC-III` dataset. These results validate the practical effectiveness and superiority of X-VFL,
particularly in scenarios involving partially missing features and locally independent inference.
## **1 Introduction**

Federated Learning (FL) is a collaboratively learning framework where multiple clients/parties
jointly train a machine learning model under the coordination of a central server without sharing
their raw data (McMahan et al., 2017; Kairouz et al., 2021). Based on the partitioning of sample

∗ Equal contribution. The work of Qinghua Yao and Xiangrui Xu was conducted during their time as Visiting
Research Students in Zhize Li’s group at SMU.

  - Corresponding author ( `zhizeli@smu.edu.sg` ).

1


-----

and feature spaces, FL can be categorized into Horizontal FL (HFL) and Vertical FL (VFL) (Yang
et al., 2019; Kairouz et al., 2021; Liu et al., 2024). In HFL, different clients typically hold different
local datasets that share the same feature space, whereas in VFL, all clients use the same set of
data samples but each holds a different subset of features.

In this paper, we focus on VFL, which is
particularly useful in domains such as finance, **Client A** **Client B**
healthcare, and e-commerce, where participating clients/parties often possess complementary
feature sets (Joshi et al., 2022; Liu et al., 2024). **(Partially) Missing Data**
For a VFL framework with *k* clients, each client
*i* contains a subset of features, denoted as ***X*** *i* =
( ***X*** *i* *[al]* *[,]* ***[ X]*** *i* *[nl]* [). Here,] ***[ X]*** *i* *[al]* represents aligned samples, where all clients have full local feature **Feature Space**
sets for these samples without any missing features. ***X*** *i* *[nl]* denotes non-aligned samples, where Figure 1: Dataset partition in VFL with *k* = 2.
at least one client has (partially) missing features. Fig. 1 illustrates this data partition in VFL with two clients *k* = 2.
While VFL represents a promising framework by integrating disjoint local feature sets from
multiple clients, its practical deployment is often hindered by two major limitations. First, both
training and inference processes in VFL depend on the complete alignment of data samples across
all clients (Castiglia et al., 2022a; Kang et al., 2022), i.e., only use the aligned samples ***X*** *[al]* .
This requirement significantly limits the volume of usable data, thereby affecting both the model
accuracy and the scalability of the model. Second, the inference phase necessitates collaboration
with all clients to complete the inference process, leading to significant communication overhead and
making real-time or locally independent inference impractical (Li et al., 2022b; Ren et al., 2022).
These limitations highlight an urgent need to develop a VFL framework to facilitate independent
inferences and enable the efficient exploitation of non-aligned data samples (i.e., ***X*** *[nl]* ) during both
training and inference processes.
In this paper, we propose X-VFL, a novel VFL framework that exploits the non-aligned data samples with missing features and supports locally independent inference of new data samples for each
client. Particularly, we design two key modules in X-VFL : *Cross Completion* ( XCom ) and *Decision*
*Subspace Alignment* ( DS-Align ). The XCom module is developed to establish cross-complementary
dependencies among disjoint local features contributed by different clients. Leveraging these relationships, local clients can complete their missing features through XCom, thereby effectively
increasing the volume of data available for training and inference. The DS-Align module is designed
to align the features across all clients within the decision subspace to support locally independent
inference for each client, while maintaining performance comparable to collaborative inferences
involving all clients.


**Client A** **Client B**





**Feature Space**


Figure 1: Dataset partition in VFL with *k* = 2.


As illustrated in the attention heatmap
(Fig. 2), the image is partitioned between two
clients (left and right). In the “Single” setting, the left-side client entirely lacks its local
features. Consequently, the model relies solely
on the right-side client’s features resulting in a
relatively low confidence score of 0.728 for the


Input Both Single Single-XCom

Confidence (‘fish’): 0.962 0.728 0.977

Figure 2: Effect of XCom .


2


-----

“fish” class. However, with the integration of
our XCom (“Single- XCom ”), the missing features in the left-side client are completed using the
information from the right-side client through XCom . As a result, “Single- XCom ” significantly improves the confidence score to 0.977. Moreover, it even slightly exceeds the 0.962 confidence score
obtained when using the full joint features (“Both”), showing that XCom may also help denoise the
original data and enhance inference accuracy. These results highlight the effectiveness of XCom in
reconstructing missing features and improving model performance.

The proposed DS-Align module enhances XVFL by aligning local individual features with
the completed and joint features across all
clients within the decision subspace. This alignment enables locally independent inference even
in the presence of missing features. As shown
in Fig. 3, the “Vanilla” setting (Fig. 3a) results

(a) Vanilla (b) DS-Align

in a poorly defined decision boundary, leading
to overlapping regions and misclassifications. In Figure 3: Effect of DS-Align .
contrast, our “ DS-Align ” setting (Fig. 3b) yields
a much clearer decision boundary, ensuring better separation between classes. This confirms the
effectiveness of DS-Align in inference under our X-VFL framework.


(a) Vanilla (b) DS-Align


Figure 3: Effect of DS-Align .

### **1.1 Our Contributions**

We would like to highlight the following contributions:

  - We propose X-VFL, a novel VFL framework designed to handle the non-aligned data samples
with (partially) missing features and to support locally independent inference for new data
samples at each client. More importantly, our X-VFL introduces two key modules: Cross
Completion ( XCom ) and Decision Subspace Alignment ( DS-Align ), which significantly enhance
the ability of VFL to address more complex and practical scenarios.



- To the best of our knowledge, we are the first to introduce a practical setting with *partially*
missing features in VFL, where a client may retain some local features rather than fully
missing all local features for non-aligned data samples. For example, consider a non-aligned
data sample *X* with *m* features evenly partitioned between two clients, each holding *m* 2

features. Previous studies have only considered the fully missing setting, where one client lacks
all *[m]*

2 [of its local features. However, we introduce the notion] *[ missing rate]* [, denoted as] *[ R]* [miss] [,]
where a client may miss *R* miss *·* *[m]* 2 [of its local features. In particular, the missing rate] *[ R]* [miss] [ = 1]

recovers the fully missing case. Given the ubiquity of partially missing features in real-world
datasets, addressing this more general and realistic setting is essential for enchancing the
practical applicability of VFL.



- Moreover, we provide theoretical convergence theorems for the algorithms used in training XVFL, showing an *O* (1 */√T* ) convergence rate for SGD-type algorithms and an *O* (1 */T* ) rate for

PAGE-type algorithms, where *T* denotes the number of training update steps (see Theorems 1
and 2 in Section 3).

- Finally, we conduct extensive experiments on real-world datasets to demonstrate that XVFL significantly outperforms existing VFL methods, e.g., achieving a 15% improvement

3


-----

in accuracy on `CIFAR-10` and a 43% improvement on the medical dataset `MIMIC-III` (see
Section 4). The experiments validate the practical effectiveness and superiority of X-VFL
in VFL, particularly in practical scenarios involving partially missing features and locally
independent inference.
### **1.2 Related Work**

Vertical Federated Learning (VFL) enables distributed learning on vertically partitioned datasets,
where clients hold disjoint feature subsets of the same dataset samples (Liu et al., 2024). A substantial body of work has focused on reducing communication overhead, which is critical for improving
the scalability and practicality of VFL, making it a promising research direction (Castiglia et al.,
2022b; Fu et al., 2022; Huang et al., 2022).
In addition, several studies have explored Split Neural Network (SplitNN)-based VFL architectures to facilitate multi-client collaboration without direct data sharing (Vepakomma et al., 2018;
Thapa et al., 2022). In these architectures, each client trains a segment of a neural network up to
a predefined cut layer. The outputs at the cut layer from each client are transmitted to a central
server, which holds the class labels and completes the forward and backward propagation. The
resulting gradients are sent back to the clients to update their local models. This iterative process
is repeated until convergence. During inference, predictions are collaboratively generated by all
clients.

Research on SplitNN-based VFL mainly focused on enabling locally independent inference and
handling non-aligned data samples (Liu et al., 2024). For instance, Ren et al. (2022) introduced
a knowledge distillation framework, IAVFL, that transfers insights from joint training to local
models, enabling independent inference based solely on local features. Subsequent studies extended
this concept through representation-level distillation to capture inter-client feature correlations and
enhance independent inference capabilities (Huang et al., 2023; Li et al., 2022a; Lin et al., 2020).
However, these distillation-based methods are restricted to aligned samples, thus providing limited
improvement in inference performance.
Another promising direction leverages non-aligned data to improve model performance (Li et al.,
2022b; Xu et al., 2023). Recent efforts have applied self-supervised learning (SSL) techniques to
enhance local model representations using non-aligned data samples (Castiglia et al., 2022a; Xu
et al., 2023). By improving the representational quality of local models, SSL typically contributes
to better global model performance in VFL frameworks. However, these methods still require
collaboration among all clients during the inference, restricting their practical applicability. Besides,
Kang et al. (2022) proposed a semi-supervised learning framework, FedCVT, that estimates the
corresponding embeddings/representations for the missing features and infers the pseudo labels for
the missing labels, thus increasing the amount of data embeddings available for training. Valdeira
et al. (2025) proposed LASER-VFL, a VFL framework that enables training and inference with
subsets of feature blocks by sharing representation models and using average aggregation with task
sampling. However, it only considers the fully missing feature setting and does not support partially
missing features. Moreover, it lacks the ability to complete/reconstruct missing features.
Therefore, designing a new VFL framework that supports independent inferences and enables
the efficient integration and completion of non-aligned data samples with partially missing features
during both training and inference is critical for advancing the practical capabilities of VFL.

4


-----

|CE Loss|Col2|
|---|---|
|DS-Align||
|||




Figure 4: The framework of X-VFL .
## **2 X-VFL Framework**

In this section, we introduce X-VFL, a novel framework that overcomes the limitations of conventional VFL by supporting locally independent inference and effectively handling datasets with
partially missing features. In Section 2.1, we first describe the setup used in our X-VFL, which serves
as the base for supporting both collaborative and independent inference under partially missing
features. Then, we provide an overview of the X-VFL framework by introducing its two core components: Cross Completion ( XCom ) and Decision Subspace Alignment ( DS-Align ). Finally, we present
the inference modes supported by our X-VFL in Section 2.2, which include both collaborative and
independent inference while effectively handling partially missing features.
### **2.1 Framework description**

As illustrated in Fig. 4, our X-VFL is built upon two key modules: Cross Completion ( XCom ) and
Decision Subspace Alignment ( DS-Align ). Each client hosts a local/bottom model *f* *i* and an XCom
module. The local model *f* *i* processes its local features to high-level embeddings/representations,
denoted as ***E*** *i* = *f* *i* ( ***x*** *i* ), while the XCom module reconstructs missing features using embeddings
received from other clients. Once the embeddings, whether derived from existing features, reconstructed features, or a combination of both, are obtained, they are either directly fed into the
top model *h* or averaged before being used as input. The final loss is computed by combining
the decision cross-entropy (CE) loss and the loss from the DS-Align module, followed by standard
backpropagation.
Note that previous VFL frameworks typically aggregates the embeddings via direct concatenation: ***E*** = [ ***E*** 1 *,* ***E*** 2 *, · · ·,* ***E*** *k* ], combining representations from all clients. However, our X-VFL
performs aggregation via averaging ***E*** avg = Avg( ***E*** 1 *,* ***E*** 2 *, · · ·,* ***E*** *k* ), and uses ***E*** avg as the input to the
top model. This design naturally enables *independent inference*, as each local embedding *E* *i* has
the same dimensionality as ***E*** avg, allowing a single client to perform inference locally using its local

5


-----

model *f* *i* with the top model *h* . In contrast, under previous concatenation-based design, the global
embedding ***E*** (input to the top model *h* ) differs in dimensionality from individual embedding ***E*** *i*,
thus requiring all clients to participate for inference (i.e., collaborative inference).

**Cross Completion.** The XCom module is designed to complete/reconstruct missing features
for non-aligned data samples by leveraging information from other clients, thereby increasing the
amount of usable training data and improving the inference performance.
To better illustrate this, we first consider a two-client setting with clients A and B (the general
case with multiple *k* clients is deferred to Appendix D). Let *f* *a* and *f* *b* denote their bottom models,
respectively. Their (partially) missing features can be completed using the embeddings of the other
client as follows:
***X*** ˜ *a* = XCom *a* ( ***E*** *b* ) *,* ***X*** ˜ *b* = XCom *b* ( ***E*** *a* ) *,* (1)

where ***E*** *b* = *f* *b* ( ***X*** *b* ) and ***E*** *a* = *f* *a* ( ***X*** *a* ), XCom *a* and XCom *b* are feature completers hosted by client
***X*** A and client B, respectively. For the fully missing feature scenario, the completed features ˜ *b* are entirely adopted. For the partially missing feature scenario, only the components of ***X*** [˜] *a* and ***X*** [˜] *a*
and ***X*** [˜] *b* for missing features are adopted, while the remaining components should be replaced by
their corresponding existing features in the original positions. The completed features ***X*** [˜] *a* and ***X*** [˜] *b*
are then input into their corresponding bottom models, ***E*** ˜ *a* and ˜ ***E*** *b*, respectively. *f* *a* and *f* *b*, to generate their embeddings
Then, the classification decision loss is formulated as follows:


*L* decision = *ℓ* ( *h* ( ***E*** *a* ) *, y* ) + *ℓ* ( *h* ( ***E*** *b* ) *, y* ) + *ℓ* ( *h* ( ***[E]*** *[a]* [ +] ***[ E]*** *[b]*


) *, y* )
2


+ *ℓ* ( *h* ( ***[E]*** *[a]* [ + ˜] ***[E]*** *[b]*


(2)

+ ***E*** *b*

) *, y* ) *,*
2



[ + ˜] ***[E]*** *[b]* ***E*** ˜ *a* + ***E*** *b*

) *, y* ) + *ℓ* ( *h* (
2 2


where *ℓ* denotes the classification loss, e.g., cross-entropy (CE) loss. *ℓ* ( *h* ( ***E*** *a* ) *, y* ) and *ℓ* ( *h* ( ***E*** *b* ) *, y* )
ensure that the model maintains the ability to make accurate predictions based on features from
a single client, thereby supporting independent inference. The term *ℓ* ( *h* ( ***[E]*** *[a]* [+] 2 ***[E]*** *[b]* ) *, y* ) reinforces the


a single client, thereby supporting independent inference. The term *ℓ* ( *h* ( ***[E]*** *[a]* [+] 2 ***[E]*** *[b]* ) *, y* ) reinforces the

model’s capacity to leverage the combined features, enhancing collaborative inference performance.The terms involving reconstructed embeddings, i.e., *ℓ* ( *h* ( ***[E]*** *[a]* [+] 2 [ ˜] ***E*** *b* ) *, y* ) and *ℓ* ( *h* ( ***E*** ˜ *a* + 2 ***E*** *b* ) *, y* ), ensure that


The terms involving reconstructed embeddings, i.e., *ℓ* ( *h* ( ***[E]*** *[a]* [+] 2 ***E*** *b* ) *, y* ) and *ℓ* ( *h* ( ***E*** *a* + 2 ***E*** *b* ) *, y* ), ensure that

the reconstructed embeddings ( ***E*** [˜] *a* or ***E*** [˜] *b* ) effectively compensate for the missing information, thus
enabling robust inference in the presence of missing features. For aligned data, all terms should
be activated. For non-aligned data, we consider two cases: 1) Client A has full local features while
client B has (partially) missing features. In this case, the loss terms *ℓ* ( *h* ( ***E*** *a* ) *, y* ) and *ℓ* ( *h* ( ***[E]*** *[a]* [+] 2 [ ˜] ***E*** *b* ) *, y* )
are activated; 2) The roles are reversed; client B has full local features, and client A has (partially)missing features. The activated terms are *ℓ* ( *h* ( ***E*** *b* ) *, y* ) and *ℓ* ( *h* ( ***E*** ˜ *a* + 2 ***E*** *b* ) *, y* ).


missing features. The activated terms are *ℓ* ( *h* ( ***E*** *b* ) *, y* ) and *ℓ* ( *h* ( ***E*** *a* + 2 ***E*** *b* ) *, y* ).

By explicitly leveraging reconstructed features, XCom effectively addresses the challenges of
training and inference with (partially) missing features in VFL.

**Decision Subspace Alignment.** DS-Align unifies the alignment between reconstructed and existing features for aligned data, as well as between local individual features and the joint averaged
features aggregated from all clients, within the decision subspace. This reinforces the effectiveness
of XCom and further enhances the performance of independent inference.
To enforce consistency between reconstructed features and their corresponding existing features,
ensuring accurate and reliable feature completion, DS-Align introduces the following alignment loss

6


-----

within the decision subspace:

*L* DSAlign 1 = *ℓ* ( *h* ( ***E*** [˜] *a* ) *, h* ( ***E*** *a* )) + *ℓ* ( *h* ( ***E*** [˜] *b* ) *, h* ( ***E*** *b* )) *,* (3)

where *ℓ* represents a similarity loss function, e.g., mean square error (MSE), which ensures that
inference based on reconstructed embeddings aligns closely with the existing embeddings.
To enhance the performance of independent inference, DS-Align aligns local individual features
and joint averaged features from all clients within the decision subspace. This alignment enables
more accurate independent inference and is formulated as:


*L* DSAlign 2 = *ℓ* ( *h* ( ***E*** *a* ) *, h* ( ***[E]*** *[a]* [ +] ***[ E]*** *[b]*


)) *.* (4)
2



[ +] ***[ E]*** *[b]*

)) + *ℓ* ( *h* ( ***E*** *b* ) *, h* ( ***[E]*** *[a]* [ +] ***[ E]*** *[b]*
2 2


This dual-component approach reinforces both feature completion and independent inference,
thus enhancing the overall performance and scalability of X-VFL .
Finally, by integrating XCom and DS-Align, we formulate the overall loss function for our X-VFL
framework as follows:
*L* = *L* decision + *λ* 1 *L* DSAlign 1 + *λ* 2 *L* DSAlign 2 *.* (5)

Here, *L* decision (see Eq. (2)) denotes the classification decision loss for both aligned and non-aligned
data samples, while *L* DSAlign 1 and *L* DSAlign 2 (see Eq. (3) and Eq. (4)) are the two loss components
introduced by our DS-Align, weighted by the hyperparameters *λ* 1 and *λ* 2, respectively.
### **2.2 Inference modes of X-VFL**

As illustrated in Fig. 5, the X-VFL framework supports multiple inference modes to highlight the
flexibility and scalability of X-VFL :

1. **Independent inference without missing features** (Fig. 5a): Each client independently
generates predictions using only its local features and model, without requiring cross-client
communication during inference.

2. **Independent inference with missing features** (Fig. 5b): Each client leverages its XCom
module to reconstruct missing features and produce predictions independently.

3. **Collaborative inference with/without missing features** (Fig. 5c): All clients contribute
their feature embeddings (obtained either from the original data or from the reconstructed
data via XCom ) to the central server, enabling comprehensive and robust predictions through
collaborative inference.




(a) Independent inference


(b) Ind. infer. with missing features


(c) Collaborative inference


Figure 5: Different inference modes of X-VFL for independent and collaborative inference,
with/without missing features.

7


-----

## **3 Theoretical Results**

In this section, we provide theoretical convergence theorems for our X-VFL framework which is
formulated as the following optimization problem (see Eq. (5) in Section 2.1):

min ***θ*** *[L]* [(] ***[θ]*** [) :=] *[ L]* [decision] [ +] *[ λ]* [1] *[L]* [DSAlign] [1] [ +] *[ λ]* [2] *[L]* [DSAlign] [2] *[,]* (6)

where ***θ*** *∈* R *[d]* denotes the X-VFL model parameters, *λ* 1 *∈* R and *λ* 2 *∈* R are two hyperparameters.
To present the thorems, we first introduce the necessary notation and assumptions.
### **3.1 Notation and assumptions**

Let ∆ 0 := *L* ( ***θ*** [0] ) *−* *L* *[∗]*, where *L* *[∗]* := min ***θ*** *L* ( ***θ*** ). Let *∇L* ( ***θ*** ) denote the gradient of function *L* at
point ***θ***, *∇* [˜] *L* ( ***θ*** ) and *∇* [˜] *b* *L* ( ***θ*** ) denote its stochastic gradient and minibatch stochastic gradients with
size *b* . Let [ *n* ] denote the set *{* 1 *,* 2 *, · · ·, n}* and *∥· ∥* denote the Euclidean norm for a vector. Let
*⟨* ***u*** *,* ***v*** *⟩* denote the inner product of two vectors ***u*** and ***v*** . We use *O* ( *·* ) to hide absolute constants.
In order to prove the convergence results, one usually needs the following standard smoothness
assumption and stochastic gradient variance assumption (see e.g., Ghadimi et al., 2016; Fang et al.,
2018; Li et al., 2021; Li and Richt´arik, 2020).

**Assumption 1 (Average smoothness)** *A function f* : R *[d]* *→* R *is average β-smooth if*

E[ *∥∇* [˜] *L* ( ***θ*** 1 ) *−* *∇* [˜] *L* ( ***θ*** 2 ) *∥* [2] ] *≤* *β* [2] *∥* ***θ*** 1 *−* ***θ*** 2 *∥* [2] *,* *∀* ***θ*** 1 *,* ***θ*** 2 *∈* R *[d]* *.* (7)

**Assumption 2 (Bounded variance)** *The stochastic gradient* *∇* [˜] *L* ( ***θ*** ) *is unbaised and has bounded*

*variance*
E[ *∥∇* [˜] *L* ( ***θ*** ) *−∇L* ( ***θ*** ) *∥* [2] ] *≤* *σ* [2] *,* *∀* ***θ*** *∈* R *[d]* *.* (8)
### **3.2 Convergence results**

***θ*** Now we present the convergence theorems for trainingˆ. The convergence theorems can also indicate the communication complexity and computation X-VFL to find a suitable target parameter
complexity of different algorithms (such as standard stochastic gradient descent (SGD) and the
optimal variance-reduced PAGE (Li et al., 2021)) for training X-VFL .

**Theorem 1 (Convergence for SGD-type algorithms)** *Suppose that Assumptions 1 and 2 hold.*
*For SGD-type algorithms, e.g., update* ***θ*** *[t]* [+1] = ***θ*** *[t]* *−η∇* [˜] *L* ( ***θ*** *[t]* ) *, after T steps, we have* *T* [1] � *Tt* =0 *−* 1 [E] *[∥∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2]


*≤* ( *η−βη* ∆ [2] 0 */* 2) *T* [+] 2 *βη* *η−* [2] *βησ* [2][2] *[ . Choose learning rate][ η][ ≤]* [min] *[{]* *β* [2] *[,]* ~~�~~


2∆ 0
*βσ* [2] *T* *[}][, we obtain]*


*T*


1

*T*


*T* *−* 1
�


*−* 1
� E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* *O*

*t* =0 � *√*


*.* (9)
�


*This means that after T* = *O* ( *ϵ* [1] [4] [ )] *[ steps, SGD-type algorithms can find a suitable parameter]* [ ˆ] ***[θ]*** *[ for]*

X-VFL *such that* E *∥∇L* ( ***θ*** [ˆ] ) *∥* [2] *≤* *ϵ* [2] *, where ϵ denotes the convergence error.*

8


-----

Note that each update step requires one round of communication in X-VFL, so the total number
of communication rounds is *T* = *O* ( *ϵ* [1] [4] [ ). Also, each step incurs a computational cost corresponding]


of communication rounds is *T* = *O* ( *ϵ* [1] [4] [ ). Also, each step incurs a computational cost corresponding]

to a stochastic gradient computation *∇* [˜] *L* ( ***θ*** *[t]* ).
We note that the large number of communication rounds is mainly due to the variance in
stochastic gradients. Thus we also show that the optimal variance-reduced PAGE method (Li
et al., 2021) can largely reduce the number of communication rounds, i.e., from *T* = *O* ( *ϵ* [1] [4] [ ) to]


et al., 2021) can largely reduce the number of communication rounds, i.e., from *T* = *O* ( *ϵ* [1] [4] [ ) to]

*T* = *O* ( *ϵ* [1] [2] [ ) by a factor of] *ϵ* 1 [2] [ . A simplified PAGE update at step] *[ t]* [ is]



[1] 1

*ϵ* [2] [ ) by a factor of] *ϵ* [2] [ . A simplified PAGE update at step] *[ t]* [ is]


***θ*** *[t]* [+1] = ***θ*** *[t]* *−* *η* ***g*** *[t]* *,* where ***g*** *[t]* =


*∇* ˜ *b* *L* ( ***θ*** *t* ) with probability *p*
(10)
� ***g*** *[t][−]* [1] + *∇* [˜] *b* *′* *L* ( ***θ*** *[t]* ) *−* *∇* [˜] *b* *′* *L* ( ***θ*** *[t][−]* [1] ) with probability 1 *−* *p* *[.]*


It uses minibatch SGD update *∇* [˜] *b* *L* ( ***θ*** *[t]* ) with probability *p*, and reuses the previous gradient ***g*** *[t][−]* [1]

with a small adjustment (lower computation cost if *b* *[′]* *≪* *b* ). This update reduces the variance in
stochatic gradients during training and thus significantly decreases the total number of communication rounds, i.e., by a fator of *ϵ* 1 [2] [ .]

**Theorem 2 (Convergence for PAGE-type algorithms)** *Suppose that Assumptions 1 and 2*
*hold. For PAGE-type algorithms, e.g., update as Eq.* (10) *, after T steps, we have* *T* [1] � *Tt* =0 *−* 1 [E] *[∥∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2]



[2∆] *ηT* [0] [+] *pbT* *[σ]* [2]



[2] 1 [2] *[σ]* [2]

*b* *[. Choose learning rate][ η][ ≤]* 2 *β* *[, minibatch sizes][ b]* [ =] *ϵ* [2]


*≤* [2∆] [0]



*[σ]* [2] *[σ]* [2]

*pbT* [+] *b*


*ϵ* *[σ]* [2][2] *[,][ b]* *[′]* *[ ≤]* *√*


*b and probability*


*b* *[′]*
*p* = *b* + *b* *[′]* *[, we obtain]*


*T*


1

*T*


*T* *−* 1
�


*−* 1
� *t* =0 E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* *O* � *T*


*.* (11)
�


*This means that after T* = *O* ( *ϵ* [1] [2] [ )] *[ steps, PAGE-type algorithms can find a suitable parameter]* [ ˆ] ***[θ]*** *[ for]*

X-VFL *such that* E *∥∇L* ( ***θ*** [ˆ] ) *∥* [2] *≤* *ϵ* [2] *, where ϵ denotes the convergence error.*


Similarly, each update step requires one round of communication in X-VFL, so the total number
of communication rounds is *T* = *O* ( *ϵ* [1] [2] [ ). Also, each step incurs a computational cost corresponding]


of communication rounds is *T* = *O* ( *ϵ* [1] [2] [ ). Also, each step incurs a computational cost corresponding]

to a minibatch stochastic gradients computation with expected size *pb* +(1 *−p* ) *b* *[′]* = *b* *b* + *[′]* *b* *b* *[′]* [ +] *b* *[bb]* + *b* *[′]* *[′]* *[ ≤]* [2] *[b]* *[′]* [.]



*[bb]*

*b* + *b* *[′]* *[ ≤]* [2] *[b]* *[′]* [.]

## **4 Experiments**

In this section, we present a comprehensive empirical evaluation of the proposed X-VFL framework using 6 real-world datasets (three image datasets: `CIFAR-10` (Krizhevsky et al., 2009),
`TinyImageNet` (restricted to five classes) (Deng et al., 2009), `UTKFace` (Zhang et al., 2017), and three
tabular datasets: `MIMIC-III` (Johnson et al., 2016), `Bank` (Liang et al., 2016), and `Avazu` (Kang
et al., 2022)). We compare our X-VFL against four methods (Vanilla Standalone, Vanilla VFL,
IAVFL (Ren et al., 2022), FedCVT (Kang et al., 2022)). We conduct experiments under a variety
of practical scenarios, including: varying feature missing rates within each client (see Section 4.1),
varying overlap ratios of aligned data samples (see Section 4.2), and imbalanced training data across
clients (see Section 4.3). More details of the experimental setup are provided in Appendix B. Additional experiments on more datasets and in the multiple clients setting are deferred to Appendix C
and D, respectively.

9


-----

MIMIC-III

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


UTKFace

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


80

60

40

20


Avazu

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


60

40

20


CIFAR-10

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


90

80

70

60

50


65

60

55

50


Vanilla Standalone IAVFL FedCVT X-VFL

Figure 6: Performance comparison under varying feature missing rates in the independent inference
mode on the `CIFAR-10`, `UTKFace`, `MIMIC-III`, and `Avazu` datasets.


Avazu

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


MIMIC-III

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


66

64

62

60


70

60

50


CIFAR-10

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


90

88

86

84


UTKFace

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


80

70

60

50

40


Vanilla VFL IAVFL FedCVT X-VFL

Figure 7: Performance comparison under varying feature missing rates in the collaborative inference
mode on the `CIFAR-10`, `UTKFace`, `MIMIC-III`, and `Avazu` datasets.



70

60

50

40

30

20


CIFAR-10


MIMIC-III


90

80

70

60


UTKFace



Avazu

64

62

60

58

56

54
Vanilla IAVFL FedCVT X-VFL


10


Figure 8: Performance comparison between independent and collaborative inference modes under

The experimental results on `CIFAR-10`, `UTKFace`, `MIMIC-III`, and `Avazu` are summarized in Fig. 6
and Fig. 7, corresponding to the independent and collaborative inference modes, respectively. A
comparison of performance between these two modes at a missing rate *R* miss = 0 *.* 9 is presented in
Fig. 8.
As shown in Fig. 6 and Fig. 7, X-VFL consistently outperforms all methods across all datasets
and feature missing rates, in both independent and collaborative modes. Moreover, the performance
gap further widens as the missing rate increases. For instance, on the `UTKFace` dataset (in Fig. 6)
with a feature missing rate *R* miss = 0 *.* 9, the independent prediction accuracies of the three baseline

10


-----

Independent


CIFAR-10
Collaborative


MIMIC-III
Collaborative

82

79

76

73

70

67


50

45

40

35

30


Independent


75

70

65

60

55

50


25


70

65

60

55

50

45





Overlap 20% Overlap 40% Overlap 80%

(a) `CIFAR-10` dataset


Overlap 20% Overlap 40% Overlap 80%

(b) `MIMIC-III` dataset


Figure 9: Performance comparison under varying overlap ratios on the `CIFAR-10` and `MIMIC-III`
datasets.

methods drop from nearly 90% (at *R* miss = 0, i.e., no missing data) to around 60%, indicating
a performance degradation of 30%. In contrast, X-VFL maintains high accuracy under the same
condition, with a decrease of less than 0.5% compared to its own performance at *R* miss = 0.
Fig. 8 highlights the advantage of X-VFL over the baseline methods by demonstrating its ability
to effectively narrow the accuracy gap between independent and collaborative inference modes.
For instance, on the `UTKFace` dataset, X-VFL exhibits a tiny performance gap of less than 0.2% between independent inference (88.15%) and collaborative inference (88.32%). In contrast, baseline
methods suffer a significant accuracy drop of over 20% when switching from collaborative to independent inference. A similar trend is observed across other datasets, indicating X-VFL ’s robustness
in maintaining consistent performance.
### **4.2 Results under varying overlap ratios of aligned data samples**

To evaluate the effect of varying overlap ratios of aligned data samples, we test these methods
with 20%, 40%, and 80% aligned samples, under a fixed feature missing rate of *R* miss = 0 *.* 5. The
performance results on `CIFAR-10` and `MIMIC-III` are presented in Fig. 9a and Fig. 9b, respectively.
As illustrated, X-VFL consistently outperforms the baseline methods across all overlap ratios
in both independent and collaborative inference modes. For instance, on the `CIFAR-10` dataset
(Fig. 9a), X-VFL achieves over 45% accuracy in the independent inference mode at a 40% overlap
ratio, whereas the three baseline methods all fall below 35%. Similarly, in the collaborative inference
mode, X-VFL attains over 65% accuracy at the same 40% overlap ratio, compared to around 55%
for the baseline methods.
Also, a similar trend is observed on the `MIMIC-III` dataset (Fig. 9b), further demonstrating
X-VFL ’s robustness and superior performance, particularly under low overlap ratios.
### **4.3 Results under imbalanced training data across clients**

To evaluate the impact of data imbalance between clients for these methods, we consider a 20% data
imbalance setting, where client A and client B hold 80% and 20% of the total data, respectively,
including both aligned and non-aligned samples. This setup enables a thorough evaluation of the
impact of imbalanced training data on classification performance. The performance gap between
the data-rich and data-poor clients also offers an interesting point of analysis. Experimental results
for independent inference under a feature missing rate of *R* miss = 0 *.* 5 are presented in Fig. 10.

11


-----

As demonstrated in Fig. 10, the baseline CIFAR-10 MIMIC-III
methods exhibit substantial performance gaps 40 +0.86 80 -8.42
between the two clients, particularly on the 35 -8.16 -9.94 70 -36.06 -35.85 -36.29
`MIMIC-III` dataset, where the data-rich client 30
A consistently outperforms the data-poor client 25 40
B by 36.06%, 35.85%, and 36.29%. These 20 30

|-8.16 -9.94|-11.90|
|---|---|
||-11.90|
|||
|||

gaps highlight the inability of baseline meth- Standalone IAVFL FedCVT X-VFL Standalone IAVFL FedCVT X-VFL
ods to effectively handle data imbalance, lead- Balance Ratio 0.8 (Party A) Balance Ratio 0.2 (Party B)
ing to significantly degraded performance for
the data-poor client B. In contrast, our X-VFL Figure 10: Performance comparison under data

imbalance on the `CIFAR-10` and `MIMIC-III`

substantially reduces this gap to just 8.42%,

datasets.

demonstrating its superior ability to balance
the performance between data-rich and datapoor clients, by completing the missing features through the XCom module. A similar yet more
compelling trend is observed on the `CIFAR-10` dataset. While baseline methods fail to adequately
support the data-poor client B, X-VFL not only narrows the performance gap but also slightly
improves the accuracy of the data-poor client B. This unexpected gain can be attributed to the
design of the XCom module, which leverages the richer data from client A to enhance the feature
representations for the data-poor client B by reconstructing its missing features, thus improving
classification performance despite the severe data imbalance.


CIFAR-10


MIMIC-III



40

35

30

25

20










Figure 10: Performance comparison under data
imbalance on the `CIFAR-10` and `MIMIC-III`

datasets.

## **5 Conclusion**

In this paper, we proposed X-VFL, a novel VFL framework that addresses key challenges of conventional VFLs by effectively handling datasets with partially missing features and enabling locally
independent inference at each client. In particular, X-VFL introduces two key modules: Cross Completion ( XCom ) and Decision Subspace Alignment ( DS-Align ). XCom is designed to complete missing
features for non-aligned data samples by exploiting cross-client information, thereby effectively
increasing the volume of data available for training and inference. DS-Align aligns local features
with completed and global features across all clients within the decision subspace, enabling each
client to perform locally independent inference, even in the presence of missing features. In addition, theoretical convergence theorems are estabilished for different algorithms used in training
X-VFL . Extensive experiments on real-world datasets demonstrate that X-VFL significantly outperforms existing VFL methods, validating its practical effectiveness and superiority in addressing key
challenges such as missing features, locally independent inference, and data imbalance.
## **Acknowledgments**

This research was supported by the Singapore Ministry of Education (MOE) Academic Research
Fund (AcRF) Tier 1 grant.
## **References**

Timothy Castiglia, Shiqiang Wang, and Stacy Patterson. Self-supervised vertical federated learning.
In *Workshop on Federated Learning: Recent Advances and New Challenges, in Conjunction with*

12


-----

*NeurIPS 2022*, 2022a.

Timothy J Castiglia, Anirban Das, Shiqiang Wang, and Stacy Patterson. Compressed-VFL:
Communication-efficient learning with vertically partitioned data. In *Proceedings of the 39th*
*International Conference on Machine Learning*, pages 2738–2766. PMLR, 17–23 Jul 2022b.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A largescale hierarchical image database. In *2009 IEEE Conference on Computer Vision and Pattern*
*Recognition*, pages 248–255. IEEE Computer Society, 2009.

Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. *Advances in neural information*
*processing systems*, 31, 2018.

Fangcheng Fu, Xupeng Miao, Jiawei Jiang, Huanran Xue, and Bin Cui. Towards communicationefficient vertical federated learning training via cache-enabled local updates. *Proceedings of the*
*VLDB Endowment*, 15(10):2111–2120, June 2022.

Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. *Mathematical Programming*, 155(1):267–
305, 2016.

Chung-ju Huang, Leye Wang, and Xiao Han. Vertical federated knowledge transfer via representation distillation for healthcare collaboration networks. In *Proceedings of the ACM Web*
*Conference 2023*, pages 4188–4199, 2023.

Lingxiao Huang, Zhize Li, Jialin Sun, and Haoyu Zhao. Coresets for vertical federated learning:
Regularized linear regression and *K* -means clustering. *Advances in Neural Information Processing*
*Systems*, 35:29566–29581, 2022.

Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii,
a freely accessible critical care database. *Scientific data*, 3(1):1–9, 2016.

Madhura Joshi, Ankit Pal, and Malaikannan Sankarasubbu. Federated learning for healthcare
domain - pipeline, applications and challenges. *ACM Transactions on Computing for Healthcare*,
3(4), 2022.

Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. *Foundations and Trends® in Machine Learning*,
14(1–2):1–210, 2021.

Yan Kang, Yang Liu, and Xinle Liang. FedCVT: Semi-supervised vertical federated learning with
cross-view training. *ACM Transactions on Intelligent Systems and Technology*, 13(4):1–16, 2022.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.

2009.

Wenjie Li, Qiaolin Xia, Hao Cheng, Kouyin Xue, and Shu-Tao Xia. Vertical semi-federated learning
for efficient online advertising. *arXiv preprint arXiv:2209.15635*, 2022a.

13


-----

Wenjie Li, Qiaolin Xia, Junfeng Deng, Hao Cheng, Jiangming Liu, Kouying Xue, Yong Cheng,
and Shu-Tao Xia. VFed-SSD: Towards practical vertical federated advertising. *arXiv preprint*
*arXiv:2205.15987*, 2022b.

Zhize Li and Peter Richt´arik. A unified analysis of stochastic gradient methods for nonconvex
federated optimization. *arXiv preprint arXiv:2006.07013*, 2020.

Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt´arik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In *International conference on machine*
*learning*, pages 6286–6295. PMLR, 2021.

Deron Liang, Chia-Chi Lu, Chih-Fong Tsai, and Guan-An Shih. Financial ratios and corporate
governance indicators in bankruptcy prediction: A comprehensive study. *European Journal of*
*Operational Research*, 252(2):561–572, 2016.

Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust
model fusion in federated learning. *Advances in Neural Information Processing Systems*, 33:
2351–2363, 2020.

Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin
Zhang, and Qiang Yang. Vertical federated learning: Concepts, advances, and challenges. *IEEE*
*Transactions on Knowledge and Data Engineering*, 2024.

H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In *International*
*Conference on Artificial Intelligence and Statistics*, pages 1273–1282. PMLR, 2017.

Zhenghang Ren, Liu Yang, and Kai Chen. Improving availability of vertical federated learning:
Relaxing inference on non-overlapping data. *ACM Transactions on Intelligent Systems and*
*Technology*, 13(4):1–20, 2022.

Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun.
Splitfed: When federated learning meets split learning. In *Proceedings of the AAAI Confer-*
*ence on Artificial Intelligence*, volume 36, pages 8485–8493, 2022.

Pedro Valdeira, Shiqiang Wang, and Yuejie Chi. Vertical federated learning with missing features
during training and inference. In *The Thirteenth International Conference on Learning Repre-*
*sentations*, 2025.

Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning
for health: Distributed deep learning without sharing raw patient data. *arXiv preprint*
*arXiv:1812.00564*, 2018.

Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized federated learning with feature alignment
and classifier collaboration. *arXiv preprint arXiv:2306.11867*, 2023.

Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. *ACM Transactions on Intelligent Systems and Technology*, 10(2):1–19, 2019.

Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial
autoencoder. In *IEEE Conference on Computer Vision and Pattern Recognition*, 2017.

14


-----

## **A Missing Proofs**

In this appendix, we provide the detailed proofs for our convergence Theorems 1 and 2 in Appendix A.1 and Appendix A.2, respectively.
### **A.1 Proof of Theorem 1**

According to SGD-type update ***θ*** *[t]* [+1] = ***θ*** *[t]* *−* *η∇* [˜] *L* ( ***θ*** *[t]* ) and the smoothness Assumption 1, we have

E *t* [ *L* ( ***θ*** *[t]* [+1] )] *≤* E *t* [ *L* ( ***θ*** *[t]* ) + *⟨∇L* ( ***θ*** *[t]* ) *,* ***θ*** *[t]* [+1] *−* ***θ*** *[t]* *⟩* + *[β]* (12)

2 *[∥]* ***[θ]*** *[t]* [+1] *[ −]* ***[θ]*** *[t]* *[∥]* [2] []]

= E *t* [ *L* ( ***θ*** *[t]* ) *−* *η⟨∇L* ( ***θ*** *[t]* ) *,* *∇* [˜] *L* ( ***θ*** *[t]* ) *⟩* + *[βη]* [2] (13)

2 *[∥][∇]* [˜] *[L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] []]


*≤* *L* ( ***θ*** *[t]* ) *−* ( *η −* *[βη]* [2]


*,* (14)
2



*[βη]* [2] *[βη]* [2] *[σ]* [2]

2 [)] *[∥∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] [ +] 2


where E *t* takes the expectation conditioned on all history before step *t*, and Eq. (14) uses Assumption 2.
Summing up Eq. (14) from *t* = 0 to *T −* 1 and rearranging terms, we get


(15)
2


( *η −* *[βη]* [2]

2 [)]

1

*T*


*T* *−* 1
�


*T* *−* 1
� E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* *L* ( ***θ*** [0] ) *−* E[ *L* ( ***θ*** *[t]* )] + *[βη]* [2] 2 *[σ]* [2] *[T]*

*t* =0


*T* *−* 1

∆ 0 *βη* [2] *σ* [2]

� *t* =0 E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* ( *η −* *βη* [2] */* 2) *T* [+] 2 *η −* *βη*


*T* *−* 1
�


(16)
2 *η −* *βη* [2] *[,]*


*βη* [2] *σ* [2] ∆ 0
where ∆ 0 := *L* ( ***θ*** [0] ) *−* *L* *[∗]* . Note that 2 *η−βη* [2] *[ ≤]* ( *η−βη* [2] */* 2) *T* [if] *[ η][ ≤]* ~~�~~


obtain, by choosing the learning rate *η ≤* min *{* *β* [2] *[,]* ~~�~~


2∆ 0
*βσ* [2] *T* *[}]* [,]


� ~~�~~


�


1
= *O*
� *√T*


2∆ 0
*βσ* [2] *T* [. Thus it is not hard to]

= *ϵ* [2] *.* (17)
�


1

*T*


*T* *−* 1
� E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* *O*

*t* =0


*β* ∆ 0 *σ* [2]

*T*


This means that after *T* = *O* ( *ϵ* [1] [4] [ ) steps, SGD-type algorithms can find a suitable parameter ˆ] ***[θ]*** [ for]

X-VFL such that E *∥∇L* ( ***θ*** [ˆ] ) *∥* [2] *≤* *ϵ* [2], where *ϵ* denotes the convergence error. ### **A.2 Proof of Theorem 2**

The variance in stochastic gradients leads to a large number of communication rounds (i.e., update
steps *T* ). We now prove that the optimal variance-reduced PAGE-type method (Li et al., 2021)
effectively reduces the variance and thus largely decreases the total number of communication
rounds (Theorem 2).
According to simplified PAGE update step Eq. (10) and Lemma 4 of PAGE (Li et al., 2021),
we have the following variance reduction lemma:

**Lemma 1** *Suppose that Assumptions 1 and 2 hold. For the probabilistic gradient estimator* ***g*** *[t]*

*defined in Eq.* (10) *, we have*


E *t* [ *∥* ***g*** *[t]* *−∇L* ( ***θ*** *[t]* ) *∥* [2] ] *≤* (1 *−* *p* ) *∥* ***g*** *[t][−]* [1] *−∇L* ( ***θ*** *[t][−]* [1] ) *∥* [2] + [(] [1] *[ −]* *[p]* [)] *[β]* [2]

*[′]*


(18)
*b* *[.]*



*[p]* [)] *[β]* [2]

*∥* ***θ*** *[t]* *−* ***θ*** *[t][−]* [1] *∥* [2] + *[p]* *[σ]* [2]
*b* *[′]* *b*


15


-----

This lemma indicates that the variance is roughly reduced by a factor 1 *−* *p* after each update step.
Then according to the descent Lemma 2 of PAGE (Li et al., 2021), we have



*[η]* [1]

2 *[∥∇][f]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] *[ −]* [(] 2 *η*



*[β]* *[η]*

2 [)] *[∥]* ***[θ]*** *[t]* [+1] *[ −]* ***[θ]*** *[t]* *[∥]* [2] [ +] 2



*[η]* (19)

2 *[∥]* ***[g]*** *[t]* *[ −∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] *[.]*


*L* ( ***θ*** *[t]* [+1] ) *≤* *L* ( ***θ*** *[t]* ) *−* *[η]*



[1]

2 *η* *[−]* *[β]* 2


Taking the expectation and combining Eq. (19) with multiple Eq. (18), we obtain

E[ *L* ( ***θ*** *[t]* [+1] ) *−* *L* *[∗]* + *[η]*

2 *p* *[∥]* ***[g]*** *[t]* [+1] *[ −∇][L]* [(] ***[θ]*** *[t]* [+1] [)] *[∥]* [2] []]



*[η]* [1]

2 *[∥∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] *[ −]* [(] 2 *η*



*[β]* *[η]*

2 [)] *[∥]* ***[θ]*** *[t]* [+1] *[ −]* ***[θ]*** *[t]* *[∥]* [2] [ +] 2



*[η]*

2 *[∥]* ***[g]*** *[t]* *[ −∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2]


*≤* E *L* ( ***θ*** *[t]* ) *−* *L* *[∗]* *−* *[η]*
� 2



[1]

2 *η* *[−]* *[β]* 2


*b*


+ *[η]*

2 *p*


+ *[η]*


�(1 *−* *p* ) *∥* ***g*** *[t]* *−∇L* ( ***θ*** *[t]* ) *∥* [2] + [(] [1] *[ −]* *b* *[p]* *[′]* [)] *[β]* [2]



*[p]* [)] *[β]* [2]

*∥* ***θ*** *[t]* [+1] *−* ***θ*** *[t]* *∥* [2] + *[p]* *[σ]* [2]
*b* *[′]* *b*


(20)
��


= E *L* ( ***θ*** *[t]* ) *−* *L* *[∗]* + *[η]*
� 2



*[η]* *[η]* *[σ]* [2]

2 *[∥∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] [ +] 2 *b*


2 *b*



*[η]*

2 *p* *[∥]* ***[g]*** *[t]* *[ −∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] *[ −]* *[η]* 2



[1]

2 *η* *[−]* *[β]* 2


*−* ( [1]



*[β]*

2 *[−]* [(] [1] *[ −]* 2 *pb* *[p]* [)] *[′]* *[ηβ]* [2]



*[p]* [)] *[ηβ]*

) *∥* ***θ*** *[t]* [+1] *−* ***θ*** *[t]* *∥* [2] [�] (21)
2 *pb* *[′]*


*≤* E *L* ( ***θ*** *[t]* ) *−* *L* *[∗]* + *[η]*
� 2



*[η]* *[η]* *[σ]* [2]

2 *[∥∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] [ +] 2 *b*


2 *b*



*[η]*

2 *p* *[∥]* ***[g]*** *[t]* *[ −∇][L]* [(] ***[θ]*** *[t]* [)] *[∥]* [2] *[ −]* *[η]* 2


*,* (22)
�



*[β]*

2 *[−]* [(] [1] *[−]* 2 *[p]* *pb* [)] *[ηβ]* *[′]* [2]


(1 *−p* ) */* ( *pb* *[′]* )) [.]


where Eq. (22) holds due to 2 1 *η* *[−]* *[β]* 2



*[−]* 2 *[p]* *pb* [)] *[ηβ]* *[′]* *≥* 0 by choosing learning rate *η ≤* *β* (1+ *[√]* (1 1 *−*


Summing up Eq. (22) from *t* = 0 to *T −* 1 and rearranging terms, we get



*[η]* *[η]* *[σ]* [2] *[T]*

2 *p* *[∥]* ***[g]*** [0] *[ −∇][L]* [(] ***[θ]*** [0] [)] *[∥]* [2] [ +] 2 *b*


2 *b*


*η*

2

1

*T*


*T* *−* 1
�


� E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* E� *L* ( ***θ*** [0] ) *−* *L* *[∗]* + 2 *[η]*

*t* =0


� E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* [2∆] [0]

*ηT*

*t* =0


(23)
�


*T* *−* 1
�



[2∆] [0] *[σ]* [2]

*ηT* [+] *pbT*



*[ σ]* (24)

*b* *[,]*



*[σ]* [2]

*pbT* [+] *[ σ]* *b* [2]


where Eq. (24) uses ∆ 0 := *L* ( ***θ*** [0] ) *−* *L* *[∗]* and the bounded variance Assumption 2 for the initial
gradient estimator *g* [0] = *∇* [˜] *b* *L* ( ***θ*** [0] ).
Then for any convergence error *ϵ*, by choosing learning rate *η ≤* 2 1 *β* [, minibatch sizes] *[ b]* [ =] [2] *ϵ* *[σ]* [2][2] *[, b]* *[′]* *[ ≤]*


Then for any convergence error *ϵ*, by choosing learning rate *η ≤* 2 1 *β* [, minibatch sizes] *[ b]* [ =] [2] *ϵ* *[σ]* [2] *[, b]* *[′]* *[ ≤]*

*b* *[′]*

*√b*, and probability *p* = *b* *b* *[′]* [, we have]


*b* *[′]*
*b*, and probability *p* = *b* + *b* *[′]* [, we have]


*T*


1

*T*


*T* *−* 1
�


*−* 1
� *t* =0 E *∥∇L* ( ***θ*** *[t]* ) *∥* [2] *≤* *O* � *T*


= *ϵ* [2] *.* (25)
�


This means that after *T* = *O* ( *ϵ* [1] [2] [ ) steps, PAGE-type algorithms can find a suitable parameter ˆ] ***[θ]***

for X-VFL such that E *∥∇L* ( ***θ*** [ˆ] ) *∥* [2] *≤* *ϵ* [2], where *ϵ* denotes the convergence error. Compared with
the SGD-type algorithms in Theorem 1, the variace-reduced PAGE-type algorithms in Theorem 2
significantly decreases the total number of communication rounds, i.e., from *T* = *O* ( *ϵ* [1] [4] [ ) to] *[ T]* [ =]


This means that after *T* = *O* ( *ϵ* [1] [2]


significantly decreases the total number of communication rounds, i.e., from *T* = *O* ( *ϵ* [1] [4] [ ) to] *[ T]* [ =]

*O* ( *ϵ* [1] [2] [ ) by a factor of] *ϵ* 1 [2] [ .] 


[1] 1 
*ϵ* [2] [ ) by a factor of] *ϵ* [2] [ .]


16


-----

## **B Experimental Details**

We now provide more details of the experimental setup in this appendix. We conduct the experiments on real-world datasets, including three image datasets: `CIFAR-10` (Krizhevsky et al., 2009),
`TinyImageNet` (restricted to five classes) (Deng et al., 2009), and `UTKFace` (Zhang et al., 2017), as
well as three tabular datasets: `MIMIC-III` (Johnson et al., 2016), `Bank` (Liang et al., 2016), and
`Avazu` (Kang et al., 2022). For the image datasets, features are vertically partitioned into disjoint
subsets and assigned to different clients. For the tabular datasets, continuous features are normalized to the range [0 *,* 1], while categorical features are transformed into one-hot encodings ( `Bank` ) or
embedding-based representations ( `Avazu` ). These preprocessed features are then partitioned into
disjoint subsets and distributed to the clients. The server retains the class labels of the datasets.
To simulate real-world scenarios where clients have partially missing features, we mask a proportion of features in the non-aligned training and test samples. Various feature missing rates
*R* miss = *{* 0 *,* 0 *.* 1 *,* 0 *.* 3 *,* 0 *.* 5 *,* 0 *.* 7 *,* 0 *.* 9 *,* 1 *}* are applied to to thoroughly assess the impact of missing fea
tures.
For the image datasets ( `CIFAR-10`, `TinyImageNet`, and `UTKFace` ), the client’s local (bottom)
model is implemented using a residual network with varying depths, characterized by different
numbers of residual blocks. The corresponding server (top) model is a six-layer fully connected
neural network (FCNN). For the tabular datasets ( `MIMIC-III`, `Bank`, and `Avazu` ), both the client
and server models are implemented as three-layer FCNNs. The input dimensions of the server
models were set to 2048 for image datasets and 512 for tabular datasets to accommodate differences in feature complexity and representation requirements. The batch size is set to 50. The
hyperparameters *λ* 1 and *λ* 2 defined in the overall loss function (see Eq. (5)) are selected from the
ranges listed in Table 1.

Table 1: Hyperparameter ranges for *λ* 1 and *λ* 2

**Dataset** ***λ*** **1** ***λ*** **2**

`CIFAR-10` *{* 0.01, 0.02, 0.05, 0.1, 0.2, 0.5 *}* *{* 1 *×* 10 *[−]* [5], 2 *×* 10 *[−]* [5], 5 *×* 10 *[−]* [5], 1 *×* 10 *[−]* [4], 2 *×* 10 *[−]* [4], 5 *×* 10 *[−]* [4] *}*
`TinyImageNet` *{* 1, 2, 5, 10, 20 *}* *{* 5 *×* 10 *[−]* [4], 1 *×* 10 *[−]* [3], 2 *×* 10 *[−]* [3], 5 *×* 10 *[−]* [3] *}*
`UTKFace` *{* 1, 2, 5, 10, 20 *}* *{* 5 *×* 10 *[−]* [4], 1 *×* 10 *[−]* [3], 2 *×* 10 *[−]* [3], 5 *×* 10 *[−]* [3] *}*
`MIMIC-III` *{* 5 *×* 10 *[−]* [4], 1 *×* 10 *[−]* [3], 2 *×* 10 *[−]* [3], 5 *×* 10 *[−]* [3], 1 *×* 10 *[−]* [2] *}* *{* 0.02, 0.05, 0.1 *}*
`Bank` *{* 1 *×* 10 *[−]* [4], 2 *×* 10 *[−]* [4], 5 *×* 10 *[−]* [4], 1 *×* 10 *[−]* [3] *}* *{* 5 *×* 10 *[−]* [5], 1 *×* 10 *[−]* [4], 2 *×* 10 *[−]* [4], 5 *×* 10 *[−]* [4] *}*
`Avazu` *{* 1 *×* 10 *[−]* [4], 2 *×* 10 *[−]* [4], 5 *×* 10 *[−]* [4], 1 *×* 10 *[−]* [3] *}* *{* 1 *×* 10 *[−]* [5], 2 *×* 10 *[−]* [5], 5 *×* 10 *[−]* [5], 1 *×* 10 *[−]* [4], 2 *×* 10 *[−]* [4] *}*

Vanilla Standalone refers to an independently trained model in which each client uses only its
own local features for training and inference. It servers as a baseline for evaluating the capability of
independent inference. Vanilla VFL enables collaborative training by concatenating local feature
representations from all clients at a central server. During inference, these concatenated representations is performed to generate predictions. This approach serves as a baseline for evaluating the
capability of collaborative inference. For other baselines, although previous studies typically either
cannot handle missing features or do not support locally independent inference, we identify two
related works, IAVFL (Ren et al., 2022) and FedCVT (Kang et al., 2022), that are relevant for
comparison, as discussed in Section 1.2. Concretely, we compare our X-VFL with these four baselines
for both independent Inference and collaborative inference modes, using test classification accuracy
as the performance metric. All results are averaged over five random seeds to ensure robustness.

17


-----

## C Additional Experimental Results for TinyImageNet and Bank

Similar to Section 4.1, this appendix provides additional experimental results on the `TinyImageNet`
and `Bank` datasets under varing feature missing rates. The results are presented in Fig. 11 and
Fig. 12. Moreover, the results demonstrate the same trend: our X-VFL significantly outperforms
the baselines, particularly in the independent inference mode—one of the key challenges that X-VFL
is designed to address.


Bank

90

80

70

60

50

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


60

50

40

30

20


TinyImageNet

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


Bank

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


60

55

50

45


TinyImageNet

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||



0.00.1 0.3 0.5 0.7 0.91.0
Feature Missing Rate


90

85

80

75

70

65


Vanilla Standalone IAVFL FedCVT X-VFL

(a) Independent inference mode


Vanilla VFL IAVFL FedCVT X-VFL

(b) Collaborative inference mode


Figure 11: Performance comparison under varying feature missing rates on the `TinyImagenet` and
`Bank` datasets.


TinyImageNet

50

40

30

20


Bank


Figure 12: Performance difference between independent and collaborative inference modes under
feature missing rate *R* miss = 0 *.* 9 on the `TinyImagenet` and `Bank` datasets.
## **D General Multiple Clients Setting**

In this appendix, we present the extension of our X-VFL framework to the general case with multiple
*k* clients. This extension enhances the practicality of X-VFL and offers a more scalable and effective
approach for broader applications. The framework formulation and corresponding experimental
results are provided in Appendix D.1 and Appendix D.2, respectively.
### **D.1 Formulation for multiple clients**

Similar to Section 2.1, let *f* 1 *, . . ., f* *k* denote the local models for the *k* clients. Then ***E*** *i* = *f* *i* ( ***x*** ***i*** )
denotes the feature embeddings/representations for client *i*, *i ∈* [ *k* ]. For the general case of *k* clients,

18


-----

the overall loss function can be formulated as the same one as in Eq. (5):

*L* = *L* decision + *λ* 1 *L* DSAlign 1 + *λ* 2 *L* DSAlign 2 *.* (26)

Compared with two-client setting, now the classification decision loss for *k* clients is formulated
as follows:


*−*

) *, y* ) *,* (27)
*k*


� *ℓ* ( *h* ( ***E*** *i* ) *, y* ) + *ℓ* ( *h* ( ***[E]*** *m* *[′]*

*i∈M*


*L* decision =
�



***[E]*** *[′]* ***E*** ˜ *[′]* + ***E*** *[′]*

*m* [)] *[, y]* [) +] *[ ℓ]* [(] *[h]* [(] *k*


) *, y* ) +
*k*


*k*
�


*k* ˜

***E*** *i* + ***E*** *−i*

� *ℓ* ( *h* ( *k*

*i* =1


where ***E*** *[′]* = [�] ***E*** *[′]* = [�]
*j∈M* ***[E]*** *[j]* [, ˜] *i∈{* 1 *,...,k}\M* ***[E]*** [˜] *i* *[′]* [,] *[ M ⊆{]* [1] *[, . . ., k][}]* [ denotes the subset of clients that]
hold full local features (i.e., clients without missing features), where *|M|* = *m* for some *m ≤* *k*,
***E*** *−i* = [�] *j∈{* 1 *,...,k}\{i}* ***[E]*** *[j]* [, and]

***E*** ˜ *i* = *f* *i* ( ˜ ***X*** *i* ) *,* ***X*** ˜ *i* = XCom *i* ( ***[E]*** *[−][i]* 1 *≤* *i ≤* *k,* (28)

*k −* 1 [)] *[,]*

***E*** ˜ *i* *[′]* [=] *[ f]* *[i]* [( ˜] ***X*** *i* *[′]* [)] *[,]* ***X*** ˜ *i* *[′]* [=][ XCom] *[i]* [(] ***[E]*** *[′]* 1 *≤* *i ≤* *k.* (29)

*m* [)] *[,]*

For aligned data, the following terms in Eq. (27) are activated: [�] *i∈M* *[ℓ]* [(] *[h]* [(] ***[E]*** *[i]* [)] *[, y]* [),] *[ ℓ]* [(] *[h]* [(] ***[E]*** *m* *[′]* [)] *[, y]* [),]


˜
and [�] *[k]* ***E*** *i* + ***E*** *−* *i*
*i* =1 *[ℓ]* [(] *[h]* [(] *k*


˜
*k* ***E*** *−* *i* ) *, y* ). The term *ℓ* ( *h* ( ***E*** *[′]* + *k* ***E*** *[′]*


and [�] *[k]* *i* =1 *[ℓ]* [(] *[h]* [(] ***E*** *i* + *k* ***E*** *−* *i* ) *, y* ). The term *ℓ* ( *h* ( ***E*** *[′]* + *k* ***E*** *[′]* ) *, y* ) is not activated, as ***E*** [˜] *[′]* is empty when *M*

includes all clients (i.e., *k* = *m* ). For non-aligned data, the actived terms are [�] *i∈M* *[ℓ]* [(] *[h]* [(] ***[E]*** *[i]* [)] *[, y]* [),]



***[E]*** *m* *[′]* [)] *[, y]* [), and] *[ ℓ]* [(] *[h]* [(] ***E*** ˜ *[′]* + *k* ***E*** *[′]*


*ℓ* ( *h* ( ***[E]*** *m* *[′]*


*ℓ* ( *h* ( ***[E]*** *m* [)] *[, y]* [), and] *[ ℓ]* [(] *[h]* [(] ***E*** + *k* ***E*** ) *, y* ).

The last two loss components introduced by our DS-Align module in the general case of *k* clients
are formulated as follows:


*L*
DSAlign 1 =


*k*
� *ℓ* ( *h* ( ***E*** [˜] *i* ) *, h* ( ***E*** *i* )) *,* (30)

*i* =1


and


= 1

)) *.* (31)
*k*


*L*
DSAlign 2 =


*k*
�


� *k* *ℓ* ( *h* ( ***E*** *i* ) *, h* ( � *k* *i* = *k* 1 ***[E]*** *[i]*

*i* =1

### **D.2 Experimental results for multiple clients**

In this appendix, we provide the experimental results for the general multiple clients setting. We
use `CIFAR-10` (Krizhevsky et al., 2009) as the image dataset and `MIMIC-III` (Johnson et al., 2016)
as the tabular dataset. In particular, we conduct experiments with *k* = 4 clients. Regarding the
baselines, since FedCVT (Kang et al., 2022) is specifically designed for the two-client scenario,
it cannot be extended to the multi-client setting. Therefore, we include three baseline methods
(Vanilla Standalone, Vanilla VFL, IAVFL) in the multi-client experiments, consistent with those
used in the two-client setting, excluding only FedCVT. The implementation details remain the
same as in the two-client case (see Appendix B), except that the batch size is set to 100. In the
multi-client setting, we simply set the hyperparameters *λ* 1 = *λ* 2 = 0 *.* 01.
Fig. 13 presents the results under varying feature missing rates, *R* miss = *{* 0 *,* 0 *.* 3 *,* 0 *.* 5 *,* 0 *.* 7 *,* 1 *.* 0 *}*,
for both independent and collaborative inference modes. A comparison of performance between
these two modes at a missing rate *R* miss = 0 *.* 7 is shown in Fig. 14. The results follow the same

19


-----

trend observed in the two-client case: our X-VFL significantly outperforms the baselines, particularly
in the independent inference mode—one of the key challenges that X-VFL is designed to address.
Moreover, Fig. 14 highlights the advantage of X-VFL in effectively narrowing the accuracy gap
between independent and collaborative inference modes.
We also evaluate the effect of varying overlap ratios of aligned data samples, using 20%, 40%,
and 80% aligned samples, as shown in Fig. 15. Similar to the two-client case, X-VFL consistently
outperforms the baseline methods across all overlap ratios in both independent and collaborative
inference modes.


MIMIC-III

70

60

50

40

30

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||



0.0 0.3 0.5 0.7 1.0
Feature Missing Rate


CIFAR-10

70

65

60

55

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||



0.0 0.3 0.5 0.7 1.0
Feature Missing Rate


MIMIC-III

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||



0.0 0.3 0.5 0.7 1.0
Feature Missing Rate


60

40

20


CIFAR-10

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||



0.0 0.3 0.5 0.7 1.0
Feature Missing Rate


85

80

75

70

65


Vanilla Standalone IAVFL X-VFL

(a) Independent inference mode


Vanilla VFL IAVFL X-VFL

(b) Collaborative inference mode


Figure 13: Performance comparison under varying feature missing rates on the `CIFAR-10` and
`MIMIC-III` datasets in the multiple clients setting.


70

60

50

40

30

20


CIFAR-10



MIMIC-III


Figure 14: Performance comparison between independent and collaborative inference modes under
feature missing rate of 0 *.* 7 on the `CIFAR-10` and `MIMIC-III` datasets in the multiple clients setting.


Independent


Independent


CIFAR-10
Collaborative


MIMIC-III
Collaborative

85

82

79

76

73

70


65

60

55

50

45


60

50

40

30

20


70

65

60

55

50




Vanilla IAVFL X-VFL


Overlap 20% Overlap 40% Overlap 80%

(a) `CIFAR-10` dataset


Overlap 20% Overlap 40% Overlap 80%

(b) `MIMIC-III` dataset


Figure 15: Performance comparison under varying overlap ratios on the `CIFAR-10` and `MIMIC-III`
datasets in the multiple clients setting.

20


-----


