{
  "title": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency",
  "detailed_summary": "This paper addresses the challenge of GUI grounding, which is mapping natural language instructions to precise screen coordinates. Existing methods rely heavily on supervised training or reinforcement learning with labeled rewards, which are limited by the cost and availability of pixel-level annotations. The authors observe that spatial overlap patterns in multiple predictions for the same GUI element reveal implicit confidence signals. They propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions. GUI-RC improves accuracy without any training. They also introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning, enabling models to iteratively refine their outputs on unlabeled data during inference. Experiments demonstrate that GUI-RC and GUI-RCPO improve performance on ScreenSpot benchmarks, highlighting the potential of test-time methods for data-efficient GUI agents.",
  "background": "GUI grounding is critical for autonomous GUI agents to understand and execute user commands. Current approaches rely on extensive train-time optimization through supervised fine-tuning with large-scale annotated datasets or reinforcement learning with carefully designed reward functions. These methods suffer from two main limitations: dependence on labeled data, which is costly to obtain, and underutilization of test-time computation for inference-time optimization. This paper aims to leverage test-time computation to enhance GUI grounding performance without additional labeled data, drawing inspiration from the success of test-time scaling methods in large language models.",
  "contributions": [
    "Propose GUI-RC, a test-time scaling method for GUI grounding that leverages spatial voting across multiple predictions to improve localization accuracy without additional training or labeled data.",
    "Introduce GUI-RCPO, a test-time reinforcement learning method that uses region consistency as a self-supervised reward signal, enabling models to improve grounding capabilities through policy optimization on unlabeled GUI screenshots.",
    "Demonstrate consistent improvements across multiple benchmarks and model architectures. GUI-RC improves accuracy by 2-3% on average, while GUI-RCPO achieves further gains of 4-5% on average through label-free optimization.",
    "Reveal that further applying GUI-RC after GUI-RCPO yields additional performance gains, demonstrating that our methods support progressive, self-bootstrapping improvement without external supervision, and provide a complementary alternative to train-time optimization for GUI automation."
  ],
  "problem": "The paper addresses the problem of improving GUI grounding accuracy without relying on expensive pixel-level annotations or extensive training. The challenge lies in the continuous, high-resolution coordinate space of GUI elements, where minor pixel deviations can lead to incorrect element selection. Visual complexity, overlapping elements, and dynamic layouts further increase prediction uncertainty.",
  "methods": [
    "**GUI-RC (GUI Region Consistency):**",
    "  - **Multi-Sample Generation:** Sample K predictions from the model using temperature-based sampling.",
    "  - **Spatial Voting Mechanism:** Construct a spatial voting grid where each sampled prediction contributes votes to the grid.",
    "  - **Consensus Extraction:** Identify the maximum vote count and extract the largest contiguous region with the maximum vote count as the consensus region.",
    "**GUI-RCPO (GUI Region Consistency Policy Optimization):**",
    "  - **Region Consistency as Reward:** Compute a reward for each sampled prediction based on the average vote density within the predicted region, normalized by the region size and maximum possible votes.",
    "  - **Policy Optimization:** Formulate GUI grounding as a reinforcement learning problem and optimize the expected region consistency reward using Group Relative Policy Optimization (GRPO)."
  ],
  "experimental_design": "The methods were evaluated on three GUI grounding benchmarks: ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro. Several VLMs were tested, including Qwen2.5-VL-3B-Instruct, Qwen2.5-VL-7B-Instruct, InternVL3-2B-Instruct, InternVL3-8B-Instruct, UGround-V1-7B, OS-Atlas-Base-7B, and UI-TARS-1.5-7B. Grounding accuracy was used as the primary evaluation metric. For GUI-RC, 64 outputs were sampled with a temperature of 0.5 and top p of 0.95. For GUI-RCPO, TTRL training was conducted on Screenspot-v2 without ground truth data, using 16 samples with a temperature of 0.7 and top p of 0.95, trained for 2 epochs with a global batch size of 64, learning rate of 1e-6, and KL penalty β = 0.04.",
  "results": "GUI-RC consistently improved grounding performance across different models. For example, OS-Atlas-Base-7B achieved an overall improvement of 2.75% on ScreenSpot-v2. GUI-RCPO also consistently improved performance and even outperformed GUI-RC. For instance, GUI-RCPO improved Qwen2.5-VL-3B-Instruct by 5.5% on ScreenSpot, while GUI-RC improved it by 1.49%. Applying GUI-RC after GUI-RCPO led to further performance gains.",
  "result_analysis": "GUI-RC improves performance by mitigating misleading and biased hallucinations. Misleading hallucinations are addressed by obtaining a consensus region through voting, leading to more confident and spatially precise answers. Biased hallucinations, stemming from the granularity mismatch between pixel-level coordinate predictions and patch-level visual encoder processing, are resolved by extracting the area the model most attends to, reducing grounding biases. Ablation studies show that GUI-RC's performance is affected by temperature, sampling number, and the expansion size hyperparameter α. GUI-RCPO demonstrated robust generalization across different screen resolutions and interface layouts.",
  "conclusions": "The paper concludes that GUI-RC and GUI-RCPO are effective test-time methods for improving GUI grounding performance. GUI-RC leverages region consistency to enhance model performance without additional training, while GUI-RCPO transforms region consistency into a self-supervised reward signal for test-time reinforcement learning. These methods consistently improve GUI grounding performance and generalize well to out-of-distribution scenarios, showing promise for more robust and data-efficient GUI automation systems.",
  "limitations": "GUI-RC brings relatively limited improvements for models with point-style outputs. Additionally, GUI-RC primarily addresses misleading and biased hallucinations, but it struggles to resolve confusion hallucinations (i.e., predicted region fails to match any valid UI element), meaning it relies on the model having a base level of understanding of the GUI elements. The method assumes the model has a certain ability to recognize the target element, but it does not require the model to be specifically trained on GUI tasks.",
  "future_work": "The paper does not explicitly state future work, but implied future work could include improvements to GUI-RC so that it works better on models with point-style outputs, and exploring methods to resolve confusion hallucinations. Also, since region-level supervision has advantages, strengthening GUI-RC's effects on the same could be an area of focus for further studies.",
  "applications": "This research has potential applications in developing more robust and data-efficient GUI automation systems. It can improve the accuracy and reliability of GUI agents, enabling them to better understand and execute user commands in various applications, such as mobile apps, web interfaces, and desktop software. By reducing the dependence on labeled data, the proposed methods can facilitate the scaling of GUI agents to new domains and applications.",
  "related_work": "The paper discusses related work in GUI grounding and test-time scaling. It mentions Seeclick, UGround, OS-Atlas, and UI-TARS as representative works in GUI grounding. It also discusses self-consistency voting and test-time reinforcement learning as key strategies in test-time scaling for LLMs.",
  "github_links": [
    "https://github.com/zju-real/gui-rcpo",
    "https://zju-real.github.io/gui-rcpo"
  ],
  "published": "2025-08-07T17:54:27+00:00"
}