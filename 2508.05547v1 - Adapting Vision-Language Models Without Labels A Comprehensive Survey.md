JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
## Adapting Vision-Language Models Without Labels: A Comprehensive Survey
##### Hao Dong [‚àó], Lijun Sheng [‚àó], Jian Liang [‚Ä†], Ran He, Eleni Chatzi, Olga Fink


***Abstract*** **‚ÄîVision-Language** **Models** **(VLMs)** **have** **demon-**
**strated remarkable generalization capabilities across a wide**
**range of tasks. However, their performance often remains sub-**
**optimal when directly applied to specific downstream scenarios**
**without task-specific adaptation. To enhance their utility while**
**preserving data efficiency, recent research has increasingly fo-**
**cused on unsupervised adaptation methods that do not rely on**
**labeled data. Despite the growing interest in this area, there**
**remains a lack of a unified, task-oriented survey dedicated to**
**unsupervised VLM adaptation. To bridge this gap, we present a**
**comprehensive and structured overview of the field. We propose a**
**taxonomy based on the availability and nature of unlabeled visual**
**data, categorizing existing approaches into four key paradigms:**
***Data-Free Transfer*** **(no data),** ***Unsupervised Domain Transfer***
**(abundant data),** ***Episodic Test-Time Adaptation*** **(batch data), and**
***Online Test-Time Adaptation*** **(streaming data). Within this frame-**
**work, we analyze core methodologies and adaptation strategies**
**associated with each paradigm, aiming to establish a systematic**
**understanding of the field. Additionally, we review representa-**
**tive benchmarks across diverse applications and highlight open**
**challenges and promising directions for future research. An**
**actively maintained repository of relevant literature is available**
**[at https://github.com/tim-learn/Awesome-LabelFree-VLMs.](https://github.com/tim-learn/Awesome-LabelFree-VLMs)**

***Index Terms*** **‚ÄîUnsupervised learning, test-time adaptation,**
**multimodal learning, vision-language models.**

I. I NTRODUCTION

ISION-language models (VLMs), such as CLIP [1],
# V ALIGN [2], Flamingo [3], and LLaVA [4] have attracted

considerable attention from both academia and industry due
to their powerful cross-modal reasoning capabilities. These
models learn joint image-text representations from large-scale
datasets [5] and have demonstrated impressive zero-shot performance and generalization across a variety of tasks. VLMs
have been successfully applied in diverse domains, including
autonomous driving [6], robotics [7], anomaly detection [8],
and cross-modal retrieval [9].
However, because the pre-training phase cannot capture
the full diversity of downstream tasks and environments,
adapting VLMs to specific applications remains a fundamental
challenge. Early efforts primarily relied on supervised finetuning [10]‚Äì[13], which explores more knowledge in annotated

*‚àó* Equal contribution. *‚Ä†* Corresponding author.
H. Dong and E. Chatzi are with ETH Z¬®urich, Switzerland. (Email:
hao.dong@ibk.baug.ethz.ch; chatzi@ibk.baug.ethz.ch).
L. Sheng is with the University of Science and Technology of China.
(Email: slj0728@mail.ustc.edu.cn).
J. Liang and R. He are with NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences. (Email: liangjian92@gmail.com;
rhe@nlpr.ia.ac.cn).
O. Fink is with EPFL, Switzerland. (Email: olga.fink@epfl.ch).
Manuscript received August 8, 2025.


examples. Despite their effectiveness, they still suffer from
high annotation costs and performance degradation under
distribution shifts [14] between training and test data. To
address these limitations, a growing body of work has explored unsupervised adaptation techniques [15]‚Äì[20]. These
approaches‚Äîoften referred to as zero-shot inference [21]‚Äì

[23], test-time methods [18], [24], [25], or unsupervised
tuning [17], [26], [27]‚Äîaim to improve VLMs‚Äô performance
in downstream tasks without relying on costly annotation.
Such methods have proven effective across a wide range of
applications, including image classification [15], [17], [18],
segmentation [16], [28], [29], medical image diagnosis [30],

[31], and action recognition [32], [33].
Given the rapid growth of this research area, this survey
provides a comprehensive and structured overview of existing
unsupervised adaptation methods for VLMs. To the best of our
knowledge, we are the first to introduce a taxonomy centered
on the availability of unlabeled visual data‚Äîan often overlooked yet practically critical factor in real-world deployment.
As illustrated in Fig. 1, we categorize existing approaches
into four paradigms: (1) Data-Free Transfer [15], [16], [21],
which adapts models using only textual class names; (2)
Unsupervised Domain Transfer [17], [34], [35], which utilizes abundant unlabeled data from the downstream tasks; (3)
Episodic Test-Time Adaptation [18], [24], [36], which adapts
models to a batch of test instances; and (4) Online Test-Time
Adaptation [19], [23], [25], which addresses the challenge
of streaming test data. This taxonomy provides a principled
framework for understanding the landscape of unsupervised
VLM adaptation, guiding practitioners in selecting suitable
techniques. We also believe our taxonomy will facilitate fair
comparisons across future work within the same paradigm.
The organization of this survey follows the structure shown
in Fig. 2. Sec. II provides an overview of several research
topics related to unsupervised learning in the context of
VLMs. Sec. III introduces zero-shot inference with VLMs

and presents a comprehensive taxonomy based on the availability of unlabeled visual data. The central focus of this
survey is discussed in Sec. IV - Sec. VII, where we analyze
existing approaches within data-free transfer, unsupervised
domain transfer, episodic test-time adaptation, and online testtime adaptation, respectively. Sec. VIII explores a variety of
application scenarios that utilize unsupervised techniques and
introduces related benchmarks, offering a broader perspective
on their practical implications and real-world utility. Finally,
we summarize emerging trends in the field and identify key
scientific questions that could inspire future work in Sec. IX.
**Comparison with previous surveys.** In recent years, sev

-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2


**1. Data-Free**

**Transfer**
#### **names T**


**3. Episodic Test-Time**

**Adaptation**




**Domain Transfer** **Adaptation** **‚Ä¶** **‚Ä¶**


**4. Online Test-Time**


**Adaptation**






**downstream tasks** **pretrained VLMs**


**downstream tasks**


Fig. 1: Illustration of our taxonomy on unsupervised adaptation with VLMs. We categorize existing unsupervised methods into
four task paradigms based on the availability of unlabeled visual data.


eral surveys [37]‚Äì[40] have explored various aspects of unsupervised adaptation and fine-tuning of VLMs. Existing
works [40]‚Äì[42] predominantly focus on unimodal model
transfer, providing a thorough analysis of this domain, but they
offer limited coverage of VLMs. An early work [37] discusses
the pre-training stage of VLMs and briefly analyzes its finetuning method for vision tasks. Another survey [38] discusses
the adaptation and generalization of multimodal models, but
at a relatively coarse-grained level. A recent work [39] uses
generalization to understand VLMs‚Äô downstream tasks and
reviews existing methods with a perspective of the parameter
space. While these surveys contribute valuable insights, our
work distinguishes itself by introducing, for the first time,
a taxonomy based on the availability of unlabeled visual
data and analyzing cutting-edge technologies in each of these
paradigms. We believe this is a novel and crucial contribution
to the field, especially in terms of the deployment of VLMs.

II. R ELATED R ESEARCH T OPICS

*A. Vision-Language Models*

Recent progress in VLMs has been remarkable, driven by
the integration of large-scale pre-training [1], [43], transformer
architectures [44], [45], and massive multimodal datasets [5],

[46]. Models such as CLIP [1], ALIGN [2], and Flamingo [3]
have pushed the boundaries by learning robust joint representations that bridge the semantic gap between vision
and language. These advancements have enabled impressive
performance across a range of tasks, including image captioning [47], visual question answering [48], text-to-image
synthesis [49], and cross-modal retrieval [50], often exhibiting
strong zero-shot and few-shot learning capabilities. For further
information on vision-language models, we refer the reader to
the recent survey papers [37], [51].

*B. Zero-Shot Learning*

Zero-shot learning (ZSL) aims to recognize unseen classes
by leveraging semantic information such as attributes or word
embeddings. Early methods relied on learning compatibility
functions between visual features and manually defined attributes [52], [53]. Subsequent works introduced embeddingbased approaches that align visual and semantic spaces using


supervised objectives [54], [55]. To address limitations in
generalization, generative models were employed to synthesize
visual features for unseen classes [56]‚Äì[58]. Recent research
emphasizes generalized settings [59], [60], aiming to improve
robustness and fair evaluation across seen and unseen cate
gories. ZSL serves as a foundational principle for unsupervised
VLM adaptation, leveraging semantic descriptions (e.g., text
prompts) to bridge seen and unseen classes. This enables
models to generalize to novel visual concepts without requiring
any labeled examples. For further information on ZSL, we
refer the reader to the recent survey papers [61], [62].

*C. Supervised Fine-Tuning of VLMs*

Supervised fine-tuning of VLMs has emerged as a key
strategy to adapt pre-trained models to downstream tasks
with task-specific supervision. Researchers have increasingly
focused on parameter-efficient fine-tuning techniques‚Äîsuch as
prompt tuning [10], [12], [63], adapter modules [64], [65],
and lightweight task-specific layers [66]‚Äîthat allow models
to adapt to new domains while preserving the generality of
their pre-trained features. Moreover, some approaches use
large language models (LLMs) to aid in adapting VLMs [21],

[67] or adapt VLMs to dense prediction tasks [16], [68]. For
further information on supervised fine-tuning of VLMs, we
refer the reader to the recent survey papers [38], [69]. Instead
of relying on explicit label supervision, this survey focuses on
unsupervised VLM adaptation, where models must adapt to
downstream tasks without access to annotated data.

*D. Source-Free Domain Adaptation*

Source-free domain adaptation (SFDA) addresses the practical setting where access to source data is restricted during
adaptation [70]. SHOT [71] initiates this paradigm by aligning
target features through information maximization and selfsupervised learning. The following works improve class-wise
feature structure via neighborhood clustering [72] and contrastive learning [73]. Other approaches leverage prototype refinement [74], self-training [75], and adversarial learning [76]
to enhance robustness. SFDA is closely related to unsupervised
VLM adaptation, as the original source data used to pretrain VLMs is often massive and impractical to access during


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3












Fig. 2: Taxonomy of unsupervised adaptation paradigms for vision-language models (VLMs).


adaptation. For further information on SFDA, we refer the
reader to the recent survey papers [77], [78].

*E. Traditional Test-Time Adaptation*

Test-time adaptation (TTA) focuses on adapting a pretrained source model online to address distribution shifts

without access to source data or target labels. Online TTA
methods [79], [80] update specific model parameters using incoming test samples, leveraging unsupervised objectives such
as entropy minimization and pseudo-labeling. Robust TTA
methods [81], [82] tackle challenging real-world scenarios,
including label shifts, single-sample adaptation, and mixed
domain shifts. Meanwhile, continual TTA approaches [83],

[84] handle evolving distribution shifts encountered over time,
which is particularly relevant in dynamic real-world applications. Although most traditional TTA methods were introduced
for vision-only architectures, their core mechanisms, such as
entropy minimization and pseudo-labeling, have been repurposed for TTA of VLMs [18], [24], [85]. For a comprehensive
review of test-time adaptation, we refer readers to the recent
survey papers [40], [41].

III. P RELIMINARIES

**Vision-Language Models (VLMs)** typically consist of an
image encoder that maps high-dimensional images into a
low-dimensional embedding space and a text encoder that
generates text representations from natural language. Since the
introduction of CLIP [1], numerous improved models have
been proposed, including ALIGN [2], EVA-CLIP [86], and
SigLIP [87], with CLIP remaining the most widely used model
in existing works. CLIP is trained on 400 million image-text
pairs and aligns image and text embeddings using contrastive
loss. Given a batch of image-text pairs, CLIP maximizes the
cosine similarity for matched pairs while minimizing it for
unmatched ones. During inference, the class names of a target
dataset are embedded using the text encoder with a prompt of
the form ‚Äúa photo of a [CLASS]‚Äú, where [CLASS] is replaced
with specific class names (e.g., cat, dog, car). The text encoder
then generates text embeddings **t** *c* for each class *c*, and the
prediction probability for an input image **x** with embedding
**f** **x** is computed as:

ex p ( cos ( **f** **x** *,* **t** *y* ) */* *œÑ* )
*p* ( *y|* **x** ) = ÔøΩ *Cc* =1 [exp (cos (] **[f]** **[x]** *[,]* **[ t]** *[c]* [)] *[ /œÑ]* [)] *,* (1)


where cos( *¬∑, ¬∑* ) measures the cosine similarity and *œÑ* is a
temperature parameter.
**Prompt Tuning.** Instead of relying on manually crafted
prompts, prompt tuning methods optimize prompts to improve
performance on downstream tasks. Specifically, prompt tuning
learns a prompt **p** = [ *V* ] 1 [ *V* ] 2 *...* [ *V* ] *M* *‚àà* R *[M]* *[√ó][d]* in the text
embedding space, where *M* is the number of tokens and *d*
is the embedding size. Given training data *D* train = *{* ( **x** *i* *, y* *i* ) *}*
from the downstream task, the objective is to generate text
inputs of the form ‚Äú[ *V* ] 1 [ *V* ] 2 ...[ *V* ] *M* [CLASS]‚Äú that provide the
model with the most relevant context information. For image
classification with cross-entropy loss *CE*, this optimization
can be formulated as:

**p** *[‚àó]* = arg min **p** [E] [(] **[x]** *[,y]* [)] *[‚àºD]* [train] *[CE]* [(] *[p]* [(] *[y][|]* **[x]** [)] *[, y]* [)] *[.]* (2)

**Taxonomy.** In this survey, we introduce a taxonomy that systematically categorizes unsupervised VLM adaptation methods
based on the availability of unlabeled visual data during the
adaptation process (Fig. 1). The proposed framework defines
four distinct adaptation paradigms, each with unique assumptions and challenges. The first, data-free transfer, represents
the most constrained setting where no visual data from the
downstream task is available. In contrast, unsupervised domain
transfer assumes access to an abundant, static collection of
unlabeled target data, enabling a more comprehensive, offline
adaptation before inference. The final two categories address
adaptation that occurs during the testing phase itself. Episodic
test-time adaptation operates on a small batch of test instances,
adapting the model specifically for that batch. Lastly, online test-time adaptation tackles the most dynamic scenario,
where the model must continuously learn from a sequential
stream of incoming data points and update itself in realtime. This taxonomy highlights the fundamental differences in
data access, computational constraints, and algorithmic design
across the spectrum of unsupervised VLM adaptation. In the
following sections, we provide a detailed overview of existing
approaches within each paradigm.

IV. D ATA -F REE T RANSFER

**Paradigm description.** Data-free transfer in the context of
VLMs refers to adapting pre-trained models to downstream
tasks *without access to any visual data (e.g., images) from the*
*downstream task* . This setting is particularly challenging, as it


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4


|External dataset Retrieval Class Names Generation Support set Generative models|Col2|Col3|
|---|---|---|
||T text features I image feature prediction||


***Inference Stage***












|Rich descriptions Class query LLMs (e.g., GPT-3, Subclass names Names ChatGPT)|Col2|Col3|
|---|---|---|
|Class hierarchy|||
||T text features prediction I image feature||


***Inference Stage***


|I|Col2|Col3|Mat|Mul x|Col6|Col7|
|---|---|---|---|---|---|---|
||||softma||||
||Class T Names text features prediction I image feature||||||


***Inference Stage***


(a) Text augmentation.


(b) Image utilization.


(c) Network modification.


Fig. 3: Three representative strategies of the **data-free transfer** paradigm.


TABLE I: Popular strategies along with their representative
works of **data-free transfer** .

**Strategies** **Representative Works**

DCLIP [15], CuPL [21],
**Text Augmentation**
CHiLS [88], TaI [89].

ReCo [28], SuS-X [90],
**Image Utilization**
Priming [91], GenCL [92].

MaskCLIP [16], CALIP [93],
**Network Modification**
SCLIP [94], ProxyCLIP [95].

relies exclusively on textual category names to guide the adaptation process. As such, data-free transfer is considered the
most difficult paradigm within unsupervised VLM adaptation.
Despite these difficulties, methods developed for this setting
are often highly generalizable and broadly applicable, offering
robust solutions for a variety of unsupervised tasks across
domains where visual data is scarce, sensitive, or unavailable.
We review existing data-free transfer methods and categorize their strategies into three primary approaches: text
augmentation, image utilization, and network modification.
These categories are summarized in Table I, and we introduce
each strategy in detail along with related methods in the
following subsections.

*A. Text Augmentation*

In data-free transfer paradigm, where only class names are
available, direct inference results in the rich semantic capacity
of the text encoder remaining underexploited. To mitigate this
limitation, several methods have been proposed that enhance
the textual input through text augmentation, aiming to generate
more informative representations, as shown in Fig. 3 (a). These
augmented texts help unlock the latent knowledge of the text
encoder, thereby improving model performance despite the
absence of visual data.

Leveraging the powerful capabilities of LLMs, such as
GPT-3 [96], several data-free transfer methods [15], [21],

[97] have adopted text augmentation strategies to enrich class
representations with more informative and discriminative descriptions. These approaches aim to replace simple class names


with richer textual content, thereby improving alignment with
visual concepts. For instance, DCLIP [15] and CuPL [21]
use GPT-3 to generate multiple semantic descriptors and full
descriptive sentences, injecting discriminative knowledge into
category representations. REAL-Prompt [98] identifies performance drops for categories with low-frequency terms in the
pretraining corpus and addresses this by prompting ChatGPT

[99] to substitute them with higher-frequency synonyms that
are more familiar to the encoder. MPVR [97] introduces a
two-step prompting mechanism, where LLMs first generate
task-relevant queries that are then used to derive categoryspecific prompts, improving both relevance and diversity. In
domain-specific applications like medical image diagnosis,
ChatGPT [99] is used to generate symptom-based descriptions
of disease classes [31]. Moreover, Parashar et al. [100] show
that replacing scientific species names with common English
terms improves classification performance. Interestingly, reliance on LLMs is not strictly necessary. WaffleCLIP [101]
demonstrates that even random word augmentations to class
names can yield results comparable to those generated with
LLMs. In a complementary direction, TAG [102] proposes an
out-of-distribution (OOD) detection approach that leverages a
scoring mechanism based on permuted prompt templates. This
score captures consistency of model predictions across varied
phrasings and enables more accurate detection.

Extending this line of work, existing studies [32], [88],

[103], [104] have also explored the use of subclass names as
an alternative to rich textual descriptions, aiming to describe
the semantic scope of each category more precisely. CHiLS

[88] leverages GPT-3 [96] to generate subclass names for
each original class and makes final predictions by aggregating
similarities between the image and both the superclass and its
associated subclasses. In the context of semantic segmentation,
subclass prompts allow for finer-grained, patch-level alignment
with the target superclass, leading to measurable improvements
in performance [103]. For video-based action recognition,
TEAR [32] decomposes complex actions into multiple subactions, generating concise descriptions for each and forming
a robust composite representation by averaging their features.
Beyond subclassing, other approaches enrich semantic understanding through category-related attributes, helping to capture


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5


nuanced intra-class variation and enhance image recognition
accuracy [104]. Furthermore, EOE [105] extends this strategy
to OOD detection by prompting LLMs to generate potential
OOD category names, thereby broadening the model‚Äôs recognition capacity beyond the training distribution.
Rather than classifying objects across all possible categories, some approaches [106]‚Äì[108] simplify the task by
organizing classes into clusters or hierarchical structures, decomposing a complex classification problem into a sequence
of hierarchical sub-tasks. An early work [106] constructs hierarchical clusters of candidate categories and employs ChatGPT

[99] to generate group-specific, discriminative textual descriptions. More recently, Lee et al. [107] leverage textual feature
similarity to identify semantically similar classes and then
prompt an LLM to generate visual descriptors that distinguish
a class from its nearest semantic neighbors. Moving beyond
accuracy alone, HAPrompts [108] introduces a hierarchical
classification framework to encourage the model to promote
better mistakes, encouraging the model to predict semantically
related labels when misclassifications occur.
Another line of data-free transfer methods [89], [109], [110]
leverage external textual data as a training signal to guide
models toward more robust task performance. For example,
TaI [89] replaces annotated images with rich textual descriptions for prompt tuning, introducing dual-grained prompts that
capture both global semantic context and local discriminative
features. In a related approach, TAP [109] constructs classspecific textual descriptions and trains a text-only classifier
using cross-entropy loss. This classifier is then integrated with
a visual encoder at inference time to enhance recognition
accuracy. Going a step further, ProText [110] optimizes deep
prompt parameters to steer the text encoder toward extracting
meaningful representations from LLM-generated descriptions,
which embed extensive linguistic knowledge and fine-grained
conceptual distinctions.

*B. Image Utilization*

In the absence of visual data, methods that rely solely on
textual information face inherent limitations, primarily due to
the modality gap that exists within VLMs. To bridge this gap,
a growing body of research [28], [91], [111] introduces visual
signals by either retrieving relevant images from external
datasets or synthesizing them using generative models, as
shown in Fig. 3 (b).
Retrieval-based methods attempt to provide visual grounding by leveraging large-scale unlabeled datasets. For example,
ReCo [28] retrieves semantically relevant images using CLIP

[1] from an external corpus and computes a reference image
embedding for each category. This embedding is then used
to guide the recognition of corresponding image patches and
refine the original zero-shot dense predictions. Neural Priming

[91] introduces a new classification head by computing the
centroids of retrieved image sets for each category from
the pre-training dataset, subsequently integrating this head
with the original zero-shot classifier to enhance recognition.
Generative approaches extend this direction by synthesizing
visual data to simulate examples for downstream tasks. For


instance, Shipard et al. [111] construct a synthetic training
set using diffusion models [49], generating a diverse set
of images that provide rich visual cues in the absence of
real data. AttrSyn [112] further boosts image diversity by
leveraging LLMs to generate a wide range of attributes, which
guide the generative model to produce class-consistent and
discriminative samples. SuS-X [90] combines both retrieval
and generation strategies by introducing a visual support set,
either constructed from large-scale datasets or generated via
advanced diffusion models [49]. This support set enables
information integration and provides auxiliary supervision
during inference. Finally, in the context of continual learning,
GenCL [92] generates synthetic images for novel classes
using prompt-guided diffusion models, and then introduces
an ensemble-based selector to curate a representative coreset
from the generated samples, supporting robust and effective
category representation over time.

*C. Network Modification*

Several data-free methods [16], [29], [93], [113] focus on
modifying the network architecture of VLMs to enhance their
suitability for downstream tasks, particularly dense prediction
such as segmentation, as shown in Fig. 3 (c). While these
approaches are primarily developed for classification-oriented
VLMs, their architectural enhancements significantly improve
dense prediction performance.
A pioneering work, MaskCLIP [16], demonstrates that value
embeddings in the final attention layers capture richer local
information than global features, making them particularly
effective for segmentation tasks. To further refine dense predictions from value embeddings, MaskCLIP introduces a keybased smoothing strategy and a denoising technique. Building
on this, CALIP [93] facilitates interaction between visual and
textual features through a parameter-free attention module,
and achieves improved classification results by ensembling
outputs from multiple feature representations. CLIP Surgery

[114] advances segmentation by introducing value-value attention to enhance local feature consistency and employing
a feature surgery strategy to suppress noisy activations, thus
improving both segmentation accuracy and interpretability.
GEM [115] generalizes value-value attention to an any-any
attention mechanism, enhancing consistency across groups
of similar tokens by ensembling outputs from the modified
attention applied to key, query, and value embeddings at every
transformer layer. SCLIP [94] introduces a correlative selfattention mechanism, which yields spatially covariant features
that better preserve fine local details. Finally, ProxyCLIP [95]
presents a training-free framework that enhances CLIP‚Äôs openvocabulary segmentation by integrating spatially consistent
proxy attention maps generated from vision foundation models
such as DINO [116] and SAM [117].

V. U NSUPERVISED D OMAIN T RANSFER

**Paradigm description.** Unsupervised domain transfer for
VLMs refers to the adaptation of pre-trained models to
downstream tasks with *abundant unlabeled data* . Compared
with data-free transfer, unsupervised domain transfer can use


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6










|Col1|C Na|las me|s s|Col5|
|---|---|---|---|---|
||||||

|Class ame|Col2|s|Col4|
|---|---|---|---|
|||||
|||||
|L|L|Ms||
|||||




|Col1|Col2|Col3|Pseudo-labelin Class T Names reference predictio pseudo label ùíö‡∑ù= ùíÇùíìùíàùíéùíÇùíô I ‚Ñí abundant prediction ùíë unlabeled data|Col5|Col6|Col7|Col8|Col9|Col10|u|Entropy Optimization Class T Names ùë™ ùíéùíäùíè‚ÑíùíîùíÜ ùíÇùíè ùíéùíï ùíëùíçùíÜ= ‡∑çùíëùíÑùíçùíêùíàùíëùíÑ ùíÑ ùë™ ùíéùíÇùíô‚ÑíùíÑùíÜ ùíçùíè ùíÇùíï ùíîùíî = ‡∑çùíë‡¥•ùíÑùíçùíêùíàùíë‡¥•ùíÑ ùíÑ I prediction ùíë abundant nlabeled data|Col13|Col14|Col15|N a unla|Class T T+ I+ ames LLMs MLLMs ‚Ñí ùë∫ùëª ‚Ñí ùë≤ùë´ I prediction bundant beled data|Col18|Col19|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||g n ùíë‡∑ù ùíë‡∑ù||||||||||||
|||||N|Clas am|s|||||||||||||
|||||||es|||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||ùë∫ùëª||||||||||||


(a) Self-training.



(b) Entropy optimization.


(c) External resource utilization.


Fig. 4: Three representative strategies of the **unsupervised domain transfer** paradigm.


TABLE II: Popular strategies along with their representative
works of **unsupervised domain transfer** .

**Strategies** **Representative Works**

UPL [17], LaFTer [118],
**Self-Training**
MUST [26], ReCLIP [119].

POUF [34], CDBN [120],
**Entropy Optimization** UEO [27], TFUP-T [121].

**External Resource Utilization** Neural Priming [91], PEST [122],
PromptKD [123], OTFusion [124].

FARE [125], OTTER [126]
**Miscellaneous**
ZLaP [35], TransCLIP [127].

the unlabeled data of downstream tasks to better grasp the
data distribution and thus achieve better performance. The
challenges of this paradigm mainly come from the filtering and
processing of unlabeled data and the unsupervised alignment
of VLM and unlabeled data.

We review existing unsupervised domain transfer methods
and categorize their strategies into three primary approaches:
self-training, entropy optimization, and external resource utilization. These categories are summarized in Table II, and we
introduce each strategy in detail along with related methods
in the following subsections.

*A. Self-Training*

Self-training is a widely used strategy in unsupervised
learning, as shown in Fig. 4 (a), where the ground truth of
the training data is absent. Using this approach, unsupervised
algorithms always manage to calculate high-quality pseudo
labels on unlabeled samples as supervision signals. How to
obtain and iteratively refine pseudo labels to make VLM better
adapts to the distribution of the unlabeled data is the key
challenge for those methods.
UPL [17] is one of the earliest efforts to explore unsupervised domain transfer for VLMs. It selects a small set of highconfidence unlabeled samples for each category and optimizes
the prompt parameters using a pseudo-labeling strategy,

arg min **p** [E] [(] **[x]** *[,][y]* [ÀÜ][)] *[‚àºD]* [select] *[L]* *[CE]* [(] *[p]* [(] **[x]** [)] *[,]* [ ÀÜ] *[y]* [)] *[,]* (3)

where *D* select represents the selected high-confidence samples
and *L* *CE* denotes the cross-entropy loss. This selective pseudolabeling approach is later adopted by a number of subsequent


works [19], [120], [128], [129]. SwapPrompt [19] extends this
self-training strategy with another swapped prediction mechanism, letting the two augmented views of the same image
provide soft pseudo-label optimization supervision through
an EMA-updated prompt for each other. RS-CLIP [130]
introduces a curriculum learning framework, beginning with
a small subset of high-confidence samples for self-training
and progressively incorporating more data as optimization
progresses, thus mitigating the noise from early-stage pseudo
labels. GTA-CLIP [131] proposes a transductive inference approach to pseudo-labeling, using iteratively refined, attributeaugmented similarities between image and text embeddings
to improve label quality. In a related approach, CPL [132]
refines candidate pseudo labels by introducing both intra- and
inter-instance label to reduce the negative impact of incorrect
hard pseudo labels typically produced by VLMs. Another
work [133] systematically investigates pseudo-labeling strategies across several unsupervised settings and demonstrates the
effectiveness of pseudo-labeling in promoting more balanced
and robust performance across diverse categories.
Inspired by FixMatch [134], several unsupervised domain
transfer methods [118], [120] apply both weak and strong
augmentations to unlabeled data to enhance consistency learning in VLMs. These approaches typically generate pseudo
labels using the weakly augmented views and consider them as
supervisory signals for self-training on the strongly augmented
ones. LaFTer [118] leverages large language models (LLMs)
to generate diverse textual data for training a text classifier,
which in turn produces high-quality pseudo labels with weak
augmented views for effective self-training. MedUnA [30]
proposes a dual-branch architecture, consisting of weak and
strong branches for the visual encoder, and jointly optimizes
them using a pseudo-labeling objective to enhance medical
image classification. NoLA [129] employs a DINO-based
labeling network fed with weak augmentations to improve
pseudo-label quality for training visual prompts. DPA [135]
introduces dual prototype representations for both visual and
textual branches, integrating their outputs to generate more
robust pseudo labels. Additionally, LP-CLIP [136] incorporates confidence estimates into the pseudo-labeling objective,
thereby improving both classification accuracy and calibration.
Rather than filtering high-confidence samples and enhancing
consistency between different augmentations, there are also
some methods that generate pseudo labels in other ways.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7


MUST [26] maintains an EMA model to produce high-qulity
pseudo labels and incorporates a masked image modeling
strategy to improve local image representation learning. PEST

[122] enhances pseudo-label quality by ensembling predictions
from multiple textual prompts and visually augmented views.
ReCLIP [119] learns a projection space to better align visual
and textual features and employs self-training with pseudo
labels refined using Label Propagation [137]. NtUA [138]
constructs a confidence-weighted key-value cache of pseudolabeled features and refines it through knowledge distillation,
effectively mitigating label noise in scenarios with limited
unlabeled data. Similarly, TFUP-T [121] improves pseudolabel quality by building a cache model with representative
samples and refining predictions based on both feature-level
and semantic-level similarities. To address the issue of low
confidence pseudo labels, FST-CBDG [139] employs soft
pseudo labels and updates them using a moving average
strategy during self-training. For regression tasks, CLIPPR

[22] trains an adapter for the image encoder using zeroshot pseudo labels, optimizing performance by minimizing the
distance between predicted and prior label distributions.

*B. Entropy Optimization*

Entropy optimization is a classic unsupervised learning objective, encouraging the model to make confident predictions
on unlabeled data, as shown in Fig. 4 (b). Unlike self-training,
entropy optimization is not affected by erroneous pseudolabels and behaves more stably on low-performance tasks.
Many algorithms minimize sample-level entropy, adapting the
model to the unlabeled data distribution [34], [120], and
also maximize category-level marginal entropy to avoid mode
collapse [34], [120], [121].
POUF [34] and CDBN [120] optimize textual prompt
parameters with sample-level entropy minimization and
category-level marginal entropy maximization. An optimal
transport objective is also incorporated into POUF for better
alignment between the distribution of textual prototypes and
the unlabeled data. In order to improve both generalization
and out-of-distribution detection performance of VLMs, UEO

[27] proposes universal entropy, utilizing marginal prediction
instead of sample prediction for entropy maximization to
stabilize the optimization process.

*C. External Resource Utilization*

Several recent approaches enhance the performance of
VLMs by incorporating external resources beyond the available unlabeled data. These resources often include retrieval
based image augmentation, introduction of (multimodal) large
language models (MLLMs), and knowledge distillation from
powerful VLMs or vision models, as shown in Fig. 4 (c).
Neural Priming [91] adopts a transductive learning paradigm
by constructing a retrieval set of images based on category
names. For each unlabeled sample, it selects the most visually
similar images to form a fine-tuning dataset, thereby adapting
the VLM to the target domain. LaFTer [118] leverages GPT-3

[96] to generate diverse textual descriptions, which are then
used to train a text classifier tailored for the downstream task.


Similarly, PEST [122] and GTA-CLIP [131] query LLMs such
as GPT-3 [96] and LLaMA [140] to create multiple prompts
per class. These prompts improve pseudo-label quality through
ensemble-based prompt inference. LatteCLIP [141] utilizes
LLaVA [4] to generate image captions, which support more
accurate textual prototype construction for VLM adaptation.
In the context of knowledge distillation, PromptKD [123]
reuses textual features from a larger teacher VLM to guide
the training of an image encoder, thereby transferring semantic knowledge. Going a step further, KDPL [142] jointly
optimizes prompts in both visual and textual input spaces,
balancing performance and efficiency. NtUA [138] improves
pseudo-label reliability by incorporating the image encoder of
a stronger VLM, enhancing both label quality and confidence
estimation. OTFusion [124] aligns VLMs‚Äô embedding with
features extracted from powerful vision models (e.g., DINO)
via optimal transport to obtain refined predictions.

*D. Miscellaneous*

There are also several recent approaches that address unsupervised domain transfer for VLMs through diverse strategies

[125], [143]. ZPE [144] designs a prompt ensembling strategy
that leverages unlabeled data to address frequency biases
in words and concepts and assigns appropriate ensembling
weights to multiple prompt templates. uCAP [145] formulates
image generation as a function of class names and latent,
domain-specific prompts. It employs an energy-based likelihood framework to infer optimal prompts from unlabeled
data. To enhance adversarial robustness while preserving performance on clean inputs, FARE [125] optimizes the vision
encoder to align the features of adversarially perturbed images
with those of their clean counterparts as computed by the original VLM. OTTER [126] addresses label distribution mismatch
by leveraging optimal transport to align model predictions
with an estimated label distribution in the target domain.
Moreover, InMaP [146] learns class proxies directly in the
vision space using refined pseudo labels derived from text
embeddings, thereby narrowing the modality gap between visual and textual representations in VLMs. Subsequent methods
exploit various strategies for modeling vision-text features,
including label propagation [35] and Dirichlet distributions

[147]. TransCLIP [127] proposes a plug-and-play transductive
framework that optimizes a KL-regularized objective with an
efficient block Majorize-Minimize algorithm, integrating the
text-encoder knowledge together.

VI. E PISODIC T EST -T IME A DAPTATION

**Paradigm description.** Episodic test-time adaptation is a
popular learning paradigm in which a pre-trained VLM is
*adapted at inference time using a single batch of unlabeled*
*test data* . The goal is to leverage the knowledge embedded
in the pre-trained VLM to accurately predict labels for the
current batch, without requiring access to multiple test batches
or labeled data during adaptation.
We review existing episodic test-time adaptation methods
and categorize their strategies into four primary approaches:
entropy minimization, feedback signal, distribution alignment,


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8






**learnable**

**prompt**















∆∏ùúá 1 **,** ‡∑úùúé 12 ∆∏ùúá 2, ‡∑úùúé 22 ∆∏ùúá 3, ‡∑úùúé 32

*Offline Computed Source Data Statistics*

(c) Distribution alignment.


**a batch of unlabeled data**

(a) Entropy minimization.


**a batch of unlabeled data**

(b) Feedback signal.


Fig. 5: Three representative strategies of the **episodic test-time adaptation** paradigm.


TABLE III: Popular strategies along with their representative
works of **episodic test-time adaptation** .

**Strategies** **Representative Works**

TPT [18], DiffTPT [85],
**Entropy Minimization**
R-TPT [148], DTS-TPT [33].

Diffusion-TTA [149],
**Feedback Signal**
RLCF [150], BPRE [151] .

PromptAlign [24], MTA [36],
**Distribution Alignment**
TAPT [152], StatA [153].

Self-TPT [154], InCPL [155],
**Self-Supervised Learning**
LoRA-TTT [156], T3AL [157].

AWT [158], RA-TTA [159],
**Miscellaneous**
SCAP [160], ZERO [161].

and self-supervised learning. These categories are summarized
in Table III, and we introduce each strategy in detail along with
related methods in the following subsections.

*A. Entropy Minimization*

Entropy minimization is a widely adopted strategy for
TTA [40] by adjusting the model‚Äôs parameters to make its output predictions more confident with lower entropy, as shown
in Fig. 5 (a). This process encourages the model to produce
low-uncertainty outputs for the test data, often improving its
performance under distribution shifts.
Shu et al. [18] introduced test-time prompt tuning (TPT)
as the first method for adapting pre-trained VLMs at test
time. TPT optimizes a text prompt **p** = [ *V* ] 1 [ *V* ] 2 *...* [ *V* ] *M* for
each test sample using entropy minimization, combined with
confidence selection to ensure consistent predictions across
augmented views. Specifically, TPT generates *N* randomly
augmented views of a test image **x** using a set of random
augmentations *A* and minimizes the entropy of the averaged
prediction probability distribution:


mask I[ **H** ( *p* *i* ) *‚â§* *Œ∑* ], where **H** denotes the entropy of predictions on augmented samples. Instead of applying random
augmentations as in TPT, DiffTPT [85], [162] leverages pretrained diffusion models to generate diverse augmentations and
employs cosine similarity-based filtering to remove spurious
samples. R-TPT [148] employs a reliability-based weighted
ensembling strategy to aggregate information from trustworthy
augmented views of the test sample. C-TPT [163] optimizes
prompts by maximizing text feature dispersion, observing that
better-calibrated predictions correlate with higher text feature
dispersion. O-TPT [164] improves calibration by enforcing
orthogonality constraints on class-specific textual prompt features during tuning to maximize their angular separation.
Furthermore, DTS-TPT [33] extends TPT to video data for
zero-shot activity recognition.
Beyond tuning text prompts to minimize entropy, several works also explore visual prompts [165], multimodal
prompts [24], low-rank attention weights [166], and learnable noise [167]. For example, PromptAlign [24] uses multimodal prompt learning to align image token distributions
between a pre-computed source proxy dataset and test samples.
TTL [166] adapts low-rank attention weights during test
time through a confidence maximization objective, enabling
efficient adaptation without altering prompts or backbone

parameters.

*B. Feedback Signal*

Some studies explore leveraging feedback signals from
diffusion [149] or CLIP-like models [150], [151] for TTA,
as shown in Fig. 5 (b). For example, Diffusion-TTA [149]
leverages generative feedback from diffusion models to adapt
pre-trained discriminative models at test time by optimizing
image likelihood, significantly improving performance across
tasks like classification, segmentation, and depth prediction.
Diffusion-TTA consists of discriminative and generative modules. Given an image **x**, the discriminative model *f* *Œ∏* predicts task output *y* (Eq. (1) for VLMs). The task output
*y* is transformed into condition **c** . For image classification,
*y* represents a probability distribution over *C* categories,
*y ‚àà* [0 *,* 1] *[C]* *, y* *[‚ä§]* **1** *C* = 1. Given the learned text embeddings
of a text-conditional diffusion model for the *C* categories
**t** *j* *‚àà* R *[d]* *, j* *‚àà{* 1 *...C}*, the diffusion condition is **c** =
ÔøΩ *Cj* =1 *[y]* *[j]* *[ ¬∑]* **[t]** *[j]* [. Finally, the generative diffusion model] *[ œµ]* *[œï]* [ is used]
to measure the likelihood of the input image, conditioned on
**c** . This consists of using the diffusion model *œµ* *œï* to predict


**p** *[‚àó]* = arg min **p** *[‚àí]*

*p* Àú **p** ( *y* *i* *|* **x** ) = *œÅN* 1


*C*
ÔøΩ *p* Àú **p** ( *y* *i* *|* **x** ) log Àú *p* **p** ( *y* *i* *|* **x** ) *,* (4)

*i* =1

*N*
ÔøΩ I[ **H** ( *p* *i* ) *‚â§* *Œ∑* ] *p* **p** ( *y|A* *i* ( **x** )) *.* (5)

*i* =1


Here, *p* **p** ( *y|A* *i* ( **x** )) represents the class probability vector for
the *i* -th augmented view of **x** under prompt **p** . TPT selects *œÅ* percentile confident samples with a prediction entropy below a
threshold *Œ∑* to filter out noisy predictions, using a confidence


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9


the added noise *œµ* from the noisy image **x** *t* and condition **c** .
The image likelihood is maximized using diffusion loss by
updating the discriminative and generative model weights via
backpropagation:


*L* = *‚àí*


encouraging the model to learn more robust class representations. The contrastive loss is formulated as:



*¬∑* **t** *j*

*œÑ* ÔøΩ


ÔøΩ


4 *C* **t** *i* *¬∑* **t** *j*
ÔøΩ *j* =1 *,j* = *Ã∏* *i* [exp] ÔøΩ *œÑ*


**t** *i* *¬∑* **t** *j*
*j‚ààP* ( *i* ) [exp] ÔøΩ *œÑ*


*i* *¬∑* **t** *j* *,* (7)

*œÑ* ÔøΩ


4 *C*
ÔøΩ log

*i* =1


*L* diff = E *t,œµ* *‚à•œµ* *œï* ( *[‚àö]* *Œ±* ¬Ø *t* *x* + *‚àö*


1 *‚àí* *Œ±* ¬Ø *t* *œµ,* **c** *, t* ) *‚àí* *œµ‚à•* [2] *,* (6)


where ¬Ø *Œ±* *t* defines how much noise is added at each time step *t* .
Differently, RLCF [150] utilizes CLIP-based feedback through
reinforcement learning and employs CLIPScore [168] as a
reward signal to provide feedback for VLMs. BPRE [151]
mitigates text-conditioned bias by using a quality-aware reward module based on intrinsic visual features, forming a selfevolving feedback loop with prototype refinement to enhance
adaptation to distribution shifts.

*C. Distribution Alignment*

Distribution alignment methods align test sample distributions with known source characteristics or refine representations for improved consistency [169], as shown in Fig. 5
(c). For example, PromptAlign [24] bridges the source-totarget distribution gap by jointly updating multimodal prompts
to align the per-layer image-token statistics of augmented
test views with offline-computed source statistics through
a combined alignment and entropy minimization loss. To
enhance adversarial robustness, TAPT [152] adapts statistical
alignment by using a loss function that, at inference, aligns
the augmented visual embeddings of a test sample with precomputed statistics from both clean and adversarially perturbed images. StatA [153] preserves text encoder knowledge
during adaptation by utilizing statistical anchors that penalize
deviations from text-derived Gaussian priors.
Complementary to these approaches, MTA [36] employs
a robust MeanShift algorithm to identify density modes in
the feature space while concurrently optimizing them with an
inlierness score to automatically assess each view‚Äôs quality.
Beyond global distribution alignment, several methods focus
on class-aware prototype alignment. PromptSync [170] performs class-aware prototype alignment of the test sample with
source class prototypes, weighted by mean class probabilities
derived from confident augmented views. Also utilizing class
prototypes, TPS [169] pre-computes class prototypes and then,
for each test sample, dynamically learns shift vectors to adjust
these prototypes directly within the shared embedding space.

*D. Self-Supervised Learning*

Self-supervised learning [171], [172] is a powerful technique for learning transferable representations. Self-TPT [154]
introduces contrastive prompt tuning as a self-supervised
learning strategy, which aims to minimize intra-class distances while maximizing inter-class separation by leveraging
contrastive learning principles. Specifically, for each class
token, multiple prompt variations are generated by altering
the insertion point of the class token (e.g., beginning, middle,
or end of the prompt sequence). This creates positive pairs
from the same class and negative pairs from different classes,


where **t** *i* and **t** *j* are the projected text features of different
views, *P* ( *i* ) denotes the set of positive samples for view *i*, and
*œÑ* is a temperature parameter. In contrast, LoRA-TTT [156]
updates only the low-rank parameters in the image encoder
using a memory-efficient reconstruction loss, computed as
the mean squared error of class tokens from top-confidence
augmented and masked views, to enhance global feature understanding. In addition, InCPL [155] enables efficient model
adaptation by optimizing visual prompts from a few labeled
examples through a context-aware unsupervised loss and a
cyclic learning strategy. T3AL [157] generates and refines
temporal action proposals by first deriving video-level pseudolabels from a pretrained VLM, then using a self-supervised
method to create initial proposals, and finally enhancing them
with frame-level textual descriptions.

*E. Miscellaneous*

Beyond the previously discussed approaches, additional
techniques have been developed for episodic test-time adaptation of VLMs [173]‚Äì[180]. One line of work uses
retrieval-based strategies [159], [181], [182]. For instance, XMoRe [181] retrieves relevant captions via a two-step crossmodal retrieval and ensembles image and text predictions
using dynamically weighted modal-confidence scores. RATTA [159] utilizes fine-grained text descriptions to guide
a two-step retrieval of relevant external images, which are
then used in a description-based adaptation process to refine
the model‚Äôs initial prediction. Another line of work tries
to improve the calibration of VLMs [161], [163], [164],

[183]. Besides retrieval and calibration, other representative
work includes optimal transport [158], spurious features erasing [184], loss landscape [185], counterattack [186], and
supportive cliques [160]. Several methods focus on improving
CLIP‚Äôs dense prediction abilities for open-vocabulary semantic
segmentation by addressing its image-level pre-training limitations [28], [29], [103], [115], [187]‚Äì[189]. The external
knowledge, such as MLLMs and LLMs, can also be used
during inference, without requiring additional training or finetuning on task-specific data [190]‚Äì[197].

VII. O NLINE T EST -T IME A DAPTATION

**Paradigm description.** Online test-time adaptation is another TTA paradigm designed for *streaming data scenarios,*
*where unlabeled data arrives sequentially in mini-batches* .
Given a pre-trained VLM, the objective is to adapt the model
online to each incoming mini-batch in order to accurately
predict its labels under potential distribution shifts. Unlike
episodic adaptation, which adapts to each batch independently, online adaptation continuously updates the model by
leveraging knowledge accumulated from previously observed


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10






**learnable**

**prompt**



memory




feature




(a) Pseudo-labeling.


**unlabeled data**


(b) Memory mechanisms. (c) Distribution modeling.


Fig. 6: Three representative strategies of the **online test-time adaptation** paradigm.


TABLE IV: Popular strategies along with their representative
works of **online test-time adaptation** .

**Strategies** **Representative Works**

DART [198], CLIPArTT [199],
**Pseudo-Labeling**
CLIP-OT [200], WATT [201].

TDA [25], DMN [202],
**Memory Mechanisms**
DPE [203], BaFTA [204].

OGA [205], DOTA [206],
**Distribution Modeling**
BCA [207], DN [208].

DynaPrompt [209], TCA [210],
**Miscellaneous**
ECALP [211], OnZeta [23].

mini-batches. This enables more effective and efficient label
prediction in dynamic, streaming environments.
We review existing online test-time adaptation methods
and categorize their strategies into three primary approaches:
pseudo-labeling, memory mechanisms, and distribution modeling. These categories are summarized in Table IV, and we
introduce each strategy in detail along with related methods
in the following subsections.

*A. Pseudo-Labeling*

Pseudo-labeling assigns class labels to unlabeled test samples and optimizes the cross-entropy loss between predictions
and pseudo-labels to guide model adaptation, as shown in
Fig. 6 (a). However, due to distribution shifts, pseudo-labels
may be noisy, which can negatively impact learning. Various
methods have been proposed to mitigate this issue. Many
methods refine the pseudo-labeling process itself; for instance,
IST [212] employs graph-based correction and non-maximum
suppression for pseudo-label refinement, stabilizing updates
with parameter moving averages. Others, like CLIPArTT [199]
dynamically construct text prompts from top- *K* predicted
classes to serve as pseudo-labels, while CLIP-OT [200] utilizes optimal transport for label assignment alongside multitemplate knowledge distillation. CTPT [213] focuses on iterative prompt updates guided by stable class prototypes
and accurate pseudo-labels. SwapPrompt [19] proposes a
dual-prompt and swapped prediction mechanism for efficient
prompt adaptation.
Several approaches enhance pseudo-labeling by integrating
it with other mechanisms. For instance, SCP [214] uses
self-text distillation with conjugate pseudo-labels to improve
robustness and minimize overfitting. WATT [201] combines
diverse text templates, pseudo-label-based updates with peri

odic weight averaging, and text ensembling. To handle noisy
target data, AdaND [215] introduces an adaptive noise detector
trained with pseudo-labels from a frozen model to decouple
noise detection from classification. DART [198] learns adaptive multimodal prompts (class-specific text and instance-level
image) while retaining knowledge from prior test samples.
ROSITA [216] employs a contrastive learning objective with
dynamically updated feature banks to enhance the discriminability of OOD samples. Finally, TIPPLE [217] adopts a
two-stage approach, first using online pseudo-labeling with an
auxiliary text classification task and diversity regularization
for task-oriented prompt learning, then refining this task-level
prompt with a tunable residual for each test instance.

*B. Memory Mechanisms*

Memory-based methods leverage dynamic or static memory
structures to store and retrieve feature representations and
pseudo-labels from test samples, as shown in Fig. 6 (b).
These methods enable progressive refinement of predictions by
utilizing confident outputs and historical information, enhancing robustness and adaptability without requiring extensive
retraining or backpropagation [218]‚Äì[221]. Inspired by TipAdapter [222], Karmanov et al. [25] propose a training-free
dynamic adapter (TDA) without requiring backpropagation.
The core of TDA is a dynamic key-value cache system
that stores pseudo-labels and corresponding feature representations from test samples. This cache enables progressive
refinement of predictions by leveraging confident test-time
outputs, facilitating efficient adaptation. Similarly, DMN [202]
leverages static memory for training data knowledge and
dynamic memory for online test feature preservation. BoostAdapter [223] leverages a lightweight key-value memory to
retrieve features from instance-agnostic historical samples and
instance-aware boosting samples. HisTPT [224] constructs
three complementary knowledge banks‚Äîlocal, hard-sample,
and global‚Äîto preserve useful information from previously
seen test samples. AdaPrompt [225] introduces a confidenceaware buffer that stores and utilizes only class-balanced, highconfidence samples to ensure the prompt updates are robust
and stable.

Other works utilize dynamically evolving class prototypes to
capture accurate multimodal representations during inference.
By continuously updating these prototypes from unlabeled test
samples, these methods enhance model adaptability, robustness, and efficiency [226], [227]. For example, DPE [203]
simultaneously evolves two sets of prototypes‚Äîtextual and


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11


visual‚Äîto progressively capture accurate multimodal representations for target classes during test time. BaFTA [204]
uses backpropagation-free online clustering to estimate class
centroids and robustly aggregate class embeddings with visualtext alignment, guided by entropy-based reliability for improved zero-shot performance. BATCLIP [228] introduces a
projection matching loss to improve alignment between visual
class prototypes and text features, and a separability loss
to increase the distance between these prototypes for more
discriminative features.

*C. Distribution Modeling*

Distribution modeling methods model the distribution of
visual or multimodal features, often using Gaussian estimations, to refine predictions during inference [229]‚Äì[231],
as shown in Fig. 6 (c). By leveraging probabilistic frameworks and incorporating zero-shot priors, these methods enhance adaptability and robustness without requiring extensive hyperparameter tuning or backpropagation. For instance,
OGA [205] models the likelihood of visual features using
multivariate Gaussian distributions and incorporates zero-shot
priors within a maximum a posteriori estimation framework.
Similarly, DOTA [206] estimates Gaussian class distributions
to compute Bayes-based posterior probabilities for adaptation,
achieves fast inference without gradient backpropagation, and
incorporates a human-in-the-loop mechanism to handle uncertain samples and enhance test-time performance. BCA [207]
continuously updates text-based class embeddings to align
likelihoods with incoming image features and concurrently
refines class priors using the resulting posterior probabilities.
On the other hand, DN [208] approximates negative sample
information using the mean representation of test samples,
enhancing alignment with the model‚Äôs optimization objective
without requiring retraining or fine-tuning.

*D. Miscellaneous*

Beyond the previously discussed approaches, additional
techniques have been developed for online test-time adaptation
of vision-language models [210], [232]‚Äì[234]. For instance,
DynaPrompt [209] mitigates error accumulation in online
adaptation by dynamically selecting and updating prompts per
test sample based on entropy and confidence scores, while
maintaining an adaptive buffer to add informative prompts
and discard inactive ones. ECALP [211] performs inference
without task-specific tuning by dynamically expanding a graph
over text prompts, few-shot examples, and test samples, using context-aware feature re-weighting to exploit the test
sample manifold without requiring additional unlabeled data.
OnZeta [23] sequentially processes test images for immediate
prediction without storage, using online label learning to
model the target distribution and online proxy learning to
bridge the image-text modality gap via class-specific vision
proxies. Besides, other representative work includes support
set [235], token condensation [210], prompt distillation [236],
and more [237]‚Äì[240].


VIII. A PPLICATIONS

*A. Object Classification*

Object classification serves as a fundamental task for
evaluating VLMs, where the objective is to assign a test
object image to one of the candidate category names. In
the context of unsupervised adaptation with VLMs, research
efforts primarily focus on two aspects: fine-grained generalization and robustness to distribution shifts. To assess finegrained classification performance, commonly used benchmark datasets include Caltech101 [241], OxfordPets [242],
StanfordCars [243], Flowers102 [244], Food101 [245], FGVCAircraft [246], SUN397 [247], DTD [248], EuroSAT [249]
and UCF101 [250]. To evaluate robustness against distributional shifts, researchers [18], [19], [101] often employ
ImageNet [251] along with its variants, such as ImageNetV2 [253], ImageNet-Sketch [255], ImageNet-A [252], and
ImageNet-R [254]. Additionally, several studies [27], [34]
incorporate datasets traditionally used in domain adaptation
to evaluate their methods, such as Office-Home [256], Office [14], and DomainNet [257].

*B. Semantic Segmentation*

Semantic segmentation aims to assign a semantic label
to each pixel in an image, playing a critical role in applications such as autonomous driving and medical image
analysis. Unsupervised segmentation methods based on VLMs
primarily focus on general and fine-grained object segmentation benchmarks, including PASCAL VOC 2012 [258], PASCAL Context [259], COCO Stuff [260], ADE20K [261], and
COCO-Object [262]. In addition, complex scene understanding datasets such as Cityscapes [263] and KITTI-STEP [264]
are often employed to evaluate the performance of unsupervised segmentation approaches. To assess the ability to identify
rare concepts, some methods [28] utilize FireNet [265]. Moreover, researchers [16] explore robustness to corruptions [285]
to find out whether segmentation algorithms preserve the
inherent robustness of VLMs. Segmentation performance is
commonly quantified using the mean intersection-over-union
(mIoU) metric.

*C. Visual Reasoning*

Context-dependent visual reasoning aims to identify
whether a test image contains a given concept, based on a
small set of support images that include both positive and
negative examples. The Bongard-HOI [266] is commonly
employed to assess the capability of VLMs to abstract the
concept of human-object interaction from a limited number of
support examples and accurately classify test samples.

*D. Out-of-Distribution Detection*

OOD detection focuses on identifying whether a test sample
belongs to an in-distribution (ID) dataset composed of candidate categories, which plays a vital role in safety-critical
applications. Based on the degree of similarity between the
OOD and ID datasets, OOD detection can be categorized into
three main types: far OOD, near OOD, and fine-grained OOD


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12

TABLE V: Overview of datasets from various tasks used in VLM-based unsupervised learning methods. (DFT=Data-Free
Transfer, UDF=Unsuperivsed Domain Transfer, ETTA=Episodic Test-Time Adaptation, OTTA=Online Test-Time Adaptation)

Dataset Task # Classes # Test sample DFT UDF ETTA OTTAPopularity

Caltech101 [241] Object Classification 100 2,465 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
OxfordPets [242] Object Classification & OOD Detection 37 3,669 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
StanfordCars [243] Object Classification & OOD Detection 196 8,041 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
Flowers102 [244] Object Classification 102 2,463 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
Food101 [245] Object Classification & OOD Detection 101 30,300 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
FGVCAircraft [246] Object Classification 100 3,333 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
SUN397 [247] Object Classification & OOD Detection 397 19,850 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
DTD [248] Object Classification & OOD Detection 47 1,692 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
EuroSAT [249] Object Classification 10 8,100 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
UCF101 [250] Object Classification & Action Recognition 101 3,783 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
ImageNet [251] Object Classification & OOD Detection 1,000 50,000 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
ImageNet-A [252] Object Classification 200 7,500 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
ImageNet-V2 [253] Object Classification 1,000 10,000 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
ImageNet-R [254] Object Classification 200 30,000 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
ImageNet-Sketch [255] Object Classification 1,000 50,889 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
Office-Home [256] Object Classification 65 15,588 ‚ãÜ ÔøΩ ‚ãÜ ‚ãÜ
Office [14] Object Classification 31 4,110 ‚ãÜ ÔøΩ ‚ãÜ ‚ãÜ
DomainNet [257] Object Classification 345 176,743 ‚ãÜ ÔøΩ ‚ãÜ ‚ãÜ
PASCAL VOC 2012 [258] Semantic Segmentation 20 1,449 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
PASCAL Context [259] Semantic Segmentation 59 5,105 ‚ãÜ ‚ãÜ ÔøΩ ‚ãÜ
COCO Stuff [260] Semantic Segmentation 172 4,172 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
ADE20K [261] Semantic Segmentation 150 2,000 ‚ãÜ ÔøΩ ÔøΩ ‚ãÜ
COCO-Object [262] Semantic Segmentation 80 5,000 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
Cityscapes [263] Semantic Segmentation 27 500 ‚ãÜ ÔøΩ ‚ãÜ ‚ãÜ
KITTI-STEP [264] Semantic Segmentation 19 2,981 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
FireNet [265] Semantic Segmentation        - 1,452 ‚ãÜ ‚ãÜ ‚ãÜ ‚ãÜ
Bongard-HOI [266] Visual Reasoning 2 13,914 ‚ãÜ ‚ãÜ ÔøΩ ‚ãÜ
CIFAR-100 [267] OOD Detection 100 10,000 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
CUB-200-2011 [268] OOD Detection 200 5,794 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
iNaturalist [269] OOD Detection 5,089 675,170 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
Places [270] OOD Detection 365 18,250 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
ImageNet-O [252] OOD Detection 200 2000 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
OpenImage-O [271] OOD Detection       - 17,632 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
MS-COCO [262] Text-Image Retrieval & Image Captioning      - 5,000 ‚ãÜ ‚ãÜ ‚ãÜ ÔøΩ
Flickr30K [272] Text-Image Retrieval & Image Captioning        - 1,000 ‚ãÜ ‚ãÜ ‚ãÜ ÔøΩ
Fashion-Gen [273] Text-Image Retrieval        - 32,528 ‚ãÜ ‚ãÜ ‚ãÜ ÔøΩ
CUHK-PEDES [274] Text-Image Retrieval      - 40,206 ‚ãÜ ‚ãÜ ‚ãÜ ÔøΩ
ICFG-PEDES [275] Text-Image Retrieval       - 54,522 ‚ãÜ ‚ãÜ ‚ãÜ ÔøΩ
Nocaps [276] Text-Image Retrieval & Image Captioning       - 15,100 ‚ãÜ ‚ãÜ ‚ãÜ ÔøΩ
Guangzhou Dataset [277] Medical Image Diagnosis 2 5,856 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
Montgomery Dataset [278] Medical Image Diagnosis 2 138 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
Shenzhen Dataset [278] Medical Image Diagnosis 2 662 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
BrainTumor Dataset [31] Medical Image Diagnosis 2 593 ÔøΩ ‚ãÜ ‚ãÜ ‚ãÜ
IDRID Dataset [279] Medical Image Diagnosis 5 516 ÔøΩ ÔøΩ ‚ãÜ ‚ãÜ
ISIC Dataset [280] Medical Image Diagnosis 7 11720 ‚ãÜ ÔøΩ ‚ãÜ ‚ãÜ
HMDB-51 [281] Action Recognition 51 6,766 ÔøΩ ‚ãÜ ÔøΩ ‚ãÜ
Kinetics-600 [282] Action Recognition 600 480,000 ÔøΩ ‚ãÜ ÔøΩ ‚ãÜ
ActivityNet [283] Action Recognition & Action Localization 200 19,994 ÔøΩ ‚ãÜ ÔøΩ ‚ãÜ
THUMOS14 [284] Action Localization 20 212 ‚ãÜ ‚ãÜ ÔøΩ ‚ãÜ


detection. Far OOD detection deals with samples that are
clearly distinct from the ID distribution. For instance, when
datasets such as CIFAR-100 [267], CUB-200-2011 [268],
StanfordCars [243], Food101 [245], OxfordPets [242], and
ImageNet [251] are used as ID data, datasets like iNaturalist [269], SUN397 [247], Places [270], and DTD [248] serve
as typical far OOD sources. Near OOD detection addresses a
more challenging setting where the OOD samples share visual
similarities with the ID data. Common experimental setups
include alternately using ImageNet-10 and ImageNet-20 as ID


and OOD datasets, as well as employing ImageNet-O [252]
and OpenImage-O [271] as near OOD sets. Fine-grained OOD
detection targets subtle distribution shifts within similar categories. For example, datasets such as CUB-200-2011 [268],
StanfordCars [243], Food101 [245], and OxfordPets [242] can
be split such that half of the classes are seen as ID data and
the other half as OOD data. Evaluation of OOD detection

performance is typically conducted using FPR95 and AUROC
metrics.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13


*E. Text-Image Retrieval*

Text-image retrieval is a fundamental task in visionlanguage research, where the goal is to retrieve relevant
images based on textual queries, or vice versa. The MSCOCO [262] and Flickr30K [272] datasets are among the
most widely used benchmarks for evaluating performance in
this domain. Besides, several specialized datasets are commonly utilized to assess retrieval performance across different
contexts, such as Fashion-Gen [273] from the e-commerce
domain, CUHK-PEDES [274], ICFG-PEDES [275] from the
person re-identification domain, and Nocaps [276] from the
natural image domain. Recall@K serves as the standard metric
for assessing the performance of retrieval algorithms.

*F. Image Captioning*

Image captioning aims to generate descriptive textual summaries of visual content. In the context of test-time adaptation, researchers [150] evaluate the adaptability of the CLIP
model across several benchmark datasets, including MSCOCO [262], Flickr30K [272], and NoCaps [276]. These
datasets provide diverse visual and textual contexts, enabling
the assessment of how effectively the model can generate
relevant captions when exposed to new domains without
additional annotated supervision. Captioning performance is
evaluated using BLEU, CIDEr, SPICE, and RefCLIPScore
metrics.

*G. Beyond Vanilla Object Images*

**Medical image diagnosis.** Medical imaging represents
a critical real-world application of VLMs in unsupervised
learning settings. Researchers [30], [31] frequently utilize
datasets such as the Guangzhou Dataset [277], Montgomery
Dataset [278], and Shenzhen Dataset [278], which focus on
chest X-ray diagnosis. Moreover, VLM-based unsupervised
methods [30], [31] have also been applied to various other
diagnostic tasks, including diabetic retinopathy [279], brain
tumor detection [31], and skin lesion classification [280].
**Videos.** Beyond static images, VLMs have also been explored in the context of video-based unsupervised learning.
Action recognition benchmarks such as HMDB-51 [281],
UCF-101 [250], Kinetics-600 [282], and ActivityNet [283] are
commonly used to evaluate performance. In addition, more
complex tasks like temporal action localization, which involve
both action classification and precise timestamp prediction,
are addressed using datasets such as ActivityNet [283] and
THUMOS14 [284].

IX. R ESEARCH C HALLENGES AND F UTURE D IRECTIONS

Despite significant progress, unsupervised VLM adaptation
remains an open and challenging problem. This section outlines key research directions, identifying gaps in the current
literature and discussing potential avenues for advancing the
field.


*A. Theoretical Analysis*

While existing research has largely focused on developing
effective unsupervised learning methods, rigorous theoretical
analyses are still lacking. Understanding the theoretical complexities of VLMs is crucial for developing more principled
adaptation methods. Future research can bridge this gap by
providing formal generalization guarantees and characterizing
the joint embedding space to explain how cross-modal alignment emerges [286].

*B. Open-world Scenarios*

Most existing approaches operate under the closed-set
assumption, which presumes identical label spaces across
domains. However, in real-world applications, test samples
often contain unknown classes, making it essential to detect
and handle them effectively. While some recent studies [27],

[215], [216] have begun addressing the open-world scenario,
this challenging yet practical setting remains underexplored.
Further research is needed to develop robust open-world
adaptation methods that can generalize across diverse domains
while accurately identifying unseen categories. Techniques
from out-of-distribution detection [287]‚Äì[289] could also be
leveraged and adapted to facilitate unknown class detection.

*C. Adversarial Robustness*

Although VLMs demonstrate strong generalization capabilities, they remain highly susceptible to adversarial attacks [290]. Several recent studies [290], [291] have drawn
inspiration from adversarial training techniques [292] to enhance the robustness of VLMs. However, these approaches
typically rely on large amounts of labeled data, leading to
substantial annotation costs. Therefore, an important research
direction is to explore robust optimization [125] and inference
strategies [148] under unsupervised settings, enabling VLMs
to operate reliably in complex, real-world environments where
adversarial threats are likely and labeled data is scarce.

*D. Privacy Considerations*

Privacy and security considerations are increasingly critical
for the adaptation of VLMs, particularly in sensitive domains
such as autonomous driving [293] and healthcare [294]. During adaptation, models may process proprietary or personal
data, raising concerns about data leakage and unauthorized
access. Additionally, the adaptation process can expose models
to adversarial attacks [292] that exploit vulnerabilities during the update phase, potentially degrading performance or
leading to harmful outcomes. To address these challenges,
future research should focus on developing privacy-preserving
adaptation techniques such as federated learning [295], which
enable models to adapt effectively without directly accessing
raw data.

*E. Efficient Inference*

The deployment of VLMs demands substantial computational resources for inference. A critical research challenge


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14


is to reduce their latency and memory footprint without
sacrificing performance. Future work may adapt techniques
like quantization [296], pruning [297], and knowledge distillation [298] for the unique cross-modal nature of these models.
The central difficulty lies in compressing the model while preserving the delicate vision-language alignments learned during
pre-training. Developing novel and efficient architectures is
crucial for enabling real-time VLM applications on resourceconstrained hardware and moving these powerful models from
the cloud to the edge.

*F. More VLMs Beyond CLIP*

While CLIP has become the de facto backbone for unsu
pervised learning of VLMs, relying solely on its contrastive
framework limits architectural and objective diversity. Future
research should investigate alternative base models‚Äîsuch as
advanced training strategies [87], masked-image modeling
with joint text encoders [299], or generative vision-language
transformers [300]‚Äîto uncover new inductive biases. Moreover, studying how different encoder-decoder pairings impact
alignment and transferability will guide the selection of more
versatile models. Broadening beyond CLIP will catalyze novel
unsupervised paradigms and improve VLM robustness across
tasks and domains.

*G. Extension to MLLMs*

Another promising research direction is to integrate TTA
into MLLMs with test-time scaling [301], [302]. TTA methods
enable models to dynamically adjust to distribution shifts
during inference, enhancing robustness without retraining. In
parallel, test-time scaling techniques allocate additional computational resources at test time‚Äîallowing models to ‚Äùthink‚Äù
longer or perform deeper reasoning on challenging or outof-distribution inputs [303]. By merging these approaches, an
MLLM could not only adapt its predictions based on the
incoming data stream but also flexibly scale its inference
compute based on sample difficulty. This synergy would
offer a balanced trade-off between efficiency and accuracy,
especially in real-world applications where both rapid response
and high adaptability are critical.

*H. New Downstream Tasks*

Although unsupervised learning of VLMs has been extensively studied in image classification and semantic segmentation tasks, its potential in other domains remains largely underexplored, including regression [304], generative models [305],
cross-modal retrieval [306], depth completion [307], misclassification detection [308], and image super-resolution [309].
Besides, the potential applications in other fields such as
medicine [310] and healthcare [311] remain underexplored and
warrant greater attention.

*I. Failure Mode and Negative Transfer*

Despite the empirical success of many unsupervised adaptation methods for VLMs, few studies have systematically
documented their failure modes or reported instances of


negative transfer. For example, entropy minimization [18],
although widely used, can reinforce incorrect predictions when
the model exhibits high uncertainty, leading to overconfident
misclassifications or even mode collapse. Similarly, prompt
generation via LLMs may introduce hallucinated or domaininappropriate descriptions [312], resulting in semantic misalignment with the visual content and degraded performance.
In continual adaptation settings [83], the accumulation of
erroneous pseudo-labels over time can distort the feature
space and destabilize the adaptation process. To advance
the field, future research should place greater emphasis on
robustness analysis, including the development of metrics to
detect adaptation failures and best practices for identifying
and reporting instability. Furthermore, sharing negative results
or counterexamples can play a critical role in uncovering
systematic weaknesses and guiding the design of more resilient
and reliable adaptation pipelines.

X. C ONCLUSION

In this survey, we have presented a comprehensive and structured overview of the rapidly advancing field of unsupervised
vision-language model adaptation. Addressing a notable gap
in existing literature, we introduced a novel taxonomy that
classifies methods based on the availability of unlabeled visual
data, a crucial factor for real-world deployment. By delineating the field into four distinct settings‚Äîdata-free transfer,
unsupervised domain transfer, episodic test-time adaptation,
and online test-time adaptation‚Äîwe provided a systematic
framework for understanding the unique challenges and assumptions inherent to each scenario. Within this structure,
we analyzed core methodologies and reviewed representative
benchmarks, offering a holistic perspective on the state of the
art. Finally, we identified several key challenges and directions
for future research, including the development of theoretical
analysis, the handling of open-world scenarios and privacy
considerations, and further exploration of new downstream
tasks and application fields. This survey will not only serve
as a valuable resource for practitioners seeking to navigate the
landscape of unsupervised VLM adaptation but also stimulate
further innovation by providing a clear basis for comparison
and identifying promising directions for future research.

R EFERENCES

[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark *et al.*, ‚ÄúLearning transferable
visual models from natural language supervision,‚Äù in *ICML*, 2021.

[2] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H.
Sung, Z. Li, and T. Duerig, ‚ÄúScaling up visual and vision-language
representation learning with noisy text supervision,‚Äù in *ICML*, 2021.

[3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson,
K. Lenc, A. Mensch, K. Millican, M. Reynolds *et al.*, ‚ÄúFlamingo: a
visual language model for few-shot learning,‚Äù in *NeurIPS*, 2022.

[4] H. Liu, C. Li, Q. Wu, and Y. J. Lee, ‚ÄúVisual instruction tuning,‚Äù in
*NeurIPS*, 2023.

[5] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman,
M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman *et al.*,
‚ÄúLaion-5b: An open large-scale dataset for training next generation
image-text models,‚Äù in *NeurIPS*, 2022.

[6] R. Chen, Y. Liu, L. Kong, X. Zhu, Y. Ma, Y. Li, Y. Hou, Y. Qiao, and
W. Wang, ‚ÄúClip2scene: Towards label-efficient 3d scene understanding
by clip,‚Äù in *CVPR*, 2023.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15



[7] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam,
‚ÄúClip-fields: Weakly supervised semantic fields for robotic memory,‚Äù
*arXiv preprint arXiv:2210.05663*, 2022.

[8] H. Sun, Y. Cao, H. Dong, and O. Fink, ‚ÄúUnseen visual anomaly
generation,‚Äù in *CVPR*, 2025.

[9] S. Chun, S. J. Oh, R. S. De Rezende, Y. Kalantidis, and D. Larlus,
‚ÄúProbabilistic embeddings for cross-modal retrieval,‚Äù in *CVPR*, 2021.

[10] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, ‚ÄúLearning to prompt for
vision-language models,‚Äù *International Journal of Computer Vision*,
vol. 130, no. 9, pp. 2337‚Äì2348, 2022.

[11] ‚Äî‚Äî, ‚ÄúConditional prompt learning for vision-language models,‚Äù in
*CVPR*, 2022.

[12] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan, ‚ÄúMaple:
Multi-modal prompt learning,‚Äù *arXiv preprint arXiv:2210.03117*, 2022.

[13] Z. Wang, J. Liang, L. Sheng, R. He, Z. Wang, and T. Tan, ‚ÄúA hard-tobeat baseline for training-free clip-based adaptation,‚Äù in *ICLR*, 2024.

[14] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, ‚ÄúAdapting visual category
models to new domains,‚Äù in *ECCV*, 2010.

[15] S. Menon and C. Vondrick, ‚ÄúVisual classification via description from
large language models,‚Äù in *ICLR*, 2023.

[16] C. Zhou, C. C. Loy, and B. Dai, ‚ÄúExtract free dense labels from clip,‚Äù
in *ECCV*, 2022.

[17] T. Huang, J. Chu, and F. Wei, ‚ÄúUnsupervised prompt learning for
vision-language models,‚Äù *arXiv preprint arXiv:2204.03649*, 2022.

[18] M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar,
and C. Xiao, ‚ÄúTest-time prompt tuning for zero-shot generalization in
vision-language models,‚Äù in *NeurIPS*, 2022.

[19] X. Ma, J. Zhang, S. Guo, and W. Xu, ‚ÄúSwapprompt: Test-time prompt
adaptation for vision-language models,‚Äù in *NeurIPS*, 2023.

[20] L. Sheng, J. Liang, R. He, Z. Wang, and T. Tan, ‚ÄúThe illusion of
progress? a critical look at test-time adaptation for vision-language
models,‚Äù *arXiv preprint arXiv:2506.24000*, 2025.

[21] S. Pratt, I. Covert, R. Liu, and A. Farhadi, ‚ÄúWhat does a platypus
look like? generating customized prompts for zero-shot image classification,‚Äù in *ICCV*, 2023.

[22] J. Kahana, N. Cohen, and Y. Hoshen, ‚ÄúImproving zero-shot models
with label distribution priors,‚Äù *arXiv preprint arXiv:2212.00784*, 2022.

[23] Q. Qian and J. Hu, ‚ÄúOnline zero-shot classification with clip,‚Äù in *ECCV*,
2024.

[24] J. Abdul Samadh, M. H. Gani, N. Hussein, M. U. Khattak, M. M.
Naseer, F. Shahbaz Khan, and S. H. Khan, ‚ÄúAlign your prompts: Testtime prompting with distribution alignment for zero-shot generalization,‚Äù in *NeurIPS*, 2023.

[25] A. Karmanov, D. Guan, S. Lu, A. El Saddik, and E. Xing, ‚ÄúEfficient
test-time adaptation of vision-language models,‚Äù in *CVPR*, 2024.

[26] J. Li, S. Savarese, and S. C. Hoi, ‚ÄúMasked unsupervised self-training
for label-free image classification,‚Äù in *ICLR*, 2023.

[27] J. Liang, L. Sheng, Z. Wang, R. He, and T. Tan, ‚ÄúRealistic unsupervised
clip fine-tuning with universal entropy optimization,‚Äù in *ICML*, 2024.

[28] G. Shin, W. Xie, and S. Albanie, ‚ÄúReco: Retrieve and co-segment for
zero-shot transfer,‚Äù in *NeurIPS*, 2022.

[29] S. Hajimiri, I. B. Ayed, and J. Dolz, ‚ÄúPay attention to your neighbours:
Training-free open-vocabulary semantic segmentation,‚Äù in *WACV*,
2025.

[30] U. Rahman, R. Imam, M. Yaqub, B. B. Amor, and D. Mahapatra,
‚ÄúCan language-guided unsupervised adaptation improve medical image
classification using unpaired images and texts?‚Äù in *ISBI*, 2025.

[31] J. Liu, T. Hu, Y. Zhang, X. Gai, Y. Feng, and Z. Liu, ‚ÄúA chatgpt
aided explainable framework for zero-shot medical image diagnosis,‚Äù
in *ICML Workshops*, 2023.

[32] M. Bosetti, S. Zhang, B. Liberatori, G. Zara, E. Ricci, and P. Rota,
‚ÄúText-enhanced zero-shot action recognition: A training-free approach,‚Äù
in *ICPR*, 2024.

[33] R. Yan, H. Qu, X. Shu, W. Li, J. Tang, and T. Tan, ‚ÄúDts-tpt: dual
temporal-sync test-time prompt tuning for zero-shot activity recognition,‚Äù in *IJCAI*, 2024.

[34] K. Tanwisuth, S. Zhang, H. Zheng, P. He, and M. Zhou, ‚ÄúPouf: Promptoriented unsupervised fine-tuning for large pre-trained models,‚Äù in
*ICML*, 2023.

[35] Y. Kalantidis, G. Tolias *et al.*, ‚ÄúLabel propagation for zero-shot
classification with vision-language models,‚Äù in *CVPR*, 2024.

[36] M. Zanella and I. Ben Ayed, ‚ÄúOn the test-time zero-shot generalization
of vision-language models: Do we really need prompt learning?‚Äù in
*CVPR*, 2024.

[37] J. Zhang, J. Huang, S. Jin, and S. Lu, ‚ÄúVision-language models for
vision tasks: A survey,‚Äù *IEEE Transactions on Pattern Analysis and*
*Machine Intelligence*, vol. 46, no. 8, pp. 5625‚Äì5644, 2024.



[38] H. Dong, M. Liu, K. Zhou, E. Chatzi, J. Kannala, C. Stachniss,
and O. Fink, ‚ÄúAdvances in multimodal adaptation and generalization:
From traditional approaches to foundation models,‚Äù *arXiv preprint*
*arXiv:2501.18592*, 2025.

[39] X. Li, J. Li, F. Li, L. Zhu, Y. Yang, and H. T. Shen, ‚ÄúGeneralizing
vision-language models to novel domains: A comprehensive survey,‚Äù
*arXiv preprint arXiv:2506.18504*, 2025.

[40] J. Liang, R. He, and T. Tan, ‚ÄúA comprehensive survey on test-time
adaptation under distribution shifts,‚Äù *International Journal of Computer*
*Vision*, vol. 133, no. 1, pp. 31‚Äì64, 2025.

[41] Z. Wang, Y. Luo, L. Zheng, Z. Chen, S. Wang, and Z. Huang, ‚ÄúIn search
of lost online test-time adaptation: A survey,‚Äù *International Journal of*
*Computer Vision*, vol. 133, no. 3, pp. 1106‚Äì1139, 2025.

[42] Z. Xiao and C. G. Snoek, ‚ÄúBeyond model adaptation at test time: A
survey,‚Äù *arXiv preprint arXiv:2411.03687*, 2024.

[43] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li,
X. Jiang, and C. Xu, ‚ÄúFilip: Fine-grained interactive language-image
pre-training,‚Äù *arXiv preprint arXiv:2111.07783*, 2021.

[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in
*NeurIPS*, 2017.

[45] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*,
‚ÄúAn image is worth 16x16 words: Transformers for image recognition
at scale,‚Äù in *ICLR*, 2021.

[46] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll¬¥ar, and
C. L. Zitnick, ‚ÄúMicrosoft coco captions: Data collection and evaluation
server,‚Äù *arXiv preprint arXiv:1504.00325*, 2015.

[47] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, ‚ÄúImage captioning with
semantic attention,‚Äù in *CVPR*, 2016.

[48] Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. Van Den Hengel,
‚ÄúVisual question answering: A survey of methods and datasets,‚Äù
*Computer Vision and Image Understanding*, vol. 163, pp. 21‚Äì40, 2017.

[49] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, ‚ÄúHighresolution image synthesis with latent diffusion models,‚Äù in *CVPR*,
2022.

[50] L. Zhen, P. Hu, X. Wang, and D. Peng, ‚ÄúDeep supervised cross-modal
retrieval,‚Äù in *CVPR*, 2019.

[51] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,
L. He *et al.*, ‚ÄúA comprehensive survey on pretrained foundation models:
A history from bert to chatgpt,‚Äù *International Journal of Machine*
*Learning and Cybernetics*, pp. 1‚Äì65, 2024.

[52] C. H. Lampert, H. Nickisch, and S. Harmeling, ‚ÄúLearning to detect
unseen object classes by between-class attribute transfer,‚Äù in *CVPR*,
2009.

[53] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth, ‚ÄúDescribing objects
by their attributes,‚Äù in *CVPR*, 2009.

[54] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato,
and T. Mikolov, ‚ÄúDevise: A deep visual-semantic embedding model,‚Äù
in *NeurIPS*, 2013.

[55] B. Romera-Paredes and P. Torr, ‚ÄúAn embarrassingly simple approach
to zero-shot learning,‚Äù in *ICML*, 2015.

[56] V. K. Verma, G. Arora, A. Mishra, and P. Rai, ‚ÄúGeneralized zero-shot
learning via synthesized examples,‚Äù in *CVPR*, 2018.

[57] R. Felix, I. Reid, G. Carneiro *et al.*, ‚ÄúMulti-modal cycle-consistent
generalized zero-shot learning,‚Äù in *ECCV*, 2018.

[58] Y. Xian, S. Sharma, B. Schiele, and Z. Akata, ‚Äúf-vaegan-d2: A feature
generating framework for any-shot learning,‚Äù in *CVPR*, 2019.

[59] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha, ‚ÄúAn empirical study
and analysis of generalized zero-shot learning for object recognition in
the wild,‚Äù in *ECCV*, 2016.

[60] Y. Xian, B. Schiele, and Z. Akata, ‚ÄúZero-shot learning-the good, the
bad and the ugly,‚Äù in *CVPR*, 2017.

[61] F. Pourpanah, M. Abdar, Y. Luo, X. Zhou, R. Wang, C. P. Lim, X.-Z.
Wang, and Q. J. Wu, ‚ÄúA review of generalized zero-shot learning methods,‚Äù *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
vol. 45, no. 4, pp. 4051‚Äì4070, 2022.

[62] W. Wang, V. W. Zheng, H. Yu, and C. Miao, ‚ÄúA survey of zero-shot
learning: Settings, methods, and applications,‚Äù *ACM Transactions on*
*Intelligent Systems and Technology*, vol. 10, no. 2, pp. 1‚Äì37, 2019.

[63] C. Ma, Y. Liu, J. Deng, L. Xie, W. Dong, and C. Xu, ‚ÄúUnderstanding
and mitigating overfitting in prompt tuning for vision-language models,‚Äù *arXiv preprint arXiv:2211.02219*, 2022.

[64] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and
Y. Qiao, ‚ÄúClip-adapter: Better vision-language models with feature
adapters,‚Äù *International Journal of Computer Vision*, vol. 132, no. 2,
pp. 581‚Äì595, 2024.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16



[65] R. Zhang, R. Fang, P. Gao, W. Zhang, K. Li, J. Dai, Y. Qiao,
and H. Li, ‚ÄúTip-adapter: Training-free clip-adapter for better visionlanguage modeling,‚Äù *arXiv preprint arXiv:2111.03930*, 2021.

[66] M. Zanella and I. Ben Ayed, ‚ÄúLow-rank few-shot adaptation of visionlanguage models,‚Äù in *CVPR Workshops*, 2024.

[67] S. Menon and C. Vondrick, ‚ÄúVisual classification via description from
large language models,‚Äù *arXiv preprint arXiv:2210.07183*, 2022.

[68] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou,
and J. Lu, ‚ÄúDenseclip: Language-guided dense prediction with contextaware prompting,‚Äù in *CVPR*, 2022.

[69] J. Gu, Z. Han, S. Chen, A. Beirami, B. He, G. Zhang, R. Liao, Y. Qin,
V. Tresp, and P. Torr, ‚ÄúA systematic survey of prompt engineering on
vision-language foundation models,‚Äù *arXiv preprint arXiv:2307.12980*,
2023.

[70] I. Nejjar, H. Dong, and O. Fink, ‚ÄúRecall and refine: A simple but
effective source-free open-set domain adaptation framework,‚Äù *arXiv*
*preprint arXiv:2411.12558*, 2024.

[71] J. Liang, D. Hu, and J. Feng, ‚ÄúDo we really need to access the source
data? source hypothesis transfer for unsupervised domain adaptation,‚Äù
in *ICML*, 2020.

[72] J. Wen, R. Liu, N. Zheng, Q. Zheng, Z. Gong, and J. Yuan, ‚ÄúExploiting
local feature patterns for unsupervised domain adaptation,‚Äù in *AAAI*,
2019.

[73] D. Chen, D. Wang, T. Darrell, and S. Ebrahimi, ‚ÄúContrastive test-time
adaptation,‚Äù in *CVPR*, 2022.

[74] J. N. Kundu, N. Venkat, and R. V. Babu, ‚ÄúUniversal source-free domain
adaptation,‚Äù in *CVPR*, 2020.

[75] J. Huang, D. Guan, A. Xiao, and S. Lu, ‚ÄúModel adaptation: Historical
contrastive learning for unsupervised domain adaptation without source
data,‚Äù in *NeurIPS*, 2021.

[76] Y. Kim, D. Cho, K. Han, P. Panda, and S. Hong, ‚ÄúDomain adaptation
without source data,‚Äù *IEEE Transactions on Artificial Intelligence*,
vol. 2, no. 6, pp. 508‚Äì518, 2021.

[77] J. Li, Z. Yu, Z. Du, L. Zhu, and H. T. Shen, ‚ÄúA comprehensive survey on
source-free domain adaptation,‚Äù *IEEE Transactions on Pattern Analysis*
*and Machine Intelligence*, vol. 46, no. 8, pp. 5743‚Äì5762, 2024.

[78] Y. Fang, P.-T. Yap, W. Lin, H. Zhu, and M. Liu, ‚ÄúSource-free unsupervised domain adaptation: A survey,‚Äù *Neural Networks*, vol. 174, p.
106230, 2024.

[79] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell, ‚ÄúTent:
Fully test-time adaptation by entropy minimization,‚Äù in *ICLR*, 2021.

[80] H. Dong, E. Chatzi, and O. Fink, ‚ÄúTowards robust multimodal open-set
test-time adaptation via adaptive entropy-aware optimization,‚Äù in *ICLR*,
2025.

[81] S. Niu, J. Wu, Y. Zhang, Y. Chen, S. Zheng, P. Zhao, and M. Tan,
‚ÄúEfficient test-time model adaptation without forgetting,‚Äù in *ICML*,
2022.

[82] T. Gong, Y. Kim, T. Lee, S. Chottananurak, and S.-J. Lee, ‚ÄúSotta:
Robust test-time adaptation on noisy data streams,‚Äù in *NeurIPS*, 2023.

[83] Q. Wang, O. Fink, L. Van Gool, and D. Dai, ‚ÄúContinual test-time
domain adaptation,‚Äù in *CVPR*, 2022.

[84] J. Song, J. Lee, I. S. Kweon, and S. Choi, ‚ÄúEcotta: Memoryefficient continual test-time adaptation via self-distilled regularization,‚Äù
in *CVPR*, 2023.

[85] C.-M. Feng, K. Yu, Y. Liu, S. Khan, and W. Zuo, ‚ÄúDiverse data
augmentation with diffusions for effective test-time prompt tuning,‚Äù
in *ICCV*, 2023.

[86] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao, ‚ÄúEva-clip: Improved
training techniques for clip at scale,‚Äù *arXiv preprint arXiv:2303.15389*,
2023.

[87] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, ‚ÄúSigmoid loss for
language image pre-training,‚Äù in *ICCV*, 2023.

[88] Z. Novack, J. McAuley, Z. C. Lipton, and S. Garg, ‚ÄúChils: Zero-shot
image classification with hierarchical label sets,‚Äù in *ICML*, 2023.

[89] Z. Guo, B. Dong, Z. Ji, J. Bai, Y. Guo, and W. Zuo, ‚ÄúTexts as images
in prompt tuning for multi-label image recognition,‚Äù in *CVPR*, 2023.

[90] V. Udandarao, A. Gupta, and S. Albanie, ‚ÄúSus-x: Training-free nameonly transfer of vision-language models,‚Äù in *ICCV*, 2023.

[91] M. Wallingford, V. Ramanujan, A. Fang, A. Kusupati, R. Mottaghi,
A. Kembhavi, L. Schmidt, and A. Farhadi, ‚ÄúNeural priming for sampleefficient adaptation,‚Äù in *NeurIPS*, 2023.

[92] M. Seo, S. Cho, M. Lee, D. Misra, H. Choi, S. J. Kim, and J. Choi,
‚ÄúJust say the name: Online continual learning with category names
only via data generation,‚Äù *arXiv preprint arXiv:2403.10853*, 2024.

[93] Z. Guo, R. Zhang, L. Qiu, X. Ma, X. Miao, X. He, and B. Cui, ‚ÄúCalip:
Zero-shot enhancement of clip with parameter-free attention,‚Äù in *AAAI*,
2023.



[94] F. Wang, J. Mei, and A. Yuille, ‚ÄúSclip: Rethinking self-attention for
dense vision-language inference,‚Äù in *ECCV*, 2024.

[95] M. Lan, C. Chen, Y. Ke, X. Wang, L. Feng, and W. Zhang, ‚ÄúProxyclip:
Proxy attention improves clip for open-vocabulary segmentation,‚Äù in
*ECCV*, 2024.

[96] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell *et al.*, ‚ÄúLanguage
models are few-shot learners,‚Äù in *NeurIPS*, 2020.

[97] M. J. Mirza, L. Karlinsky, W. Lin, S. Doveh, J. Micorek, M. Kozinski,
H. Kuhene, and H. Possegger, ‚ÄúMeta-prompting for automating zeroshot visual recognition with llms,‚Äù in *ECCV*, 2024.

[98] S. Parashar, Z. Lin, T. Liu, X. Dong, Y. Li, D. Ramanan, J. Caverlee,
and S. Kong, ‚ÄúThe neglected tails in vision-language models,‚Äù in *CVPR*,
2024.

[99] O. Blog, ‚ÄúIntroducing chatgpt,‚Äù 2023.

[100] S. Parashar, Z. Lin, Y. Li, and S. Kong, ‚ÄúPrompting scientific names
for zero-shot species recognition,‚Äù in *EMNLP*, 2023.

[101] K. Roth, J. M. Kim, A. Koepke, O. Vinyals, C. Schmid, and Z. Akata,
‚ÄúWaffling around for performance: Visual classification with random
words and broad concepts,‚Äù in *ICCV*, 2023.

[102] X. Liu and C. Zach, ‚ÄúTag: Text prompt augmentation for zero-shot
out-of-distribution detection supplementary material,‚Äù in *ECCV*, 2024.

[103] W. Sun, Y. Du, G. Liu, R. Kompella, and C. G. Snoek, ‚ÄúTrainingfree semantic segmentation via llm-supervision,‚Äù *arXiv preprint*
*arXiv:2404.00701*, 2024.

[104] M. Moayeri, M. Rabbat, M. Ibrahim, and D. Bouchacourt, ‚ÄúEmbracing
diversity: Interpretable zero-shot classification beyond one vector per
class,‚Äù in *ACM Conference on Fairness, Accountability, and Trans-*
*parency*, 2024.

[105] C. Cao, Z. Zhong, Z. Zhou, Y. Liu, T. Liu, and B. Han, ‚ÄúEnvisioning outlier exposure by large language models for out-of-distribution
detection,‚Äù in *ICML*, 2024.

[106] Z. Ren, Y. Su, and X. Liu, ‚ÄúChatgpt-powered hierarchical comparisons
for image classification,‚Äù in *NeurIPS*, 2023.

[107] H. Lee, G. Seo, W. Choi, G. Jung, K. Song, and J. Jung, ‚ÄúEnhancing
visual classification using comparative descriptors,‚Äù in *WACV*, 2025.

[108] T. Liang and J. Davis, ‚ÄúMaking better mistakes in clip-based zero-shot
classification with hierarchy-aware language prompts,‚Äù *arXiv preprint*
*arXiv:2503.02248*, 2025.

[109] M. J. Mirza, L. Karlinsky, W. Lin, H. Possegger, R. Feris, and
H. Bischof, ‚ÄúTap: Targeted prompting for task adaptive generation
of textual training instances for visual classification,‚Äù *arXiv preprint*
*arXiv:2309.06809*, 2023.

[110] M. U. Khattak, M. F. Naeem, M. Naseer, L. Van Gool, and F. Tombari,
‚ÄúLearning to prompt with text only supervision for vision-language
models,‚Äù in *AAAI*, 2025.

[111] J. Shipard, A. Wiliem, K. N. Thanh, W. Xiang, and C. Fookes,
‚ÄúDiversity is definitely needed: Improving model-agnostic zero-shot
classification via stable diffusion,‚Äù in *CVPR*, 2023.

[112] S. Wang, L. Song, R. Shimizu, M. Goto, and H. Wu, ‚ÄúAttributed
synthetic data generation for zero-shot domain-specific image classification,‚Äù in *ICME*, 2025.

[113] M. Lan, C. Chen, Y. Ke, X. Wang, L. Feng, and W. Zhang, ‚ÄúClearclip:
Decomposing clip representations for dense vision-language inference,‚Äù
in *ECCV*, 2024.

[114] Y. Li, H. Wang, Y. Duan, J. Zhang, and X. Li, ‚ÄúA closer look at
the explainability of contrastive language-image pre-training,‚Äù *Pattern*
*Recognition*, vol. 162, p. 111409, 2025.

[115] W. Bousselham, F. Petersen, V. Ferrari, and H. Kuehne, ‚ÄúGrounding
everything: Emerging localization properties in vision-language transformers,‚Äù in *CVPR*, 2024.

[116] M. Caron, H. Touvron, I. Misra, H. J¬¥egou, J. Mairal, P. Bojanowski, and
A. Joulin, ‚ÄúEmerging properties in self-supervised vision transformers,‚Äù
in *ICCV*, 2021.

[117] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, ‚ÄúSegment anything,‚Äù
in *ICCV*, 2023.

[118] M. J. Mirza, L. Karlinsky, W. Lin, H. Possegger, M. Kozinski, R. Feris,
and H. Bischof, ‚ÄúLafter: Label-free tuning of zero-shot classifier using
language and unlabeled image collections,‚Äù in *NeurIPS*, 2023.

[119] X. Hu, K. Zhang, L. Xia, A. Chen, J. Luo, Y. Sun, K. Wang, N. Qiao,
X. Zeng, M. Sun *et al.*, ‚ÄúReclip: Refine contrastive language image
pre-training with source free domain adaptation,‚Äù in *WACV*, 2024.

[120] Y. Li, Y. Cao, J. Li, Q. Wang, and S. Wang, ‚ÄúData-efficient clippowered dual-branch networks for source-free unsupervised domain
adaptation,‚Äù *arXiv preprint arXiv:2410.15811*, 2024.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17



[121] S. Long, L. Wang, Z. Zhao, Z. Tan, Y. Wu, S. Wang, and J. Wang,
‚ÄúTraining-free unsupervised prompt for vision-language models,‚Äù *arXiv*
*preprint arXiv:2404.16339*, 2024.

[122] J. Huang, J. Zhang, H. Qiu, S. Jin, and S. Lu, ‚ÄúPrompt ensemble
self-training for open-vocabulary domain adaptation,‚Äù *arXiv preprint*
*arXiv:2306.16658*, 2023.

[123] Z. Li, X. Li, X. Fu, X. Zhang, W. Wang, S. Chen, and J. Yang, ‚ÄúPromptkd: Unsupervised prompt distillation for vision-language models,‚Äù in
*CVPR*, 2024.

[124] Q. Xu, W. Chen, Z. Hu, H. Li, and Y. Tai, ‚ÄúOtfusion: Bridging visiononly and vision-language models via optimal transport for transductive
zero-shot learning,‚Äù *arXiv preprint arXiv:2506.13723*, 2025.

[125] C. Schlarmann, N. D. Singh, F. Croce, and M. Hein, ‚ÄúRobust clip:
Unsupervised adversarial fine-tuning of vision embeddings for robust
large vision-language models,‚Äù in *ICML*, 2024.

[126] C. Shin, J. Zhao, S. Cromp, H. Vishwakarma, and F. Sala, ‚ÄúOtter: Effortless label distribution adaptation of zero-shot models,‚Äù in *NeurIPS*,
2024.

[127] M. Zanella, B. G¬¥erin, and I. Ayed, ‚ÄúBoosting vision-language models
with transduction,‚Äù in *NeurIPS*, 2024.

[128] Y. Zhang, C. Zhang, X. Hu, and Z. He, ‚ÄúUnsupervised prototype
adapter for vision-language models,‚Äù in *PRCV*, 2023.

[129] M. F. Imam, R. F. Marew, J. Hassan, M. Fiaz, A. F. Aji, and
H. Cholakkal, ‚ÄúClip meets dino for tuning zero-shot classifier using
unlabeled image collections,‚Äù *arXiv preprint arXiv:2411.19346*, 2024.

[130] X. Li, C. Wen, Y. Hu, and N. Zhou, ‚ÄúRs-clip: Zero shot remote sensing
scene classification via contrastive vision-language supervision,‚Äù *Inter-*
*national Journal of Applied Earth Observation and Geoinformation*,
vol. 124, p. 103497, 2023.

[131] O. Saha, L. Lawrence, G. Van Horn, and S. Maji, ‚ÄúGenerate, transduct,
adapt: Iterative transduction with vlms,‚Äù in *ICCV*, 2025.

[132] J. Zhang, Q. Wei, F. Liu, and L. Feng, ‚ÄúCandidate pseudolabel learning:
Enhancing vision-language models by prompt tuning with unlabeled
data,‚Äù in *ICML*, 2024.

[133] C. Menghini, A. Delworth, and S. Bach, ‚ÄúEnhancing clip with clip:
Exploring pseudolabeling for limited-label prompt tuning,‚Äù in *NeurIPS*,
2023.

[134] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel,
E. D. Cubuk, A. Kurakin, and C.-L. Li, ‚ÄúFixmatch: Simplifying semisupervised learning with consistency and confidence,‚Äù in *NeurIPS*,
2020.

[135] E. Ali, S. Silva, and M. H. Khan, ‚ÄúDpa: Dual prototypes alignment for
unsupervised adaptation of vision-language models,‚Äù in *WACV*, 2025.

[136] C. Laroudie, A. Bursuc, M. L. Ha, and G. Franchi, ‚ÄúImproving clip
robustness with knowledge distillation and self-training,‚Äù *arXiv preprint*
*arXiv:2309.10361*, 2023.

[137] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum, ‚ÄúLabel propagation for
deep semi-supervised learning,‚Äù in *CVPR*, 2019.

[138] E. Ali and M. H. Khan, ‚ÄúNoise-tolerant few-shot unsupervised adapter
for vision-language models,‚Äù in *BMVC*, 2023.

[139] H. Yan and Y. Guo, ‚ÄúLightweight unsupervised federated learning
with pretrained vision language model,‚Äù in *International Workshop on*
*Trustworthy Federated Learning*, 2024.

[140] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar *et al.*, ‚ÄúLlama:
Open and efficient foundation language models,‚Äù *arXiv preprint*
*arXiv:2302.13971*, 2023.

[141] A.-Q. Cao, M. Jaritz, M. Guillaumin, R. de Charette, and L. Bazzani,
‚ÄúLatteclip: Unsupervised clip fine-tuning via lmm-synthetic texts,‚Äù in
*WACV*, 2025.

[142] M. Mistretta, A. Baldrati, M. Bertini, and A. D. Bagdanov, ‚ÄúImproving
zero-shot generalization of learned prompts via unsupervised knowledge distillation,‚Äù in *ECCV*, 2024.

[143] Y. Benigmim, M. Fahes, T.-H. Vu, A. Bursuc, and R. de Charette,
‚ÄúFloss: Free lunch in open-vocabulary semantic segmentation,‚Äù in
*ICCV*, 2025.

[144] J. U. Allingham, J. Ren, M. W. Dusenberry, X. Gu, Y. Cui, D. Tran,
J. Z. Liu, and B. Lakshminarayanan, ‚ÄúA simple zero-shot prompt
weighting technique to improve prompt ensembling in text-image
models,‚Äù in *ICML*, 2023.

[145] A. T. Nguyen, K. S. Tai, B.-C. Chen, S. N. Shukla, H. Yu, P. Torr,
T.-P. Tian, and S.-N. Lim, ‚Äúucap: An unsupervised prompting method
for vision-language models,‚Äù in *ECCV*, 2024.

[146] Q. Qian, Y. Xu, and J. Hu, ‚ÄúIntra-modal proxy learning for zero-shot
visual categorization with clip,‚Äù in *NeurIPS*, 2023.

[147] S. Martin, Y. Huang, F. Shakeri, J.-C. Pesquet, and I. Ben Ayed,
‚ÄúTransductive zero-shot and few-shot clip,‚Äù in *CVPR*, 2024.



[148] L. Sheng, J. Liang, Z. Wang, and R. He, ‚ÄúR-tpt: Improving adversarial
robustness of vision-language models through test-time prompt tuning,‚Äù
in *CVPR*, 2025.

[149] M. Prabhudesai, T.-W. Ke, A. C. Li, D. Pathak, and K. Fragkiadaki,
‚ÄúDiffusion-tta: Test-time adaptation of discriminative models via generative feedback,‚Äù in *NeurIPS*, 2023.

[150] S. Zhao, X. Wang, L. Zhu, and Y. Yang, ‚ÄúTest-time adaptation with
clip reward for zero-shot generalization in vision-language models,‚Äù in
*ICLR*, 2023.

[151] X. Qiao, P. Huang, J. Yuan, X. Guo, B. Ye, Z. Sun, and X. Li,
‚ÄúBidirectional prototype-reward co-evolution for test-time adaptation
of vision-language models,‚Äù *arXiv preprint arXiv:2503.09394*, 2025.

[152] X. Wang, K. Chen, J. Zhang, J. Chen, and X. Ma, ‚ÄúTapt: Testtime adversarial prompt tuning for robust inference in vision-language
models,‚Äù in *CVPR*, 2024.

[153] M. Zanella, C. Fuchs, C. De Vleeschouwer, and I. B. Ayed, ‚ÄúRealistic
test-time adaptation of vision-language models,‚Äù in *CVPR*, 2025.

[154] Y. Zhu, G. Zhang, C. Xu, H. Shen, X. Chen, G. Wu, and L. Wang,
‚ÄúEfficient test-time prompt tuning for vision-language models,‚Äù *arXiv*
*preprint arXiv:2408.05775*, 2024.

[155] J. Yin, X. Zhang, L. Wu, and X. Wang, ‚ÄúIn-context prompt learning for
test-time vision recognition with frozen vision-language model,‚Äù *arXiv*
*preprint arXiv:2403.06126*, 2024.

[156] Y. Kojima, J. Xu, X. Zou, and X. Wang, ‚ÄúLora-ttt: Low-rank test-time
training for vision-language models,‚Äù in *ICML Workshops*, 2025.

[157] B. Liberatori, A. Conti, P. Rota, Y. Wang, and E. Ricci, ‚ÄúTest-time
zero-shot temporal action localization,‚Äù in *CVPR*, 2024.

[158] Y. Zhu, Y. Ji, Z. Zhao, G. Wu, and L. Wang, ‚ÄúAwt: Transferring visionlanguage models via augmentation, weighting, and transportation,‚Äù in
*NeurIPS*, 2024.

[159] Y. Lee, D. Kim, J. Kang, J. Bang, H. Song, and J.-G. Lee, ‚ÄúRA-TTA:
Retrieval-augmented test-time adaptation for vision-language models,‚Äù
in *ICLR*, 2025.

[160] C. Zhang, K. Xu, Z. Liu, Y. Peng, and J. Zhou, ‚ÄúScap: Transductive
test-time adaptation via supportive clique-based attribute prompting,‚Äù
in *CVPR*, 2025.

[161] M. Farina, G. Franchi, G. Iacca, M. Mancini, and E. Ricci, ‚ÄúFrustratingly easy test-time adaptation of vision-language models,‚Äù in *NeurIPS*,
2024.

[162] C.-M. Feng, Y. He, J. Zou, S. Khan, H. Xiong, Z. Li, W. Zuo, R. S. M.
Goh, and Y. Liu, ‚ÄúDiffusion-enhanced test-time adaptation with text
and image augmentation,‚Äù *International Journal of Computer Vision*,
pp. 1‚Äì16, 2025.

[163] H. S. Yoon, E. Yoon, J. T. J. Tee, M. Hasegawa-Johnson, Y. Li,
and C. D. Yoo, ‚ÄúC-tpt: Calibrated test-time prompt tuning for visionlanguage models via text feature dispersion,‚Äù in *ICLR*, 2024.

[164] A. Sharifdeen, M. A. Munir, S. Baliah, S. Khan, and M. H. Khan, ‚ÄúOtpt: Orthogonality constraints for calibrating test-time prompt tuning in
vision-language models,‚Äù in *CVPR*, 2025.

[165] J. Sun, M. Ibrahim, M. Hall, I. Evtimov, Z. M. Mao, C. C. Ferrer,
and C. Hazirbas, ‚ÄúVpa: Fully test-time visual prompt adaptation,‚Äù in
*ACM-MM*, 2023.

[166] R. Imam, H. Gani, M. Huzaifa, and K. Nandakumar, ‚ÄúTest-time low
rank adaptation via confidence maximization for zero-shot generalization of vision-language models,‚Äù in *WACV*, 2024.

[167] R. Imam, A. Hanif, J. Zhang, K. W. Dawoud, Y. Kementchedjhieva, and
M. Yaqub, ‚ÄúNoise is an efficient learner for zero-shot vision-language
models,‚Äù *arXiv preprint arXiv:2502.06019*, 2025.

[168] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, ‚ÄúClipscore:
A reference-free evaluation metric for image captioning,‚Äù in *EMNLP*,
2021.

[169] E. Sui, X. Wang, and S. Yeung-Levy, ‚ÄúJust shift it: Test-time prototype
shifting for zero-shot generalization with vision-language models,‚Äù in
*WACV*, 2024.

[170] A. Khandelwal, ‚ÄúPromptsync: Bridging domain gaps in vision-language
models through class-aware prototype alignment and discrimination,‚Äù
in *CVPR Workshops*, 2024.

[171] J. Gui, T. Chen, J. Zhang, Q. Cao, Z. Sun, H. Luo, and D. Tao,
‚ÄúA survey on self-supervised learning: Algorithms, applications, and
future trends,‚Äù *IEEE Transactions on Pattern Analysis and Machine*
*Intelligence*, vol. 46, no. 12, pp. 9052‚Äì9071, 2024.

[172] D. Hendrycks, M. Mazeika, S. Kadavath, and D. Song, ‚ÄúUsing selfsupervised learning can improve model robustness and uncertainty,‚Äù in
*NeurIPS*, 2019.

[173] M. Liu, T. L. Hayes, M. Mancini, E. Ricci, R. Volpi, and G. Csurka,
‚ÄúTest-time vocabulary adaptation for language-driven object detection,‚Äù
in *ICIP*, 2025.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18



[174] L. Cai, J. Kang, S. Li, W. Ma, B. Xie, Z. Qin, and J. Liang, ‚ÄúFrom
local details to global context: Advancing vision-language models with
attention-based selection,‚Äù in *ICML*, 2025.

[175] C. Yi, L. Ren, D.-C. Zhan, and H.-J. Ye, ‚ÄúLeveraging cross-modal
neighbor representation for improved clip classification,‚Äù in *CVPR*,
2024.

[176] B. An, S. Zhu, M.-A. Panaitescu-Liess, C. K. Mummadi, and F. Huang,
‚ÄúPerceptionclip: Visual classification by inferring and conditioning on
contexts,‚Äù in *ICLR*, 2023.

[177] Y. Ge, J. Ren, A. Gallagher, Y. Wang, M.-H. Yang, H. Adam, L. Itti,
B. Lakshminarayanan, and J. Zhao, ‚ÄúImproving zero-shot generalization and robustness of multi-modal models,‚Äù in *CVPR*, 2023.

[178] S. Shen, Z. Zhu, L. Fan, H. Zhang, and X. Wu, ‚ÄúDiffclip: Leveraging
stable diffusion for language grounded 3d classification,‚Äù in *WACV*,
2024.

[179] M. M. Rahaman, E. K. Millar, and E. Meijering, ‚ÄúLeveraging visionlanguage embeddings for zero-shot learning in histopathology images,‚Äù
*IEEE Journal of Biomedical and Health Informatics*, 2025.

[180] J. Li, M. Yang, Y. Tian, L. Zhang, Y. Lu, J. Liu, and W. Wang,
‚ÄúWavedn: A wavelet-based training-free zero-shot enhancement for
vision-language models,‚Äù in *ACM-MM*, 2024.

[181] S. Eom, N. Ho, J. Oh, and S.-Y. Yun, ‚ÄúCross-modal retrieval meets inference: Improving zero-shot classification with cross-modal retrieval,‚Äù
*arXiv preprint arXiv:2308.15273*, 2023.

[182] A. Conti, E. Fini, M. Mancini, P. Rota, Y. Wang, and E. Ricci,
‚ÄúVocabulary-free image classification and semantic segmentation,‚Äù
*arXiv preprint arXiv:2404.10864*, 2024.

[183] B. Murugesan, J. Silva-Rodr¬¥ƒ±guez, I. B. Ayed, and J. Dolz, ‚ÄúRobust
calibration of large vision-language adapters,‚Äù in *ECCV*, 2024.

[184] H. Ma, Y. Zhu, C. Zhang, P. Zhao, B. Wu, L.-K. Huang, Q. Hu, and
B. Wu, ‚ÄúSpurious feature eraser: Stabilizing test-time adaptation for
vision-language foundation model,‚Äù in *AAAI*, 2024.

[185] A. Li, L. Zhuang, X. Long, M. Yao, and S. Wang, ‚ÄúTest-time loss
landscape adaptation for zero-shot generalization in vision-language
models,‚Äù *arXiv preprint arXiv:2501.18864*, 2025.

[186] S. Xing, Z. Zhao, and N. Sebe, ‚ÄúClip is strong enough to fight back:
Test-time counterattacks towards zero-shot adversarial robustness of
clip,‚Äù in *CVPR*, 2025.

[187] M. Wysocza¬¥nska, M. Ramamonjisoa, T. Trzci¬¥nski, and O. Sim¬¥eoni,
‚ÄúClip-diy: Clip dense inference yields open-vocabulary semantic segmentation for-free,‚Äù in *WACV*, 2024.

[188] T. Shao, Z. Tian, H. Zhao, and J. Su, ‚ÄúExplore the potential of clip for
training-free open vocabulary semantic segmentation,‚Äù in *ECCV*, 2024.

[189] D. Kang and M. Cho, ‚ÄúIn defense of lazy visual grounding for openvocabulary semantic segmentation,‚Äù in *ECCV*, 2024.

[190] J. Li, H. Li, S. Erfani, L. Feng, J. Bailey, and F. Liu, ‚ÄúVisual-text cross
alignment: Refining the similarity score in vision-language models,‚Äù in
*ICML*, 2024.

[191] X. Yin, Q. Wang, B. Cao, and Q. Hu, ‚ÄúS3: Synonymous semantic space
for improving zero-shot generalization of vision-language models,‚Äù
*arXiv preprint arXiv:2412.04925*, 2024.

[192] A. Abdelhamed, M. Afifi, and A. Go, ‚ÄúWhat do you see? enhancing
zero-shot image classification with multimodal large language models,‚Äù
*arXiv preprint arXiv:2405.15668*, 2024.

[193] C. Wei, ‚ÄúEnhancing fine-grained image classifications via cascaded
vision language models,‚Äù in *EMNLP Findings*, 2024.

[194] A. Munir, F. Z. Qureshi, M. H. Khan, and M. Ali, ‚ÄúTlac: Two-stage
lmm augmented clip for zero-shot classification,‚Äù in *CVPR Workshops*,
2025.

[195] K. Miller, S. Mishra, A. Gangrade, K. Saenko, and V. Saligrama,
‚ÄúSparc: Score prompting and adaptive fusion for zero-shot multi-label
recognition in vision-language models,‚Äù in *CVPR*, 2025.

[196] R. Esfandiarpoor and S. H. Bach, ‚ÄúFollow-up differential descriptions:
Language models resolve ambiguities for image classification,‚Äù in
*ICLR*, 2023.

[197] Z. Guo, B. Dong, Z. Ji, J. Bai, Y. Guo, and W. Zuo, ‚ÄúTexts as images
in prompt tuning for multi-label image recognition,‚Äù in *CVPR*, 2023.

[198] Z. Liu, H. Sun, Y. Peng, and J. Zhou, ‚ÄúDart: Dual-modal adaptive
online prompting and knowledge retention for test-time adaptation,‚Äù in
*AAAI*, 2024.

[199] G. A. V. Hakim, D. Osowiechi, M. Noori, M. Cheraghalikhani,
A. Bahri, M. Yazdanpanah, I. B. Ayed, and C. Desrosiers, ‚ÄúClipartt:
Adaptation of clip to new domains at test time,‚Äù in *WACV*, 2024.

[200] S. Mishra, J. Silva-Rodrƒ±guez, I. B. Ayed, M. Pedersoli, and J. Dolz,
‚ÄúWords matter: Leveraging individual text embeddings for code generation in clip test-time adaptation,‚Äù *arXiv preprint arXiv:2411.17002*,
2024.



[201] D. Osowiechi, M. Noori, G. A. V. Hakim, M. Yazdanpanah,
A. Bahri, M. Cheraghalikhani, S. Dastani, F. Beizaee, I. B. Ayed, and
C. Desrosiers, ‚ÄúWatt: Weight average test-time adaption of clip,‚Äù in
*NeurIPS*, 2024.

[202] Y. Zhang, W. Zhu, H. Tang, Z. Ma, K. Zhou, and L. Zhang, ‚ÄúDual
memory networks: A versatile adaptation approach for vision-language
models,‚Äù in *CVPR*, 2024.

[203] C. Zhang, S. Stepputtis, K. Sycara, and Y. Xie, ‚ÄúDual prototype
evolving for test-time generalization of vision-language models,‚Äù in
*NeurIPS*, 2024.

[204] X. Hu, K. Zhang, M. Sun, A. Chen, C.-H. Kuo, and R. Nevatia, ‚ÄúBafta:
Backprop-free test-time adaptation for zero-shot vision-language models,‚Äù *arXiv preprint arXiv:2406.11309*, 2024.

[205] C. Fuchs, M. Zanella, and C. De Vleeschouwer, ‚ÄúOnline gaussian test-time adaptation of vision-language models,‚Äù *arXiv preprint*
*arXiv:2501.04352*, 2025.

[206] Z. Han, J. Yang, J. Li, Q. Hu, Q. Xu, M. Z. Shou, and C. Zhang,
‚ÄúDota: Distributional test-time adaptation of vision-language models,‚Äù
*arXiv preprint arXiv:2409.19375*, 2024.

[207] L. Zhou, M. Ye, S. Li, N. Li, X. Zhu, L. Deng, H. Liu, and Z. Lei,
‚ÄúBayesian test-time adaptation for vision-language models,‚Äù in *CVPR*,
2025.

[208] Y. Zhou, J. Ren, F. Li, R. Zabih, and S. N. Lim, ‚ÄúTest-time distribution
normalization for contrastively learned visual-language models,‚Äù in
*NeurIPS*, 2023.

[209] Z. Xiao, S. Yan, J. Hong, J. Cai, X. Jiang, Y. Hu, J. Shen, C. Wang,
and C. G. M. Snoek, ‚ÄúDynaprompt: Dynamic test-time prompt tuning,‚Äù
in *ICLR*, 2025.

[210] Z. Wang, D. Gong, S. Wang, Z. Huang, and Y. Luo, ‚ÄúIs less more?
exploring token condensation as training-free test-time adaptation,‚Äù in
*ICCV*, 2025.

[211] Y. Li, Y. Su, A. Goodge, K. Jia, and X. Xu, ‚ÄúEfficient and contextaware label propagation for zero-/few-shot training-free adaptation of
vision-language model,‚Äù in *ICLR*, 2025.

[212] J. Ma, ‚ÄúImproved self-training for test-time adaptation,‚Äù in *CVPR*,
2024.

[213] F. Wang, Z. Han, X. Liu, Y. Yin, and X. Gao, ‚ÄúCtpt: Continual testtime prompt tuning for vision-language models,‚Äù *Pattern Recognition*,
vol. 161, p. 111300, 2025.

[214] R. Wang, H. Zuo, Z. Fang, and J. Lu, ‚ÄúTowards robustness prompt tuning with fully test-time adaptation for clip‚Äôs zero-shot generalization,‚Äù
in *ACM-MM*, 2024.

[215] C. Cao, Z. Zhong, Z. Zhou, T. Liu, Y. Liu, K. Zhang, and B. Han,
‚ÄúNoisy test-time adaptation in vision-language models,‚Äù in *ICLR*, 2025.

[216] M. Sreenivas and S. Biswas, ‚ÄúEffectiveness of vision language models
for open-world single image test time adaptation,‚Äù *arXiv preprint*
*arXiv:2406.00481*, 2024.

[217] Z. Lu, J. Bai, X. Li, Z. Xiao, and X. Wang, ‚ÄúTask-to-instance prompt
learning for vision-language models at test time,‚Äù *IEEE Transactions*
*on Image Processing*, vol. 34, pp. 1908‚Äì1920, 2025.

[218] F. Huang, J. Jiang, Q. Jiang, H. Li, F. N. Khan, and Z. Wang, ‚ÄúCosmic:
Clique-oriented semantic multi-space integration for robust clip testtime adaptation,‚Äù in *CVPR*, 2025.

[219] C. Ding, X. Gao, S. Dong, Y. He, Q. Wang, X. Song, A. Kot, and
Y. Gong, ‚ÄúSpace rotation with basis transformation for training-free
test-time adaptation,‚Äù *arXiv preprint arXiv:2502.19946*, 2025.

[220] R. Wang, H. Zuo, Z. Fang, and J. Lu, ‚ÄúPrompt-based memory bank
for continual test-time domain adaptation in vision-language models,‚Äù
in *IJCNN*, 2024.

[221] B. Tong, K. Song, and H. Lai, ‚ÄúTest-time alignment-enhanced adapter
for vision-language models,‚Äù *arXiv preprint arXiv:2411.15735*, 2024.

[222] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li,
‚ÄúTip-adapter: Training-free adaption of clip for few-shot classification,‚Äù
in *ECCV*, 2022.

[223] T. Zhang, J. Wang, H. Guo, T. Dai, B. Chen, and S.-T. Xia, ‚ÄúBoostadapter: Improving vision-language test-time adaptation via regional
bootstrapping,‚Äù in *NeurIPS*, 2024.

[224] J. Zhang, J. Huang, X. Zhang, L. Shao, and S. Lu, ‚ÄúHistorical test-time
prompt tuning for vision foundation models,‚Äù in *NeurIPS*, 2024.

[225] D.-C. Zhang, Z. Zhou, and Y.-F. Li, ‚ÄúRobust test-time adaptation for
zero-shot prompt tuning,‚Äù in *AAAI*, 2024.

[226] H. Zhai, X. Chen, C. Zhang, T. Sha, and R. Li, ‚ÄúMitigating cache
noise in test-time adaptation for large vision-language models,‚Äù in *ICLR*
*Workshops*, 2025.

[227] J. Yi, R. Pan, J. Yang, and X. Yang, ‚ÄúMint: Memory-infused prompt
tuning at test-time for clip,‚Äù *arXiv preprint arXiv:2506.03190*, 2025.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19



[228] S. K. Maharana, B. Zhang, L. Karlinsky, R. Feris, and Y. Guo, ‚ÄúBatclip:
Bimodal online test-time adaptation for clip,‚Äù in *ICCV*, 2025.

[229] Z. Xiao, J. Shen, M. M. Derakhshani, S. Liao, and C. G. Snoek, ‚ÄúAnyshift prompting for generalization over distributions,‚Äù in *CVPR*, 2024.

[230] S. Cui, J. Xu, Y. Li, X. Tang, J. Li, J. Zhou, F. Xu, F. Sun,
and H. Xiong, ‚ÄúBayestta: Continual-temporal test-time adaptation for
vision-language models via gaussian discriminant analysis,‚Äù *arXiv*
*preprint arXiv:2507.08607*, 2025.

[231] Q. Dai and S. Yang, ‚ÄúFree on the fly: Enhancing flexibility in test-time
adaptation with online em,‚Äù in *CVPR*, 2025.

[232] H. Chen, Y. Xu, Y. Xu, Y. Zhang, and L. Cui, ‚ÄúTest-time medical image
segmentation using clip-guided sam adaptation,‚Äù in *BIBM*, 2024.

[233] M. Lafon, G. A. V. Hakim, C. Rambour, C. Desrosier, and N. Thome,
‚ÄúCliptta: Robust contrastive vision-language test-time adaptation,‚Äù
*arXiv preprint arXiv:2507.14312*, 2025.

[234] H. Han, A. J. Wang, and F. Liu, ‚ÄúNegation-aware test-time adaptation
for vision-language models,‚Äù *arXiv preprint arXiv:2507.19064*, 2025.

[235] R. Yan, J. Wang, H. Qu, X. Du, D. Zhang, J. Tang, and T. Tan, ‚ÄúTest-v:
Test-time support-set tuning for zero-shot video classification,‚Äù *arXiv*
*preprint arXiv:2502.00426*, 2025.

[236] Q. Zhang, M. Zhao, J. Liu, F. Zhang, Y. Xu, and Z.-J. Zha, ‚ÄúHierarchical knowledge prompt tuning for multi-task test-time adaptation,‚Äù
in *CVPR*, 2025.

[237] B. Tong, H. Lai, Y. Pan, and J. Yin, ‚ÄúOn the zero-shot adversarial
robustness of vision-language models: A truly zero-shot and trainingfree approach,‚Äù in *CVPR*, 2025.

[238] X. Chen, J. Huang, Q. Jiang, F. Huang, X. Fu, J. Jiang, and Z. Wang,
‚ÄúSmall aid, big leap: Efficient test-time adaptation for vision-language
models with adaptnet,‚Äù *arXiv preprint arXiv:2506.02671*, 2025.

[239] K. Adachi, S. Yamaguchi, and T. Hamagami, ‚ÄúUniformity first:
Uniformity-aware test-time adaptation of vision-language models
against image corruption,‚Äù *arXiv preprint arXiv:2505.12912*, 2025.

[240] D. Sarkar, A. Chakrabartty, B. Bhanja, and A. Das, ‚ÄúActive test
time prompt learning in vision-language models,‚Äù 2024. [Online].
[Available: https://openreview.net/forum?id=pdzHpQbGrn](https://openreview.net/forum?id=pdzHpQbGrn)

[241] L. Fei-Fei, R. Fergus, and P. Perona, ‚ÄúLearning generative visual
models from few training examples: An incremental bayesian approach
tested on 101 object categories,‚Äù in *CVPR Workshops*, 2004.

[242] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar, ‚ÄúCats and
dogs,‚Äù in *CVPR*, 2012.

[243] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, ‚Äú3d object representations
for fine-grained categorization,‚Äù in *ICCV Workshops*, 2013.

[244] M.-E. Nilsback and A. Zisserman, ‚ÄúAutomated flower classification
over a large number of classes,‚Äù in *ICVGIP*, 2008.

[245] L. Bossard, M. Guillaumin, and L. Van Gool, ‚ÄúFood-101‚Äìmining
discriminative components with random forests,‚Äù in *ECCV*, 2014.

[246] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi,
‚ÄúFine-grained visual classification of aircraft,‚Äù *arXiv* *preprint*
*arXiv:1306.5151*, 2013.

[247] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, ‚ÄúSun
database: Large-scale scene recognition from abbey to zoo,‚Äù in *CVPR*,
2010.

[248] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi,
‚ÄúDescribing textures in the wild,‚Äù in *CVPR*, 2014.

[249] P. Helber, B. Bischke, A. Dengel, and D. Borth, ‚ÄúEurosat: A novel
dataset and deep learning benchmark for land use and land cover
classification,‚Äù *IEEE Journal of Selected Topics in Applied Earth*
*Observations and Remote Sensing*, vol. 12, no. 7, pp. 2217‚Äì2226, 2019.

[250] K. Soomro, A. R. Zamir, and M. Shah, ‚ÄúUcf101: A dataset of
101 human actions classes from videos in the wild,‚Äù *arXiv preprint*
*arXiv:1212.0402*, 2012.

[251] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:
A large-scale hierarchical image database,‚Äù in *CVPR*, 2009.

[252] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, ‚ÄúNatural
adversarial examples,‚Äù in *CVPR*, 2021.

[253] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, ‚ÄúDo imagenet
classifiers generalize to imagenet?‚Äù in *ICML*, 2019.

[254] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo,
R. Desai, T. Zhu, S. Parajuli, M. Guo *et al.*, ‚ÄúThe many faces of
robustness: A critical analysis of out-of-distribution generalization,‚Äù in
*ICCV*, 2021.

[255] H. Wang, S. Ge, Z. Lipton, and E. P. Xing, ‚ÄúLearning robust global
representations by penalizing local predictive power,‚Äù in *NeurIPS*,
2019.

[256] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan,
‚ÄúDeep hashing network for unsupervised domain adaptation,‚Äù in *CVPR*,
2017.



[257] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, ‚ÄúMoment
matching for multi-source domain adaptation,‚Äù in *ICCV*, 2019.

[258] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,
and A. Zisserman, ‚ÄúThe pascal visual object classes challenge: A
retrospective,‚Äù *International Journal of Computer Vision*, vol. 111, pp.
98‚Äì136, 2015.

[259] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler,
R. Urtasun, and A. Yuille, ‚ÄúThe role of context for object detection
and semantic segmentation in the wild,‚Äù in *CVPR*, 2014.

[260] H. Caesar, J. Uijlings, and V. Ferrari, ‚ÄúCoco-stuff: Thing and stuff
classes in context,‚Äù in *CVPR*, 2018.

[261] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and
A. Torralba, ‚ÄúSemantic understanding of scenes through the ade20k
dataset,‚Äù *International Journal of Computer Vision*, vol. 127, pp. 302‚Äì
321, 2019.

[262] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in
context,‚Äù in *ECCV*, 2014.

[263] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset for
semantic urban scene understanding,‚Äù in *CVPR*, 2016.

[264] M. Weber, J. Xie, M. Collins, Y. Zhu, P. Voigtlaender, H. Adam,
B. Green, A. Geiger, B. Leibe, D. Cremers *et al.*, ‚ÄúStep: Segmenting
and tracking every pixel,‚Äù *arXiv preprint arXiv:2102.11859*, 2021.

[[265] F. Panella, V. Melatti, and J. Boehm, ‚ÄúFirenet dataset,‚Äù http://www.](http://www.firenet.xyz)
[firenet.xyz, accessed: 2022-05-17.](http://www.firenet.xyz)

[266] H. Jiang, X. Ma, W. Nie, Z. Yu, Y. Zhu, and A. Anandkumar,
‚ÄúBongard-hoi: Benchmarking few-shot visual reasoning for humanobject interactions,‚Äù in *CVPR*, 2022.

[267] A. Krizhevsky, G. Hinton *et al.*, ‚ÄúLearning multiple layers of features
from tiny images,‚Äù 2009.

[268] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, ‚ÄúThe
caltech-ucsd birds-200-2011 dataset,‚Äù 2011.

[269] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard,
H. Adam, P. Perona, and S. Belongie, ‚ÄúThe inaturalist species classification and detection dataset,‚Äù in *CVPR*, 2018.

[270] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, ‚ÄúPlaces:
A 10 million image database for scene recognition,‚Äù *IEEE Transactions*
*on Pattern Analysis and Machine Intelligence*, vol. 40, no. 6, pp. 1452‚Äì
1464, 2017.

[271] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija,
A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit *et al.*, ‚ÄúOpenimages: A public dataset for large-scale multi-label and multi-class image
classification,‚Äù *Dataset available from https://github. com/openimages*,
vol. 2, no. 3, p. 18, 2017.

[272] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, ‚ÄúFlickr30k entities: Collecting region-to-phrase
correspondences for richer image-to-sentence models,‚Äù in *ICCV*, 2015.

[273] N. Rostamzadeh, S. Hosseini, T. Boquet, W. Stokowiec, Y. Zhang,
C. Jauvin, and C. Pal, ‚ÄúFashion-gen: The generative fashion dataset
and challenge,‚Äù *arXiv preprint arXiv:1806.08317*, 2018.

[274] S. Li, T. Xiao, H. Li, B. Zhou, D. Yue, and X. Wang, ‚ÄúPerson search
with natural language description,‚Äù in *CVPR*, 2017.

[275] Z. Ding, C. Ding, Z. Shao, and D. Tao, ‚ÄúSemantically self-aligned
network for text-to-image part-aware person re-identification,‚Äù *arXiv*
*preprint arXiv:2107.12666*, 2021.

[276] H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra,
D. Parikh, S. Lee, and P. Anderson, ‚ÄúNocaps: Novel object captioning
at scale,‚Äù in *ICCV*, 2019.

[277] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L.
Baxter, A. McKeown, G. Yang, X. Wu, F. Yan *et al.*, ‚ÄúIdentifying medical diagnoses and treatable diseases by image-based deep learning,‚Äù
*Cell*, vol. 172, no. 5, pp. 1122‚Äì1131, 2018.

[278] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. W¬¥ang, P.-X. Lu, and
G. Thoma, ‚ÄúTwo public chest x-ray datasets for computer-aided screening of pulmonary diseases,‚Äù *Quantitative Imaging in Medicine and*
*Surgery*, vol. 4, no. 6, p. 475, 2014.

[279] P. Porwal, S. Pachade, R. Kamble, M. Kokare, G. Deshmukh, V. Sahasrabuddhe, and F. Meriaudeau, ‚ÄúIndian diabetic retinopathy image
dataset (idrid): a database for diabetic retinopathy screening research,‚Äù
*Data*, vol. 3, no. 3, p. 25, 2018.

[280] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti,
S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler *et al.*, ‚ÄúSkin
lesion analysis toward melanoma detection: A challenge at the 2017
international symposium on biomedical imaging (isbi), hosted by the
international skin imaging collaboration (isic),‚Äù in *ISBI*, 2018.


-----

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20



[281] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, ‚ÄúHmdb: a
large video database for human motion recognition,‚Äù in *ICCV*, 2011.

[282] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman,
‚ÄúA short note about kinetics-600,‚Äù *arXiv preprint arXiv:1808.01340*,
2018.

[283] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles,
‚ÄúActivitynet: A large-scale video benchmark for human activity understanding,‚Äù in *CVPR*, 2015.

[284] H. Idrees, A. R. Zamir, Y.-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar, and M. Shah, ‚ÄúThe thumos challenge on action recognition
for videos ‚Äúin the wild‚Äù,‚Äù *Computer Vision and Image Understanding*,
vol. 155, pp. 1‚Äì23, 2017.

[285] D. Hendrycks and T. Dietterich, ‚ÄúBenchmarking neural network robustness to common corruptions and perturbations,‚Äù in *ICLR*, 2019.

[286] V. W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Y. Zou, ‚ÄúMind
the gap: Understanding the modality gap in multi-modal contrastive
representation learning,‚Äù in *NeurIPS*, 2022.

[287] H. Dong, Y. Zhao, E. Chatzi, and O. Fink, ‚ÄúMultiood: Scaling out-ofdistribution detection for multiple modalities,‚Äù in *NeurIPS*, 2024.

[288] S. Li, H. Gong, H. Dong, T. Yang, Z. Tu, and Y. Zhao, ‚ÄúDpu: Dynamic
prototype updating for multimodal out-of-distribution detection,‚Äù in
*CVPR*, 2025.

[289] M. Liu, H. Dong, J. Kelly, O. Fink, and M. Trapp, ‚ÄúExtremely
simple multimodal outlier synthesis for out-of-distribution detection
and segmentation,‚Äù *arXiv preprint arXiv:2505.16985*, 2025.

[290] C. Mao, S. Geng, J. Yang, X. Wang, and C. Vondrick, ‚ÄúUnderstanding
zero-shot adversarial robustness for large-scale models,‚Äù in *ICLR*, 2023.

[291] L. Li, H. Guan, J. Qiu, and M. Spratling, ‚ÄúOne prompt word is enough
to boost adversarial robustness for pre-trained vision-language models,‚Äù
in *CVPR*, 2024.

[292] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, ‚ÄúTowards
deep learning models resistant to adversarial attacks,‚Äù *arXiv preprint*
*arXiv:1706.06083*, 2017.

[293] H. Dong, X. Chen, M. Dusmanu, V. Larsson, M. Pollefeys, and
C. Stachniss, ‚ÄúLearning-based dimensionality reduction for computing
compact and effective local feature descriptors,‚Äù in *ICRA*, 2023.

[294] A. Qayyum, J. Qadir, M. Bilal, and A. Al-Fuqaha, ‚ÄúSecure and
robust machine learning for healthcare: A survey,‚Äù *IEEE Reviews in*
*Biomedical Engineering*, vol. 14, pp. 156‚Äì180, 2020.

[295] W. Bao, T. Wei, H. Wang, and J. He, ‚ÄúAdaptive test-time personalization for federated learning,‚Äù in *NeurIPS*, 2023.

[296] J. Lee, D. Kim, and B. Ham, ‚ÄúNetwork quantization with element-wise
gradient scaling,‚Äù in *CVPR*, 2021.

[297] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, ‚ÄúRethinking the
value of network pruning,‚Äù *arXiv preprint arXiv:1810.05270*, 2018.

[298] J. Gou, B. Yu, S. J. Maybank, and D. Tao, ‚ÄúKnowledge distillation: A
survey,‚Äù *International Journal of Computer Vision*, vol. 129, no. 6, pp.
1789‚Äì1819, 2021.

[299] G. Kwon, Z. Cai, A. Ravichandran, E. Bas, R. Bhotika, and S. Soatto,
‚ÄúMasked vision and language modeling for multi-modal representation
learning,‚Äù *arXiv preprint arXiv:2208.02131*, 2022.

[300] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu,
Z. Chen, Z. Yang, and M. Z. Shou, ‚ÄúShow-o: One single transformer
to unify multimodal understanding and generation,‚Äù *arXiv preprint*
*arXiv:2408.12528*, 2024.

[301] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi,
L. Zettlemoyer, P. Liang, E. Cand`es, and T. Hashimoto, ‚Äús1: Simple
test-time scaling,‚Äù *arXiv preprint arXiv:2501.19393*, 2025.

[302] C. Snell, J. Lee, K. Xu, and A. Kumar, ‚ÄúScaling llm test-time compute
optimally can be more effective than scaling model parameters,‚Äù *arXiv*
*preprint arXiv:2408.03314*, 2024.

[303] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye,
H. Tian, Z. Liu *et al.*, ‚ÄúExpanding performance boundaries of opensource multimodal models with model, data, and test-time scaling,‚Äù
*arXiv preprint arXiv:2412.05271*, 2024.

[304] I. Nejjar, Q. Wang, and O. Fink, ‚ÄúDare-gram: Unsupervised domain
adaptation regression by aligning inverse gram matrices,‚Äù in *CVPR*,
2023.

[305] C. Yang, Y. Shen, Z. Zhang, Y. Xu, J. Zhu, Z. Wu, and B. Zhou,
‚ÄúOne-shot generative domain adaptation,‚Äù in *ICCV*, 2023.

[306] H. Li, P. Hu, Q. Zhang, X. Peng, X. Liu, and M. Yang, ‚ÄúTest-time
adaptation for cross-modal retrieval with query shift,‚Äù in *ICLR*, 2024.

[307] H. Park, A. Gupta, and A. Wong, ‚ÄúTest-time adaptation for depth
completion,‚Äù in *CVPR*, 2024.

[308] H. Dong, M. Liu, J. Liang, E. Chatzi, and O. Fink, ‚ÄúTo trust or
not to trust your vision-language model‚Äôs prediction,‚Äù *arXiv preprint*
*arXiv:2505.23745*, 2025.



[309] Z. Deng, Z. Chen, S. Niu, T. Li, B. Zhuang, and M. Tan, ‚ÄúEfficient
test-time adaptation for super-resolution with second-order degradation
and reconstruction,‚Äù in *NeurIPS*, 2023.

[310] B. C. Kalp¬¥elb¬¥e, A. G. Adaambiik, and W. Peng, ‚ÄúVision language
models in medicine,‚Äù *arXiv preprint arXiv:2503.01863*, 2025.

[311] K. Zhang, R. Zhou, E. Adhikarla, Z. Yan, Y. Liu, J. Yu, Z. Liu, X. Chen,
B. D. Davison, H. Ren *et al.*, ‚ÄúA generalist vision‚Äìlanguage foundation
model for diverse biomedical tasks,‚Äù *Nature Medicine*, pp. 1‚Äì13, 2024.

[312] V. Rawte, A. Sheth, and A. Das, ‚ÄúA survey of hallucination in large
foundation models,‚Äù *arXiv preprint arXiv:2309.05922*, 2023.

**Hao Dong** is a Ph.D. student at ETH Z¬®urich, Z¬®urich, Switzerland. He
received the B.S. degree from Xi‚Äôan Jiaotong University, Xi‚Äôan, China, in
2020, and the M.S. degree from Aalto University, Espoo, Finland, in 2022.
His research interest lies in multimodal learning and sensor fusion, including
their applications in robotics, computer vision, and anomaly detection.

**Lijun Sheng** received the B.E. degree in Automation from the University of
Science and Technology of China (USTC), in July 2020. He is now a Ph.D.
candidate in the Department of Automation at the University of Science and
Technology of China. His research interests include transfer learning, domain
adaptation, and trustworthy AI.

**Jian Liang** received his B.E. degree in Electronic Information and Technology
from Xi‚Äôan Jiaotong University in July 2013 and his Ph.D. degree in Pattern
Recognition and Intelligent Systems from the National Laboratory of Pattern
Recognition (NLPR), CASIA, in January 2019. From June 2019 to April 2021,
he worked as a research fellow at the National University of Singapore. He
is currently an associate professor at NLPR, CASIA. His research interests
include transfer learning, pattern recognition, and computer vision.

**Ran He** received the BE degree in computer science from the Dalian
University of Technology, in 2001, the MS degree in computer science
from the Dalian University of Technology, in 2004, and the PhD degree
in pattern recognition and intelligent systems from CASIA, in 2009. Since
September 2010, he joined NLPR, where he is currently a full professor. His
research interests include information theoretic learning, pattern recognition,
and computer vision. He serves as the editor board member of IEEE TPAMI,
TIP, TIFS, TCSVT, and TBIOM, and serves on the program committee of
several conferences. He is also a fellow of IEEE and IAPR.

**Eleni Chatzi** received the Diploma and M.Sc. degrees in civil engineering
from the Department of Civil Engineering, National Technical University of
Athens (NTUA), Athens, Greece, in 2004 and 2006, respectively, and the
Ph.D. degree (with distinction) from the Department of Civil Engineering and
Engineering Mechanics, Columbia University, New York, NY, USA, in 2010.
She is currently a Full Professor at the Chair of Structural Mechanics with the
Department of Civil, Environmental and Geomatic Engineering, ETH Z¬®urich,
Z¬®urich, Switzerland. Her research spans a broad range of topics, including
applications on emerging sensor technologies and structural control, methods
for curbing uncertainties in structural diagnostics and lifecycle assessment, as
well as advanced schemes for nonlinear/nonstationary dynamics simulations.

**Olga Fink** received the Diploma degree in industrial engineering from the
Hamburg University of Technology, Hamburg, Germany, in 2008, and the
Ph.D. degree from ETH Z¬®urich, Z¬®urich, Switzerland, in 2014. She has been
an Assistant Professor of Intelligent Maintenance and Operations Systems
with EPFL, Lausanne, Switzerland, since March 2022. Before joining EPFL
Faculty, she was an Assistant Professor of Intelligent Maintenance Systems
with ETH Z¬®urich from 2018 to 2022. From 2014 and 2018, she was heading
the research group ‚ÄúSmart Maintenance‚Äù with the Zurich University of Applied Sciences, Winterthur, Switzerland. And, she is also a Research Affiliate
with the Massachusetts Institute of Technology, Cambridge, CA, USA. Her
research focuses on hybrid algorithms fusing physics-based models and deep
learning algorithms, hybrid operational digital twins, transfer learning, selfsupervised learning, deep reinforcement learning, and multiagent systems for
intelligent maintenance and operations of infrastructure and complex assets.


-----


