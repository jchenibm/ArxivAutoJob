**LEARNING GEOMETRIC-AWARE QUADRATURE RULES**

**FOR FUNCTIONAL MINIMIZATION**

COSTAS SMARAGDAKIS

Abstract. Accurate numerical integration over non-uniform point clouds is a challenge for modern
mesh-free machine learning solvers for partial differential equations (PDEs) using variational principles.
While standard Monte Carlo (MC) methods are not capable of handling a non-uniform point cloud,
modern neural network architectures can deal with permutation-invariant inputs, creating quadrature
rules for any point cloud. In this work, we introduce `QuadrANN`, a Graph Neural Network (GNN)
architecture designed to learn optimal quadrature weights directly from the underlying geometry of point
clouds. The design of the model exploits a deep message-passing scheme where the initial layer encodes
rich local geometric features from absolute and relative positions as well as an explicit local density
measure. In contrast, the following layers incorporate a global context vector. These architectural
choices allow the `QuadrANN` to generate a data-driven quadrature rule that is permutation-invariant and
adaptive to both local point density and the overall domain shape. We test our methodology on a series
of challenging test cases, including integration on convex and non-convex domains and estimating the
solution of the Heat and Fokker-Planck equations. Across all the tests, `QuadrANN` reduces the variance of
the integral estimation compared to standard Quasi-Monte Carlo methods by warping the point clouds
to be more dense in critical areas where the integrands present certain singularities. This enhanced
stability in critical areas of the domain at hand is critical for the optimization of energy functionals,
leading to improved deep learning-based variational solvers.

1 Introduction

Variational principles are fundamental to the mathematics and physical sciences, creating the ground for
solving challenging problems in fields covering classical and statistical mechanics to modern probability
theory. These principles often require the minimization a functional, which typically involves an integral
over a particular domain Ω:


*J* [ *u* ] = *L* ( *x, u* ( *x* ) *, ∇u* ( *x* )) *dx,* *u ∈* *H* [1] (Ω) *,* (1)
� Ω


where *L* is a given operator. The objective is to find a function *u* *[∗]* *∈* *H* [1] (Ω) that minimizes this
functional, i.e., *J* [ *u* *[∗]* ] *≤J* [ *u* ] for any function *u ∈* *H* [1] (Ω). Such a problem is equivalent to solving the
corresponding Euler-Lagrange partial differential equation (PDE), and the pursuit of its solution is a
cornerstone of computational science.

For many years, the numerical solution of these problems has been driven by mesh-based methods like
the Finite Element Method (FEM)[1]. While powerful these methods are, they heavily rely on domain
meshing, which can be computationally restrictive for problems in high-dimensional spaces or with
complex geometries. In recent years, we have witnessed a shift in the methodologies that are used to
solve problems, thanks to the introduction of deep learning-based solvers. Methods spanning the Deep
Ritz Method (DRM) [2] and Physics-Informed Neural Networks (PINNs) [3] have been introduced as
mesh-free alternatives. These modern approaches reformulate the variational problem as an optimisation

Department of Statistics and Actuarial-Financial Mathematics, University of the Aegean, Samos,
83200 Karlovassi, Greece & Institute of Applied and Computational Mathematics, FORTH, 70013 Heraklion, Greece
*E-mail address* : `kesmarag@aegean.gr` .
2020 *Mathematics Subject Classification.* 65D32,65N12,68T07.
*Key words and phrases.* Numerical integration, Quadrature rules, Graph Neural Networks, Deep learning, Partial
differential equations, Variational methods, Mesh-free numerical methods.

1


-----

2 2 INTRODUCTION

problem, where now the parameters of an Artificial Neural Network (ANN), representing the solution
*u*, are trained to minimize the functional *J* [ *u* ].

A significant aspect of deep learning solvers is the accurate numerical approximation of the integral
shown in Equation (ˆ 1). The solving process depends heavily on the minimization of the discretized
functional, *J* [ *u* ], typically evaluated with respect to a given set of sample points. The accuracy of
this numerical integration is critical for obtaining an accurate solution. As this work will underscore,
unless the approximation *J* [ˆ] [ *u* ] is highly accurate, the minimizer ˆ *u* *[∗]* obtained from training the ANN
will generally not be a reasonable estimate of the actual minimizer *u* *[∗]* of the original functional.

The simplest and common approach for approximating such integrals is, of course, the Monte Carlo (MC)
method. In an MC-driven approach, the integral is estimated by averaging the values of the integrand
over a set of randomly sampled points. While the universal MC approach is this method suffers from slow
convergence, with an error rate of *O* ( *N* *[−]* [1] *[/]* [2] ), requiring a large number of sample points *n* for acceptable
accuracy. Variants like Quasi-Monte Carlo (QMC) methods offer faster convergence for certain classes
of integrands [4]. However, QMC methods may not be optimal for the complex/non-linear integrands
that often arise when training neural networks to solve high-dimensional PDEs, a challenge this work
aims to address.

This challenge has motivated research into more sophisticated quadrature strategies. For instance,
Bayesian Quadrature models the integrand as a Gaussian process to provide a posterior distribution
over the value of the integral, which can be more sample-efficient [5]. Additionally, the concept of
adaptive quadrature that concentrates more points to areas where the integrand is complex or varies
rapidly, is well-established in classical numerical analysis [6] and is beginning to find its way into deep
learning contexts.

In this work, we claim that for variational problems solved with Artificial Neural Networks (ANNs), a
static quadrature rule is generally insufficient. Modern deep learning solvers often employ non-uniform
point distributions that are chosen adaptively to minimize the PDE residual. This dynamic nature of
point sampling necessitates a quadrature rule that can also adapt to arbitrary non-uniformity.

We therefore develop a ANN-based approach for numerical integration, focusing on learning optimal
quadrature rules directly from the sample points. Instead of relying on predefined static weights, we
propose a model that learns a direct mapping from a set of points to a corresponding set of optimal
quadrature weights. This is achieved by training the ANN to produce weights that guarantee the exact
(or almost exact) integration of a set of basis functions, creating a highly accurate and data-driven
quadrature rule.

Our work builds upon a growing interest in machine learning for numerical integration. This field
includes probabilistic methods like Bayesian Quadrature, which models the integrand as a Gaussian
process to estimate the uncertainty [7] of the integral, and direct approaches that train neural networks
to predict optimal quadrature weights for specific function classes or domains [2, 8, 9]. Unlike methods
that focus on learning adaptive sampling policies, our approach learns a direct mapping from a point
cloud configuration to its corresponding weights. Our method is particularly suited for the non-uniform
sampling policies inherent in modern deep learning solvers that aim to close the gap between the
theoretical functional and its practical, discretised approximations.

The remainder of this work is as follows. In Section 2, we recap the neural quadrature problem for
functional integrals. Section 3 introduces our approach for generating non-uniform point clouds model
for model training and testing. In Section 4, we present the proposed QuadrANN architecture and
analyze our architectural decisions. Section 5 presents applications to demonstrate the performance of
QuadrANN, and finally, Section 6 provides the conclusion.


-----

3


2 Neural Quadrature for Functional Integrals


Let domain Ω *⊂* R *[d]* . The core idea in a quadrature rule is to estimate an integral using a weighted sum
over a set of *n* points *X* = *{x* 1 *, x* 2 *, . . ., x* *n* *}* sampled from Ω:

*N*

*L* ( *x, u* ( *x* ) *, ∇u* ( *x* )) *dx ≈* � *w* *i* *L* ( *x* *i* *, u* ( *x* *i* ) *, ∇u* ( *x* *i* )) *.* (2)

� Ω


*L* ( *x, u* ( *x* ) *, ∇u* ( *x* )) *dx ≈*
Ω


*N*
�


� *w* *i* *L* ( *x* *i* *, u* ( *x* *i* ) *, ∇u* ( *x* *i* )) *.* (2)

*i* =1


The main challenge here is to find the optimal weights ( *w* 1 *, . . ., w* *n* ) for a given set of points *X* . These
weights should depend on the overall geometry of the point cloud but be independent of the ordering
in which the points are provided to a model. To achieve this, we represent the mapping from the point
set *X* to the weights ( *w* *i* ) *[n]* *i* =1 [by means of an Artificial Neural Network (ANN) that is insensitive to the]
permutation of its inputs. We denote this network as *W* ( *·* ; *θ* ), which takes the entire set *X* as input
and produces *N* corresponding weights. The integral approximation is then:


*L* ( *x, u* ( *x* ) *, ∇u* ( *x* )) *dx ≈*

� Ω


*n*
� *W* *i* ( *X* ; *θ* ) *L* ( *x* *i* *, u* ( *x* *i* ) *, ∇u* ( *x* *i* )) *,* (3)

*i* =1


where *W* *i* ( *·* ; *θ* ) is the *i* -th output of the network corresponding to the point *x* *i* . While the underlying
framework is applicable to more general domains, the theoretical development of our ANN quadrature
rule will henceforth be mainly focused on the *d* -dimensional hypercube Ω= [0 *,* 1] *[d]* . However, we will
expand our analysis to include more complex geometries, such as a non-convex L-shaped domain in 2D
(see Subsection 5.1.2).

The network is trained to produce model parameters that guarantee a good integration performance
for a predefined basis set of test functions. This basis is designed to be highly versatile, combining
polynomial and trigonometric functions to effectively approximate a broad class of integrands. The core
of the basis consists of multivariate polynomials of total degree at most *J*, which is fixed at 5 throughout
this work, formed by tensor products of normalized Hermite polynomials, *{* Φ ***j*** ( *·* ) *}* :


*d*
� *j* *k* *≤* *J.* (4)

*k* =1


Φ ***j*** ( *x* ) =


*d*
� *H* ˜ *e* *jk* ( *x* *k* ) *,* where

*k* =1


The polynomials *H* [˜] *e* *jk* are normalised such that their integral over [0 *,* 1] is unity. To increase the
capability of the basis for oscillatory functions, the degree-zero Hermite polynomial is reformulated as
a trigonometric term:

˜
*H* *e* 0 ( *x* ; *κ, φ* ) = 1 + cos( *κπx* + *φ* ) *−* cos( *κπx* + *φ* ) *dx,* (5)
� Ω

where *κ* is an integer randomly sampled from *{* 1 *,* 2 *, . . ., K* max *}* during training and *φ ∈* [0 *,* 2 *π* ) is a
random phase . This hybrid construction, controlled by hyperparameters *J* and *K* max, creates a powerful
approximation space. The polynomial components are well-suited for capturing smooth, low-frequency
behaviour, while the randomized trigonometric terms are explicitly included to represent oscillatory
patterns that are difficult to approximate efficiently with a pure polynomial basis. It is important to
emphasize that this reformulation does not affect the ability of the model to integrate constants. This
property is instead enforced by the architecture of the network, as the final `softmax` activation layer
constrains the output weights to sum to unity ( [�] *W* *i* = 1).

The training objective is operationalized through the following discrete loss function, which penalizes
deviation from exactness on the test basis:


�


*C* ( *X* ; *θ* ) = �

***j*** : [�] *[d]* *k* =1 *[j]* *[k]* *[≤][J]*


*n*
�
�� *i* =1


� *W* *i* ( *X* ; *θ* )Φ ***j*** ( *x* *i* )

*i* =1


*−* 1


2

*.* (6)
�


The training phase consists of multiple iterations where a new set of points *X* is obtained by resampling,
and the model parameters *θ* are updated using the ADAM optimizer.


-----

4 4 NEURAL QUADRATURE FOR FUNCTIONAL INTEGRALS

3 Generating Non-Uniform Point Clouds

To ensure robust training and evaluation of our neural network-based quadrature model, it is essential to
be capable of generating point clouds that not only cover the entire domain but also feature non-uniform
densities. This approach simulates the conditions commonly encountered in practical applications,
such as adaptive mesh refinement or scenarios where physical phenomena are concentrated in specific
areas. Our procedure for generating these point clouds is a two-step process that combines the superior
uniformity of Quasi-Monte Carlo sequences with a deterministic, non-linear transformation.

**3.1** **Quasi-Random Base Sampling** Instead of using entirely random numbers, which can lead to
the formation of gaps, we are generating sets of points with low-discrepancy characteristics using a QuasiMonte Carlo (QMC) method. QMC sequences are designed to cover the sampling space more evenly
than entirely random sampling strategies. Specifically, in this work, we employ the Sobol sequence[10],
a widely-used standard for QMC integration.

Given *n*, we generate a base point set *S* = *{* **s** 1 *,* **s** 2 *, . . .,* **s** *n* *}*, where each point **s** *i* is drawn from the Sobol
sequence within the unit hypercube Ω= [0 *,* 1] *[d]* . This set *S* provides a basis for our sampling method.

**3.2** **Non-Linear Warping** While the Sobol sequence provides a well-behaved uniform distribution,
our goal is to create point clouds with varying densities. To achieve this in a controllable and general
manner, we can apply a bijective, non-linear transformation **G** : [0 *,* 1] *[d]* *→* [0 *,* 1] *[d]* to uniformly distributed
points. This vector-valued function warps the entire unit hypercube, allowing for complex, non-separable
distortions beyond simple coordinate-wise stretching.

This transformation deterministically modifies the local density of points. Regions where the transformation compresses space lead to higher point density, while regions that are expanded result in lower
density.

Each point **s** *i* *∈S* is to be mapped to a transformed point **x** *i* to create the final non-uniform point cloud
*X* = *{* **x** 1 *,* **x** 2 *, . . .,* **x** *n* *}* . The transformation is defined as follows:

**x** *i* = **G** ( **s** *i* ) *,* for *i* = 1 *, . . ., n.* (7)

This two-step procedure enables us to create on demand non-uniform point clouds with certain characteristics by simply modifying the warping function **G** ( *·* ). The resulting unordered set *X* serves then as
the direct input for our quadrature model, creating a robust framework to learn quadrature weights for
arbitrary point clouds.

4 The QuadrANN Architecture

To address the challenge of learning quadrature rules directly from point clouds, we introduce a new
neural network architecture, `QuadrANN` . It is a deep Graph Neural Network (GNN) architecture [11, 12]
that builds on principles from graph models for point clouds [13] and is designed to capture and utilize
geometric information at multiple scales effectively. Our approach is based on the principle that the
quadrature weight for a given point depends not only on its position but also on its role within the
geometry and structure of the domain. The architecture is composed of three primary stages: an initial
geometric encoding layer, a number of globally-aware propagation layers, and a final weight prediction
neural network.

Several critical architectural decisions were made to optimize `QuadrANN` for this task. First, to capture rich local information, we combine two powerful features: Positional Encoding (PE), a technique
popularized by Transformers and coordinate-based networks [14, 15], provides a high-frequency understanding of relative positions, while an explicit density estimate provides a direct, robust measure
of local point concentration. Second, to ensure the entire shape informs the learned weights, each
message-passing step is conditioned on a global context vector, a strategy used in seminal point cloud
models [16]. Third, inspired by `DenseNets` [17], we employ a dense concatenation of feature maps from


-----

4.2 Geometric Encoding Layer 5

all layers. This gives the prediction network simultaneous access to multi-scale features, mitigating the
over-smoothing problem, common in deep GNNs [18].

Ultimately, the complete model network is trained using a global softmax function in the final layer. This
activation function guarantees that the output weights are positive and sum to one, which satisfies the
mathematical properties of a quadrature rule. The following sections will provide a detailed explanation
of each component of this architecture.

The complete formulation of the model is presented in Algorithm 1.

**4.1** **Input Feature Engineering** In order to prepare the raw point cloud, we perform an initial
feature engineering on the input coordinates. The raw coordinates *{* **x** *i* *}* *[n]* *i* =1 [, assumed to lie within the]
unit hypercube [0 *,* 1] *[d]* (or in a subset of this domain). We first normalize the raw coordinates to the
typical domain [ *−* 1 *,* 1] *[d]* via the linear transformation **x** *i* *←* 2 **x** *i* *−* 1.

To enable the network to learn high-frequency patterns and to improve its ability to reason about
both absolute and relative positions of the points, we employ Positional Encoding (PE). Using PE is
a common technique that maps a low-dimensional continuous space into a higher-dimensional feature
space by employing a set of sinusoidal functions with various frequencies.

For a general input vector **x** *∈* R *[d]*, the positional encoding PE( **x** ) is constructed by applying the encoding
to each component of the vector and concatenating the results:

PE *p* ( **x** ) = ( *γ* ( *x* 1 ; *p* ) *, γ* ( *x* 2 ; *p* ) *, . . ., γ* ( *x* *d* ; *p* )) (8)

The encoding function *γ* ( *·* ; *p* ) for a single scalar component *α* is defined as:

*γ* ( *α* ; *p* ) = �sin(2 [0] *πα* ) *,* cos(2 [0] *πα* ) *, . . .,* sin(2 *[p][−]* [1] *πα* ) *,* cos(2 *[p][−]* [1] *πα* )�

where *p* is a hyperparameter representing the number of frequency bands. This multi-scale representation
provides the model with a rich, unambiguous signal for both fine-grained detail and global position,
resulting in a feature vector of dimension *d ×* 2 *p* .

**4.2** **Geometric Encoding Layer** The first layer of our network creates a high-dimensional latent
representation for each point **x** *i* *∈* R *[d]* . The dimensionality of these representations is denoted by *Q* 1,
which we constrain to be an even number. An even number ensures that the feature dimension can be
cleanly halved, as the final node features are formed by concatenating two vectors of equal size ( *Q* 1 */* 2).
The resulting latent vector is denoted as **o** [(1)] *i* *∈* R *[Q]* [1] .

We begin by defining two *k* -NN graphs. First, a graph with *k* *[′]* neighbours is used to compute a local
density feature, *ρ* *i* *∈* R, for each point. The density feature is defined to be the reciprocal of the mean
distance of a point’s neighbours to their local centroid, providing a robust measure of point concentration.
1
Specifically, for each point **x** *i*, we define its neighborhood centroid as **c** *i* = *k* *[′]* � *j* : **x** *j* *∈V* *k′* ( **x** *i* ) **[x]** *[j]* [, and the]

*−* 1
1
density is then *ρ* *i* = � *k* *[′]* � *j* : **x** *j* *∈V* *k′* ( **x** *i* ) *[∥]* **[x]** *[j]* *[ −]* **[c]** *[i]* *[∥]* [2] � . For computational efficiency, we can always set

*k* *[′]* = *k*, requiring the *k* -NN search to be performed only once for each point **x** *i* .

By incorporating the explicit density feature, *ρ* *j*, into the message-passing scheme, the model acquires
essential information about local point concentration. Although a GNN can implicitly deduce density
from its inputs, supplying a precise measure enables the model to reason about point density directly and
frees up model capacity, enabling the subsequent MLP to learn the more complex relationships between
geometry, density, and the resulting quadrature weights. Ultimately, this leads to a more robust and
accurate quadrature rule.

Subsequently, a graph with *k* neighbours determines the neighbourhood *V* *k* ( **x** *i* ), which facilitates the
main message passing among the points. Within this neighbourhood, a message vector, *G* *ij*, is generated
by augmenting the geometric features with the pre-computed density feature of the neighbours:

*G* *ij* = [ **x** *j* *,* PE *p* 1 ( **x** *j* ) *,* **x** *j* *−* **x** *i* *,* PE *p* 2 ( **x** *j* *−* **x** *i* ) *, ρ* *j* ] (9)

This message combines five key features:


-----

6 5 THE QUADRANN ARCHITECTURE

(1) the **absolute position** of the neighbor ( **x** *j* ) and its **positional encoding** (PE *p* 1 ( **x** *j* )) to provide
global context;

(2) the **relative position vector** ( **x** *j* *−* **x** *i* ) to capture their Euclidean relationship;

(3) the **positional encoding of the relative vector** (PE *p* 2 ( **x** *j* *−* **x** *i* )) to provide a high-frequency
representation of the local relationship;

(4) the **local density explicit feature** ( *ρ* *j* ) at the neighboring point to provide explicit information
about the local concentration of the points.

A dedicated MLP then processes this augmented data, denoted MLP 1 . This network consists of three
layers and its role is to transform the rich input data into a meaningful latent feature space:

(1) a linear transformation that maps the input *G* *ij* to a hidden representation, followed by a GELU
activation function;

(2) a second linear transformation and GELU activation further process the features;

(3) a final linear transformation that produces the output message **m** [(1)] *ij* *[∈]* [R] *[Q]* [1] *[/]* [2] [.]

Finally, all learned messages are aggregated using a higher-order statistical function to form the initial
node feature **o** [(1)] *i* *∈* R *[Q]* [1] :

**o** [(1)] *i* = � ***µ*** *i* ( **m** [(1)] *ij* [)] *[,]* ***[ σ]*** *[i]* [(] **[m]** [(1)] *ij* [)] � (10)

This initial encoding provides a rich, geometrically-aware basis for subsequent layers.

**4.3** **Globally-Aware Propagation Layers** Following the initial encoding, a stack of *L* *−* 1 propagation layers refines the node features. At each layer *l ∈{* 2 *, . . ., L}*, a global context vector **g** [(] *[l][−]* [1)] *∈* R *[Q]* [1] is
first computed by averaging all node features from the previous layer. A message is then computed by a
layer-specific MLP *l*, which has a compact two-layer structure sufficient for operating on already-learned
features:

(1) a linear tranformation maps the concatenated input vector [ **o** [(] *j* *[l][−]* [1)] *,* **o** [(] *j* *[l][−]* [1)] *−* **o** [(] *i* *[l][−]* [1)] *,* **g** [(] *[l][−]* [1)] ] of dimension 3 *Q* 1 to a hidden representation, followed by a GELU activation function;

(2) a final linear transformation produces the output message **m** [(] *ij* *[l]* [)] *[∈]* [R] *[Q]* [1] *[/]* [2] [.]

The new node features, **o** [(] *i* *[l]* [)] *∈* R *[Q]* [1], are then formed by concatenating the mean and standard deviation of
the messages. After the final propagation layer, we create a comprehensive representation, **o** [final] *i* *∈* R *[L][·][Q]* [1],
by concatenating the feature vectors from all layers:

**o** [final] *i* = **o** [(1)] *i* *[,]* **[ o]** [(2)] *i* *[, . . .,]* **[ o]** [(] *i* *[L]* [)] *.* (11)
� �

**4.4** **Weight Prediction Network** The final stage of the model maps the comprehensive feature
vector **o** [final] *i* to a quadrature weight *w* *i* . This is performed by a point-wise MLP, MLP out, whose structure
is defined by a second hyperparameter, *Q* 2 . We require again *Q* 2 to be an even positive integer. This
network is designed as follows:

(1) an input linear transformation takes the *L × Q* 1 -dimensional vector **o** [final] *i* ;

(2) a first hidden linear layer maps the input to a *Q* 2 -dimensional space, followed by a GELU
activation and a dropout (= 0 *.* 5) layer for regularization;

(3) a second hidden layer reduces the dimension from *Q* 2 to *Q* 2 */* 2, followed again by GELU and
dropout (= 0 *.* 5);

(4) an output layer maps the final hidden representation to a single scalar logit, *v* *i* .

The vector of all logits, **v** = ( *v* 1 *, . . ., v* *n* ) *[T]*, is then normalized using a global `softmax` function to ensure
the weights are positive and sum to unity ( *w* *i* *>* 0 *,* [�] *i* *[w]* *[i]* [ = 1][).]


-----

7

**Al** **g** **orithm 1** The `QuadrANN` Architecture

1: **Input:** Point cloud *X* = *{* **x** *i* *}* *[n]* *i* =1 *[⊆]* [R] *[d]* [, density neighbors] *[ k]* *[′]* [, message neighbors] *[ k]* [, layers] *[ L]* [, PE]
bands *p* 1 *, p* 2, node dimension *Q* 1, prediction network parameter *Q* 2 .
**Learnable Parameters:** Geometric MLP MLP 1, Propagation MLPs *{* MLP *l* *}* *[L]* *l* =2 [, Prediction MLP]
MLP out .
2: **Output:** Quadrature weights **w** *∈* R *[n]* .
3: **procedure** QuadrANN( *{* **x** *i* *}* *[n]* *i* =1 [)]
4: *▷* **Stage 1: Geometric Encoding Layer**
5: Find density neighborhood *V* *k* *′* ( **x** *i* ) for each point **x** *i* .
6: **for** each point **x** *i* *, i* = 1 *, . . ., n* **do** *▷* Pre-compute density feature
7: **c** *i* *←* *k* [1] *[′]* � *j* : **x** *j* *∈V* *k′* ( **x** *i* ) **[x]** *[j]* *▷* Neighborhood centroid

8: *ρ* *i* *←* ( *k* [1] *[′]* � *j* : **x** *j* *∈V* *k′* ( **x** *i* ) *[∥]* **[x]** *[j]* *[ −]* **[c]** *[i]* *[∥]* [2] [)] *[−]* [1] *▷* Density feature is the inverse of mean distance

9: **end for**

10: Find message passing neighborhood *V* *k* ( **x** *i* ) for each point **x** *i* . *▷* Can be skipped if *k* *[′]* = *k*
11: **for** each point **x** *i* *, i* = 1 *, . . ., n* **do**
12: **for** each neighbor **x** *j* *∈* *V* *k* ( **x** *i* ) **do**
13: *G* *ij* *←* [ **x** *j* *,* PE *p* 1 ( **x** *j* ) *,* **x** *j* *−* **x** *i* *,* PE *p* 2 ( **x** *j* *−* **x** *i* ) *, ρ* *j* ]

14: **m** [(1)]
*ij* *[←]* [MLP] [1] [(] *[G]* *[ij]* [)]
15: **end for**
16: ***µ*** [(1)] *i* *←* *j* : **x** Mean *j* *∈V* *k* ( **x** *i* ) [(] *[{]* **[m]** *ij* [(1)] *[}]* [)][;] ***σ*** *i* [(1)] *←* *j* : **x** *j* Std *∈V* *k* ( **x** *i* ) [(] *[{]* **[m]** *ij* [(1)] *[}]* [)]

17: **o** [(1)] *i* *←* [ ***µ*** [(1)] *i* *[,]* ***[ σ]*** *i* [(1)] ]
18: **end for**

19: *▷* **Stage 2: Globally-Aware Propagation Layers**
20: **for** *l* = 2 *, . . ., L* **do**

21: **g** [(] *[l][−]* [1)] *←* *n* [1] � *np* =1 **[o]** *p* [(] *[l][−]* [1)]

22: **for** each *i* = 1 *, . . ., n* **do**
23: **for** each *j* : **x** *j* *∈* *V* *k* ( **x** *i* ) **do**

24: **m** [(] *ij* *[l]* [)] *[←]* [MLP] *[l]* [([] **[o]** [(] *j* *[l][−]* [1)] *,* **o** [(] *j* *[l][−]* [1)] *−* **o** [(] *i* *[l][−]* [1)] *,* **g** [(] *[l][−]* [1)] ])
25: **end for**
26: **o** [(] *i* *[l]* [)] *←* [ *j* : **x** Mean *j* *∈V* *k* ( **x** *i* ) [(] *[{]* **[m]** *ij* [(] *[l]* [)] *[}]* [)] *[,]* *j* : **x** *j* Std *∈V* *k* ( **x** *i* ) [(] *[{]* **[m]** *ij* [(] *[l]* [)] *[}]* [)]]

27: **end for**

28: **end for**

29: *▷* **Stage 3: Final Representation and Weight Prediction**
30: **for** each *i* = 1 *, . . ., n* **do**

31: **o** [final] *i* *←* [ **o** [(1)] *i* *[,]* **[ o]** [(2)] *i* *[, . . .,]* **[ o]** [(] *i* *[L]* [)] ]

32: *v* *i* *←* MLP out ( **o** [final] *i* )
33: **end for**
34: **w** *←* Softmax(( *v* 1 *, . . ., v* *n* ) *[T]* )

35:

36: **return w**

37: **end procedure**

5 Applications

In this section, we demonstrate the robustness of the `QuadrANN` framework through a number of numerical experiments. We begin by directly evaluating its performance on pure integration problems and


-----

8 5 APPLICATIONS

then showcase its utility as a component within complex variational solvers for time-dependent Partial
Differential Equations (PDEs).

A challenging point in numerical integration, particularly for functions with sharp gradients or localised
peaks, is the fact that standard sampling methods may not adequately cover the regions of interest.
Methods like Monte Carlo or even Quasi-Monte Carlo (QMC) that typically distribute points uniformly
may undersample those critical parts of the domain where the contribution of the integrand is most
significant, leading to slow convergence or high variance approximations. To achieve high accuracy with
a limited number of points, the sampling distribution should ideally be concentrated in these important
regions. Our approach is designed to excel in precisely this scenario by learning the correct quadrature
weights for a given, potentially non-uniform, point distribution.

In order to ensure the robustness of our model, we test it in the context of non-uniform point clouds as
described in the previous sections. These clouds are generated by first creating a low-discrepancy point
set using a scrambled Sobol sequence and then applying a non-linear, coordinate-wise warping function,
**G** ( **x** ) = ( *g* ( *x* 1 ) *, . . ., g* ( *x* *d* )). The function *g* : [0 *,* 1] *→* [0 *,* 1] is defined as:

*g* ( *s* ) = 0 *.* 95 *s* + 0 *.* 05 �4( *s −* 0 *.* 5) [3] + 0 *.* 5� *.* (12)

This transformation intentionally concentrates points around the center of the domain, simulating a
scenario where the energy of the integrand is localized around that point. By training `QuadrANN` on a
constantly changing set of warped point clouds, the network learns a quadrature rule that is not tied to
any single cloud but is instead a general-purpose function adaptable to local point density variations.
Note that this exact transformation is used for all examples that follow.

Our numerical examples are divided into two parts. First, we present numerical integration examples
to directly benchmark the accuracy and stability of `QuadrANN` against standard Sobol-QMC. These
tests include a simple 2D unit square, a non-convex 2D L-shaped domain, and a 4D hypercube to test
the performance of the model under the curse of dimensionality. Second, we apply `QuadrANN` to solve
PDEs: the Heat equation in 2D and the Fokker-Planck equation in 4D. These examples demonstrate the
practical advantage of using an adaptive, data-driven quadrature rule within variational, time-stepping
schemes that are common in modern scientific computing.

**5.1** **Numerical Integration** To test the performance of `QuadrANN`, we conducted a number of numerical examples. For each example, we trained a dedicated model to learn a quadrature rule for a
specific domain and according to a certain point cloud generation policy. In particular, at each training epoch, we generate a batch of point clouds from a scrambled Sobol sequence and then apply the
coordinate-wise warping function *g* ( *s* ) to create non-uniform distributions. The network is updated
based on these generated, non-uniform clouds. This ensures that the model learns a general quadrature
weight estimator adaptable to point density variations, rather than overfitting to a single point cloud.

The performance of the model is evaluated over a large number of independent trials ( *M* = 2 [9] ). In each
trial, a new, “unseen” point cloud is generated, and `QuadrANN` is used to compute the integral. The final
results are the mean and standard deviation of these *M* integral estimates. For a straightforward comparison, the standard Quasi-Monte Carlo (QMC) baseline computes its estimates on the corresponding
*M* original (unwarped) Sobol point clouds considering uniform weights. In all cases, the objective is to
integrate a known function, allowing for explicit error analysis. The actual value of the integral is 1 for
the first and third examples, and 3 */* 4 for the second one.

*5.1.1* *Exam* *p* *le 1: 2D Inte* *g* *ration on a Unit S* *q* *uare* The first example serves as a reference for performance in a simple domain, Ω= [0 *,* 1] [2] . The integrand here is a 2D Gaussian probability density function
(PDF) selected for its smoothness and localized characteristics:

*f* 1 ( **x** ) = *N* ( **x** *|* ***µ*** *, σ* [2] **I** ) *.* (13)

For this example, the parameters are set to ***µ*** = (0 *.* 5 *,* 0 *.* 5) *[T]* and *σ* = 0 *.* 025 to center the function in
the domain. The `QuadrANN` model was configured with *L* = 3 layers, feature dimensions of *Q* 1 = 2 [7],
*Q* 2 = 2 [8], *k* = *k* *[′]* = 16 nearest neighbors,positional encoding bands *p* 1 = 3 *, p* 2 = 5, and *K* max = 10.


-----

5.1 Numerical Integration 9

The results of this test case are summarized as follows. The `QuadrANN` model yielded a mean integral of
1 *.* 0004 with a standard deviation of 0 *.* 0839. These results compare to the Sobol–QMC method, which
resulted in a mean of 0 *.* 9958 and a standard deviation of 0 *.* 0964. In the presented realization, `QuadrANN`
achieved an absolute error of 0 *.* 0004, while the corresponding Sobol–QMC error was 0 *.* 0042. Moreover,
our model demonstrated a higher precision by attaining about 12 *.* 9% reduction in standard deviation
over the Sobol–QMC. Figure 1 visualizes the learned quadrature weights corresponding to a random
“unseen” point cloud.

Figure 1. Interpolated quadrature weights learned by `QuadrANN` for the 2D unit square
example. The color map indicates the weight value, and the dots refer to the locations
of the points.

*5.1.2* *Exam* *p* *le 2: 2D Inte* *g* *ration on a Non-Convex Domain* To challenge the model with complex geometry, the second example is conducted on a non-convex, L-shaped domain, defined as Ω= [0 *,* 1] [2] *\*

[0 *.* 5 *,* 1] [2] . The inner corner of this domain makes it particularly difficult for standard methods. The
integrand is the same Gaussian PDF defined in Equation (13).

This setup tests the capacity of `QuadrANN` to adapt its weights to complex boundary. For this complex
geometry, we increased the network depth to *L* = 4. The additional propagation layer provides the
model with a larger receptive field and a greater capacity for feature extraction, which is necessary to
resolve the intricate non-convex boundary. The other model hyperparameters were set as in the previous
case *Q* 1 = 2 [7], *Q* 2 = 2 [8], *k* = *k* *[′]* = 16, *p* 1 = 3 *, p* 2 = 5, and *K* max = 10. Each point cloud consists of
*n* = 2 [10] = 1024 points. It should be noted that in this test case, the base functions were re-normalized
so that their integral over the L-shaped domain evaluates to one. The model is trained for 256 epochs

For this test case, `QuadrANN` achieved a mean integral value of 0 *.* 7464 with a standard deviation of
0 *.* 09164. The Sobol–QMC produced a mean of 0 *.* 7434 with a standard deviation of 0 *.* 09699. Here, the
`QuadrANN` model achieved an absolute error of 0 *.* 0036, whereas the corresponding Sobol–QMC error was
0 *.* 0066. Additionally, our model exhibited improved precision, achieving approximately a 5 *.* 5% reduction
in standard deviation when compared to the Sobol–QMC approach.

The learned weights, shown in Figure 2, demonstrate the ability of the model to adjust its weights for
adapting to the non-convex geometry.


-----

10 5 APPLICATIONS

Figure 2. Visualization of the quadrature weights for the L-shaped domain. The model
accurately assigns the weight of the points to adapt to the non-convex geometry. The
color map indicates the weight value, and the dots refer to the locations of the points.

*5.1.3* *Exam* *p* *le 3: Hi* *g* *h-Dimensional Integration* The final pure integration example tests the performance of the model in a higher-dimensional space scenario, Ω= [0 *,* 1] [4], where the curse of dimensionality
is more pronounced. We compute the integral of a 4D Gaussian probability density function:

*f* 3 ( **x** ) = *N* ( **x** *|* ***µ*** *, σ* [2] **I** ) *.* (14)

with a mean ***µ*** = (0 *.* 5 *,* 0 *.* 5 *,* 0 *.* 5 *,* 0 *.* 5) *[T]*, and a standard deviation *σ* = 0 *.* 075. The network for this 4D
case used *L* = 4 layers, feature dimensions of *Q* 1 = 2 [7], *Q* 2 = 2 [8], *k* = *k* *[′]* = 32 neighbors, PE bands
*p* 1 = 5 *, p* 2 = 7, and *K* max = 6. The model is trained for 512 epochs, using point clouds of size *n* = 2 [12] .

In this high-dimensional test, `QuadrANN` achieved a mean integral of 0 *.* 9997 with a standard deviation of
0 *.* 0993. In contrast, standard Sobol-QMC produced a mean of 1 *.* 0020 with a higher standard deviation of
0 *.* 1203. In this example, `QuadrANN` achieved an absolute error of 0 *.* 0003, while the associated Sobol–QMC
error was 0 *.* 002. Moreover, our model again exhibited enhanced accuracy by achieving approximately a
17 *.* 4% decrease in standard deviation compared to the Sobol–QMC.

*5.1.4* *Discussion on Stabilit* *y* The results across all the examples underscore the benefits of our approach, particularly in reducing the variance of the integral estimator. The consistent reduction in the
standard deviation of the integral estimates highlights a crucial feature of `QuadrANN` . While these examples address pure integration problems, the implications for extending to solve PDEs via functional
minimization are clear. The optimization process at the core of such solvers seeks the minimum of a
discretized integral. A quadrature rule with high variance provides a noisy estimate of this functional,
which can mislead the optimization algorithm. By decreasing the variance, `QuadrANN` ensures a more
stable loss behaviour for the optimizer. This stability is critical for ensuring the process converges
robustly to the actual minimizer of the functional and yields a more accurate final solution.

**5.2** **Application to PDE Solvers** We now demonstrate the practical efficacy of our ANN-based
quadrature rule within variational solvers for time-dependent PDEs. In each scenario, the task is to find


-----

5.2 Application to PDE Solvers 11

the solution by minimizing a corresponding functional, where the accuracy of the numerical integration
is critical. Through these applications, we aim to validate that a more accurate, data-driven quadrature
rule leads to a more faithful optimization of the underlying functional and, consequently, a more precise
solution. The variational time-stepping approach used here is inspired by minimizing movement schemes
(IMEXs), which have proven effective in other complex deep learning contexts [19].

*5.2.1* *PDE Solution Network and Setup* For the forthcoming PDE examples, the time-dependent solution *u* ( *t* *k* *,* **x** ) is approximated by a neural network, *U* ( *t* *k* *,* **x** ; *θ* ). We employ a DGM-type architecture

[20], detailed in Algorithm 2, which is proper for high-dimensional problems.

**Al** **g** **orithm 2** The DGM-t yp e Solution Network Architecture

**Input:** Spatial coordinates **x** *∈* R *[d]*, time *t* .
**Learnable Parameters:** *{* **W** [(][in][)] *,* **b** [(][in][)] *, {* **V** [(] *[s,l]* [)] *,* **W** [(] *[s,l]* [)] *,* **b** [(] *[s,l]* [)] *}* *[L]* *l* =1 *[,]* **[ W]** [(][out][)] *[}]* [ for] *[ s][ ∈{][g, z, r, h][}]* [.]
**Hyperparameters:** Number of layers *L* .
**Output:** Approximate solution *U* ( *t,* **x** ; *θ* ).
**procedure** U( *t,* **x** )

*▷* Input Layer
*S* [(0)] *←* tanh( **W** [(][in][)] [ *t,* **x** ] *[T]* + **b** [(][in][)] ) *▷* Concatenate time and space

*▷* DGM Hidden Layers
**for** *l* = 1 *, . . ., L* **do**

*G* [(] *[l]* [)] *←* tanh( **V** [(] *[g,l]* [)] **x** + **W** [(] *[g,l]* [)] *S* [(] *[l][−]* [1)] + **b** [(] *[g,l]* [)] )
*Z* [(] *[l]* [)] *←* tanh( **V** [(] *[z,l]* [)] **x** + **W** [(] *[z,l]* [)] *S* [(] *[l][−]* [1)] + **b** [(] *[z,l]* [)] )
*R* [(] *[l]* [)] *←* tanh( **V** [(] *[r,l]* [)] **x** + **W** [(] *[r,l]* [)] *S* [(] *[l][−]* [1)] + **b** [(] *[r,l]* [)] )
*H* [(] *[l]* [)] *←* tanh( **V** [(] *[h,l]* [)] **x** + **W** [(] *[h,l]* [)] ( *S* [(] *[l][−]* [1)] *⊙* *R* [(] *[l]* [)] ) + **b** [(] *[h,l]* [)] )
*S* [(] *[l]* [)] *←* (1 *−* *G* [(] *[l]* [)] ) *⊙* *H* [(] *[l]* [)] + *Z* [(] *[l]* [)] *⊙* *S* [(] *[l][−]* [1)] *▷* The new hidden state
**end for**

*▷* Estimated solution
*U ←* Softplus( **W** [(][out][)] *S* [(] *[L]* [)] )

**return** *U*

**end procedure**

The solution network *U* ( *t,* **x** ; *θ* ) is configured with *L* = 3 hidden layers and a width of 32 nodes per
layer. For the 2D Heat Equation, we use a time step of *τ* = 0 *.* 01. The optimization for the first time
step ( *k* = 1) is performed for 2 [11] epochs, while all subsequent steps are trained for 2 [7] epochs with a
batch size of 8. For the 4D Fokker-Planck equation, the time step is *τ* = 0 *.* 01. The first step is trained
for 2 [11] epochs and the rest for 2 [7] epochs, using now larger point clouds of *n* = 2 [12] points, with a batch
size of 4.


*5.2.2* *Exam* *p* *le: Heat E* *q* *uation* We wish to solve the heat equation on Ω= [0 *,* 1] [2] . The problem is to
find *u* ( *t,* **x** ) for *t ∈* [0 *,* 1] and **x** = ( *x* 1 *, x* 2 ) *∈* Ω, such that:

 *∂* *t* *u −* *c* ∆ *u* = *f* ( *t,* **x** ) in (0 *,* 1] *×* Ω *, c* = 1 */* 8

*u* (0 *,* **x** ) = 0 in Ω (15)




*∂* *t* *u −* *c* ∆ *u* = *f* ( *t,* **x** ) in (0 *,* 1] *×* Ω *, c* = 1 */* 8
*u* (0 *,* **x** ) = 0 in Ω (15)
*∂u*
*∂* **n** [(] *[t,]* **[ x]** [) = 0] on (0 *,* 1] *× ∂* Ω






The source term *f* ( *t,* **x** ) is defined as:


*f* ( *t,* **x** ) =


*πr* ( **x** )
50 1 + cos

*R*

� �

0

�


*R*

*,* (16)
0 if *r* ( **x** ) *≥* *R*


( **x** )

*R* (2 + cos(4 *πt* )) *e* *[−]* [3] *[t]* if *r* ( **x** ) *< R*
��


-----

12 5 APPLICATIONS

Figure 3. Comparison of solutions for the 2D Heat Equation. A 1D slice of the computed solution *u* ( *T,* **x** ) at the final time *T* = 1, taken along the *x* 1 -axis at *x* 2 = 0 *.* 5. The
plot compares the solution obtained using our proposed `QuadrANN` integration with the
standard Sobol-based Monte Carlo (MC) method against the Finite Difference Method
(FDM) benchmark. The `QuadrANN` -based solution shows a closer agreement with the
benchmark.

where *r* ( **x** ) = ~~�~~ ( *x* 1 *−* 0 *.* 5) [2] + ( *x* 2 *−* 0 *.* 5) [2] and *R* = 0 *.* 05. We estimate the solution *u* *[k]* ( **x** ) at time *t* *k* = *kτ*

by minimizing the cost functional *J* [ *u* *[k]* ]:


*J* [ *u* *[k]* ] =
� Ω


1

2


2 2
*u* *[k]* ( *x* ) *−* *u* *[k][−]* [1] ( *x* ) + [1] *∇u* *[k]* ( *x* ) *−* *τf* ( *t* *[k]* *, x* ) *u* *[k][−]* [1] ( *x* ) *dx.* (17)
� � 2 *[τc]* � �


The quadrature model is first trained on the [0 *,* 1] [2] domain for 256 epochs, where each batch consists of
a point cloud of 2 [10] points created using the adopted warping policy. The trained model is then used to
provide the quadrature weights for minimizing *J* [ *u* *[k]* ]. The solution obtained using `QuadrANN` integration
achieved a mean absolute error of 1 *.* 527 *×* 10 *[−]* [3], a clear improvement over the error of 4 *.* 329 *×* 10 *[−]* [3] from
the solver using standard Sobol-based Monte Carlo integration. This confirms the advantage of our
approach for about 15% computational overhead. A qualitative comparison is presented in Figure 3.

*5.2.3* *Exam* *p* *le: Fokker-Planck E* *q* *uation* To test our method in a higher-dimensional scenario, we focus
on the Fokker-Planck equation within the Ω= [0 *,* 1] [4] .

The problem describes the probability density function, *u* ( *t,* **x** ), for the state of particle whose position
*X* *t* evolves according to an Ornstein-Uhlenbeck type Stochastic Differential Equation (SDE):

*d* **X** *t* = *−γ* ( **X** *t* *−* **x** 0 ) *dt* + *σd* **W** *t* *,* (18)

where the drift is *γ* = 0 *.* 5, the mean-reversion level is **x** 0 = (0 *.* 5 *,* 0 *.* 5 *,* 0 *.* 5 *,* 0 *.* 5) *[T]*, the diffusion is *σ* = 0 *.* 2,
and **W** *t* is a 4D Brownian motion. We assume again Neumann boundary conditions on *∂* Ω. The
corresponding Fokker-Planck equation, which governs the time evolution of the probability density
function *u* ( *t,* **x** ), is given by:

*∂* *t* *u* = *∇·* [ *γ* ( **x** *−* **x** 0 ) *u* ] + *[σ]* [2] (19)

2 [∆] *[u.]*


-----

5.2 Application to PDE Solvers 13

Figure 4. Comparison of solutions for the 4D Fokker-Planck equation. A 1D slice of the
computed density *u* ( *T,* **x** ) at the final time *T* = 1 */* 2, taken along the *x* 1 -axis with other
coordinates fixed at the center of the domain ( *x* 2 = *x* 3 = *x* 4 = 0 *.* 5). The plot compares
the density profile from our `QuadrANN` integration scheme with a standard Sobol-based
Monte Carlo (Quasi-MC) method against the benchmark density constructed via SDE
path simulation and Kernel Density Estimation (KDE).

The considered initial condition, *u* (0 *,* **x** ) = *u* 0 ( **x** ), is a mixture of two multivariate Gaussian distributions
creating a bimodal landscape:

*u* 0 ( **x** ) = 0 *.* 5 *N* ( **x** *|* ***µ*** 1 *, σ* 1 [2] **[I]** [) + 0] *[.]* [5] *[ N]* [(] **[x]** *[|]* ***[µ]*** [2] *[, σ]* 2 [2] **[I]** [)] *[,]* (20)

with means ***µ*** 1 = (0 *.* 35 *,* 0 *.* 35 *,* 0 *.* 35 *,* 0 *.* 35) *[T]*, ***µ*** 2 = (0 *.* 65 *,* 0 *.* 65 *,* 0 *.* 65 *,* 0 *.* 65) *[T]*, and standard deviations of 0 *.* 1
and 0 *.* 11, respectively.

Following a variational time-stepping scheme, we approximate the solution *u* *[k]* ( **x** ) at each time step by
minimizing the cost functional *J* [ *u* *[k]* ]:


*J* [ *u* *[k]* ] =
� Ω


1
� 2


(21)
*−* *τγ* ( **x** *−* **x** 0 ) *· ∇u* *[k][−]* [1] ( **x** ) *u* *[k]* ( **x** ) *−* 2 *τγu* *[k]* ( **x** ) *d* **x** *.*
� �
�


2


2
*u* *[k]* ( **x** ) *−* *u* *[k][−]* [1] ( **x** ) + *[τσ]* [2]
� � 4



*[τσ]*

4 *[|∇][u]* *[k]* [(] **[x]** [)] *[|]* [2]


A benchmark solution is obtained by simulating the SDE for 10 [7] particles and using Kernel Density
Estimation (KDE) with a bandwidth parameter of 0 *.* 03, a standard non-parametric technique [21], on
the final particle positions to construct a high-fidelity density. The error of our solver is quantified by
the Hellinger distance to this benchmark.

For this 4D problem, a non-uniform point cloud is generated by applying a coordinate-wise warping
function *g* ( *s* ) to a 4D Sobol sequence. A dedicated `QuadrANN` model is trained for 512 epochs on point
clouds of *n* = 2 [12] . This trained model then supplies the weights for the variational solver. The final
solution using `QuadrANN` integration achieved a Hellinger distance of 0 *.* 0769 to the KDE benchmark.
This is considered an improvement over the Hellinger distance of 0 *.* 0917 obtained with standard SobolQMC integration, achieved at a modest 20% increase in computational time. Figure 4 provides a visual
comparison of the obtained solutions.


-----

14 6 APPLICATIONS

6 Conclusions

In this work, we introduced `QuadrANN`, a deep learning architecture designed to overcome the critical
challenge of numerical integration on non-uniform point clouds, a common task in modern mesh-free
variational solvers for PDEs. Mainstream methods like Quasi-Monte Carlo struggle to handle such
distributions, while the traditional permutation-variant neural networks make them unsuitable for unordered point sets. Our approach treats these limitations by adopting a Graph Neural Network (GNN)
to learn data-driven, permutation-invariant quadrature rules directly from geometry. The `QuadrANN`
architecture introduces a rich geometric message-passing scheme that combines absolute and relative
positional features as well as a density feature in its initial layer. In contrast, the subsequent layers
exploit a global context vector to make the learned rule aware of both local point density and the overall
domain shape.

We demonstrated the efficiency of our method through a series of numerical experiments. Direct integration tests on simple, non-convex, and high-dimensional domains confirmed that `QuadrANN` consistently
and significantly reduces the variance of the integral estimate compared to standard Sobol-QMC integration. The uncertainty reduction led to improved performance in solving PDE problems. In particular,
when the quadrature model was integrated into variational ML-solvers for the Heat and Fokker-Planck
equations, our method yielded more accurate final solutions than those obtained using standard QMC
quadrature, validating its practical merit.

The core benefit of this work is that it provides a stable framework for the optimization process used
in mesh-free variational solvers. Particularly, `QuadrANN` ensures a more reliable convergence to the
energy minimizer by reducing the variance of the estimations of the integral, even while providing mean
estimates of the integrals comparable to those from Sobol-QMC.

QuadrANN Implementation

We have developed `QuadrANN` using `Pytorch-Geometric` [22] on top of `PyTorch` [23]. Interested readers
can refer to the following `GitHub` [repository (github.com/kesmarag/QuadrANN) for the source code.](https://github.com/kesmarag/QuadrANN)

References

[1] O. C. Zienkiewicz, R. L. Taylor, and J. Z. Zhu. *The Finite Element Method: Its Basis and Funda-*
*mentals* . Butterworth-Heinemann, 2005.

[2] Weinan E and Bing Yu. “The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm
for Solving Variational Problems”. In: *Communications in Mathematics and Statistics* 6.1 (2018),
pp. 1–17. doi: `10.1007/s40304-018-0127-z` .

[3] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. “Physics-Informed Neural Networks:
A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial
Differential Equations”. In: *Journal of Computational Physics* 378 (2019), pp. 686–707. doi: `10.`
`1016/j.jcp.2018.10.045` .

[4] Harald Niederreiter. *Random Number Generation and Quasi-Monte Carlo Methods* . SIAM, 1992.

[5] Chris J. Oates and Tim J. Sullivan. “A Modern Retrospective on Probabilistic Numerics”. In:
*Statistics and Computing* 29 (2019), pp. 1335–1351. doi: `10.1007/s11222-018-9834-6` .

[6] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. *Numerical*
*Recipes: The Art of Scientific Computing* . 3rd. Cambridge University Press, 2007.

[7] François-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne, and Dino Sejdinovic.
“Probabilistic Integration: A Role for Statisticians in Numerical Analysis?” In: *Statistical Science*
34.1 (2019), pp. 1–22. doi: `10.1214/18-STS667` .

[8] Luis Zepeda-Núñez, Jiequn Han, and Andrew M. Stuart. “Deep Quadrature: A Deep LearningBased Method for High-Dimensional Numerical Integration”. In: *Research in the Mathematical*
*Sciences* 8.3 (2021), pp. 1–33. doi: `10.1007/s40687-021-00264-8` .

[9] Jon A. Rivera, Jamie M. Taylor, Ángel J. Omella, and David Pardo. “On Quadrature Rules for
Solving Partial Differential Equations Using Neural Networks”. In: *Computer Methods in Applied*
*Mechanics and Engineering* 398 (2022), p. 114710. doi: `10.1016/j.cma.2022.114710` .


-----

REFERENCES 15

[10] Ilya M. Sobol’. “On the Distribution of Points in a Cube and the Approximate Evaluation of
Integrals”. In: *USSR Computational Mathematics and Mathematical Physics* 7.4 (1967), pp. 86–
112. doi: `10.1016/0041-5553(67)90144-9` .

[11] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
“The Graph Neural Network Model”. In: *IEEE Transactions on Neural Networks* 20.1 (2009),
pp. 61–80. doi: `10.1109/TNN.2008.2005605` .

[12] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. “A
Comprehensive Survey on Graph Neural Networks”. In: *IEEE Transactions on Neural Networks*
*and Learning Systems* 32.1 (2021), pp. 4–24. doi: `10.1109/TNNLS.2020.2978386` .

[13] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M.
Solomon. “Dynamic Graph CNN for Learning on Point Clouds”. In: *ACM Transactions on Graphics*
*(TOG)* 38.5 (2019), p. 146. doi: `10.1145/3326362` .

[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need”. In: *Advances in Neural Infor-*
*mation Processing Systems (NeurIPS)* . Vol. 30. 2017, pp. 5998–6008.

[15] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi,
and Ren Ng. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis”. In: *Eu-*
*ropean Conference on Computer Vision (ECCV)* . Vol. 12346. Lecture Notes in Computer Science.
Springer, 2020, pp. 405–421.

[16] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. “PointNet: Deep Learning on Point
Sets for 3D Classification and Segmentation”. In: *Proceedings of the IEEE Conference on Computer*
*Vision and Pattern Recognition (CVPR)* . 2017, pp. 652–660.

[17] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. “Densely Connected
Convolutional Networks”. In: *Proceedings of the IEEE Conference on Computer Vision and Pattern*
*Recognition (CVPR)* . 2017, pp. 4700–4708.

[18] Qimai Li, Zhichao Han, and Xiao-Ming Wu. “Deeper Insights into Graph Convolutional Networks
for Semi-Supervised Learning”. In: *Proceedings of the Thirty-Second AAAI Conference on Artificial*
*Intelligence* . Vol. 32. 1. 2018, pp. 3539–3546.

[19] Emmanuil H. Georgoulis, Antonis Papapantoleon, and Costas Smaragdakis. *A Deep Implicit-*
*Explicit Minimizing Movement Method for Option Pricing in Jump-Diffusion Models* . arXiv preprint.
2025. arXiv: `2401.06740` .

[20] Justin Sirignano and Konstantinos Spiliopoulos. “DGM: A Deep Learning Algorithm for Solving
Partial Differential Equations”. In: *Journal of Computational Physics* 375 (2018), pp. 1339–1364.
doi: `10.1016/j.jcp.2018.08.029` .

[21] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. *The Elements of Statistical Learning:*
*Data Mining, Inference, and Prediction* . 2nd. Springer, 2009. doi: `10.1007/978-0-387-84858-7` .

[22] Matthias Fey and Jan E. Lenssen. *Fast Graph Representation Learning with PyTorch Geometric* .
arXiv preprint. 2019. arXiv: `1903.02428` .

[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. “PyTorch: An Imperative Style, High-Performance
Deep Learning Library”. In: *Advances in Neural Information Processing Systems* . Vol. 32. Curran
Associates, Inc., 2019, pp. 8024–8035.


-----

