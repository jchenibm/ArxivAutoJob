## Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification
#### Thorsten Peinemann [1], Paula Arnold [1], Sebastian Berndt [2], Thomas Eisenbarth, [1] and Esfandiar Mohammadi [1] 1 University of L¨ubeck, L¨ubeck, Germany 2 Technische Hochschule L¨ubeck, L¨ubeck, Germany 1 {t.peinemann, p.arnold, thomas.eisenbarth, esfandiar.mohammadi}@uni-luebeck.de 2 sebastian.berndt@th-luebeck.de

### **Abstract**

Backdoor injection attacks are a threat to machine learning models that are trained on large data collected from
untrusted sources; these attacks enable attackers to inject
malicious behavior into the model that can be triggered by
specially crafted inputs. Prior work has established bounds
on the success of backdoor attacks and their impact on the
benign learning task, however, an open question is what
amount of poison data is needed for a successful backdoor
attack. Typical attacks either use few samples, but need
much information about the data points or need to poison
many data points.
In this paper, we formulate the one-poison hypothesis: an
adversary with one poison sample and limited background
knowledge can inject a backdoor with zero backdooringerror and without significantly impacting the benign learning task performance. Moreover, we prove the one-poison
hypothesis for linear regression and linear classification.
For adversaries that utilize a direction that is unused by the
benign data distribution for the poison sample, we show
that the resulting model is functionally equivalent to a
model where the poison was excluded from training. We
build on prior work on statistical backdoor learning to show
that in all other cases, the impact on the benign learning
task is still limited. We also validate our theoretical results

experimentally with realistic benchmark data sets.
### **1 Introduction**

Machine learning on publicly available data is highly prone
to data poisoning attacks that aim to inject backdoors into
a model. Malicious parties could, e.g., randomly sprinkle
poisoned data points on the Internet, hoping for a successful backdoor injection. Yet, data poisoning attacks are theoretically not sufficiently understood to assess whether such
attacks will succeed, not even for linear models.
Prior work that provides theoretical bounds on backdoor
injection attacks requires a significant fraction of the data
to be poisoned in order to establish a bound on the ac

curacy of a backdoor attack. Poisoning such a large number of data points raises multiple problems for an attacker.
One particular issue is the fact that such a large number
of untypical data points can be easily detected and point
to malicious behaviour, revealing the attack. In contrast,
the modification of only few data points allows for plausible deniability, as these modification might, e.g., be due
to measurement errors. Hence, Wang et al. [16] raise the
question of “when will a backdoor attack succeed with a
vanishing [fraction] *ρ* ” of poisoned data points? Chaudhari
et al. [2], Zhong et al. [19], Tan et al. [14] empirically illustrate that a one-poison attack is possible for specific tasks
with rich data points. Their results, however, leave the question open for which other tasks such a one-poison attack
can succeed, what the theoretical bounds for the success
of the backdoor attack are, and how well the original, benign task is learned. For an omniscient attacker knowing *all*
training data points, Hoang [6] provide initial bounds for a
one-poison attack.
Hence, previous backdoor attacks were either able to perform a one-poison attack but needed exact knowledge about
*all* data points or needed to poison a significant fraction of
the data points, but needed less knowledge about the other
data points. One might thus wonder whether this tradeoff
between the number of poisoned data points and the needed
knowledge is inherent. We study this open question from
the literature that we pose as the *one-poison hypothesis* :

**One-Poison Hypothesis.** *Any machine learning model can*
*be backdoored by a non-omniscient attacker with zero*
*backdooring-error with a single poison sample with no sig-*
*nificant harm to benign learning with probability almost 1.*
#### **1.1 Main contributions**

To understand this problem space, we confirm this hypothesis for linear regression and linear classification.

  - We prove that with little knowledge of the training
data, a single poisoned sample suffices to backdoor a


1


-----

Symbol Description

*x* data point
*n* size of training data
*d* Dimensionality of data space, i.e. R *[d]*

*η* poison pattern strength
1 *−* *φ* targeted prediction of poisoned sample
(regression)
*x* *p* single poison sample
*C* regularization parameter
*µ* cl *f* /ˆ *µ* cl / bd ˆ *f* poi / *µ* poi benign/backdoored/poisoned data distributionclassifier trained on benign/poisoned data
*L* [L2H] / *l* L2H regularized/per sample Hinge loss
*rL* *n* [cl][SE] � *f* /ˆ *l* poiSE � regularized/per sample squared error lossstatistical risk on benign data,
expectation taken over training data
*r* *n* [poi] � *f* ˆ poi � statistical risk on poisoned data,
expectation taken over training data

Table 1: Notation Table

linear classification or linear regression model with no
attack error with probability almost 1. In contrast to
previous work, our bounds are proven and confirmed
on real-world data.

  - We prove that, if all samples from the benign data distribution have zero magnitude projected onto some direction, then both clean and poisoned model are functionally equivalent for any clean data sample, if the attacker chooses that direction for its single poison sample.

  - We build on the prior work by Wang et al. [16] for
classification and extend their work for regression to
show that in all other cases, the impact of the poison
sample on the benign learning task is still limited.

  - We validate our theoretical results by evaluating them
on realistic benchmark.
### **2 Overview**

We provide an overview of our technique to confirm the
one-poison hypothesis for linear classification and linear
regression. Our proofs guarantee a successful backdoor attack and limited impact, in some cases even no impact, on
the benign learning task. In our proof for guaranteeing a
successful backdoor (cf. Theorem 2), we construct an attacker whose goal it is to leave a sufficiently large imprint
on the classifier / regressor in any one direction. During inference stage, the adversary then triggers the backdoor behavior using a poison patch that activates the imprint and
thus changes the prediction. Our proof for linear classification follows a similar idea as for linear regression. Hence,
we only provide an intuition for linear regression.
Our main contribution is a one-poison attack for nonomniscient attackers. [1] We identify a simple sufficient prop
1 For an omniscient attacker, Hoang [6] has shown that a one-poison

2


erty for injecting a backdoor through a single poisonous
data point. For that we need the notion of the magnitude
of a data set in a direction that is characterized by a vector *u ∈* R *[D]* . The magnitude of a data set *D* in a direction *u* is the sum over each data point’s length projected
onto *u*, i.e., [�] *x∈D* *[∥]* [proj][(] *[x, u]* [)] *[∥]* [2] [ =][ �] *x∈D* *[∥]* *[x]* *[T]* *[ u]* *[/]* *[∥][u][∥]* 2 [2] *[·][u][∥]* 2 [.]

The sufficient condition that we identified requires that
mean and variance of this magnitude for a single direction *u* are known to the adversary. We show that using this
knowledge, an adversary can inject a backdoor with zero
backdoor-error and if the magnitude is 0 (in mean and variance), the poison sample does not affect the benign learning
task at all (cf. Theorem 3). We also show that if this magnitude is not 0, the impact on the benign learning task is
limited (cf. Corollary 4).
In the simplest case of our proof for linear regression,
assume that there is a direction *u ∈* R *[D]* such that the sam
ples from the benign data distribution have zero projected
magnitude onto *u* . The attacker constructs their single poison along the direction of *u* . They desire a specific property
of the regressor, that the poison sample is predicted by the
regressor with large enough value 1 *−* *φ* ( *φ ∈* (0 *,* 1)). To
obtain this property, the adversary enforces the loss’s gradient to be non-zero, whenever this property is not satisfied.
For training a linear regressor with regularized squared error loss, the gradient of the loss function for a classifier
*T ∈* R *[D]* is the difference of the classifier *T* and a sum
of training data points, weighted with a regularization factor *C* . Only the classifier disturbs the impact of the poison
sample in the gradient as benign training data points have
no magnitude projected onto *u* by assumption. We show
that if the magnitude of the poison sample is now larger
than ~~�~~ *φ* (1 *−* *φ* ) */* *φ* ~~*√*~~ *C* for regularization factor *C*, then it will

outdo the regularization in the gradient.
During inference, the attacker patches the pattern with
increased magnitude onto any test data point to change prediction of the data point. The increased magnitude of the
poison patch increases the impact of the imprinted poison
sample in the prediction of the regressor. This can arbitrarily change the prediction of the test data point when the
poison patch is applied.
For the special case of zero magnitude of training data in
direction *u*, we also show functional equivalence between
clean and poisoned regressor. The high-level idea is that
no benign data interferes with poisoned data in the learning phase and vice versa, because they are orthogonal. We
show, that the task of optimizing the model can the be split
into two independent tasks and their results are added together: One task is optimizing a benign model part and the
other optimizing a poison model part. The poison model
part is orthogonal to the benign model part, so predicting
a benign test sample is not influenced by the benign model
part, resulting in functional equivalence.
For bounding the impact of the poison sample on the benign learning task in general, we build on results on statistical poisoned learning for linear classification by Wang
et al. [16] and extend their work to linear regression. These


*x∈D* *[∥]* [proj][(] *[x, u]* [)] *[∥]* [2] [ =][ �]


*φ* (1 *−* *φ* ) */* *φ* ~~*√*~~


attack basically follows the same idea as in the impossibility of byzantine
agreement [1].


-----

proofs bound the discrepancy between poisoned model and
optimal clean model by bounding first the discrepancy between poisoned model and optimal poisoned model and
then the discrepancy between optimal poisoned model and
optimal clean model. The first discrepancy will be expected
to be small when a good training algorithm is deployed and
the second discrepancy will also be expected to be small
since there is only a single sample that differs between the
poisoned task and the benign task.
#### **2.1 Related work**

**Data poisoning backdoor attacks.** Data poisoning
backdoor attacks add maliciously crafted samples to the
training data of a machine learning model to implant hidden triggers into the model, that when activated, leads to
malicious behavior while the model continues to perform
normally in the absence of such an active trigger. To the
best of our knowledge the first work to explore data poisoning attacks is that of Gu et al. [5], which similar to our
work crafts a poison sample with a target label that is specified by the adversary.
**One poison sample backdoor.** There are only few papers on backdoor injection specifically with one poison
sample, to the best of our knowledge. Theoretical works
on that topic assume that the adversary is omniscient, i.e.,
knows all training data, whereas we do not make such an assumption. Blanchard et al. [1] show, that a sum of elements
can be arbitrarily changed by controlling a single element
when all other elements are known. Hoang [6] apply the
principle of Blanchard et al.’s work to linear and logistic
regression, since the gradient of the loss functions for linear regression and logistic regression are both essentially
a weighted sum of training data points. Hoang can make
the gradient a **0** -gradient for an attacker chosen regressor,
so that this regressor is optimal for the optimization task.
For achieving the **0** -gradient Hoang use gradient inversion,
i.e., generating a data point for a specific desired gradient,
which is straightforward for linear and logistic regression.
Tan et al. [14], Zhong et al. [19], Chaudhari et al. [2] empirically demonstrate that a single poison document / passage suffices to backdoor retriever models. Their evaluation
shows attack success rates ranging from 28% *−* 100%.
**Theoretical** **understanding** **of** **backdoor** **attacks.**
Manoj and Blum [10] find that the excess memorization
capacity of a model, i.e., the fact that a model can learn
more tasks than the benign learning tasks, is necessary
for a backdoor. They show that overparameterized linear
models, i.e., linear models of dimension *d* where the input
data from R *[d]* resides in a smaller subspace of dimension
*s < d*, can be poisoned with learning error on benign data
of *ε* clean and backdoor data of *ε* adv . The number of poison
samples required is Ω( *ε* *[−]* adv [1] [((] *[d]* [ + 1) + log] [ 1] *[/]* *[δ]* [))][ where] *[ d]* [ + 1]
is the VC-dimension of linear classifiers and *δ* is the failure
probability. This number can be a very large constant for
small learning error.
Xian et al. [17] propose the ’adaptability hypothesis’ to
explain the success of a backdoor attack: A good backdoor
attack should not change the predicted value too much before and after the backdoor attack. Their findings suggest


The gradient of regularized squared error loss is

*∇* *f* ˆ cl *[L]* [sq] [(] *[D]* [cl] *[,]* [ ˆ] *[f]* [cl] [) = ˆ] *[f]* [cl] [ +] *[ C][ ·]* � ( *f* [ˆ] cl *[T]* *[x]* *[i]* *[−]* *[y]* *[i]* [)] *[x]* *[i]* *[.]*

( *x* *i* *,y* *i* ) *∈D* cl

We can predict test samples *x* in inference stage: *f* [ˆ] cl *[T]* *[x]* [.]
The following lemma will be useful throughout the paper.


that a good attack should have direction of low variance.
For kernel smoothing algorithms and a specific benign data
distributions that follows a multivariate normal distribution

they prove that for *n →∞*, a large poison strength can
counter a small poison ratio and yield no harm on the benign learning task and zero backdoor error.
Wang et al. [16] provide generalization bounds for benign task learning and backdoor learning for data poisoning. Their bound on the statistical risk of the poisoned classifier on data with backdoor trigger scales with [1] */* *ρ* *·* *r* poi, ignoring further additive error terms, where *r* poi is the statistical risk on input from training distribution. For [1] */* *ρ* = *n*, i.e.,
a single poison sample, this bound can quickly become impractical due to the factor *n* on the statistical risk on training
data. Wang et al. further show for benign data distributions
that follow a multivariate normal, the direction of smallest

variance is most successful for a backdoor attack. For the

case of distributions with a direction where the distribution

is a point, they show that any backdoor attack will be successful but only asymptotically. A backdoor attack is considered successful by Wang et al. if for dataset size *n*, one
has lim sup *n→∞* *r* bd */r* cl *≤* *C*, where *r* bd is the statistical
risk of a poisoned classifier on data with backdoor trigger
and *r* cl is the statistical risk of a clean model on benign data.
Consequently, a very large constant *C* might fulfill this definition, while the risk on data with backdoor trigger might
not be meaningfully bounded.
Li and Liu [8] provide bounds on benign learning task
accuracy and backdoor attack success rate of backdoor attacks on overparameterized CNNs assuming data is constructed from random patches that follow a multivariate
normal distribution for a poison ratio that is sub-linear in
the training dataset size *n* .
Yu et al. [18] propose a generalization bound for benign learning and backdoor learning for neural networks
and also more general hypothesis spaces. Their bound on
the statistical risk of the poisoned model on data with backdoor patch roughly scales with *[L]* */* *ρ*, where *L* is the crossentropy loss on poisoned training data. For *ρ* = [1] */* *n*, i.e., a
single poison sample, this bound can also quickly become
impractical due to the factor *n* on the cross-entropy loss.
### **3 Preliminaries**

**Linear regression.** We train a linear regressor minimizing the regularized squared error loss *L* sq with regularization parameter *C* :


min ˆ *L* sq ( *D* cl *,* *f* [ˆ] cl ) = min ˆ
*f* cl *f* cl


1
2 *[∥][f]* [ˆ] [cl] *[∥]* 2 [2]


+ *C ·* � ( *y* *i* *−* *f* [ˆ] cl *[T]* *[x]* *[i]* [)] [2] *[.]*

( *x* *i* *,y* *i* ) *∈D* cl


3


-----

**Lemma 1.** *Let a and b be two orthogonal vectors. Then*
*∥a* + *b∥* 2 [2] [=] *[ ∥][a][∥]* [2] 2 [+] *[ ∥][b][∥]* [2] 2 *[.]*

*Proof.* We have *∥a* + *b∥* 2 [2] [= (] *[a]* [+] *[b]* [)] *[T]* [ (] *[a]* [+] *[b]* [) =] *[ a]* *[T]* *[ a]* [+2] *[a]* *[T]* *[ b]* [+]
*b* *[T]* *b* = *∥a∥* 2 [2] [+ 2] *[a]* *[T]* *[ b]* [ +] *[ ∥][b][∥]* [2] 2 [. As] *[ a]* [ and] *[ b]* [ are orthogonal,]
*a* *[T]* *b* = 0, which concludes the proof.

**Linear classification.** We train a linear classifier minimizing the regularized Hinge loss *L* Hinge with regularization parameter *C* (training stage):

min ˆ *L* Hinge ( *D* cl *,* *f* [ˆ] cl )
*f* cl


= min ˆ
*f* cl


1
2 *[∥][f]* [ˆ] [cl] *[∥]* 2 [2] [+] *[ C][ ·]* � max (0 *,* 1 *−* *y* *i* *f* [ˆ] cl *[T]* *[x]* *[i]* [)] *[.]*

( *x* *i* *,y* *i* ) *∈D* cl

(1)


The gradient of regularized Hinge loss is

*∇* *f* ˆ cl *[L]* [Hinge] [(] *[D]* [cl] *[,]* [ ˆ] *[f]* [cl] [) = ˆ] *[f]* [cl] *[ −]* *[C][ ·]* � *x* *i* *y* *i* *.*

( *x* *i* *,y* *i* ) *∈D* cl : *f* [ˆ] cl *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]

We can predict test samples *x* in inference stage: *f* [ˆ] cl *[T]* *[x]* [. In]
this work, we also consider the Lagrangian dual of the linear classification problem of Equation (1). For obtaining the
Lagrangian dual, we first formulate the primal optimization
problem which is equivalent to Equation (1):


1
min 2 [+] *[ C][ ·]*
*w,ξ* 2 *[∥][w][∥]* [2]


*n*
� *ξ* *i* (2)

*i* =1


s.t. *−* *ξ* *i* *≤* 0 and (1 *−* *y* *i* *w* *[T]* *x* *i* ) *−* *ξ* *i* *≤* 0 for *i* = 1 *, ..., n.*

We can then construct the Lagrangian of Equation (2):


**Threat model.** The adversary provides a single maliciously crafted sample to the training data set to plant
a backdoor into the trained model. The backdoor should
have no backdoor error, i.e., 100% of samples with injected
backdoor patch get classified as positive (classification) or,
respectively, get labeled with +1 (regression). The single
poison sample should also not degrade learning the benign task. The adversary does not need to know the exact training data or even the exact distribution of the benign data. They need, however, an estimate on mean and
variance of the benign data when projected onto some direction *u ∈* R *[D]* that the adversary uses to launch their
attack. To this end, we define a random variable *s* [(] *m* *[u]* [)] =

*m*
� *i* =1 *[∥]* *[x]* *i* *[T]* *[u]* */* *[∥][u][∥]* 2 [2] *[·][ u][∥]* 2 [that for] *[ m]* [ data points] *[ x]* *i* *[∼]* *[µ]* cl
( *i* = 1 *,* 2 *, ..., m* ) from benign data distribution *µ* cl, sums
up the lengths when those data points are projected onto a
direction *u ∈* R *[d]*, and let *µ* signal *, σ* signal [2] [be mean and vari-]

ance of *s* [(] *m* *[R]* [)] [. We assume the size] *[ n]* [ of the benign training]
data to be fixed and known to the adversary.
### **4 One-poison hypothesis for linear** **regression**

In this section, we show validity of the one-poison hypothesis for linear regression. Our proof for linear classification
follows a similar idea. We briefly discuss linear classification in Section 4.3, for the complete proofs, we refer to the
appendix.
#### **4.1 Single poison sample backdoor for linear** **regression**

Our first result shows that a single poison sample suffices to
backdoor a linear regressor with zero backdoor error with
probability almost 1:

**Theorem 2.** *Let* 0 *< δ, φ <* 1 *. Let the size n of the benign*
*data D* *cl* *be fixed. Let R ∈* R *[d][×][d]* : � det ( *R* ) = 1 *∧* *R* *[T]* *R* =
*I* *d* � *, then define the poison sample x* *p* = *R ·* (0 *,* 0 *, ....,* 0 *, η* ) *.*
*Let s* [(] *m* *[u]* [)] *be defined as in the threat model (cf. Sec-*
*tion 3) with mean and variance µ* *signal* *, σ* *signal* [2] *[. Assume]*
*that there is some bound* [ *−K, K* ] *(K* *∈* R [+] *) on*
*regression labels and regressor output. With poison*
*pattern strength η* *>* 2 1 *φ* [(2] *[K]* [(] *[µ]* *[signal]* [ +] *[ σ]* *signal* [2] *[/]* *√δ* ) +


*L* ( *w, ξ, α, r* ) = [1] 2 [+] *[ C][ ·]*

2 *[∥][w][∥]* [2]


*n*
� *ξ* *i* (3)

*i* =1


*n*
� *r* *i* *ξ* *i* *,*

*i* =1


*−*


*n*
� *α* *i* � *y* *i* *w* *[T]* *x* *i* *−* 1 + *ξ* *i* � *−*

*i* =1


where the *α* *i* and *r* *i* are Lagrange multipliers constrained to
being *≥* 0. We formulate the primal of Equation (2) equivalently as

min *w,ξ* [max] *α,r* *[L]* [(] *[w, ξ, α, r]* [)] *[.]* (4)

Then, the dual of the problem is defined as

max *α,r* [min] *w,ξ* *[L]* [(] *[w, ξ, α, r]* [)] *[ .]* (5)

For solving the dual, we first set the derivative of *L* with
respect to *w* to zero. From the derivative, we obtain that for
any *α, r*, the minimizing *w* is *w* = [�] *[n]* *i* =1 *[α]* *[i]* *[x]* *[i]* *[y]* *[i]* [. Inserting]
into the Lagrangian of Equation (3) yields the following
form of the dual optimization problem of Equation (5):


4( *µ* *signal* + *σ* *signal* [2] *[/]* *√*


*δ* ) [2] + [4] *[K]* [2] *[φ]* [(1] *[ −]* *[φ]* [)] */* *C* ) *and* *patch*


2


*pattern strength η* *>* 2 1 *φ* [(2] *[K]* [(] *[µ]* *[signal]* [ +] *[ σ]* *signal* [2] *[/]* *√δ* ) +

~~�~~ 4( *µ* *signal* + *σ* *signal* [2] *[/]* *√δ* ) [2] + [4] *[K]* [2] *[φ]* [(1] *[ −]* *[φ]* [)] */* *C* ) *and* *patch*

*function* patch( *x* ) = *x* + *R ·* (0 *,* 0 *, ...,* 0 *, |* ( *−f* [ˆ] *poi* *[T]* *[x]* [ +]
1) */* ( *f* [ˆ] *poi* *[T]* *[x]* *[p]* [)] *[ ·][ η][|]* [)] *[, the adversary achieves with probability]*
1 *−* *δ that ∀x ∈* Ω *|y* = *−* 1 : *f* [ˆ] *poi* *[T]* [patch(] *[x]* [) = 1] *[.]*

*Proof.* We define the gradient of squared error loss w.r.t.
poisoned model *f* [ˆ] poi as *g* := *∇L* ( *f* [ˆ] poi ), which is equal to

*C ·* � ( *f* [ˆ] poi *[T]* *[x]* *[i]* *[−]* *[y]* *[i]* [)] *[x]* *[i]* [+] *[ C][ ·]* [ ( ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[−]* [1)] *[x]* *[p]* [+ ˆ] *[f]* [poi] *[.]*

*x* *i* *∈D* cl

To exploit the backdoor during inference stage, the adversary desires the property *f* [ˆ] poi *[T]* *[x][ ≥]* [1][. The adversary con-]
structs a single poison sample *x* *p* *∈* R *[d]* *\ {* **0** *}*, so that for


max
( *α* ) *[n]* *i* =1


*n*
�


� *α* *i* *−* [1] 2

*i* =1


*n*
� *α* *i* *α* *j* *y* *i* *y* *j* *x* *[T]* *i* *[x]* *[j]* (6)

*i* =1 *,j* =1


s.t. 0 *≤* *α* *i* *≤* *C* for *i* = 1 *, ..., n.*


4


-----

any regressor that does not satisfy this property, its gradient *g* will have *g* *[T]* *x* *p* *<* 0 almost certainly which implies
*g ̸* = **0**, i.e., that regressor is not optimal. We investigate the
probability of the adversary being successful:

Pr[ *g* *[T]* *x* *p* *<* 0] = Pr[� *C ·* � ( *f* [ˆ] poi *[T]* *[x]* *[i]* *[−]* *[y]* *[i]* [)] *[x]* *[i]*

*x* *i* *∈D* cl


+ *C ·* ( *f* [ˆ] poi *[T]* *[x]* *[p]* *[−]* [1)]
~~�~~ � ~~�~~ �

*<* 0 since *f* [ˆ] poi *[T]* *[x]* *[p]* *[<]* [1]


*T*
*x* *p* + *f* [ˆ] poi � *x* *p* *<* 0]


= Pr[ *C ·* � *x* *[T]* *p* [( ˆ] *[f]* poi *[ T]* *[x]* *[i]* *[−]* *[y]* *[i]* [)] *[x]* *[i]*

*x* *i* *∈D* cl


+ *C · x* *[T]* *p* [( ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[−]* [1)] *[x]* *[p]* [+ ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[<]* [ 0]]

= Pr[ *C ·* � ( *R ·* (0 *, ...,* 0 *, η* )) *[T]* ( *f* [ˆ] poi *[T]* *[x]* *[i]* *[−]* *[y]* *[i]* [)(] *[R][ ·][ x]* [(] *i* *[R]* [)] )

*x* *i* *∈D* cl

+ *C · x* *[T]* *p* *[x]* *[p]* [( ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[−]* [1) + ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[<]* [ 0]]

Here, *x* [(] *i* *[R]* [)] := *R* *[−]* [1] *x* *i* and *x* [(] *i,d* *[R]* [)] [are the] *[ d]* [’th component.]


= Pr[ *C · η ·* � *x* [(] *i,d* *[R]* [)] [( ˆ] *[f]* poi *[ T]* *[x]* *[i]* *[−]* *[y]* *[i]* [)]

*x* *i* *∈D* cl

~~�~~ � ~~�~~ �
L [SE] :=
+ *C · x* *[T]* *p* *[x]* *[p]* ( *f* [ˆ] poi *[T]* *[x]* *[p]* *[−]* [1)] + *f* [ˆ] poi *[T]* *[x]* *[p]* *<*
~~�~~ � ~~�~~ � ~~�~~ �� � ~~�~~ ���
= *η* [2] *<−φ* *<* 1 *−φ*


( *f* [ˆ] poi *[T]* *[x]* *[p]* *[−]* [1)]
~~�~~ �� �
*<−φ*


+ *f* [ˆ] poi *[T]* *[x]* *[p]*
~~�~~ ���
*<* 1 *−φ*


*<* 0]


*≥* Pr[ *C ·* L [SE] *· η −* *C · η* [2] *· φ* + 1 *−* *φ <* 0] *.* (7)

The adversary aims to bound L [SE] to obtain a magnitude
of positive *η* that guarantees *g* *[T]* *x* *p* *<* 0 with high probability. To this end, for *n* data points in training data, the adversary establishes a bound on the sum of the magnitudes
of benign training data points projected onto *x* *p*, and uses
*µ* signal and *σ* signal [2] [and Chebyshev’s inequality [3]: From]

!
Pr[ *|s* [(] *|D* *[u]* [)] cl *|* *[−]* *[µ]* [signal] *[| ≥]* *[k]* []] *[ ≤]* *[σ]* signal [2] *[/k]* [2] = *δ*

it follows that *k* = *σ* signal [2] *[/]* *√δ.* The adversary using the as
sumption of bounded regressor output and bounded regression label on [ *−K, K* ] bounds *|* ( *f* [ˆ] poi *[T]* *[x]* *[i]* *[ −]* *[y]* *[i]* [)] *[| ≤]* [2] *[ ·][ K]* [ in-]
side L SE, then sets ˆ *s* := 2 *· K ·* ( *µ* signal + *k* ) and obtains
Pr[L [SE] *<* ˆ *s* ] *≥* 1 *−* *δ* . Solving the quadratic ine q uality of


*−*
The prior theorem assumes a bound [ *K, K* ] on regressor output and regression label, but note that this is merely
a technical assumption because otherwise the impact of benign data could be arbitrarily large. In reality, regressor output and regression label will certainly always have some
upper and lower bound. While the adversary can pick any
rotation *R* for the poison sample *x* *p*, when *R* is selected
such that all samples from benign data distribution projected onto the poison are zero-centered and have zero magnitude, then *µ* signal = 0 and *σ* signal [2] [= 0][. In this special case,]
the adversary does not need to bound the impact of benign
training data and their success probability is exactly 1. The
bound on *η* simplifies to *η >* � *φ* (1 *−* *φ* ) */* ( *φ* ~~*√*~~ *C* ) since all

terms with *µ* signal *, σ* signal [2] [can be dropped.]
#### **4.2 Impact on benign learning task**

We now analyze the impact of the single poison sample
on the benign learning task. First, we show that if all samples from benign data distribution projected onto the poison
sample are zero-centered and have zero magnitude, then the
backdoor attack described in Theorem 2 does not impact
the benign learning task at all.

**Theorem 3.** *Assume that µ* *signal* = 0 *and σ* *signal* [2] [= 0] *[, i.e.,]*
*all samples from benign data distribution projected onto the*
*poison are zero-centered and have zero magnitude. Then*
*for all x ∈* *X, an optimal regressor* *f* [ˆ] *poi* *is functionally*
*equivalent to an optimal* *f* [ˆ] *cl* *that is obtained when the single*
*poison x* *p* *is omitted in training.*

*Proof.* Let *x* *p* be defined as in Theorem 2. From the definition, it follows that min *f* ˆ poi *[L]* [sq] [(] *[D]* [poi] *[,]* [ ˆ] *[f]* [poi] [)][ is equal to]


= min *f* ˆ poi


1
2 *[∥][f]* [ˆ] [poi] *[∥]* 2 [2] [+] *[ C][ ·]* � ( *y* *i* *−* *f* [ˆ] cl *[T]* *[x]* *[i]* [)] [2]

( *x* *i* *,y* *i* ) *∈D* cl


*s* ˆ+ �
Equation (7) yields the solution *η* *[∗]* *>*


*s* ˆ [2] + [4] *[φ]* [(] [1] *C* *[−]* *[φ]* [)] and

2 *φ*


Pr[ *C ·* L [SE] *· η −* *C · η* [2] *· φ* + 1 *−* *φ <* 0] *≥* 1 *−* *δ.*

Consequently, when the poisoned regressor attains opti*f* mum, i.e., its gradientˆ poi *[T]* *[x]* *[p]* *[ ≥]* [1] *[−][φ]* [ or else with high probability] *g* satisfies *g* = **0**, it has to hold that *[ g][ ̸]* [=] **[ 0]** [. Now the]
backdoor can be triggered via the patch function for every
sample *x ∈* *X* and *f* [ˆ] poi *[T]* [patch(] *[x]* [)][ thus equals]

= *f* [ˆ] poi *[T]* � *x* + *R ·* (0 *,* 0 *, ...,* 0 *, |* ( *−f* [ˆ] poi *[T]* *[x]* [ + 1)] *[/]* [( ˆ] *[f]* poi *[ T]* *[x]* *[p]* [)] *[ ·][ η][|]* [)] �


+ *C ·* (1 *−* *f* [ˆ] cl *[T]* *[x]* *[p]* [)] [2] *[ .]*

*f* Now, we split the classifierˆ poi projected onto *x* *p*, i.e., proj *f* [ˆ] poi *x* into two parts: the part of *p* ( ˆ *f* poi ) := ˆ *f* poi *[T]* *[x]* *[p]* *[/][∥][x]* *[p]* *[∥]* [2] *[ ·]*

*x* *p*, and the remainder of *f* [ˆ] poi, i.e., rem *x* *p* ( *f* [ˆ] poi ) := *f* [ˆ] poi *−*
proj *x* *p* ( *f* [ˆ] poi ). We can thus write min *f* ˆ poi *[L]* [sq] [(] *[D]* [poi] *[,]* [ ˆ] *[f]* [poi] [)][ as]


= *f* [ˆ] poi *[T]* *[x]* [ +] *[ |]* [(] *[−][f]* [ˆ] poi *[ T]* *[x]* [ + 1)] *[/]* [( ˆ] *[f]* poi *[ T]* *[x]* *[p]* [)] *[| ·]* [ ˆ] *[f]* poi *[ T]*
� *R ·* (0 *,* 0 *, ...,* 0 *, |η|* ) �
~~�~~ � ~~�~~ �

= *x* *p*

= *f* [ˆ] poi *[T]* *[x]* [ +] *[ |]* [(] *[−][f]* [ˆ] poi *[ T]* *[x]* [ + 1)] *[/]* [( ˆ] *[f]* poi *[ T]* *[x]* *[p]* [)] *[| · |][f]* [ ˆ] poi *[ T]* *[x]* *[p]* *[|]*
~~�~~ � ~~�~~ �

= *−f* [ˆ] poi *[T]* *[x]* [+1]


= min *f* ˆ poi


1
2 *[∥]* [proj] *[x]* *[p]* [( ˆ] *[f]* [poi] [) +][ rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2]


= 1 *.*


+ *C ·* � ( *y* *i* *−* (proj *x* *p* ( *f* [ˆ] poi ) + rem *x* *p* ( *f* [ˆ] poi )) *[T]* *x* *i* ) [2]

( *x* *i* *,y* *i* ) *∈D* cl

+ *C ·* (1 *−* (proj *x* *p* ( *f* [ˆ] poi ) + rem *x* *p* ( *f* [ˆ] poi )) *[T]* *x* *p* ) [2]


5


-----

By assumption, *µ* signal = *σ* signal [2] [= 0][ and thus]


**Corollary 4.** *Corollary of [16, Theorem 1] Let µ* *cl* *be the*
*benign data distribution. We define the backdoor and poi-*
*soned distributions as*

*µ* *bd* ( *x* ) = 1 *{x* = *x* *p* *}, µ* *poi* ( *x* ) = (1 *−* [1] */* *n* ) *µ* *cl* + [1] */* *n* *µ* *bd* *.*

*Let n be fixed. Let* *f* [ˆ] *poi* *be the regressor trained on n*
*samples from the poisoned distribution µ* *poi* *. Let l* ( *., .* ) :

[0 *,* 1] *×* [0 *,* 1] *�→* R [+] *be a general loss function that is*
( *C, α* ) *-H¨older continuous for* 0 *< α ≤* 1 *that measures*
*the discrepancy between two regressors. The statistical risk*
*on benign input is bounded as*

ˆ 1 ˆ
*r* *n* *[cl]* � *f* *poi* � *≤* *n* � *f* *poi* � + *C* ( *µ* *cl* ( *x* *p* ) *·* [1] */* *n* *·* 2) *[α]* *.*
1 *−* [1] */* *n* *[r]* *[ poi]*

*Proof.* First, since *l* is ( *C, α* )-H¨older continuous:


= min *f* ˆ poi


1
2 *[∥]* [proj] *[x]* *[p]* [( ˆ] *[f]* [poi] [) +][ rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2] (*)


+ *C ·* � ( *y* *i* *−* (proj *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *i*

( *x* *i* *,y* *i* ) *∈D* cl ~~�~~ � ~~�~~ �
=0

rem *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *i* )) [2]


+


+ *C ·* (1 *−* (proj *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *p* + rem ~~�~~ *x* *p* ( �� *f* [ˆ] poi ) *[T]* *x* *p* �
=0


)) [2]


= min *f* ˆ poi


1 2 [+] [1] 2
2 *[∥]* [proj] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* [2] 2 *[∥]* [rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* [2]


+ *C* � ( *y* *i* *−* rem *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *i* ) [2]

( *x* *i* *,y* *i* ) *∈D* cl

+ *C ·* (1 *−* proj *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *p* ) [2]

= min *L* sq (rem *x* *p* ( *f* [ˆ] poi ) *, D* cl )
rem *xp* ( *f* [ˆ] poi )

+ proj min *xp* ( *f* [ˆ] poi ) *L* sq (proj *x* *p* ( *f* [ˆ] poi ) *, {x* *p* *}* )

Here, we used that proj *x* *p* ( *f* [ˆ] poi ) and rem *x* *p* ( *f* [ˆ] poi ) are

orthogonal. Lemma 1 thus implies *∥* proj *x* *p* ( *f* [ˆ] poi ) +

rem *x* *p* ( *f* [ˆ] poi ) *∥* 2 [2] [=] *[ ∥]* [proj] *x* *p* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2] [+] *[ ∥]* [rem] *[x]* *p* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2] [. Using]
this equality in (*) gives us the term completely separating
the projection from the remainder.
To deduce the functional equivalence, we need to
show that minimizing rem *x* *p* ( *f* [ˆ] poi ) *[∗]*, which is defined
as arg min rem *xp* ( ˆ *f* poi ) *[L]* [sq] [(][rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[, D]* [cl] [)][, is the same as]
minimizing *f* *[∗]* = arg min *f* *L* sq ( *f, D* cl ). Intuitively, this is
true, as any vector of the form *α · x* *p* ( *α ∈* R *\ {* 0 *}* ) is
orthogonal to rem *x* *p* ( *f* [ˆ] poi ) *[∗]* . Adding *α · x* *p* to rem *x* *p* ( *f* [ˆ] poi ) *[∗]*

does not change the prediction of any benign training data
point ( *x, y* ) *∈* Ω and can thus not reduce the data point’s
loss. More formally, let *l* SE be a data point’s squared error
loss, then

*l* SE (rem *x* *p* ( *f* [ˆ] poi ) *[∗]* + *αx* *p* *, x* )

=( *y −* (rem *x* *p* ( *f* [ˆ] poi ) *[∗]* + *αx* *p* ) *[T]* *x* ) [2]

=( *y −* ((rem *x* *p* ( *f* [ˆ] poi ) *[∗]* ) *[T]* *x* + *αx* *[T]* *p* *[x]* [))] [2]

= *l* SE (rem *x* *p* ( *f* [ˆ] poi ) *[∗]* *, x* ) *.*

Due to the orthogonality, we can again use Lemma 1
to show that the addition of *α · x* *p* only increases the
norm of the classifier, as *∥* rem *x* *p* ( *f* [ˆ] poi ) *[∗]* + *αx* *p* *∥* 2 [2] =
*∥* rem *x* *p* ( *f* [ˆ] poi ) *[∗]* *∥* 2 [2] [+] *[ ∥][αx]* *[p]* *[∥]* 2 [2] *[>][ ∥]* [rem] *[x]* *p* [( ˆ] *[f]* [poi] [)] *[∗]* *[∥]* 2 [2] [.]
We thus obtain the functional equivalence for *x ∈* *X* by
using (min proj *xp* ( ˆ *f* poi ) *[L]* [sq] [(][proj] *x* *p* [( ˆ] *[f]* [poi] [)] *[,][ {][x]* *[p]* *[}]* [))] *[T]* *[ x]* [ = 0][, as]

(min ˆ *L* sq ( *D* poi *,* *f* [ˆ] poi )) *[T]* *x* = (min ˆ *L* sq ( *D* cl *,* *f* [ˆ] poi )) *[T]* *x .*
*f* poi *f* poi

Now we move on to the most general case where the
benign data distribution can be any distribution. For this,
we extend prior work [16] to regression:


ˆ
*r* *n* [cl] � *f* poi � = E *D* poi � *x∼* E *µ* cl


*l* ( *f* [ˆ] poi ( *x* ) *, f* cl *[∗]* [(] *[x]* [))]
� � [�]


*≤* E E
*D* poi � *x∼µ* cl

*≤* E E
*D* poi � *x∼µ* cl


� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [)) +] *[ C][ |][f]* poi *[ ∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* *[α]* [��]

� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


+ *C* E
*x∼µ* cl


� *|f* poi *[∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* *[α]* [�] *.*


Using Jensen’s inequality, this is at most


*≤* E
*D* poi


� *x∼* E *µ* cl � *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


+ *C* � *x∼* E *µ* cl


*α*
� *|f* poi *[∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* �� *.*


We bound each term on the right-hand side independently.
For the first term, we can bound


E
*D* poi


� *x∼* E *µ* cl � *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


E
� *x∼µ* poi


*≤* (1 *−* [1] */* *n* ) *[−]* [1] E
*D* poi


� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


ˆ
*≤* (1 *−* [1] */* *n* ) *[−]* [1] *r* *n* [poi] � *f* poi � *.*

As for the second term, by definition of the Bayes optimal regressor for squared error loss [20], we have *f* cl *[∗]* [(] *[x]* [) =]
E
*µ* cl [[] *[Y][ |][X]* [ =] *[ x]* []][ and, similarly,]


*f* poi *[∗]* [(] *[x]* [) =][ E]
*µ* poi [[] *[Y][ |][X]* [ =] *[ x]* [] =]

 E

*µ* cl [[] *[Y][ |][X]* [ =] *[ x]* []]

(1 *−* [1] */* *n* ) E [[] *[Y][ |][X]* [ =] *[ x]*


E if *x ̸* = *x* *p*,
*µ* cl [[] *[Y][ |][X]* [ =] *[ x]* []]

(1 *−* [1] */* *n* ) E
*µ* cl [[] *[Y][ |][X]* [ =] *[ x]* *[p]* [] +] [ 1] *[/]* *[n]* [ E] *µ* bd [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []][ if] *[ x]* [ =] *[ x]* *[p]* [.]





Combining prior equations, we get


0 if *x ̸* = *x* *p*,


*f* poi *[∗]* *[−]* *[f]* cl *[ ∗]* [=]





1 */* *n* *·* E
*µ* bd [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []]


*−* [1] */* *n* E if *x* = *x* *p* .
*µ* cl [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []]


6


-----

Accordingly,


E

*x∼µ* cl


� *|f* poi *[∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* �


= Pr
*µ* cl [[] *[X]* [ =] *[ x]* *[p]* []] *[ · |]* [1] *[/]* *[n]* [ E] *µ* bd [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []]

*−* E
*µ* cl [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []] *[|]* [ + (1] *[ −]* [Pr] *µ* cl [(] *[X]* [ =] *[ x]* *[p]* [)] *[ ·]* [ 0]

= *µ* cl ( *x* *p* ) [1] */* *n* *|* E
*µ* bd [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []] *[ −]* *µ* [E] cl [[] *[Y][ |][X]* [ =] *[ x]* *[p]* []] *[|]*

*≤* *µ* cl ( *x* *p* ) *·* [1] */* *n* *·* 2 *.*

Now, we plug the two bounds together:

ˆ 1 ˆ
*r* *n* [cl] � *f* poi � *≤* *n* � *f* poi � + *C* ( *µ* cl ( *x* *p* ) *·* [1] */* *n* *·* 2) *[α]* *.*
1 *−* [1] */* *n* *[r]* [ poi]

The attentive reader might be wondering why we consider i.i.d. sampling of the data set that is different to our
prior assumption that the poison sample is guaranteed to
be added to training data. Slightly increasing the poison
ratio in the proof from [1] */* *n* to *[k]* */* *n* for a small constant *k*
gives the probability of sampling at least one poison sample 1 *−* (1 *−* *[k]* */* *n* ) *[n]* . Under large enough data size *n*, this
turns to 1 *−* (1 *−* *[k]* */* *n* ) *[n]* *→*
*n→∞* [1] *[ −]* *[e]* *[−][k]* [ which quickly]
goes to 1 when increasingrisk on benign data for 1 ˆ *k* poison samples is *k* . The bound on the statistical *r* *n* [cl] � *f* ˆ poi � *≤*
1 *−* *[k]* */* *n* *[r]* [ poi] *n* � *f* poi � + *C* ( *µ* cl ( *x* *p* ) *·* *[k]* */* *n* ) *[α]*, so the bound is only
slightly higher.
#### **4.3 One-poison hypothesis for linear classifi-** **cation**

We extend all of our results stated here to the case of lin
ear classification using regularized Hinge loss, both in primal optimization and dual optimization. The attack works
mostly similar, but there are a few differences. The loss
function is different than regularized squared error loss: In
the gradient w.r.t. classifier *f* each data point *x* with label *y* is weighted by either 1 if *f* *[T]* *xy <* 1 and 0 otherwise. Consequently, the poison sample *x* *p* ’s impact on
gradient is not scaled down when *f* *[T]* *x* *p* approaches 1, so
the adversary does not need to account for this. Also, during inference stage, the adversary must construct a poison
patch for test data point *x* that changes prediction from
*f* *[T]* *x <* 0 to *f* *[T]* patch( *x* ) *≥* 0 so there is no need to aim
for a specific predicted value. Linear classification is often
performed using dual optimization, for instance using the
popular LIBLINEAR library [4]. We show using the strong
duality property of linear classification, i.e., that maximum
objective value of primal and dual are equal, that our results
for primal optimization also follow for dual optimization.
For full proofs, we refer to the appendix.
### **5 Experiments**
#### **5.1 Experimental setup**

All experiments were run on an Intel Xeon Platinum 8168
2.7 GHz CPU with 32GB of RAM.


**Model.** We validate our developed theory for linear
regression and linear classification using the scikit-learn
Python library [13], trained for 1,000 iterations with regularization *C* = 1. For our functional equivalence ablation
for linear classification we use a vanilla implementation of
liblinear [4] instead of the one of scikit-learn to remove any
source of randomness.

**Data sets.** We evaluate regression data sets Parkinsons

[15] and Abalone [12], and classification data sets Spambase [7] and Phishing [11], each partitioned half and half
in training and test data. The data sets demand deducing motor impairment of patients from biomedical voice
measurements, predicting the age of abalone from physical measurements, detecting spam mail from word counts,
and detecting phishing websites from website meta data.
We choose these data sets as they represent realistic data
sets of real-world measurements.

**Baselines.** We train a clean model only on benign data.
We also evaluate a mean regressor producing the mean regression label from training data for linear regression, and
a majority-vote classifier outputting the majority label from
training data for linear classification. For linear regression,
we report the mean squared error, for linear classification,
we report accuracy. Both metrics correspond to their learning task, i.e., reducing squared error (linear regression) and
improving accuracy (linear classification). We report mean
and standard deviation over five runs of random data splits.
**Attack.** We set the poison label to 1 and the magnitude of poison patch *η* = 1 for Parkinsons and Abalone,
*η* = 10 for Spambase and Phishing. For poison direction,
we compute a principal component analysis on each data
set and extract an eigenvector in direction with smallest
variance. This variance is 6 *.* 5 *e* *[−]* [12] for Parkinsons, 0 *.* 0001
for Abalone, 0 *.* 0004 for Spambase and 0 *.* 03 for Phishing.
A single poison sample is added to the training data set.
#### **5.2 Results**

**Single poison sample backdoor.** Table 2 and Table 3
show that a single poison sample suffices to inject a backdoor in linear regressors and classifiers. All test samples
with poison patches are predicted with the poison label instead of the correct label. The single poison sample does not
significantly increase prediction error, showing bounded
impact on the benign learning task.
**Ablation: Functional equivalence of clean model and**
**poisoned model with 0-dimension in data.** To validate
our theoretic results, we add a dimension to the benign data
and set each benign sample’s value in that dimension to 0,
then calculate for 100 random test samples the L1-distance
of predictions between clean model and poisoned model.
We clip the predictions to two decimal places. In all settings, the L1 distance is 0, showing functional equivalence
of clean model and poisoned model.
### **6 Conclusion**

In this paper, we proved the one-poison hypothesis for both
linear regression and linear classification. Our attack shows


7


-----

Parkinsons

Regressor Benign Task MSE Backdoor MSE

Mean regr. 0 *.* 202 *±* 0 *.* 002
Clean regr. 0 *.* 165 *±* 0 *.* 002 3 *.* 852 *±* 1 *.* 954
Poisoned re g r. 0 *.* 166 *±* 0 *.* 002 0 *.* 000 *±* 0 *.* 000

Abalone

Regressor Benign Task MSE Backdoor MSE

Mean regr. 0 *.* 054 *±* 0 *.* 001
Clean regr. 0 *.* 033 *±* 0 *.* 001 7 *.* 391 *±* 0 *.* 043
Poisoned regr. 0 *.* 034 *±* 0 *.* 001 0 *.* 000 *±* 0 *.* 000

Table 2: Clean and poisoned regressor test MSE of benign
and backdoor task. Single poison added to training data.
Mean regressor outputs mean of labels from training data.

Spambase

Classifier Benign Task (%) Backdoor Task (%)

Majority vote 60 *.* 00 *±* 0 *.* 04
Clean 82 *.* 89 *±* 0 *.* 05 18 *.* 33 *±* 36 *.* 65

Poisoned 81 *.* 81 *±* 0 *.* 04 100 *.* 00 *±* 0 *.* 00

Phishing

Classifier Benign Task (%) Backdoor Task (%)

Majority vote 56 *.* 44 *±* 0 *.* 01
Clean 92 *.* 47 *±* 0 *.* 01 0 *.* 02 *±* 0 *.* 05

Poisoned 92 *.* 37 *±* 0 *.* 01 100 *.* 00 *±* 0 *.* 00

Table 3: Clean and poisoned classifier test accuracy of benign and backdoor task. Single poison added to training
data. Majority vote predicts majority label from training
data.

that such models can be successfully attacked by poisoning
a single data point with limited knowledge about the other
data points. Our bounds are formally proven, apply to realworld instance sizes, and are verified experimentally also.
While typical countermeasures such as differential privacy look very promising [9], they also come with a large
performance or accuracy penalty. Promising future directions are thus the development of efficient countermeasures
and the transfer of our results to more complex models.
### **References**

[1] Peva Blanchard, El Mahdi El Mhamdi, Rachid
Guerraoui, and Julien Stainer. Machine learning with
adversaries: Byzantine tolerant gradient descent. In
I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors,
*Advances in Neural Information Processing Systems*,
volume 30. Curran Associates, Inc., 2017. URL https:
//proceedings.neurips.cc/paper files/paper/2017/file/
f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf.



[2] Harsh Chaudhari, Giorgio Severi, John Abascal,
Matthew Jagielski, Christopher A Choquette-Choo,
Milad Nasr, Cristina Nita-Rotaru, and Alina Oprea.
Phantom: General trigger attacks on retrieval augmented language generation. *arXiv* *preprint*
*arXiv:2405.20485*, 2024.

[3] Pafnutii Lvovich Chebyshev. Des valeurs moyennes.
*Journal de Math´ematique Pures et Appliqu´ees*, 12(2):
177–184, 1867.

[4] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. Liblinear: A library
for large linear classification. *the Journal of machine*
*Learning research*, 9:1871–1874, 2008.

[5] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and
Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. *IEEE Access*, 7:
47230–47244, 2019. doi: 10.1109/ACCESS.2019.

2909068.

[6] Lˆe-Nguyˆen Hoang. The poison of dimensionality.
*arXiv preprint arXiv:2409.17328*, 2024.

[7] Mark Hopkins, Erik Reeber, George Forman, and
Jaap Suermondt. Spambase. UCI Machine Learning Repository, 1999. URL https://archive.ics.uci.
edu/dataset/94/spambase.

[8] Boqi Li and Weiwei Liu. A theoretical analysis
of backdoor poisoning attacks in convolutional neural networks. In *Proceedings of the 41st Interna-*
*tional Conference on Machine Learning*, ICML’24.
JMLR.org, 2024.

[9] Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners: attacks
and defenses. In *Proceedings of the 28th Interna-*
*tional Joint Conference on Artificial Intelligence*, IJCAI’19, page 4732–4738. AAAI Press, 2019. ISBN
9780999241141.

[10] Naren Manoj and Avrim Blum. Excess capacity and
backdoor poisoning. *Advances in Neural Information*
*Processing Systems*, 34:20373–20384, 2021.

[11] Rami Mustafa A. Mohammad, Fadi A. Thabtah, and
Lee McCluskey. An assessment of features related
to phishing websites using an automated technique.
*2012 International Conference for Internet Tech-*
*nology and Secured Transactions*, pages 492–497,
2012. URL https://api.semanticscholar.org/CorpusID:
5716727.

[12] Warwick Nash, Tracy Sellers, Simon Talbot, Andrew
Cawthorn, and Wes Ford. Abalone. UCI Machine
Learning Repository, 1995. URL https://archive.ics.
uci.edu/dataset/1/abalone.

[13] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,


8


-----

D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
*Journal of Machine Learning Research*, 12:2825–
2830, 2011.

[14] Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan
Li, Song Wang, Jundong Li, Tianlong Chen, and Huan
Liu. Glue pizza and eat rocks - exploiting vulnerabilities in retrieval-augmented generative models. In
*EMNLP*, pages 1610–1626. Association for Computational Linguistics, 2024.

[15] Athanasios Tsanas, Max A. Little, Patrick E. McSharry, and Lorraine O. Ramig. Accurate telemonitoring of parkinson’s disease progression by noninvasive
speech tests. *IEEE Transactions on Biomedical Engi-*
*neering*, 57(4):884–893, 2010. doi: 10.1109/TBME.
2009.2036000.

[16] Ganghua Wang, Xun Xian, Jayanth Srinivasa,
Xuan Bi Ashish Kundu, Mingyi Hong,, and Jie Ding.
Demystifying Poisoning Backdoor Attacks from a
Statistical Perspective. In *International Conference*
*on Learning Representations (ICLR)*, 2024.

[17] Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish
Kundu, Xuan Bi, Mingyi Hong, and Jie Ding. Understanding backdoor attacks through the adaptability
hypothesis. In *International Conference on Machine*
*Learning*, pages 37952–37976. PMLR, 2023.

[18] Lijia Yu, Shuang Liu, Yibo Miao, Xiao-Shan Gao,
and Lijun Zhang. Generalization bound and new algorithm for clean-label backdoor attack. In Ruslan
Salakhutdinov, Zico Kolter, Katherine Heller, Adrian
Weller, Nuria Oliver, Jonathan Scarlett, and Felix
Berkenkamp, editors, *Proceedings of the 41st Inter-*
*national Conference on Machine Learning*, volume
235 of *Proceedings of Machine Learning Research*,
pages 57559–57596. PMLR, 21–27 Jul 2024. URL
https://proceedings.mlr.press/v235/yu24i.html.

[19] Zexuan Zhong, Ziqing Huang, Alexander Wettig, and
Danqi Chen. Poisoning retrieval corpora by injecting adversarial passages. In *EMNLP*, pages 13764–
13775. Association for Computational Linguistics,
2023.

[20] Eric R Ziegel. The elements of statistical learning,
2003.


9


-----

### **A One-poison hypothesis for linear** **classification**

In this section, we answer the one-poison hypothesis for
linear classification: We first show that a single poison sample suffices to backdoor a linear classifier with zero backdoor error with probability almost 1. We extend this result
to the case where the learner uses dual optimization to fit
their classifier. We then show that the poison sample does
not impact the benign learning task at all when all data from
the benign data distribution projected onto the poison sample is zero-centered and has zero magnitude. For all other
cases, we show that the impact is still limited.
#### **A.1 Single poison sample backdoor for lin-** **ear classification**

Our first result shows that a single poison sample suffices
to backdoor a linear classifier with zero backdoor error with
probability almost 1:

**Theorem 5.** *Let* 0 *<* *δ* *<* 1 *. Let the size n of*
*the benign data D* *cl* *be fixed. Let R* *∈* R *[d][×][d]* :
� det ( *R* ) = 1 *∧* *R* *[T]* *R* = *I* *d* � *, then define the poison sam-*
*ple x* *p* = *R ·* (0 *,* 0 *, ....,* 0 *, η* ) *. Let s* [(] *m* *[u]* [)] *be defined as in*
*the threat model (cf. Section 3) with mean and variance*
*µ* *signal* *, σ* *signal* [2] *[. With poison pattern strength][ η >]* [1] 2 [(] *[µ]* *[signal]* [ +]


Let *x* [(] *i* *[R]* [)] := *R* *[−]* [1] *x* *i* and *x* [(] *i,d* *[R]* [)] [its] *[ d]* [’th component, then]

( *R · x* [(] *i* *[R]* [)] ) *y* *i*

*−* *C · x* *[T]* *p* *[x]* *[p]* [+ ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[<]* [ 0]]

= Pr[ *C ·* � *−* (0 *, ...,* 0 *, η* ) *[T]* *x* [(] *i* *[R]* [)] *y* *i*

( *x* *i* *,y* *i* ) *∈D* cl : *f* [ˆ] poi *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]

*−* *C · x* *[T]* *p* *[x]* *[p]* [+ ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[<]* [ 0]]


= Pr[ *C · η ·* � *x* [(] *i,d* *[R]* [)] *[y]* *[i]*

*x* *i* *∈D* cl : *f* [ˆ] poi *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]

~~�~~ �� �
L [L2H] :=


*−* *C · x* *[T]* *p* *[x]* *[p]*
~~�~~ ���
= *η* [2]


+ *f* [ˆ] poi *[T]* *[x]* *[p]*
�� ~~�~~ �
*<* 1


*<* 0] *≥* Pr[ *C ·* L [L2H] *· η −* *C · η* [2] + 1 *<* 0] *.* (8)


The adversary aims to bound L [L2H] in order to obtain a magnitude of positive *η* that guarantees *g* *[T]* *x* *p* *<* 0 with high
probability. To this end, for *n* data points in training data,
the adversary establishes a bound on the sum of the magnitudes of benign training data points projected onto *x* *p*, and
uses *µ* signal and *σ* signal [2] [and Chebyshev’s inequality [3]:]

!
Pr[ *|s* [(] *|D* *[u]* [)] cl *|* *[−]* *[µ]* [signal] *[| ≥]* *[k]* []] *[ ≤]* *[σ]* signal [2] *[/k]* [2] = *δ*


*⇒* *k* = *σ* [2] *√*
signal *[/]*


*δ.*


*σ* [2] *√*
*signal* *[/]*


*δ* +
~~�~~


( *µ* *signal* + *σ* *signal* [2] *[/]* *√*


The adversary assumes all data points to be active in the
gradient and sets ˆ *s* := *µ* signal + *k* and obtains

Pr[L [L2H] *<* ˆ *s* ] *≥* 1 *−* *δ.*

Then, solving the quadratic ine q uality of Equation (8)


*σ* *signal* [2] *[/]* *√δ* + ~~�~~ ( *µ* *signal* + *σ* *signal* [2] *[/]* *√δ* ) [2] + 4 */C* ) *and patch*

*function* patch( *x* ) = *x* + *R ·* (0 *,* 0 *, ...,* 0 *, K · η* ) *, the ad-*
*versary achieves with probability* 1 *−* *δ that*

*∀x ∈* Ω *|y* = *−* 1 : *f* [ˆ] *poi* *[T]* *[x >][ −][K][ ⇒]* *[f]* [ˆ] *poi* *[ T]* [patch(] *[x]* [)] *[ ≥]* [0] *[ .]*

*Proof.* We define the gradient w.r.t. poisoned model *f* [ˆ] poi as


*s* ˆ+ *[√]*
yields the solution *η* *[∗]* *>*


*s* ˆ [2] + *C* [4]


*C* and with that
2


*g* := *∇* *f* ˆ poi *[L]* [( ˆ] *[f]* [poi] [) =] *[ C][ ·]* � *−x* *i* *y* *i*

( *x* *i* *,y* *i* ) *∈D* cl : *f* [ˆ] poi *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]

*−* *C · x* *p* *·* 1 *{f* [ˆ] poi *[T]* *[x]* *[p]* *[<]* [ 1] *[}]* [ + ˆ] *[f]* [poi] *[.]*

To exploit the backdoor during inference stage, the adversary desires the property *f* [ˆ] poi *[T]* *[x][ ≥]* [1][. The adversary con-]
structs a single poison sample *x* *p* *∈* R *[d]* *\ {* **0** *}*, so that for
any classifier that does not satisfy this property, its gradient *g* will have *g* *[T]* *x* *p* *<* 0 almost certainly which implies
*g ̸* = **0**, i.e., that classifier is not optimal. We investigate the
probability of the adversary being successful:

Pr[ *g* *[T]* *x* *p* *<* 0] = Pr[� *C ·* � *−x* *i* *y* *i*

( *x* *i* *,y* *i* ) *∈D* cl : *f* [ˆ] poi *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]


Pr[ *C ·* L [L2H] *· η* *[∗]* *−* *C ·* ( *η* *[∗]* ) [2] + 1 *<* 0] *≥* 1 *−* *δ.*

Consequently, when the poisoned classifier attains opti*f* mum, i.e., its gradientˆ poi *[T]* *[x]* *[p]* *[ ≥]* [1][ or else with high probability] *g* satisfies *g* = **0**, it has to hold that *[ g][ ̸]* [=] **[ 0]** [. Now the]
backdoor can be triggered via the patch function for every
sample *x ∈* *X* : *f* [ˆ] poi *[T]* *[x >][ −][K]* [:]

*f* ˆ poi *[T]* [patch(] *[x]* [) = ˆ] *[f]* poi *[ T]* � *x* + *R ·* (0 *,* 0 *, ...,* 0 *, K · η* )�

= *f* [ˆ] poi *[T]* *[x]* [ +] *[ K][ ·]* [ ˆ] *[f]* poi *[ T]* � *R ·* (0 *,* 0 *, ...,* 0 *, η* )�


= *f* [ˆ] poi *[T]* *[x]* [ +] *[ K][ ·]* [ ˆ] *[f]* poi *[ T]* *[x]* *[p]*
�� ~~�~~ �
*≥* 1


*≥* *f* [ˆ] poi *[T]*
����
*≥−K*


*x* + *K ≥* 0 *.*


*−* *C · x* *p* *·* 1 *{f* [ˆ] poi *[T]* *[x]* *[p]* *[<]* [ 1] *[}]*
~~�~~ �� �
=1 by assumption


*T*
+ *f* [ˆ] poi � *x* *p* *<* 0]


= Pr[ *C ·* � *−x* *[T]* *p* *[x]* *[i]* *[y]* *[i]*

( *x* *i* *,y* *i* ) *∈D* cl : *f* [ˆ] poi *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]

*−* *C · x* *[T]* *p* *[x]* *[p]* [+ ˆ] *[f]* poi *[ T]* *[x]* *[p]* *[<]* [ 0]]

= Pr[ *C ·* � *−* ( *R ·* (0 *, ...,* 0 *, η* )) *[T]*

( *x* *i* *,y* *i* ) *∈D* cl : *f* [ˆ] poi *[T]* *[x]* *[i]* *[y]* *[i]* *[<]* [1]


In the prior theorem, the adversary can pick any rotation
*R* for the poison sample *x* *p* . When *R* is selected such that
all samples from benign data distribution projected onto
the poison are zero-centered and have zero magnitude, then
*µ* signal = 0 and *σ* signal [2] [= 0][. In this special case, the adver-]
sary does not need to bound the impact of benign training
data. Consequently, the bound on *η* simplifies to *η >* [1] */* ~~*√*~~ *C*

and the backdoor success probability is always exactly 1.


10


-----

#### **A.2 Single poison sample backdoor for dual** **optimization of linear classification**

For linear classification instead of solving the task directly,
one often solves the dual of the problem using dual optimization, e.g., via dual optimizers like liblinear. We
show that solving the dual exhibits the same susceptibility
to backdoor attacks as in the original task.

**Corollary 6.** *Consider the same adversary as in Theo-*
*rem 5. If the learner utilizes dual optimization for obtaining*
*a linear classifier, then with probability* 1 *−* *δ the learned*
*model is backdoored.*

*Proof.* The dual learner obtains a solution ( *α, w* ) with *w* =
� *ni* =1 *[α]* *[i]* *[x]* *[i]* *[y]* *[i]* [ that is dual optimal, i.e., is optimal for Equa-]
tion (6). Because strong duality holds for linear SVM, the
objective value *d* *[∗]* that ( *α, w* ) attains in the dual is also an
optimal objective value for the primal of Equation (4), so
( *α, w* ) is also a minimizer of the primal of Equation (4).
Now for the sake of contradiction, assume that *w* does not
satisfy the adversary’s desired backdoor property, i.e., assume that *w* *[T]* *x* *p* *<* 1. By Theorem 5, when choosing *η*
appropriately, with probability arbitrarily close to 1, the
gradient *g* *t* projected onto poison direction is not zero and
so *g* *t* *̸* = 0. This implies that *w* can be further changed
to achieve smaller objective value in Equation (1). This
further implies that changing *w* can also reduce objective
value in Equation (2), and also in Equation (4). This contradicts that ( *α, w* ) is a minimizer of the primal of Equation (4).
#### **A.3 Impact on benign learning task**

We now analyze the impact of the single poison sample
on the benign learning task. First, we show that if all samples from benign data distribution projected onto the poison
sample are zero-centered and have zero magnitude, then the
backdoor attack described in Theorem 5 does not impact
the benign learning task.

**Theorem 7.** *Assume that µ* *signal* = 0 *and σ* *signal* [2] [= 0] *[, i.e.,]*
*all samples from benign data distribution projected onto the*
*poison are zero-centered and have zero magnitude. Then*
*for all x ∈* *X, an optimal linear classifier w is functionally*
*equivalent to an optimal w* *[′]* *that is obtained when the single*
*poison x* *p* *is omitted in training.*

*Proof.* Let *x* *p* be defined as in Theorem 5. By definition,
we have

min ˆ *L* Hinge ( *D* poi *,* *f* [ˆ] poi )
*f* poi


*x* *p*, and the remainder of *f* [ˆ] poi, i.e., rem *x* *p* ( *f* [ˆ] poi ) := *f* [ˆ] poi *−*
proj *x* *p* ( *f* [ˆ] poi ). We can thus write min *f* ˆ poi *[L]* [Hinge] [(] *[D]* [poi] *[,]* [ ˆ] *[f]* [poi] [)][ as]


= min *f* ˆ poi


1
2 *[∥]* [proj] *[x]* *[p]* [( ˆ] *[f]* [poi] [) +][ rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2]


+ *C ·*
�

( *x* *i* *,y* *i* ) *∈D* cl

max (0 *,* 1 *−* *y* *i* (proj *x* *p* ( *f* [ˆ] poi ) + rem *x* *p* ( *f* [ˆ] poi )) *[T]* *x* *i* )

+ *C ·* max (0 *,* 1 *−* (proj *x* *p* ( *f* [ˆ] poi ) + rem *x* *p* ( *f* [ˆ] poi )) *[T]* *x* *p* ) *.*

By assumption, *µ* signal = *σ* signal [2] [= 0][, and we thus conclude]


= min *f* ˆ poi


1
2 *[∥]* [proj] *[x]* *[p]* [( ˆ] *[f]* [poi] [) +][ rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2] (*)


+ *C ·*
�

( *x* *i* *,y* *i* ) *∈D* cl

max (0 *,* 1 *−* *y* *i* (proj *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *i*
~~�~~ � ~~�~~ �
=0


+rem *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *i* ))


+ *C ·* max (0 *,* 1 *−* (proj *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *p*


+ rem *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *p*
~~�~~ �� �
=0


)) *.*


As proj *x* *p* ( *f* [ˆ] poi ) and rem *x* *p* ( *f* [ˆ] poi ) are orthogonal, Lemma 1
implies

*∥* proj *x* *p* ( *f* [ˆ] poi ) + rem *x* *p* ( *f* [ˆ] poi ) *∥* 2 [2]

=
*∥* proj *x* *p* ( *f* [ˆ] poi ) *∥* 2 [2] [+] *[ ∥]* [rem] *[x]* *p* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2] *[.]*

Using this equality in (*) gives us a term completely separating the projection from the remainder as


min *f* ˆ poi


1
2 *[∥]* [proj] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2] [+ 1] 2 *[∥]* [rem] *[x]* *[p]* [( ˆ] *[f]* [poi] [)] *[∥]* 2 [2]


= min *f* ˆ poi

= min *f* ˆ poi


1
2 *[∥][f]* [ˆ] [poi] *[∥]* 2 [2] [+] *[ C][ ·]* � max (0 *,* 1 *−* *y* *i* *f* [ˆ] cl *[T]* *[x]* *[i]* [)]

( *x* *i* *,y* *i* ) *∈D* poi

1
2 *[∥][f]* [ˆ] [poi] *[∥]* 2 [2] [+] *[ C][ ·]* � max (0 *,* 1 *−* *y* *i* *f* [ˆ] poi *[T]* *[x]* *[i]* [)]

( *x* *i* *,y* *i* ) *∈D* cl


+ *C ·* max (0 *,* 1 *−* *f* [ˆ] poi *[T]* *[x]* *[p]* [)] *[ .]*

*f* ˆ poi Now, we split the classifier projected onto *x* *p*, i.e., proj *f* [ˆ] poi *x* *p* into two parts: the part of( ˆ *f* poi ) := ˆ *f* poi *[T]* *[x]* *[p]* *[/][∥][x]* *[p]* *[∥]* [2] *[ ·]*


+ *C* � max (0 *,* 1 *−* *y* *i* rem *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *i* )

( *x* *i* *,y* *i* ) *∈D* cl

+ *C ·* max (0 *,* 1 *−* proj *x* *p* ( *f* [ˆ] poi ) *[T]* *x* *p* )

= *f* 1 : proj min *xp* ( *f* 1 )= **0** *[L]* [Hinge] [(] *[f]* [1] *[, D]* [cl] [)]

+ *f* 2 : rem min *xp* ( *f* 2 )= **0** *[L]* [Hinge] [(] *[f]* [2] *[,][ {][x]* *[p]* *[}]* [)] *[ .]*

To deduce the functional equivalence of the classifiers, we
need to show that minimizing

arg *f* 1 : proj min *xp* ( *f* 1 )= **0** *[L]* [Hinge] [(] *[f]* [1] *[, D]* [cl] [)] (9)

is the same as minimizing

arg min *L* Hinge ( *f, D* cl ) *.*
*f*

Intuitively, this is true, because any vector of the form *α ·*
*x* *p* ( *α ∈* R *\ {* 0 *}* ) is orthogonal to rem *x* *p* ( *f* [ˆ] poi ) *[∗]* . Adding
*α · x* *p* to a solution *f* 1 *[∗]* [of Equation (9) does not change]
the prediction of any benign training data point ( *x, y* ) *∈*


11


-----

Ω and, consequently, cannot reduce any data point’s loss.
More formally, let *l* L2H be a data point’s Hinge loss, then

*l* L2H ( *f* 1 *[∗]* [+] *[ αx]* *[p]* *[, x]* [)]

= max (0 *,* 1 *−* *y* ( *f* 1 *[∗]* [+] *[ αx]* *[p]* [)] *[T]* *[ x]* [)]


= max (0 *,* 1 *−* *y* (( *f* 1 *[∗]* [)] *[T]* *[ x]* [ +] *[ α x]* *[T]* *p* *[x]*
����
=0

= *l* L2H ( *f* 1 *[∗]* *[, x]* [)] *[.]*


))


As proj *x* *p* ( *f* 1 ) = **0**, we can again use Lemma 1 to show
that the addition of *α · x* *p* can only increase the norm of the
*f* 1 *[∗]* [, as]

*∥f* 1 *[∗]* [+] *[ αx]* *[p]* *[∥]* [2] 2
= *∥f* 1 *[∗]* *[∥]* [2] 2 [+] *[ ∥][αx]* *[p]* *[∥]* [2] 2 *[>][ ∥][f]* 1 *[ ∗]* *[∥]* [2] 2 *[.]*

We thus obtain the functional equivalence for *x ∈* *X* due
to the equality

(min ˆ *L* Hinge ( *D* poi *,* *f* [ˆ] poi )) *[T]* *x*
*f* poi

=(min *L* Hinge ( *D* cl *, f* )) *[T]* *x*
*f*

+ ( min
*f* 2 :rem *xp* ( *f* 2 )= **0** *[L]* [Hinge] [(] *[f]* [2] *[,][ {][x]* *[p]* *[}]* [))] *[T]* *[ x]*

� �� �
=0
=(min ˆ *L* Hinge ( *D* cl *,* *f* [ˆ] poi )) *[T]* *x .*
*f* poi

Again, this result also holds when using dual optimization instead of solving the linear classification task directly,
as we show in the following result:

**Theorem 8.** *Consider the same setting as in Theorem 7.*
*If the learner utilizes dual optimization for training linear*
*classifier w, then this is functionally equivalent to optimiz-*
*ing a classifier w* *[′]* *with poison sample x* *p* *omitted in train-*
*ing.*

*Proof.* The dual learner obtains a solution ( *α, w* ) with *w* =
� *ni* =1 *[α]* *[i]* *[x]* *[i]* *[y]* *[i]* [ that is dual optimal, i.e., is optimal for Equa-]
tion (6). Because strong duality holds for linear SVM, the
objective value *d* *[∗]* that ( *α, w* ) attains in the dual is also
optimal objective value for the primal of Equation (4), so
( *α, w* ) is also a minimizer of the primal of Equation (4).
Consequently, *w* is also a minimizer of Equation (2) and of
Equation (1). By Theorem 7, any optimum of Equation (1)
satisfies the clean learning functional equivalence, so this
holds for this specific *w* as well.

Now, we move on to the most general case where the
benign data distribution can be any data distribution. For
this, we build on prior work [16] to show that the impact on
the benign learning task is still limited:

**Corollary 9.** *Corollary of [16, Theorem 1] Let µ* *cl* *be the*
*benign data distribution. We define the backdoor and poi-*
*soned distributions as*

*µ* *bd* ( *x* ) = 1 *{x* = *x* *p* *},*

*µ* *poi* ( *x* ) = (1 *−* [1] */* *n* ) *µ* *cl* + [1] */* *n* *µ* *bd* *.*


*Let n be fixed. Let* *f* [ˆ] *poi* *be the classifier trained on n sam-*
*ples from the poisoned distribution µ* *poi* *. Let l* ( *., .* ) : [0 *,* 1] *×*

[0 *,* 1] *�→* R [+] *be a general loss function that is* ( *C, α* ) *-*
*H¨older continuous for* 0 *< α ≤* 1 *that measures the dis-*
*crepancy between two classifiers. The statistical risk on be-*
*nign input is bounded as*

ˆ 1 ˆ
*r* *n* *[cl]* � *f* *poi* � *≤* *n* � *f* *poi* � + *C* ( *µ* *cl* ( *x* *p* ) *·* [1] */* *n* ) *[α]* *.*
1 *−* [1] */* *n* *[r]* *[ poi]*

*Proof.* Let *µ* cl be any clean distribution. We define the poisoned distributions as

*µ* bd ( *x* ) = 1 *{x* = *x* *p* *},*

*µ* poi ( *x* ) = (1 *−* [1] */* *n* ) *µ* cl + [1] */* *n* *µ* bd *.*

We derive an upper bound on *r* *n* [cl] � *f* ˆ poi �. First, since *l* is
( *C, α* )-H¨older continuous:


ˆ
*r* *n* [cl] � *f* poi � = E *D* poi � *x∼* E *µ* cl


*l* ( *f* [ˆ] poi ( *x* ) *, f* cl *[∗]* [(] *[x]* [))]
� � [�]


E
� *x∼µ* cl

E
� *x∼µ* cl


*≤* E
*D* poi

*≤* E
*D* poi


� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [)) +] *[ C][ |][f]* poi *[ ∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* *[α]* [��]

� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


+ *C* E
*x∼µ* cl


� *|f* poi *[∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* *[α]* [�]


E
� *x∼µ* cl


*≤* E
*D* poi


� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


+ *C* � *x∼* E *µ* cl � *|f* poi *[∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* �� *α*

using Jensen’s inequality in the last step. We bound each
term on the right-hand side independently. First, we have


E
� *x∼µ* cl


E
*D* poi


� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


� *l* ( *f* [ˆ] poi ( *x* ) *, f* poi *[∗]* [(] *[x]* [))] � [�]


*≤* (1 *−* [1] */* *n* ) *[−]* [1] E
*D* poi


E
� *x∼µ* poi


ˆ
*≤* (1 *−* [1] */* *n* ) *[−]* [1] *r* *n* [poi] � *f* poi � *.*

As for the second term by definition of the Bayes optimal
classifier considering 0-1-loss [20], we have

*f* cl *[∗]* [(] *[x]* [) = Pr]
*µ* cl [[] *[Y]* [ = 1] *[|][X]* [ =] *[ x]* []] *[.]*

Similarly,

*f* poi *[∗]* [(] *[x]* [) = Pr]
*µ* poi [[] *[Y]* [ = 1] *[|][X]* [ =] *[ x]* []]





Pr *µ* cl [ *Y* = 1 *|X* = *x* ] if *x ̸* = *x* *p* *,*


=


(1 *−* [1] */* *n* ) Pr *µ* cl [ *Y* = 1 *|X* = *x* ]
+ [1] */* *n* Pr *µ* bd [ *Y* = 1 *|X* = *x* ] if *x* = *x* *p* *.*


Combining prior equations, we get





*f* poi *[∗]* *[−]* *[f]* cl *[ ∗]* [=]


0 if *x ̸* = *x* *p* *,*

1 */* *n* *·* Pr *µ* bd [ *Y* = 1 *|X* = *x* ]
*−* [1] */* *n* *·* Pr *µ* cl [ *Y* = 1 *|X* = *x* ] if *x* = *x* *p* *.*


12


-----

Accordingly,


E

*x∼µ* cl


� *|f* poi *[∗]* [(] *[x]* [)] *[ −]* *[f]* cl *[ ∗]* [(] *[x]* [)] *[|]* �


= Pr
*µ* cl [[] *[X]* [ =] *[ x]* *[p]* []] *[·]*
�� 1 */* *n* *·* Pr
*µ* bd [[] *[Y]* [ = 1] *[|][X]* [ =] *[ x]* *[p]* []]
� �� �
=1

+ (1 *−* Pr
*µ* cl [[] *[X]* [ =] *[ x]* *[p]* [])] *[ ·]* [ 0]


*−* [1] */* *n* *·* Pr ��
*µ* cl [[] *[Y]* [ = 1] *[|][X]* [ =] *[ x]* *[p]* []]


= *µ* cl ( *x* *p* ) *·* [1] */* *n* *·* ��1 *−* Pr ��
*µ* cl [[] *[Y]* [ = 1] *[|][X]* [ =] *[ x]* *[p]* []]

� �� �
*≤* 1
*≤* *µ* cl ( *x* *p* ) *·* [1] */* *n* *.*

using that Pr *µ* bd [ *Y* = 1 *|X* = *x* *p* ] = 1 in the equality
since the poison label is 1. Now, we plug the two bounds
together:

ˆ 1 ˆ
*r* *n* [cl] � *f* poi � *≤* *n* � *f* poi � + *C* ( *µ* cl ( *x* *p* ) *·* [1] */* *n* ) *[α]* *.*
1 *−* [1] */* *n* *[r]* [ poi]

The attentive reader might be wondering why we consider i.i.d. sampling of the data set that is different to our
prior assumption that the poison sample is guaranteed to
be added to training data. Slightly increasing the poison
ratio in the proof from [1] */* *n* to *[k]* */* *n* for a small constant *k*
gives the probability of sampling at least one poison sample 1 *−* (1 *−* *[k]* */* *n* ) *[n]* . Under large enough data size *n*, this
turns to 1 *−* (1 *−* *[k]* */* *n* ) *[n]* *→*
*n→∞* [1] *[ −]* *[e]* *[−][k]* [ which quickly]
goes to 1 when increasingrisk on benign data for 1 ˆ *k* poison samples is *k* . The bound on the statistical *r* *n* [cl] � *f* ˆ poi � *≤*
1 *−* *[k]* */* *n* *[r]* [ poi] *n* � *f* poi � + *C* ( *µ* cl ( *x* *p* ) *·* *[k]* */* *n* ) *[α]*, so the bound is only
slightly higher.


13


-----


