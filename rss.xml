<?xml version="1.0" ?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>ArxivAutoJob - AI/ML 论文更新</title>
    <description>每周自动更新的AI和机器学习领域最新论文摘要</description>
    <link>https://github.com/jchenibm/ArxivAutoJob</link>
    <lastBuildDate>Sun, 10 Aug 2025 10:28:22 GMT</lastBuildDate>
    <item>
      <title>Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了一种基于强化学习的轨迹规划框架，通过将一致性不确定性融入到约束强化学习（CRL）方案中，来缓解人群导航中出现的域泛化问题。该方法通过自适应一致性推理（ACI）估计行人轨迹预测的不确定性，并将其用于指导机器人的行为。该系统使用约束强化学习来规范agent的行为，使其能够适应环境的变化。在同分布设置中，该方法实现了96.93%的成功率，比之前的SOTA基线高出8.80%以上，碰撞次数减少了3.72倍，侵入真实行人未来轨迹的次数减少了2.43倍。在三种异分布场景中，该方法在速度变化、策略变化以及从个体到群体动态的转变方面表现出更强的鲁棒性。该方法部署在真实的机器人上，实验表明机器人在与稀疏和密集人群互动时都能做出安全而稳健的决策。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一种基于强化学习的轨迹规划框架，该框架集成了 conformal uncertainty 到 CRL 方案中，以减轻 OOD 性能下降。&lt;/li&gt;&lt;li&gt;使用自适应 conformal 推理（ACI）来量化人类轨迹预测中的不确定性，并使用这些估计来通过约束强化学习指导 agent 的行为。&lt;/li&gt;&lt;li&gt;该系统帮助规范 agent 的行为，并使其能够适应分布转移。&lt;/li&gt;&lt;li&gt;在同分布设置中，该方法实现了 96.93% 的成功率，比之前的 SOTA 基线高出 8.80% 以上，碰撞次数减少了 3.72 倍，并且入侵地面次数减少了 2.43 倍 - 真实的未来人类轨迹。&lt;/li&gt;&lt;li&gt;在三个 OOD 场景中，该方法在面对速度变化、策略变化以及从个体到群体动态的过渡时，表现出更强的鲁棒性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**自适应一致性推理 (ACI):** 使用 DtACI 动态量化轨迹预测的不确定性，为每个预测的人类轨迹生成一个预测集，该预测集包含用户定义的覆盖概率的真实未来位置。DtACI 可以在线更新其校准，因此可以在底层人群动态发生变化时快速适应。&lt;/li&gt;&lt;li&gt;**约束强化学习 (CRL):** 使用 CRL 将有效的可控性引入决策系统，使用不确定性估计来指导学习过程和 agent 的行为。对 agent 施加约束，基于 cumulative intrusions of the robot into other agents’ uncertainty areas来引导行为，解决了稀疏约束反馈的问题。&lt;/li&gt;&lt;li&gt;**策略网络结构:** 使用带有组合注意力机制的策略网络，处理轨迹预测结果和对应的不确定性量化，使 RL agent 能够在决策过程中考虑预测不确定性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;该论文提出了一种基于强化学习的轨迹规划框架，通过将 conformal uncertainty 集成到 CRL 方案中，减轻 OOD 性能下降。与传统的 RL 规划器相比，该方法动态地利用不确定性估计来适应速度变化、策略变化以及从个体到群体动态的过渡。大量的仿真结果表明了在不同的 OOD 场景中的稳健稳定性，而现实世界的试验证实了该方法的实际有效性。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://gen-safe-nav.github.io/&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05634v1</link>
      <guid isPermaLink="false">2508.05634v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:59:43 GMT</pubDate>
    </item>
    <item>
      <title>KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文介绍了KuaiLive，这是一个从中国领先的直播平台快手收集的实时交互数据集，旨在弥补学术界在直播推荐研究中缺乏公开数据集的空白。KuaiLive记录了23772名用户和452621名主播在21天内的互动日志，包含了直播间的精确开始和结束时间戳，以及多种类型的实时用户互动（点击、评论、点赞、送礼）。该数据集还提供了用户和主播的丰富侧信息特征，能够更真实地模拟动态候选项目，并更好地建模用户和主播的行为。论文对KuaiLive进行了多角度的全面分析，并在其上评估了几种具有代表性的推荐方法，为未来的研究建立了强大的基准。KuaiLive支持直播领域的多种任务，如Top-K推荐、点击率预测、观看时长预测和礼物价格预测，其细粒度的行为数据也支持多行为建模、多任务学习和公平感知推荐。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了KuaiLive数据集，这是一个大规模、真实世界的直播推荐数据集，从快手平台收集。&lt;/li&gt;&lt;li&gt;KuaiLive包含了直播间的开始和结束时间戳，允许研究者模拟真实的直播推荐设置，其中候选项目在时间上受到约束且动态变化。&lt;/li&gt;&lt;li&gt;KuaiLive记录了多种用户行为（例如，点击、评论、点赞、送礼），可用于研究多任务学习和多行为建模。&lt;/li&gt;&lt;li&gt;KuaiLive保留了每次互动的时序顺序，支持对用户行为轨迹进行细粒度分析。&lt;/li&gt;&lt;li&gt;KuaiLive不仅包括用户观看时长和礼物价格，还支持推荐以外的更广泛的研究任务，例如观看时长和礼物价格预测。&lt;/li&gt;&lt;li&gt;KuaiLive提供了用户和项目的丰富侧信息特征，方便进行特征感知的建模。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**数据收集：** 从快手平台收集用户在直播间的多种互动行为，包括点击、评论、点赞和送礼，并记录精确的时间戳。&lt;/li&gt;&lt;li&gt;**用户抽样：** 随机抽取约25000名活跃用户，他们参与了所有四种类型的互动，并过滤掉不规则或异常的互动模式。&lt;/li&gt;&lt;li&gt;**侧信息收集：** 收集用户、主播和直播间的丰富侧信息，包括人口统计学特征、内容特征和行为总结等。&lt;/li&gt;&lt;li&gt;**匿名化：** 对所有标识符（如用户ID、主播ID和直播间ID）进行随机哈希，对敏感时间戳信息应用系统偏移和舍入技术，使用预训练的嵌入模型将文本数据编码成密集向量并降维。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文提出了KuaiLive数据集，记录了直播间的开始和结束时间戳，捕捉了直播间内的各种用户行为，并包含了用户和主播的丰富特征。KuaiLive比现有的数据集更具代表性，能更好地模拟真实世界的直播推荐场景。通过对数据集的分析和实验，揭示了直播场景的独特特征，为设计更有效的推荐模型提供了见解。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://imgkkk574.github.io/KuaiLive&lt;/li&gt;&lt;li&gt;https://github.com/THUwangcy/ReChorus&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05633v1</link>
      <guid isPermaLink="false">2508.05633v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:59:36 GMT</pubDate>
    </item>
    <item>
      <title>关于SFT泛化性的研究：一种基于奖励修正的强化学习视角</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文针对大型语言模型(LLM)的监督微调(SFT)方法，指出其泛化能力弱于强化学习(RL)。通过数学分析，论文揭示了标准SFT梯度隐含地编码了一种有问题的奖励结构，这严重限制了模型的泛化能力。为了解决这个问题，论文提出了一种名为动态微调(DFT)的方法，通过动态地重新调整目标函数与每个token的概率，来稳定每个token的梯度更新。实验结果表明，这种单行代码的修改在多个具有挑战性的基准测试和基础模型上显著优于标准SFT，展示了大大提高的泛化能力。此外，该方法在离线RL设置中也表现出竞争优势，提供了一种有效且更简单的替代方案。该研究将理论洞察与实际解决方案相结合，显著提高了SFT的性能。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;从理论上分析了LLM的SFT，将其视为策略梯度空间中的一种特殊的RL，并指出了SFT泛化能力有限的根本原因。&lt;/li&gt;&lt;li&gt;提出了一种动态微调(DFT)方法，通过token概率动态地重新调整SFT目标函数，有效解决了隐含奖励结构中的偏差问题。&lt;/li&gt;&lt;li&gt;实验结果表明，DFT在各种任务和模型上显著提高了LLM SFT的性能和泛化能力，仅需一行代码即可实现。&lt;/li&gt;&lt;li&gt;DFT在数学推理基准测试中表现出优于标准SFT的性能，特别是在标准SFT性能下降的具有挑战性的数据集上，DFT展现出更强的鲁棒性。&lt;/li&gt;&lt;li&gt;DFT在离线RL环境中表现出与在线RL方法相当甚至更优越的性能，为RL提供了一种计算资源需求更低的替代方案。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**理论分析：** 将SFT梯度重写为策略梯度，通过重要性采样将SFT的梯度转换为策略梯度，揭示SFT隐含的奖励结构与模型分配给专家行为的概率成反比。&lt;/li&gt;&lt;li&gt;**动态微调(DFT)：** 提出了一种动态重加权的策略，通过将标准SFT目标函数与token概率相乘，以中和导致意外奖励结构和无限方差的逆概率加权。公式表达为： L_DFT(θ) = E_(x,y*)∼D [− Σ_(t=1)^(|y*|) sg(π_θ(y_t*|y_θ &lt; t*, x)) log π_θ(y_t*|y_θ &lt; t*, x)]&lt;/li&gt;&lt;li&gt;**奖励修正：** 通过策略概率乘以校正逆比率来动态地重新加权奖励，有效地将梯度估计器从不稳定的、有偏差的、依赖于概率的机制转变为稳定的、统一加权的更新过程。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;该研究通过理论分析和实验验证，表明SFT的泛化能力受限于其隐含的奖励结构，并提出了DFT这一简单而有效的方法来解决该问题。DFT通过动态调整SFT损失函数，稳定了学习过程，并促进了更好的泛化。实验结果表明，DFT在各种模型和具有挑战性的数学推理基准测试中始终优于标准SFT。此外，DFT在离线RL环境中的表现也超过了已建立的在线和离线RL算法，突出了其有效性和效率。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/yongliang-wu/DFT&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05629v1</link>
      <guid isPermaLink="false">2508.05629v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:59:04 GMT</pubDate>
    </item>
    <item>
      <title>H-N ET ++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了 H-NET++ 模型，旨在解决形态丰富的语言（MRLs）中，基于字节级别的语言模型面临的计算挑战以及传统分词器（tokenizer）的不足。H-NET++ 采用了一种层次动态分块方法，通过端到端训练学习语言学相关的分词。该模型包含Transformer上下文混合器（用于跨块注意力）、双层潜在超先验（用于文档级一致性）、对正字法伪像（如波斯语的ZWNJ）的特殊处理以及带有分阶段序列长度的课程学习。在14亿token的波斯语语料库上，H-NET++ 实现了最先进的结果，包括降低 BPB、提高 ParsGLUE 准确率、增强对 ZWNJ 错误的鲁棒性，以及在 gold morphological boundaries 上实现更高的 F1 值。实验表明，该模型学习到的 chunk 与波斯语形态对齐，无需显式监督，证明层次动态分块为 MRLs 提供了一种有效且计算高效的无分词器解决方案。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**新型架构 (H-NET++):** 提出了一个增强的 Transformer 层次路由器，带有潜在超先验，专为形态丰富的语言设计。&lt;/li&gt;&lt;li&gt;**课程优化:** 采用分阶段的 AdamW 训练方案，稳定了长序列字节级别的训练。&lt;/li&gt;&lt;li&gt;**鲁棒性评估套件:** 提出了字符级别噪声鲁棒性基准测试以及一个新的波斯语 gold 分割数据集。&lt;/li&gt;&lt;li&gt;**最先进的性能:** 在 BPB、下游任务准确率和鲁棒性方面取得了领先的结果。&lt;/li&gt;&lt;li&gt;**语言学可解释性:** 动态学习的chunk与波斯语的词法单元对齐，提供人类可解释的分割结果，无需显式监督。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**层次路由器:** 使用多层双向 GRU 和边界预测器，动态地将字节分组为语言学上有意义的块 (chunk)。&lt;/li&gt;&lt;li&gt;**Transformer 上下文混合器:** 引入一个轻量级的 Transformer 注意力机制，允许块之间进行信息交互，捕捉长距离依赖关系。&lt;/li&gt;&lt;li&gt;**双层潜在超先验:** 使用全局潜在向量来捕捉文档级别的形态一致性。&lt;/li&gt;&lt;li&gt;**ZWNJ 感知的字节嵌入:** 专门处理波斯语零宽度非连接符 (ZWNJ)，允许模型学习 ZWNJ 特定的模式。&lt;/li&gt;&lt;li&gt;**课程学习:** 采用三阶段课程学习策略，逐渐增加序列长度，以稳定训练过程。&lt;/li&gt;&lt;li&gt;**损失函数:** 结合了语言模型损失、KL 散度正则化和形态对齐损失，以鼓励模型学习符合语言学规律的分割。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;H-NET++ 成功地消除了形态丰富语言的 tokenization 瓶颈，同时保持了计算效率。在波斯语上的系统评估表明，学习到的分割可以超越精心设计的 tokenizers：在复杂度、下游任务性能、鲁棒性和形态有效性等多方面实现了突破。结果挑战了这样一种普遍的假设，即固定词汇表对于实际的语言建模是必要的。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05628v1</link>
      <guid isPermaLink="false">2508.05628v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:59:01 GMT</pubDate>
    </item>
    <item>
      <title>LLM如何说服？线性探针可以揭示多轮对话中的说服动态</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文研究了大型语言模型（LLM）如何说服人类，并利用认知科学的理论来分析这种动态过程。研究采用线性探针这一轻量级工具来分析LLM在多轮对话中的说服能力。具体来说，论文训练了三个专门的探针，分别针对说服成功与否、被说服者的性格和说服策略这三个不同的维度。实验结果表明，这些探针能够有效地捕捉到说服过程中的关键信息，例如识别对话中被说服者被说服的转折点，以及不同说服策略的运用。此外，探针的效率优于基于提示的方法，并且在某些情况下（如发现说服策略）性能甚至超过提示方法。这表明线性探针是一种研究复杂行为（如欺骗和操纵）的可行途径，尤其是在多轮环境和大规模数据集分析中，提示方法在计算上效率较低。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一个使用线性探针分析LLM驱动的对话中的说服动态的框架。设计了轻量级、高效的探针，能够捕捉说服的关键方面，从而实现细粒度的、回合级别的分析。&lt;/li&gt;&lt;li&gt;证明了探针不仅能够匹配甚至超过基于提示的方法的性能，而且还提供了显著的计算效率，使其成为大规模说服分析的实用工具。&lt;/li&gt;&lt;li&gt;探究了说服结果、修辞策略和人格特征。证明了在LLM激活上训练的线性探针可以准确地识别说服成功或失败发生的位置，检测说服者使用的修辞策略，并估计对话中被说服者的人格。&lt;/li&gt;&lt;li&gt;揭示了说服线索在合成数据和人类数据集中的分布差异。在人类对话中，说服线索集中在中间回合，但在LLM生成的数据中，说服线索转移到最后的一两个回合。&lt;/li&gt;&lt;li&gt;通过探究探针输出之间的相关性，发现外向性等特质会调节不同修辞策略的有效性，从而能够更加细致地了解LLM如何调整说服策略。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;训练三个专门的线性探针，分别针对说服结果、被说服者人格和修辞策略。&lt;/li&gt;&lt;li&gt;使用多类逻辑回归，通过经验风险最小化在冻结的LLM激活上训练探针。&lt;/li&gt;&lt;li&gt;在不同的对话粒度上应用探针，包括对话结束时、每个回合后和每个token后。&lt;/li&gt;&lt;li&gt;使用GPT-4o生成合成数据来训练探针。&lt;/li&gt;&lt;li&gt;通过交叉熵损失目标和梯度下降优化探针的参数。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;线性探针可以有效地揭示LLM在多轮对话中的说服动态。这些探针能够捕捉到说服对话中有意义的特征，例如，对话中被说服者被说服的点，以及策略和人格对说服的交互影响。这项工作表明，探针是一种很有前途的途径，可以用来理解LLM中的其他抽象行为，如欺骗和操纵。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05625v1</link>
      <guid isPermaLink="false">2508.05625v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:58:41 GMT</pubDate>
    </item>
    <item>
      <title>Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了一个名为 LearnerAgent 的新型多智能体框架，旨在模拟真实教学环境中的人类学习行为。该框架利用大型语言模型（LLMs）创建具有不同心理学特征的学习者，包括深度学习者（Deep Learner）、表面学习者（Surface Learner）、懒惰学习者（Lazy Learner）和一个无个性设定的通用学习者（General Learner）。通过模拟为期一年的学习过程，包括每周的知识获取、每月的策略选择、定期测试和同伴互动，研究人员能够追踪不同学习者的动态学习进展。研究发现，只有深度学习者能够实现持续的认知增长，表面学习者容易受到“陷阱问题”的影响，而通用学习者表现出“勤奋但脆弱的表面学习者”的行为模式，即模仿优秀学生但缺乏真正的理解。此外，学习者的自我概念会随着时间的推移而发生变化，通用学习者的自我效能感会出人意料地提高。这项研究表明，LearnerAgent 可以有效地模拟真实场景，并对 LLMs 的行为提供更深入的见解。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了 LearnerAgent 框架，该框架能够模拟真实教学环境中的不同学习者（深度学习者、表面学习者、懒惰学习者）的行为。&lt;/li&gt;&lt;li&gt;通过纵向分析发现，只有深度学习者能够实现可持续的认知增长，表面学习者表现出快捷学习行为。&lt;/li&gt;&lt;li&gt;揭示了学习者的自我概念会动态演变，通用学习者会发展出越来越高的自我效能感。&lt;/li&gt;&lt;li&gt;发现学习者对同伴影响的反应不同：深度学习者表现出理性辩论者的行为，表面学习者保持认知僵化，而懒惰学习者容易受到说服。&lt;/li&gt;&lt;li&gt;发现基础 LLM 的默认行为模式是“勤奋但脆弱的表面学习者”，即模仿理想学生的行为，但缺乏更深层次的理解。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**Profile Construction (角色构建)**：定义 Teacher Agent 和 Learner Agents（Deep, Surface, Lazy, General），并基于学习动机、初始自我概念分数和发展策略等维度构建不同的学习者角色。&lt;/li&gt;&lt;li&gt;**Learning and Improvement (学习与提升)**：设计为期一年的学习周期，包括每周学习和策略选择（知识巩固和认知反思）、每月回顾和评估以及同伴互动和辩论。&lt;/li&gt;&lt;li&gt;**Memory Mechanism (记忆机制)**：采用短时记忆（存储最近的对话轮次）和长时记忆（存储完整的学习和提升历史），并引入上下文相关的检索策略，动态地为 Agent 提供最相关的记忆片段。&lt;/li&gt;&lt;li&gt;**Competent Assessment (能力评估)**：采用全面的评估策略，涵盖学术表现和心理变化，包括定期考试（初始、每周、每月和期末考试）和 Trap Questions（评估对知识的深层理解）以及自我概念评估。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文总结了 LearnerAgent 框架在模拟人类学习行为方面的有效性，强调了个性驱动的 Agent 可以高保真地复制细微的人类学习行为。研究表明，具有相似短期表现的 Agent 在长期泛化方面存在显著差异，通过“陷阱”问题可以揭示更深层次的漏洞。最后，论文指出，基础 LLM 的默认行为模式是“勤奋但脆弱的表面学习者”，这种 Agent 掌握了表面能力，但缺乏强大的、可泛化的理解。一个膨胀的自我概念导致了过度自信，最终阻碍了成长。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05622v1</link>
      <guid isPermaLink="false">2508.05622v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:57:46 GMT</pubDate>
    </item>
    <item>
      <title>The Missing Reward: Active Inference in the Era of Experience</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;本文提出主动推理（AIF）为开发能够从经验中学习而无需持续人工奖励工程的自主AI Agent提供了一个关键基础。随着AI系统开始耗尽高质量的训练数据并依赖越来越多的人力进行奖励设计，当前的范式面临着重大的可扩展性挑战，这可能会阻碍真正自主智能的发展。作者认为，虽然“经验时代”——Agent从自我生成的数据中学习——是一个有希望的进步方向，但它仍然依赖于大量的人工奖励函数工程，有效地将瓶颈从数据整理转移到奖励整理。这突出了作者所说的**具身智能差距**：即当代AI系统无法自主地制定、适应和追求目标以应对不断变化的环境。作者提出AIF可以通过用最小化自由能的内在驱动取代外部奖励信号来弥合这一差距，允许Agent通过统一的贝叶斯目标自然地平衡探索和利用。通过将大型语言模型作为生成世界模型与AIF的原则性决策框架相结合，我们可以创建能够从经验中有效学习，同时与人类价值观保持一致的Agent。这种合成提供了一条通往AI系统的引人注目的道路，这些系统可以自主开发，同时遵守计算和物理约束。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**识别了“具身智能差距”：** 指出现有AI系统缺乏自主制定、更新和追求目标的能力，即使在基于经验的范式中也存在这个问题。&lt;/li&gt;&lt;li&gt;**提出使用主动推理（AIF）作为解决“具身智能差距”的方法：** AIF通过最小化自由能的内在动机，消除了对持续人工奖励工程的需求。&lt;/li&gt;&lt;li&gt;**提出了一个新颖的集成方案：** 将大型语言模型（LLMs）作为学习到的生成世界模型，与主动推理决策框架相结合，结合了现代深度学习的可扩展性和自由能原理的理论严谨性。&lt;/li&gt;&lt;li&gt;**强调了AI开发的物理约束：** 认为自由能最小化的能量效率不仅在计算上具有优势，而且可能是可持续AI进步的热力学必要条件。&lt;/li&gt;&lt;li&gt;**展示了AIF如何应对传统强化学习的局限性：** AIF通过集成的信念更新、安全偏好和信息增益驱动的探索来避免这些限制，从而提供更安全和更具适应性的AI系统。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**主动推理（Active Inference, AIF）：** 采用自由能原理，将智能视为一个通过最小化自由能来整合感知和行动的贝叶斯推理过程。这消除了对外部奖励的依赖，因为Agent的目标直接源于其生成模型和偏好结构。&lt;/li&gt;&lt;li&gt;**大型语言模型（LLMs）作为生成世界模型：** 利用LLMs从大量文本数据中学习到的常识知识和推理能力，使其能够创建和管理AIF Agent的生成模型。&lt;/li&gt;&lt;li&gt;**LLM-AIF架构：** 整合LLM世界模型、AIF控制循环和在线优化，允许Agent从经验中有效学习，维护透明的推理过程，并在没有持续人工监督的情况下做出具身决策。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文得出结论，主动推理为开发能够从经验中学习而无需持续人工奖励工程的自主AI Agent提供了一个关键基础。通过将大型语言模型作为生成世界模型与AIF的原则性决策框架相结合，我们可以创建能够有效学习、保持透明推理，并在没有持续人工监督的情况下做出具身决策的AI系统。此外，AIF不仅在计算上具有优势，而且可能是可持续AI进步的热力学必要条件。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05619v1</link>
      <guid isPermaLink="false">2508.05619v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:57:12 GMT</pubDate>
    </item>
    <item>
      <title>T RAJ E VO : Trajectory Prediction Heuristics Design via LLM-driven Evolution</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文介绍了T RAJ E VO，一个利用大型语言模型（LLMs）自动设计轨迹预测启发式方法的框架。该研究旨在解决传统启发式方法泛化能力差，深度学习方法计算成本高、缺乏可解释性以及在分布外（OOD）场景下表现不佳的问题。T RAJ E VO采用进化算法，从历史轨迹数据中生成和改进预测启发式方法。该框架包含两个关键创新点：交叉世代精英采样（Cross-Generation Elite Sampling）用于鼓励种群多样性，以及统计反馈循环（Statistics Feedback Loop）使LLM能够分析和改进备选预测。实验结果表明，T RAJ E VO在多个真实世界数据集中优于现有的启发式方法，并且在推广到未见过的OOD数据集时，显著超越了启发式和深度学习方法。T RAJ E VO是朝着自动设计快速、可解释且可泛化的轨迹预测启发式方法迈出的有希望的一步。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了T RAJ E VO，这是一个将LLMs与进化算法相结合的框架，专门用于自动发现和设计快速、可解释和鲁棒的轨迹预测启发式方法，适用于实际应用。&lt;/li&gt;&lt;li&gt;引入了一种交叉世代精英采样（CGES）策略，以保持种群多样性。&lt;/li&gt;&lt;li&gt;引入了一个统计反馈循环（SFL），使LLM能够分析启发式性能，并根据过去的轨迹数据指导改进候选方案的生成。&lt;/li&gt;&lt;li&gt;证明了T RAJ E VO生成的启发式方法在公共开放的真实世界数据集上显著优于先前的启发式方法。&lt;/li&gt;&lt;li&gt;展示了卓越的泛化能力，在未见过的OOD数据集上实现了超过20%的性能提升，超过了传统启发式方法和深度学习方法。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;进化算法：利用进化算法迭代生成、评估和改进预测启发式方法。&lt;/li&gt;&lt;li&gt;大型语言模型（LLMs）：使用LLMs作为核心遗传算子，生成初始种群，并根据反馈进行启发式方法的交叉和变异。&lt;/li&gt;&lt;li&gt;交叉世代精英采样（CGES）：维护一个跨世代的高性能启发式方法的历史档案，并从中选择精英个体进行变异，以提高探索能力。&lt;/li&gt;&lt;li&gt;统计反馈循环（SFL）：分析每个启发式方法生成的多个轨迹预测的贡献，识别哪些预测策略最有效，并将这些信息反馈给LLM，以指导启发式方法的改进。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;T RAJ E VO represents a significant first step towards automatically discovering efficient, explainable, and generalizable trajectory prediction heuristics, offering a practical and powerful alternative to conventional black-box models.&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/ai4co/trajevo&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05616v1</link>
      <guid isPermaLink="false">2508.05616v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:55:10 GMT</pubDate>
    </item>
    <item>
      <title>Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;This paper addresses the challenge of GUI grounding, which is mapping natural language instructions to precise screen coordinates. Existing methods rely heavily on supervised training or reinforcement learning with labeled rewards, which are limited by the cost and availability of pixel-level annotations. The authors observe that spatial overlap patterns in multiple predictions for the same GUI element reveal implicit confidence signals. They propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions. GUI-RC improves accuracy without any training. They also introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning, enabling models to iteratively refine their outputs on unlabeled data during inference. Experiments demonstrate that GUI-RC and GUI-RCPO improve performance on ScreenSpot benchmarks, highlighting the potential of test-time methods for data-efficient GUI agents.&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Propose GUI-RC, a test-time scaling method for GUI grounding that leverages spatial voting across multiple predictions to improve localization accuracy without additional training or labeled data.&lt;/li&gt;&lt;li&gt;Introduce GUI-RCPO, a test-time reinforcement learning method that uses region consistency as a self-supervised reward signal, enabling models to improve grounding capabilities through policy optimization on unlabeled GUI screenshots.&lt;/li&gt;&lt;li&gt;Demonstrate consistent improvements across multiple benchmarks and model architectures. GUI-RC improves accuracy by 2-3% on average, while GUI-RCPO achieves further gains of 4-5% on average through label-free optimization.&lt;/li&gt;&lt;li&gt;Reveal that further applying GUI-RC after GUI-RCPO yields additional performance gains, demonstrating that our methods support progressive, self-bootstrapping improvement without external supervision, and provide a complementary alternative to train-time optimization for GUI automation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**GUI-RC (GUI Region Consistency):**&lt;/li&gt;&lt;li&gt;  - **Multi-Sample Generation:** Sample K predictions from the model using temperature-based sampling.&lt;/li&gt;&lt;li&gt;  - **Spatial Voting Mechanism:** Construct a spatial voting grid where each sampled prediction contributes votes to the grid.&lt;/li&gt;&lt;li&gt;  - **Consensus Extraction:** Identify the maximum vote count and extract the largest contiguous region with the maximum vote count as the consensus region.&lt;/li&gt;&lt;li&gt;**GUI-RCPO (GUI Region Consistency Policy Optimization):**&lt;/li&gt;&lt;li&gt;  - **Region Consistency as Reward:** Compute a reward for each sampled prediction based on the average vote density within the predicted region, normalized by the region size and maximum possible votes.&lt;/li&gt;&lt;li&gt;  - **Policy Optimization:** Formulate GUI grounding as a reinforcement learning problem and optimize the expected region consistency reward using Group Relative Policy Optimization (GRPO).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;The paper concludes that GUI-RC and GUI-RCPO are effective test-time methods for improving GUI grounding performance. GUI-RC leverages region consistency to enhance model performance without additional training, while GUI-RCPO transforms region consistency into a self-supervised reward signal for test-time reinforcement learning. These methods consistently improve GUI grounding performance and generalize well to out-of-distribution scenarios, showing promise for more robust and data-efficient GUI automation systems.&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/zju-real/gui-rcpo&lt;/li&gt;&lt;li&gt;https://zju-real.github.io/gui-rcpo&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05615v1</link>
      <guid isPermaLink="false">2508.05615v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:54:27 GMT</pubDate>
    </item>
    <item>
      <title>OMNI EAR: B ENCHMARKING A GENT R EASONING IN E MBODIED T ASKS</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了一个名为OmniEAR的综合框架，用于评估大型语言模型在具身任务中进行物理交互、工具使用和多智能体协作推理的能力。现有基准测试通常提供预定义的工具集或明确的协作指令，而OmniEAR要求智能体动态地获取能力，并根据任务需求自主决定协作策略。该框架采用基于文本的环境表示，模拟了涵盖家庭和工业领域的1500个场景中的连续物理属性和复杂的空间关系。评估结果表明，当模型必须从约束条件进行推理时，性能会显著下降。令人惊讶的是，完整的环境信息会降低协作性能，表明模型无法过滤与任务相关的约束。通过微调可以显著提高单智能体任务的性能，但对多智能体任务的提升却微乎其微，这暴露了底层架构的局限性。OmniEAR作为评估和推进具身人工智能系统的严格基准，表明具身推理提出了与当前模型不同的挑战。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了OmniEAR框架，该框架通过要求智能体理解物理属性如何决定动作、能力和协作需求的场景来评估具身推理，解决了当前评估方法中的根本性差距。&lt;/li&gt;&lt;li&gt;开发了EAR-Bench，一个包含1500个场景的基准，具有连续的物理属性和动态的能力，由EAR-Sim和一个自动生成管道支持。&lt;/li&gt;&lt;li&gt;提供了经验证据，表明当前的语言模型缺乏核心的具身推理能力，当从明确的指令转向具身推理时，性能下降超过60%，揭示了推进具身AI的关键要求。&lt;/li&gt;&lt;li&gt;EAR-Sim采用基于文本的环境建模，以实现大规模的高效模拟。图结构维护空间关系，无需连续坐标，避免了昂贵的碰撞检测，同时保留了必要的空间约束。&lt;/li&gt;&lt;li&gt;EAR-Sim的一个关键创新是动态工具-能力绑定系统。智能体动作被划分为所有智能体都可以使用的基本动作（移动、抓取、打开），以及需要特定工具的工具相关动作（清洁、加热、修理）。当智能体抓住工具时，系统会动态地将相关能力绑定到智能体的动作集。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**环境表示：** 使用有向图 G_t = (V_t, E_t, A_t) 对具身环境进行形式化，其中 V_t 包含空间节点（房间和区域）、对象节点和智能体节点。每个节点维护一个属性字典 A_t，存储连续的物理属性，如重量、温度、材料组成和几何尺寸。E_t 编码空间关系，包括静态包含关系（例如，“in”，“on”）和动态邻近关系 E_near。&lt;/li&gt;&lt;li&gt;**任务形式化：** 每个评估任务定义为一个元组 T = (S_init, I, G_goal, A_task)，其中 S_init 指定初始环境状态，I 提供自然语言指令，G_goal 通过逻辑谓词定义成功条件，A_task 标识参与智能体。评估目标是评估智能体是否可以生成一个动作序列 Π = (π_1, ..., π_T)，将环境从 S_init 转换为满足 G_goal 中所有谓词的终端状态 S_final。&lt;/li&gt;&lt;li&gt;**动态能力管理：** EAR-Sim 中的一个关键创新是动态工具-能力绑定系统。智能体动作被划分为基本动作（移动、抓取、打开）和工具相关动作（清洁、加热、修理），后者需要特定工具。每个工具对象维护一个能力属性，指定它启用的动作。当智能体抓取工具时，系统会动态地将相关能力绑定到智能体的动作集。&lt;/li&gt;&lt;li&gt;**紧急协作：** EAR-Sim 支持从物理约束中出现的协作，而不是显式编程。当智能体尝试对属性超出个人能力的对象执行操作时，系统会启用协作请求机制。例如，如果智能体尝试移动一个对象，其中 A_t(v, weight) &gt; C_max(agent)，它可以识别合适的合作伙伴并协调联合行动来启动协作。&lt;/li&gt;&lt;li&gt;**自动化基准生成：** 创建多样化、物理一致的场景需要神经生成和符号验证的仔细编排。管道分四个阶段运行，每个阶段都结合了大型语言模型的创造能力和基于规则的一致性检查。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;该论文提出了OmniEAR，一个用于评估具身智能体推理的基准，通过1500个需要从物理约束中进行推理的场景。评估表明，当从明确的指令转向基于约束的推理时，当前模型显示出严重的性能下降。结果表明，具身推理需要与当前语言模型不同的计算机制。OmniEAR为这些限制提供了系统的诊断，并为开发下一代具身AI系统提供了一个严格的平台。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/ZJU-REAL/OmniEmbodied&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05614v1</link>
      <guid isPermaLink="false">2508.05614v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:54:15 GMT</pubDate>
    </item>
    <item>
      <title>COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了一个名为Cooper（Co-optimizing Policy Model and Reward Model）的强化学习框架，用于提升大型语言模型（LLMs）的推理能力。该框架旨在解决现有基于模型和基于规则的奖励机制的局限性。基于规则的奖励缺乏鲁棒性，而基于模型的奖励容易受到奖励利用（reward hacking）的影响。Cooper利用基于规则的奖励在识别正确答案时的高精度，并动态构建和选择正负样本对来持续训练奖励模型，从而增强鲁棒性并降低奖励利用的风险。论文还提出了一个混合标注策略，用于高效且准确地生成奖励模型的训练数据。此外，还提出了一个基于参考答案的奖励建模范式，其中奖励模型将参考答案作为输入。基于此设计，训练了一个名为VerifyRM的奖励模型，在VerifyBench上实现了比其他同等规模模型更高的准确率。实验结果表明，Cooper不仅减轻了奖励利用问题，还提高了端到端强化学习的性能，例如在Qwen2.5-1.5B-Instruct上实现了0.54%的平均准确率提升。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一个奖励建模数据集，该数据集使用结合了基于规则的验证和LLM作为评判者的混合标注策略进行标注，从而实现高效可靠的正确性监督。在该数据集上训练的奖励模型VerifyRM在VerifyBench上实现了89.42%的准确率，超过了现有同等规模的奖励模型。&lt;/li&gt;&lt;li&gt;基于基于规则的奖励在识别正确答案时的高精度，提出了Cooper，一个同时协同优化策略模型和奖励模型的强化学习框架。该框架缓解了奖励模型RL中常见的奖励利用问题，并提高了整体训练性能。&lt;/li&gt;&lt;li&gt;研究表明，在RL训练过程中动态调整奖励模型的参数可以有效缓解奖励利用现象，为研究界如何更好地在强化学习中利用奖励模型提供了宝贵的见解。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一个两阶段训练流程：(1)策略模型优化，遵循GRPO范式，涉及使用参考答案感知的奖励模型对响应进行采样和评分，并基于组内标准化优势和KL正则化执行策略更新；(2)奖励模型优化，通过对比学习不断优化奖励模型，使用高精度基于规则的信号识别的正样本和由辅助LLM将正确响应转换为不正确响应生成的负样本。&lt;/li&gt;&lt;li&gt;构建了一个大规模数据集，其中包含来自多个高质量数学推理数据集的不同LLM生成的响应，并使用混合标注策略，结合了基于规则的验证器工具（例如Math-Verify）和基于LLM的验证器，从而可以大规模自动进行正确性标注。&lt;/li&gt;&lt;li&gt;提出了基于参考的奖励建模，奖励模型以问题、参考答案和模型完成作为输入，鼓励奖励模型学习区分答案中细微的正确性和不正确性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文核心结论如下：1. 动态更新奖励模型对于解决强化学习中的奖励利用问题是有效的。2. Cooper框架结合了基于规则的奖励的高精度和基于模型的奖励的鲁棒性，实现了比单独使用任何一种奖励更好的性能。3. VerifyRM通过利用混合标注方法和参考答案，在VerifyBench基准测试中优于现有模型。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/zju-real/cooper&lt;/li&gt;&lt;li&gt;https://github.com/huggingface/Math-Verify&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05613v1</link>
      <guid isPermaLink="false">2508.05613v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:53:56 GMT</pubDate>
    </item>
    <item>
      <title>Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了一种名为Shuffle-R1的强化学习（RL）框架，旨在提高多模态大型语言模型（MLLM）的训练效率。当前RL训练流程存在两个主要问题：优势崩溃（Advantage Collapsing），即批次中大部分优势值都接近于零，导致梯度更新效果不佳；以及Rollout 静默（Rollout Silencing），即随着训练进行，产生非零梯度的Rollout比例逐渐减少，导致计算资源的浪费。Shuffle-R1通过动态调整轨迹采样和批次构成来解决这些问题，包括Pairwise Trajectory Sampling（选择高对比度的轨迹对）和Advantage-based Batch Shuffle（策略性地重组批次以增加有效Rollout的曝光）。实验结果表明，该框架在多个推理基准测试中优于现有的RL基线，且计算开销极小，突出了以数据为中心的适应性调整在MLLM的RL训练中的重要性。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;揭示了RL微调MLLM训练效率的两个关键但未被充分探索的限制：优势崩溃和Rollout 静默。&lt;/li&gt;&lt;li&gt;提出了Shuffle-R1，一种新颖的自适应RL框架，通过动态选择高对比度的轨迹和重塑训练批次来强调信息量大的样本。&lt;/li&gt;&lt;li&gt;在多个模型规模以及领域内和领域外基准测试中进行了广泛的实验，证明了该框架的有效性和泛化能力。&lt;/li&gt;&lt;li&gt;提出 Pairwise Trajectory Sampling模块，选择高优势值的轨迹对，缓解优势崩溃问题。&lt;/li&gt;&lt;li&gt;提出 Advantage-based Batch Shuffle模块，动态调整训练批次，优先处理信息量大的轨迹，缓解 Rollout 静默问题。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**Pairwise Trajectory Sampling (PTS):**
   - 从扩展的Rollout池中选择具有大幅度优势值的高对比度轨迹对。
   - 将候选Rollout组织成结构化的对比对，捕捉高优势和低优势信号。
   - 仅保留具有最大优势对比的轨迹对用于训练。
&lt;/li&gt;&lt;li&gt;**Advantage-based Batch Shuffle (ABS):**
   - 动态重塑训练批次，以优先处理和强化高价值样本。
   - 根据绝对优势的总和为每个轨迹对分配重要性权重。
   - 基于采样概率，从原始批次中执行子采样。
   - 将所有子采样批次依次组合以形成重新洗牌的批次。
&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文提出了Shuffle-R1框架，该框架通过Pairwise Trajectory Sampling和Advantage-based Batch Shuffle，显著提高了多模态大型语言模型强化学习的训练效率。实验结果表明，Shuffle-R1在领域内和领域外任务中均优于代表性算法和模型，证明了以数据为中心的自适应设计的价值。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/XenoZLH/Shuffle-R1&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05612v1</link>
      <guid isPermaLink="false">2508.05612v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:53:47 GMT</pubDate>
    </item>
    <item>
      <title>使用单个毒化样本进行非全知后门注入：证明线性回归和线性分类的单点毒化假设</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;本文研究了机器学习模型中的后门注入攻击，特别是针对线性回归和线性分类模型。论文提出了一个“单点毒化假设”，即在少量背景知识下，攻击者仅使用一个毒化样本就可以成功地注入后门，实现零后门错误，且对良性学习任务的性能影响不大。论文为线性回归和线性分类证明了这一假设，并指出，如果毒化样本利用了良性数据分布未使用的方向，则生成的模型在功能上等同于排除了毒化样本的模型。论文还基于先前的统计后门学习工作，表明在所有其他情况下，对良性学习任务的影响仍然有限。最后，通过实际的基准数据集验证了理论结果。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;证明了在对训练数据了解有限的情况下，单个毒化样本足以对线性分类或线性回归模型进行后门攻击，且攻击错误率几乎为零。与以往工作不同的是，该结论经过了严格证明，并在真实世界的数据上得到了验证。&lt;/li&gt;&lt;li&gt;证明了如果良性数据分布的所有样本在某个方向上的投影幅度为零，那么当攻击者选择该方向作为其单个毒化样本的方向时，干净模型和投毒模型在功能上是等价的。&lt;/li&gt;&lt;li&gt;基于Wang等人的先前工作，针对分类问题进行了扩展，并将其推广到回归问题，证明了在所有其他情况下，毒化样本对良性学习任务的影响仍然是有限的。&lt;/li&gt;&lt;li&gt;通过在实际基准上评估来验证理论结果。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**理论证明：**  针对线性回归和线性分类，分别设计了单点投毒攻击方法，并通过数学推导证明了该攻击方法在一定条件下能够成功注入后门，且对良性学习任务的影响可控或无影响。&lt;/li&gt;&lt;li&gt;**梯度分析：** 通过分析损失函数的梯度，证明了只要毒化样本对梯度的影响足够大，就可以克服正则化项的约束，从而在模型中留下后门。&lt;/li&gt;&lt;li&gt;**功能等价性分析：** 通过数学方法证明了在特定条件下（例如，良性数据在特定方向上的投影幅度为零），投毒后的模型与未投毒的模型在功能上是等价的。&lt;/li&gt;&lt;li&gt;**统计风险分析：**  借鉴并扩展了前人工作，对投毒攻击对良性学习任务的影响进行了量化分析，并给出了统计风险的界限。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;本文成功证明了线性回归和线性分类的单点投毒假设。结果表明，即使在对其他数据点了解有限的情况下，此类模型也可以通过投毒单个数据点成功地进行攻击。文中给出的界限经过了正式证明，适用于真实世界的实例大小，并且也经过了实验验证。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05600v1</link>
      <guid isPermaLink="false">2508.05600v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:41:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 Kolmogorov-Arnold 网络 (KANs) 优化物联网威胁检测</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;本研究探讨了 Kolmogorov-Arnold 网络 (KANs) 在物联网 (IoT) 网络入侵检测中的潜力，作为传统机器学习模型的替代方案。物联网的快速增长带来了巨大的安全隐患，物联网网络成为网络攻击的主要目标。KANs 采用可学习的激活函数，优于传统的多层感知器 (MLP)，并且与诸如随机森林和 XGBoost 等最先进的模型相比，具有竞争力的准确性，同时为物联网网络中的入侵检测提供卓越的可解释性。研究结果表明，KANs 通过学习激活函数能够动态适应复杂的数据模式，在不断演变的物联网威胁环境中具有关键优势。该研究还通过特征选择和 KANs 的结合，旨在优化实时物联网环境中的检测性能和计算效率。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;将 KANs 应用于 CIC IoT 2023 数据集，展示了边缘处的可学习激活函数如何提高模型的准确性和可解释性。&lt;/li&gt;&lt;li&gt;评估 KANs 与传统模型（例如，随机森林、XGBoost）的性能，以证明其具有竞争力的性能和对物联网入侵检测的适用性。&lt;/li&gt;&lt;li&gt;通过符号公式生成展示 KANs 的可解释性，从而在安全关键型物联网系统中实现透明的决策制定。&lt;/li&gt;&lt;li&gt;研究结果表明，通过使用可学习的激活函数，KANs 可以比基于固定激活函数的模型更有效地捕获复杂的非线性关系。这种灵活性和适应性使 KANs 成为物联网安全应用的一个有前途的替代方案，在物联网中，网络流量可能表现出需要动态解释的各种行为。&lt;/li&gt;&lt;li&gt;这项工作有助于不断增长的物联网安全知识体系，展示了 KANs 在入侵检测任务中的有效性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**数据预处理：** 使用 Pandas、PyTorch 和 Scikit-Learn 等库加载数据集，并将数据集分成训练集 (67%) 和测试集 (33%)。&lt;/li&gt;&lt;li&gt;**模型评估：** 评估各种机器学习算法，包括 Logistic Regression, Random Forest, Decision Trees, K-Nearest Neighbors (KNN), Gradient Boosting, XGBoost, Naive Bayes, Multi-Layer Perceptron (MLP), 和 AdaBoost，并生成一个分类性能报告，包括精确率、召回率、F1 分数和整体准确率等指标。&lt;/li&gt;&lt;li&gt;**KAN 模型构建：** 定义网络拓扑结构，包括输入层、两个隐藏层（分别有 16 和 8 个神经元）和一个输出层（2 个神经元，代表“BenignTraffic”和“MaliciousTraffic”的分类），使用 MultiKAN 架构以允许特征之间的加性和乘性交互。使用 StandardScaler 对特征进行归一化，并根据其重要性对特征进行排序，保留前 N 个最相关的特征。&lt;/li&gt;&lt;li&gt;**训练过程：** 使用 Adam 优化器和 CrossEntropyLoss 训练 KAN 模型，在训练数据集上进行多次迭代，通过调整参数来最小化损失函数。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;该研究表明，Kolmogorov-Arnold 网络 (KANs) 在捕获物联网环境中的复杂非线性关系方面非常有效，明显优于传统的机器学习模型。这项研究强调了优化特征选择的关键重要性，这不仅通过减少变量的数量来提高模型性能，而且最大限度地减少了训练时间和计算开销，从而有助于在资源受限的物联网系统中实现实时应用。KANs 与可学习激活函数的集成代表了物联网安全框架领域的重大进步。这种集成提供了一个强大的解决方案，可以提高网络流量分类的准确性和可解释性，这对于保护敏感数据免受不断演变的 Cyber 威胁至关重要。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05591v1</link>
      <guid isPermaLink="false">2508.05591v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:29:10 GMT</pubDate>
    </item>
    <item>
      <title>使用多种负采样解决方案增强PyKEEN知识图嵌入模型</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文针对知识图嵌入(KGE)模型训练中负采样策略的局限性，提出了一种增强PyKEEN框架的模块化扩展。KGE模型依赖正负样本进行训练，但负样本通常需要人工生成，而现有的PyKEEN框架只支持基本的负采样策略。该论文的主要目标是为PyKEEN集成一系列先进的负采样器，包括静态和动态策略，以生成更有意义的负样本，同时保持与现有PyKEEN工作流程的兼容性。该扩展不仅提升了PyKEEN本身的功能，还促进了KGE方法及定制化的开发。论文通过全面的实验研究，验证了所开发扩展在不同嵌入方法性能上的影响，并为设计更有效的负采样策略提供了有用的见解。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**提出了一个模块化的PyKEEN扩展**，该扩展无缝集成了多种高级负采样技术，增强了KGE模型构建的灵活性。&lt;/li&gt;&lt;li&gt;**实现了五种新的负采样器**（Corrupt、Typed和Relational），这些采样器利用结构和语义标准进行实体选择，并在PyKEEN生态系统中提供高效且用户友好的解决方案。&lt;/li&gt;&lt;li&gt;**引入了动态负采样策略**，该策略利用预训练的辅助模型来指导信息量更大的负样本的选择，从而允许上下文感知的负样本生成。&lt;/li&gt;&lt;li&gt;**提供了一个定制的数据加载器**，可以加载外部语义元数据以及标准三元组集，从而可以轻松地将语义元数据与知识图三元组集成在一起。&lt;/li&gt;&lt;li&gt;**对负采样策略进行了全面的实证比较研究**，分析了它们对链接预测任务性能的影响，并为负采样器选择和定制提供了有价值的见解。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**静态负采样:** 基于固定的结构或语义信息预先计算负样本池。具体策略包括：
   - **随机采样:** 从整个实体集中随机选择实体。
   - **伯努利采样:** 根据关系连接实体的倾向（例如，一对多或多对一）调整破坏头实体或尾实体的概率。
   - **Corrupt采样:** 基于在正实例集中每个关系中作为头或尾出现的实体来定义可用候选负三元组的池。
   - **Typed采样:** 利用KG中存在的强类型关系和实体，例如DBpedia和YAGO。
   - **Relational采样:** 假设每个头-尾对仅参与一个关系。&lt;/li&gt;&lt;li&gt;**动态负采样:** 利用预训练的辅助模型来指导信息量更大的负样本的选择。具体策略包括：
   - **最近邻采样:** 使用辅助预训练模型生成实体的向量表示。当在头（尾）上进行破坏时，通过检索最接近头（尾）向量表示的 k 个实体来选择hard negatives。
   - **对抗采样:** 与之前的动态采样器采用完全相同的公式，但是使用模型在向量空间中的预测代替实体嵌入。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;该论文介绍并验证了PyKEEN框架的一个模块化扩展。该扩展旨在在使用KGE方法时提供广泛的标准负采样器实现覆盖，从而填补了更高级的已实现负采样器可用性的重要空白。论文特别提供了五种具有静态和动态破坏策略的负采样器的完全兼容实现。通过一系列实验证明了其实用性，展示了如何将扩展无缝集成到KGE的训练、评估和超参数优化管道中。此外，论文还提供了四个常用数据集的负样本池统计数据，突出了各种采样策略在不同条件下的操作约束和行为。通过遵守PyKEEN架构和设计标准，所提出的扩展支持可重现性、模块化和易于实验。通过减少开发和评估新采样策略的障碍，论文的目标是促进一个更加统一和高效的研究生态系统。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/ivandiliso/refactor-negative-sampler/&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05587v1</link>
      <guid isPermaLink="false">2508.05587v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:24:34 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型迭代学习治疗耐药性高血压的可计算表型</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文探讨了大型语言模型（LLM）在生成可解释的可计算表型（CPs）方面的潜力，特别是针对高血压及其亚型。研究提出了一种*合成、执行、调试、指导*（SEDI）策略，利用LLM生成和迭代改进CPs，并使用数据驱动的反馈进行优化。研究结果表明，LLM结合迭代学习能够生成具有良好可解释性和相当准确的程序，其性能接近最先进的机器学习方法，同时所需的训练样本数量显著减少。研究通过对高血压相关的三种表型进行实验，证明了LLM在自动生成CPs方面的有效性，并强调了其在临床决策支持中的潜在应用价值，尤其是在需要快速适应不同临床环境或随时间变化的场景下。此外，使用LLM生成CPs而非黑盒预测，增强了模型行为的可解释性。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一种新颖的*合成、执行、调试、指导*（SEDI）策略，用于迭代改进LLM生成的可计算表型（CPs）。&lt;/li&gt;&lt;li&gt;验证了LLM在生成高血压及其亚型（包括治疗耐药性高血压）的可解释CPs方面的能力。&lt;/li&gt;&lt;li&gt;证明了LLM生成的CPs在性能上可以与传统的机器学习方法相媲美，同时需要更少的训练数据。&lt;/li&gt;&lt;li&gt;强调了使用LLM生成CPs而非黑盒预测，可以增强模型的可解释性，并便于临床医生理解和信任。&lt;/li&gt;&lt;li&gt;提供了一个公开可用的SEDI框架，便于研究人员和临床医生开发其他疾病的CPs。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**LLM-Guided CP Generation:** 使用LLM生成Python代码形式的CPs，该代码基于对表型的自然语言描述和可用的EHR特征进行预测。&lt;/li&gt;&lt;li&gt;**Zero-shot Prompts:** LLM在不接收任何反馈的情况下生成CPs。&lt;/li&gt;&lt;li&gt;**SEDI Prompts:** 采用*合成、执行、调试、指导*（SEDI）循环，迭代地接收关于CPs在训练数据集上的性能反馈。如果CPs执行失败，LLM会收到包含错误回溯的消息。如果CPs执行成功，LLM会收到性能指标以及假阳性（FP）和假阴性（FN）的示例，并被指示改进其表型定义以提高程序的性能。&lt;/li&gt;&lt;li&gt;**Problem Definition Prompts:** 构建系统提示和用户提示，引导LLM生成符合特定表型描述的Python函数。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;最先进的LLM可以为高血压表型生成相当准确和简洁的CP，即使是简单提示。如果给出详细而集中的提示并配备数据驱动的迭代反馈（即，SEDI），则LLM生成的CP可以与使用监督ML训练的CP竞争。我们观察到，通常，利用chart-review示例的传统监督ML方法仍然优于LLM推导的CP，但生成的模型更大，并且需要访问更大的专家标记数据集。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/cavalab/htn-phenotyping-with-llms&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05581v1</link>
      <guid isPermaLink="false">2508.05581v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:15:17 GMT</pubDate>
    </item>
    <item>
      <title>Fairy ±i : the First 2-bit Complex LLM with All Parameters in {± 1, ±i}</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文提出了Fairy ±i，一种新颖的2比特复数LLM量化框架。该框架旨在通过提高全精度模型的精度上限，从而突破现有量化方法的性能瓶颈。Fairy ±i利用复数域的表示优势来提升全精度模型的准确性，并将权重映射到单位的四次方根{±1, ±i}，形成一种信息论上最优的2比特表示。这种量化方案的关键在于，每个量化后的权重都具有零实部或零虚部，从而实现仅使用加法和元素交换的无乘法推理。实验结果表明，Fairy ±i在PPL和下游任务方面均优于现有2比特量化方法的性能上限，同时保持严格的存储和计算效率。这项工作为在极低比特约束下构建高精度且实用的LLM开辟了一个新的方向。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一种新的低比特量化视角：通过提高全精度模型（精度上限）来提高量化模型的准确性。&lt;/li&gt;&lt;li&gt;设计了一种复数LLM架构，该架构利用复数域的表示优势，而无需增加参数存储。&lt;/li&gt;&lt;li&gt;设计了一种2比特量化方案，将复数权重映射到单位的四次方根{±1, ±i}，充分利用比特容量，同时保留对称性和稀疏性等关键属性。&lt;/li&gt;&lt;li&gt;实验结果表明，在PPL和下游理解任务方面，我们的量化模型优于现有2比特量化方法的性能上限。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**复数Transformer架构扩展：** 将标准的Transformer架构扩展到复数域，所有模型参数和中间表示都采用复数形式。&lt;/li&gt;&lt;li&gt;**双通道投影嵌入层：** 使用两个并行的嵌入层，分别生成实部和虚部，从而将离散的token空间映射到连续的复数表示空间。&lt;/li&gt;&lt;li&gt;**高效的复数自注意力机制：** 采用Hermitian内积的实部作为注意力得分，确保所有四个组成部分（实部和虚部）都参与计算，并使用优化的实值FlashAttention内核。&lt;/li&gt;&lt;li&gt;**复数前馈网络：** 使用平方ReLU（ReLU^2）作为非线性激活函数，在保持非线性的同时，将操作限制在实数域。&lt;/li&gt;&lt;li&gt;**PhaseQuant量化方案：** 将每个复数权重映射到单位的四次方根{±1, ±i}，基于其在复平面中的相位。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;Fairy ±i是第一个参数全部位于{±1, ±i}的2比特复数LLM。通过将复数表示集成到Transformer中，并通过提出的PhaseQuant将权重量化为单位的四次方根{±1, ±i}，Fairy ±i充分利用了2比特空间，同时保留了对称性、效率和硬件兼容性。实验结果表明，在等效模型大小下，Fairy ±i在困惑度和任务准确度方面均优于所有现有量化方法的精度上限。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/PKULab1806/Fairy-plus-minus-i&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05571v1</link>
      <guid isPermaLink="false">2508.05571v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:02:23 GMT</pubDate>
    </item>
    <item>
      <title>具有 Richardson-Romberg 外推的马尔可夫 LSA 的高阶误差界限</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;本文研究了马尔可夫噪声下具有 Polyak-Ruppert (PR) 平均的线性随机逼近 (LSA) 算法的偏差和高阶误差界限。 该研究重点关注具有恒定步长 α 的算法版本，并通过线性化技术提出了一种新颖的偏差分解方法。 分析了偏差的结构，表明主导项是 α 的线性函数，不能通过 PR 平均消除。 为了解决这个问题，应用了 Richardson-Romberg (RR) 外推程序，有效地消除了主导偏差项。推导了 RR 迭代的高阶矩界限，并表明主导误差项与 vanilla 平均 LSA 迭代的渐近最优协方差矩阵对齐。这项研究为马尔科夫 LSA 提供了更精确的误差分析，并提出了一种有效的偏差消除方法。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一种量化 θn(α) 渐近偏差的新技术。这种方法考虑了联合马尔可夫链 {(θk(α), Zk+1)}k∈N 的极限分布 Πα，并分析了偏差 Πα(θ0) − θ*。然后，应用 Aguech、Moulines 和 Priouret (2000) 中的 θk(α) 线性化方法。这允许我们研究分量的极限分布，这些分量的平均值显示为 α 的幂的阶数。&lt;/li&gt;&lt;li&gt;建立了 Richardson-Romberg 方法的高阶矩误差界限，其中主导项与渐近最优协方差 Σ∞ 对齐。分析了它对步数 n、步长 α 和混合时间 tmix 的依赖性。&lt;/li&gt;&lt;li&gt;提出的偏差展开的线性项中的系数与 Lauand 和 Meyn (2023a, Theorem 2.5) 中导出的表示相匹配，但该工作没有分析具有减少偏差的 MSE。&lt;/li&gt;&lt;li&gt;结果表明，主要误差项与 Polyak-Ruppert 迭代的局部渐近极小极大最优值对齐，并与 CLT 协方差矩阵 Σ∞ (参见 (7)) 对齐。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;线性化方法：通过线性化技术对偏差进行分解，分析偏差的结构，并识别出主导项。&lt;/li&gt;&lt;li&gt;Richardson-Romberg (RR) 外推：应用 RR 外推程序来消除主导偏差项，从而提高算法的精度。&lt;/li&gt;&lt;li&gt;Wasserstein 距离：证明了所提出的马尔可夫链在Wasserstein距离度量下的收敛性。&lt;/li&gt;&lt;li&gt;Rosenthal 不等式：使用马尔可夫链的 Rosenthal 不等式来控制误差项的矩。&lt;/li&gt;&lt;li&gt;Lyapunov 方程：分析线性SA迭代中随机矩阵的稳定性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;这项研究调查了马尔可夫线性随机逼近设置中 Richardson-Romberg 外推法的高阶误差界限。 通过应用偏差表征的新技术，研究人员能够获得与渐近最优协方差矩阵 Σ∞ 对齐的主导项。对于进一步的工作，研究人员考虑将获得的结果推广到非线性马尔可夫 SA 和具有状态相关噪声的 SA。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05570v1</link>
      <guid isPermaLink="false">2508.05570v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:02:11 GMT</pubDate>
    </item>
    <item>
      <title>X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文提出了X-VFL，一个新的垂直联邦学习（VFL）框架，旨在解决VFL中两个关键挑战：1）需要所有客户端数据样本完全对齐（不允许缺失特征）；2）需要所有客户端参与的联合协同推理/预测（不支持单个客户端本地独立推理）。X-VFL通过引入两个新模块来应对这些挑战：交叉补全（XCom）和决策子空间对齐（DS-Align）。XCom利用其他客户端的信息补全非对齐数据样本的缺失特征。DS-Align将本地特征与补全后的全局特征在决策子空间内对齐，从而支持每个客户端本地独立推理。论文还提供了不同训练算法的收敛性定理，证明了SGD类型算法的O(1/√T)收敛速度和PAGE类型算法的O(1/T)收敛速度。在真实数据集上的大量实验表明，X-VFL显著优于现有方法，例如在CIFAR-10图像数据集上准确率提高了15%，在MIMIC-III医疗数据集上提高了43%。这些结果验证了X-VFL的实际有效性和优越性，特别是在涉及部分缺失特征和本地独立推理的场景中。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了X-VFL，一种新的VFL框架，旨在处理具有（部分）缺失特征的非对齐数据样本，并支持每个客户端上新数据样本的本地独立推理。X-VFL引入了两个关键模块：交叉补全（XCom）和决策子空间对齐（DS-Align），显著增强了VFL处理更复杂和实际场景的能力。&lt;/li&gt;&lt;li&gt;据我们所知，我们首次在VFL中引入了具有部分缺失特征的实际设置，其中客户端可以保留一些本地特征，而不是完全缺少非对齐数据样本的所有本地特征。这更贴近实际应用场景。&lt;/li&gt;&lt;li&gt;为用于训练X-VFL的算法提供了理论收敛性定理，表明SGD类型算法的收敛速度为O(1/√T)，PAGE类型算法的收敛速度为O(1/T)，其中T表示训练更新步骤的数量。&lt;/li&gt;&lt;li&gt;在真实数据集上进行了大量实验，证明了X-VFL显著优于现有的VFL方法，例如，在CIFAR-10上实现了15%的准确率提升，在医疗数据集MIMIC-III上实现了43%的准确率提升。实验验证了X-VFL在VFL中的实际有效性和优越性，尤其是在涉及部分缺失特征和本地独立推理的实际场景中。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**交叉补全 (XCom):** 设计用于利用来自其他客户端的信息来完成/重建非对齐数据样本的缺失特征。该模块通过建立不同客户端贡献的局部特征之间的互补依赖关系来工作。通过这些关系，本地客户端可以通过 XCom 模块完成其缺失的特征，从而有效地增加可用于训练和推理的数据量。&lt;/li&gt;&lt;li&gt;**决策子空间对齐 (DS-Align):** 旨在将所有客户端的特征在决策子空间内对齐，以支持每个客户端的本地独立推理，同时保持与涉及所有客户端的协作推理相当的性能。它包含两个损失分量：
    *   ***L***DSAlign 1: 确保基于重建嵌入的推理与现有嵌入紧密对齐，从而增强特征补全的准确性和可靠性。
    *   ***L***DSAlign 2: 在决策子空间中对齐来自所有客户端的本地单独特征和联合平均特征，从而实现更准确的独立推理。&lt;/li&gt;&lt;li&gt;**整体损失函数:** 整合 XCom 和 DS-Align，将分类决策损失、特征补全一致性损失和独立推理增强损失结合在一起，形成一个统一的优化目标。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文提出了 X-VFL，一种新颖的 VFL 框架，它通过有效处理具有部分缺失特征的数据集并在每个客户端启用本地独立推理来解决传统 VFL 的关键挑战。 具体来说，X-VFL 引入了两个关键模块：交叉补全 (XCom) 和决策子空间对齐 (DS-Align)。 XCom 旨在通过利用跨客户端信息来完成非对齐数据样本的缺失特征，从而有效地增加可用于训练和推理的数据量。 DS-Align 将本地特征与所有客户端中已完成和全局特征的决策子空间对齐，使每个客户端即使在缺少特征的情况下也能执行本地独立推理。 此外，还为用于训练 X-VFL 的不同算法建立了理论收敛定理。 真实世界数据集上的大量实验表明，X-VFL 显着优于现有的 VFL 方法，验证了其在解决关键挑战（例如缺失特征、本地独立推理和数据不平衡）方面的实际有效性和优越性。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05568v1</link>
      <guid isPermaLink="false">2508.05568v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:00:47 GMT</pubDate>
    </item>
    <item>
      <title>L1正则化函数支持向量机</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文研究了函数型数据分析中多元函数协变量的二元分类问题，提出了L1正则化函数支持向量机（L1-fSVM）。该方法旨在解决当每个个体有多个函数协变量时如何进行有效分类并识别相关特征。论文开发了一种算法来拟合该分类器，通过施加L1惩罚项，能够识别与二元响应相关的函数协变量。该算法通过B样条回归估计每个轨迹的系数函数，将函数协变量的影响表示为在系数函数上的投影，然后在这些投影分数上构建SVM分类器。仿真和真实数据应用表明，该分类器在预测和特征选择方面都表现良好，性能优于现有的函数分类器。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了L1正则化函数支持向量机（L1-fSVM），用于处理多元函数协变量的二元分类问题。&lt;/li&gt;&lt;li&gt;开发了一种迭代算法来拟合L1-fSVM分类器，该算法可以同时更新系数函数和SVM中的向量。&lt;/li&gt;&lt;li&gt;通过施加L1惩罚项，实现了函数协变量的特征选择，能够识别与二元响应相关的函数协变量。&lt;/li&gt;&lt;li&gt;通过仿真实验和真实数据分析，验证了L1-fSVM在预测精度和特征选择方面的有效性。&lt;/li&gt;&lt;li&gt;该方法使用B样条回归估计每个轨迹的系数函数，并将函数协变量的影响表示为在系数函数上的投影，从而避免了直接惩罚原始函数。&lt;/li&gt;&lt;li&gt;为了解决α_j和β_j的不可识别性问题，论文添加了约束条件β_j(0)=1。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**L1正则化:** 在SVM的目标函数中引入L1范数惩罚项，以实现特征选择，即通过将不相关函数协变量的系数压缩为零，从而选择出对分类有用的函数。&lt;/li&gt;&lt;li&gt;**B样条基函数:** 使用B样条基函数来近似系数函数，将无限维的函数估计问题转化为有限维的参数估计问题，简化计算。&lt;/li&gt;&lt;li&gt;**坐标下降算法:** 开发了一种坐标下降算法来迭代更新模型参数，包括SVM的参数以及B样条基函数的系数。该算法交替优化不同的参数，直到收敛。&lt;/li&gt;&lt;li&gt;**平方铰链损失 (Squared Hinge Loss):** 为了简化优化过程，使用了平方铰链损失函数代替标准的铰链损失函数。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文提出了L1正则化函数支持向量机（L1-fSVM）用于函数型数据分类，并开发了一种坐标下降算法进行模型估计和特征选择。实验结果表明，L1-fSVM在预测精度和特征选择方面具有良好的性能，可以应用于实际问题，如脑电信号分析。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05567v1</link>
      <guid isPermaLink="false">2508.05567v1</guid>
      <pubDate>Thu, 07 Aug 2025 17:00:29 GMT</pubDate>
    </item>
    <item>
      <title>Expressive and Trainable Pulse-based Quantum Machine Learning Models的设计</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文深入研究了基于脉冲的量子机器学习（QML）模型的设计，旨在实现模型既具有高表达性又易于训练。论文首先指出，虽然基于动态对称性的脉冲模型可以有效训练，但如果设计不当，其表达性可能会受到影响。为了解决这一问题，论文提出了一个必要条件，涉及系统的初始状态、测量算符以及潜在的动态对称李代数，并通过数值模拟验证了这一条件。研究的核心在于利用Dyson级数展开分析脉冲模型的表达性，并在此基础上提出了一个李代数工具，用于设计具有动态对称性的、表达性强的脉冲模型。论文的发现为设计实用的脉冲QML模型提供了一个框架，能够在表达性和可训练性之间取得平衡，对于推动量子机器学习的实际应用具有重要意义。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了基于Dyson级数展开的脉冲QML模型表达性分析方法，不同于传统的傅里叶分析。&lt;/li&gt;&lt;li&gt;推导出了脉冲QML模型具有表达性的必要条件，涉及初始状态、测量算符和动态对称李代数。&lt;/li&gt;&lt;li&gt;提供了一个李代数工具，用于设计具有动态对称性的、表达性强的脉冲模型。&lt;/li&gt;&lt;li&gt;通过数值模拟验证了理论分析，展示了在动态对称性下脉冲模型的表达性和可训练性。&lt;/li&gt;&lt;li&gt;建立了一个综合框架，用于设计实用的、既具有表达性又易于训练的脉冲QML模型。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Dyson级数展开（Fliess级数）：将脉冲模型的输出函数展开为关于输入变量的多项式级数，从而分析模型的非线性特性。&lt;/li&gt;&lt;li&gt;李代数理论：利用李代数工具分析模型的动态对称性，并推导出模型具有表达性的必要条件。&lt;/li&gt;&lt;li&gt;数值模拟：通过数值实验验证理论分析，并评估不同动态对称性下脉冲模型的表达性和可训练性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文提出了一个综合框架，用于设计实用的、既具有表达性又易于训练的脉冲QML模型。该框架结合了Dyson多项式级数展开和现有的关于量子系统中动态对称性所引起的贫瘠高原现象的李代数理论。理论分析和数值模拟表明，利用动态对称性可以构建适用于NISQ设备的、硬件效率高的、表达性强且易于训练的脉冲模型。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05559v1</link>
      <guid isPermaLink="false">2508.05559v1</guid>
      <pubDate>Thu, 07 Aug 2025 16:40:09 GMT</pubDate>
    </item>
    <item>
      <title>MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文提出了一种名为MV-Debate的多视角Agent辩论框架，用于在社交媒体中检测多模态有害内容。由于社交媒体中图像、文本和其他信号相互作用，导致讽刺、仇恨言论或虚假信息等有害意图难以识别，该框架旨在解决这一挑战。MV-Debate集成了四个互补的辩论Agent，分别从表面分析、深度推理、模态对比和社会背景四个角度分析内容。通过迭代辩论和反思，Agent们根据∆-gain准则改进回应，确保准确性和效率。在三个基准数据集上的实验表明，MV-Debate显著优于强大的单模型和现有的多Agent辩论基线模型。这项工作突出了多Agent辩论在推进高可靠性的社交意图检测方面的潜力，对于在线安全领域至关重要。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了MV-Debate，一个多Agent辩论框架，引导Agent采用多样化的推理视角来检测社交媒体中的多模态有害内容。&lt;/li&gt;&lt;li&gt;设计了四个特定视角的辩论Agent，并结合动态反射门控机制来提高性能。&lt;/li&gt;&lt;li&gt;通过多个多模态有害内容基准数据集，验证了所提出方法的有效性。&lt;/li&gt;&lt;li&gt;利用Top-k ∆-Reflection Gating 机制，权衡辩论质量与效率，实现性能提升。&lt;/li&gt;&lt;li&gt;通过消融实验验证了不同辩论Agent和反射机制的有效性，并分析了模型大小和辩论轮数对性能的影响。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**问题形式化:** 给定一个由文本 *x*[text] 和相关视觉内容 *x*[img] 组成的多模态社交媒体帖子，目标是预测其潜在的 *社会意图* 标签 *y ∈Y*，其中 *Y* = {*Yes, No*} 表示是否存在讽刺、仇恨内容或虚假信息。目标是最大化预测准确率。&lt;/li&gt;&lt;li&gt;**系统架构:** MV-Debate系统由四种类型的专业辩论Agent以及三个附加的控制Agent组成。 每个辩论Agent都需要用相应的指定视角回答问题，而控制Agent旨在对推理路径进行评分和反思，并最终做出最终预测。&lt;/li&gt;&lt;li&gt;**专业辩论Agent:**
 - **表面分析师Agent (SA):** 此Agent专门关注显式的文本和视觉线索以进行检测。
 - **深度推理师Agent (DR):** 此Agent揭示了隐含的含义和隐藏的意图以进行检测。
 - **模态对比Agent (MC):** 此Agent评估文本和视觉模态之间的对齐或矛盾以进行检测。
 - **社会背景主义者Agent (SC):** 此Agent利用外部文化和社会背景知识进行检测。&lt;/li&gt;&lt;li&gt;**法官Agent:** 此Agent评估辩论Agent生成的论点。 它根据逻辑连贯性、一致性和合理性分配分数，其中较好的响应会获得更高的分数。&lt;/li&gt;&lt;li&gt;**反思Agent:** 此Agent生成结构化的反馈，突出显示逻辑缺陷和改进建议。&lt;/li&gt;&lt;li&gt;**总结Agent:** 此Agent汇总辩论历史并提供最终预测。&lt;/li&gt;&lt;li&gt;**多视角辩论:** 在辩论的第一轮，给定一个图像-文本对，每个专业辩论Agent都会生成其响应 *r* *i,* 1，其中下标“1”表示第一轮，由相应的任务视角提示 *p* *i* 指导：

*r* *i,* 1 = *M* *i* ( *x* [*text*] *, x* [*img*] *|h* *i* *, p* *i* ) *, i* = 1 *,* 2 *, ...,* 4 *.* (1)

其中 *h* *i* 是第 *i* 个Agent的历史消息，并初始化为空列表。 *r* *i,* 1 输出为结构化的JSON对象，其中包含二进制决策（“YES”或“NO”）和简要推理。 它们特定角色的提示严格执行不同的分析视角，从而确保整个推理过程中的多样性和互补性。
因此，法官Agent会收集所有辩论Agent的解决过程和对问题的回答，并为每个Agent的响应分配一个分数 *s* *i,* 1，其中较好的响应会获得更高的分数。&lt;/li&gt;&lt;li&gt;**Top-** *k* ∆ **-反思门控:** 由于这些Agent的初始响应可能包含不正确的信息，因此，按照以前的工作，我们引入了一种反思机制来自我改进响应质量。

为了减少计算开销，我们引入了一种Top-*k* ∆-反思门控策略。 在每一轮中，反思Agent都会收到所有辩论Agent的响应，并检查每个Agent的推理过程。 然后，它将指出推理错误并提供修改建议。 接下来，选择由法官Agent评分的最高的*k*个响应。 然后，每个选定的原始辩论Agent都会生成一个新的响应ˆ*r* *i,* 1，其中包含查询实例、初始响应和修改建议。 之后，法官Agent将重新评分新的响应，表示为 *s* ˆ*i,* 1。

然后，我们通过比较具有和不具有反思反馈的Agent的分数来估计反思的预期效用。 正式地，反思增益∆*i,* 1的计算方式如下：

∆ *i,* 1 = [1]

*k*

 (ˆ*s* *i,* 1 *−* *s* *i,* 1 ) (2)

*i∈* Top *k*

仅当∆*i,* 1超过预定义的阈值*τ*时，才触发反思，即∆*i,* 1 *≥* *τ*。 否则，我们将使用原始响应。
在我们的实验中，我们凭经验设置*k* = 2和*τ* = 0 *.* 1以实现效率提升。 与反映所有辩论Agent相比，它可以减少超过60％的冗余反思调用，同时保持或提高准确性，与无条件反思基线相比。&lt;/li&gt;&lt;li&gt;**历史更新:** 反思之后，如果未采用较新的响应，则法官Agent将收集得分最高的响应，并将其附加到历史记录中。 否则，我们还将推理错误和修改建议*ϕ* 1附加到历史记录中。&lt;/li&gt;&lt;li&gt;**辩论循环:** 从第二轮开始，将上一轮得分最高的响应（包括推理过程和答案）纳入每个Agent的历史记录 *h* *i* 中。 在接下来的回合中，每个Agent都利用这些推理轨迹和解决方案作为附加输入，有选择地从不同的角度提取有用的信息，以完善自己的答案。 此迭代辩论过程遵循与上述相同的过程，直到达到最大回合数*N*或Agent收敛到相同的判断为止。 在我们的实验中，我们将*N*设置为3。最后，在辩论结束时，总结Agent将汇总辩论历史并提供最终预测ˆ*y*。 算法1总结了迭代辩论和反思过程。&lt;/li&gt;&lt;li&gt;建议的反射门控多视图辩论框架（MV-Debate）为多模式有害内容检测提供了三个主要优势。首先，将专门的角色分配给辩论代理，从而可以采用不同的推理视角。与单视图提示不同，此设计结合了表面水平、深度语义、跨模态和社文化分析，从而降低了错过隐式或特定于上下文的有害线索的风险。&lt;/li&gt;&lt;li&gt;其次，Top- *k* ∆ -反射门控机制可在保持效率的同时提高可靠性。 通过仅在期望有显着改进时自适应地触发反射，该框架避免了冗余计算，但实现了与无条件反射相当或更好的准确性。 这与可伸缩性和成本效益至关重要的实际部署相关。&lt;/li&gt;&lt;li&gt;第三，迭代辩论循环鼓励累积推理。 代理通过将高质量的响应和结构化反馈集成到他们的历史记录中来完善其预测，从而促进了代理间多样性和代理内改进，同时减少了重复错误。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;论文提出了一种名为MV-Debate的多视角辩论框架，用于检测社交媒体上的多模态有害内容。通过协调四个具有互补推理策略和动态反射门控机制的特定视角Agent，MV-Debate有效地整合了跨模态证据和上下文线索，以识别复杂的社会意图，例如讽刺、仇恨言论和虚假信息。在多个基准测试中进行的大量实验证实了其与强大的基线相比具有卓越的准确性、效率和可解释性。除了性能提升之外，MV-Debate还生成了透明的辩论记录，从而支持模型调试、审计和用户信任。展望未来，我们的框架为将多Agent辩论方法扩展到更广泛的安全关键型多模态推理任务奠定了基础。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05557v1</link>
      <guid isPermaLink="false">2508.05557v1</guid>
      <pubDate>Thu, 07 Aug 2025 16:38:25 GMT</pubDate>
    </item>
    <item>
      <title>Adapting Vision-Language Models Without Labels: A Comprehensive Survey</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文全面调研了视觉语言模型 (VLMs) 的无监督自适应方法。VLMs 在各种任务中表现出卓越的泛化能力，但在特定下游场景中直接应用时，性能通常欠佳。为了提高 VLM 的实用性，同时保持数据效率，最近的研究越来越关注不依赖于标记数据的无监督自适应方法。该论文针对无监督 VLM 自适应领域，提出了一个统一的、面向任务的综述。论文根据未标记视觉数据的可用性和性质，将现有方法分为四种关键范式：无数据迁移 (Data-Free Transfer)、无监督域迁移 (Unsupervised Domain Transfer)、情景测试时自适应 (Episodic Test-Time Adaptation) 和在线测试时自适应 (Online Test-Time Adaptation)。论文分析了与每种范式相关的核心方法和自适应策略，旨在系统地理解该领域，并回顾了跨各种应用的代表性基准，突出了开放的挑战和有希望的未来研究方向。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一个新颖的分类体系，该体系基于无标签视觉数据的可用性对现有的 VLM 无监督自适应方法进行分类。这一定义了四个不同的适应范式：无数据迁移、无监督域迁移、情景测试时适应和在线测试时适应。&lt;/li&gt;&lt;li&gt;对每种范式下的核心方法和自适应策略进行了详细分析，并提供了系统性的理解。&lt;/li&gt;&lt;li&gt;回顾了各种应用中的代表性基准，并对实际应用和真实世界的效用进行了广泛的展望。&lt;/li&gt;&lt;li&gt;总结了该领域的新兴趋势，并确定了可以激发未来工作的关键科学问题。&lt;/li&gt;&lt;li&gt;提供了一个积极维护的相关文献库，供研究人员参考学习。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Data-Free Transfer: 通过文本增强、图像利用和网络修改等策略，在不使用任何下游任务的视觉数据的情况下，适应预训练模型。文本增强方法利用 LLM 生成更丰富的文本描述。 图像利用方法检索相关图像或者使用生成模型合成图像，引入视觉信号。 网络修改方法主要修改 VLM 的网络结构，增强其对下游任务的适用性。&lt;/li&gt;&lt;li&gt;Unsupervised Domain Transfer: 利用下游任务中大量的未标记数据来更好地掌握数据分布，从而提高性能。 自训练方法通过伪标签反复优化模型。 熵优化方法鼓励模型对未标记数据做出自信的预测。 外部资源利用方法纳入检索的图像增强、多模态大型语言模型以及来自强大 VLM 或视觉模型的知识蒸馏等资源。&lt;/li&gt;&lt;li&gt;Episodic Test-Time Adaptation: 在推理时使用单个批次的未标记测试数据来调整预先训练的 VLM。 熵最小化方法调整模型的参数以降低熵，从而使其输出预测更加自信。 反馈信号策略探索利用来自扩散模型或类 CLIP 模型的反馈信号来进行 TTA。 分布对齐方法将测试样本分布与已知的源特征对齐或改进表示以提高一致性。&lt;/li&gt;&lt;li&gt;Online Test-Time Adaptation: 专为流数据场景设计，其中未标记数据以小批量形式顺序到达。 伪标记方法将类标签分配给未标记的测试样本，并优化预测和伪标签之间的交叉熵损失。 内存机制方法利用动态或静态内存结构来存储和检索测试样本中的特征表示和伪标签。 分布建模方法使用高斯估计对视觉或多模态特征的分布进行建模，以在推理期间优化预测。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;本文全面且有条理地概述了快速发展的无监督视觉语言模型自适应领域。该论文引入了一个新的分类法，该分类法根据未标记视觉数据的可用性对方法进行分类，这是实际部署的关键因素。通过将该领域划分为四个不同的设置，即无数据迁移、无监督域迁移、情景测试时自适应和在线测试时自适应，论文为理解每个场景中固有的独特挑战和假设提供了一个系统的框架。&lt;/p&gt;&lt;h3&gt;GitHub链接&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;https://github.com/tim-learn/Awesome-LabelFree-VLMs&lt;/li&gt;&lt;/ul&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05547v1</link>
      <guid isPermaLink="false">2508.05547v1</guid>
      <pubDate>Thu, 07 Aug 2025 16:27:37 GMT</pubDate>
    </item>
    <item>
      <title>Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;这篇论文提出了一种在黑盒环境下进行多项选择题（MCQA）回答的不确定性量化方法。大型语言模型（LLMs）在MCQA任务中表现出了显著的进步，但其固有的不可靠性，如幻觉和过度自信，限制了它们在高风险领域的应用。为了解决这个问题，研究人员利用共形预测（CP），提出了一种基于频率的不确定性量化方法，以确保可证明的覆盖保证。该方法包括对每个输入的模型输出分布进行多次独立采样，并将最频繁的样本作为参考来计算预测熵（PE）。通过在六个LLMs和四个数据集（MedMCQA、MedQA、MMLU、MMLU-Pro）上进行的实验评估表明，基于频率的PE在区分正确和不正确的预测方面优于基于logit的PE，以AUROC作为衡量标准。此外，该方法有效地控制了用户指定风险水平下的经验未覆盖率。这项工作为MCQA中可靠的不确定性量化提供了一个无分布、模型无关的框架，具有保证的覆盖率，从而增强了LLMs在实际应用中的可信度。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;提出了一种基于频率的预测熵（PE）方法，用于在黑盒设置下量化LLMs在MCQA任务中的不确定性。&lt;/li&gt;&lt;li&gt;利用共形预测（CP）框架，构建了具有可证明覆盖保证的预测集，增强了模型预测的可靠性。&lt;/li&gt;&lt;li&gt;实验结果表明，在多个数据集和模型上，基于频率的PE在区分正确和错误预测方面优于基于logit的PE。&lt;/li&gt;&lt;li&gt;验证了在黑盒LLMs中，采样频率可以作为logit概率的可行替代方案，为不确定性量化提供了一种有效手段。&lt;/li&gt;&lt;li&gt;提供了一个无分布、模型无关的框架，用于在MCQA中进行可靠的不确定性量化，并具有保证的覆盖率。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;**多次独立采样：** 对每个输入问题，独立采样M次模型输出分布，获得候选答案集E。&lt;/li&gt;&lt;li&gt;**频率计算：** 计算候选答案集中每个答案的频率，选择频率最高的答案作为参考。&lt;/li&gt;&lt;li&gt;**频率-基于预测熵（PE）计算：** 基于采样得到的经验频率分布，计算PE，公式为：PE_freq = -∑ P̂(a) * log(P̂(a))，其中P̂(a)是候选答案a的频率。&lt;/li&gt;&lt;li&gt;**共形预测（CP）：** 利用CP框架，根据非一致性得分构建预测集，确保预测集包含正确答案的概率不低于预先设定的置信水平1-α。&lt;/li&gt;&lt;li&gt;**非一致性得分计算：** 对于校准集中的每个样本，非一致性得分定义为：s_i = 1 - F(x_i)y*_i，其中F(x_i)y*_i是模型对于真实标签y*_i的输出得分。&lt;/li&gt;&lt;li&gt;**分位数计算：**  计算校准集中非一致性得分的分位数q̂，用于构建预测集。&lt;/li&gt;&lt;li&gt;**预测集构建：** 对于测试样本x_{n+1}，构建预测集C(x_{n+1}) = {y | s_{n+1}(x_{n+1}, y) ≤ q̂}。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;本研究提出了一种基于频率的预测熵（PE）方法，用于在黑盒环境下量化LLMs在MCQA任务中的不确定性。该方法通过对模型输出分布进行多次独立采样，并将最频繁的样本作为参考来量化不确定性。结合CP框架，构建了具有可证明覆盖保证的预测集。实验结果表明，该方法在多个数据集和模型上表现良好，并验证了采样频率可以作为logit概率的可行替代方案，为黑盒LLMs的不确定性量化提供了一种有效手段。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05544v1</link>
      <guid isPermaLink="false">2508.05544v1</guid>
      <pubDate>Thu, 07 Aug 2025 16:22:49 GMT</pubDate>
    </item>
    <item>
      <title>Tractable Sharpness-Aware Learning of Probabilistic Circuits</title>
      <description>&lt;![CDATA[&lt;h3&gt;摘要&lt;/h3&gt;&lt;p&gt;该论文研究了概率电路（Probabilistic Circuits，PCs）的过拟合问题，并从对数似然景观的角度分析了PCs的过拟合现象，指出其通常是由于收敛到泛化能力差的尖锐最优解所致。受神经网络中锐度感知最小化（sharpness aware minimization）的启发，该论文提出了一种基于Hessian矩阵的正则化方法来训练PCs。该方法的一个关键贡献是，论文证明了PCs的对数似然Hessian矩阵的迹可以高效计算，这与深度神经网络中通常难以处理的情况形成对比。通过最小化Hessian矩阵的迹，论文提出了一种基于梯度范数的正则化方法，该方法可以为EM算法产生简单的闭式参数更新，并且可以无缝地与基于梯度的学习方法集成。在合成和真实数据集上的实验表明，该方法能够引导PCs找到更平坦的最小值，从而提高泛化性能。总之，该论文为概率电路的学习提供了一种新颖且高效的锐度感知正则化方法。&lt;/p&gt;&lt;h3&gt;主要贡献&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;推导了树状结构PC对数似然精确完整Hessian矩阵的闭式表达式，并证明了它可以被有效地计算。&lt;/li&gt;&lt;li&gt;对于一般（DAG结构）PC，论文证明虽然完整的Hessian可能是难以处理的，但它的迹仍然可以在时间和参数及数据集大小上呈线性关系地精确计算，从而为大规模PC提供了第一个实际的曲率度量。&lt;/li&gt;&lt;li&gt;提出了一种新颖的锐度感知正则化方法，用于学习PCs，该正则化方法来源于Hessian矩阵的迹。&lt;/li&gt;&lt;li&gt;表明虽然通过EM直接最小化Hessian矩阵的迹会导致一个三次更新方程，但可以把这个目标重新表达成一个等价的梯度范数最小化问题，从而产生具有闭式参数更新的二次方程。&lt;/li&gt;&lt;li&gt;在多个合成和真实世界的数据集上进行了详尽的实验，表明正则化器强制收敛到更平坦的最优值，并有助于减少过拟合，尤其是在有限的数据设置中。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;技术方法&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;推导了树结构概率电路(TS-PC)的对数似然Hessian矩阵的闭式解&lt;/li&gt;&lt;li&gt;证明了一般有向无环图(DAG)结构的概率电路(PC)的Hessian矩阵的迹可以在线性时间内计算&lt;/li&gt;&lt;li&gt;提出了一种新颖的锐度感知正则化方法，通过最小化Hessian矩阵的迹来引导PC的学习&lt;/li&gt;&lt;li&gt;将Hessian矩阵的迹作为正则项加入到EM算法的M步，约束每个和节点的平方梯度&lt;/li&gt;&lt;li&gt;设计了一种等价的梯度范数表达形式，可将三次参数更新方程简化为具有闭式解的二次方程&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;主要结论&lt;/h3&gt;&lt;p&gt;该论文提出了一种新的研究方向，通过对数似然表面几何的视角来研究PC的训练。论文推导了树状结构PC中对数似然精确完整Hessian的闭式表达式，并证明了其可处理性。对于一般DAG结构的PC，论文表明，虽然完整的Hessian可能是难以处理的，但其迹仍然可以精确有效地计算——为训练大型PC提供了第一个可扩展的曲率度量。在此基础上，论文设计了一种新颖的正则化器，其等效梯度范数公式产生了闭式二次更新，从而实现了高效优化。实验证实，该方法将训练引导到更平坦的最小值并减少了过拟合，尤其是在低数据情况下。总的来说，该论文的工作为研究PC开辟了一个有希望的新方向。&lt;/p&gt;]]&gt;</description>
      <link>https://arxiv.org/pdf/2508.05537v1</link>
      <guid isPermaLink="false">2508.05537v1</guid>
      <pubDate>Thu, 07 Aug 2025 16:13:24 GMT</pubDate>
    </item>
  </channel>
</rss>
