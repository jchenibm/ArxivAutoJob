## **Shuffle-R1: Efficient RL framework for Multimodal Large Language Models** **via Data-centric Dynamic Shuffle**
### Linghao Zhu [1] [∗], Yiran Guan [1] [∗], Dingkang Liang [1] [∗], Jianzhong Ju [2], Zhenbo Luo [2], Bin Qin [2], Jian Luan [2], Yuliang Liu [1], Xiang Bai [1][�]

1 Huazhong University of Science and Technology
2 MiLM Plus, Xiaomi Inc.
*{* lh zhu, yiranguan, dkliang, ylliu, xbai *}* @hust.edu.cn
*{* jujianzhong, luozhenbo, qinbin, luanjian *}* @xiaomi.com


**Abstract**


Reinforcement learning (RL) has emerged as an effective
post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near
zero, and Rollout Silencing, where the proportion of rollouts
contributing non-zero gradients diminishes over time. These
issues lead to suboptimal gradient updates and hinder longterm learning efficiency. To address these issues, we propose
**Shuffle-R1**, a simple yet principled framework that improves
RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal
quality, and (2) Advantage-based Batch Shuffle, which increases exposure of valuable rollouts through strategic batch
reshuffling. Experiments across multiple reasoning benchmarks demonstrate that our framework consistently outperforms strong RL baselines with minimal computational overhead. These results highlight the importance of data-centric
adaptations for more efficient RL training in MLLM.

**Code** [— https://github.com/XenoZLH/Shuffle-R1](https://github.com/XenoZLH/Shuffle-R1)



### **1 Introduction**

Inspired by the slow and deliberate thinking process in human cognition, reasoning capabilities empower Large Language Model (LLM) to plan, reflect, and generalize beyond
mere memorization (Chu et al. 2025a; Kang et al. 2024). Recent advances have demonstrated that integrating reinforcement learning (RL) into the training of LLMs can significantly enhance such capabilities, particularly in complex domains such as mathematical problem solving and code generation (Guo et al. 2025; Deepmind 2025; OpenAI 2024;
Seed et al. 2025). Notably, DeepSeek-R1 (Guo et al. 2025)
leverage reward signals derived exclusively from verifiable
outcomes to yield impressive performance gains. Beyond
textual tasks, RL has also seen increasing application in multimodal domains, including object detection, segmentation,

� Corresponding author. *[∗]* This work was done during the
research internship of Linghao Zhu, Yiran Guan, and Dingkang
Liang at Xiaomi.


**(a)** **(b)**

Figure 1: (a): Advantage Collapsing, where most advantages
concentrate near zero. (b): Rollout Silencing, where the ratio
of rollouts with non-zero gradient consistently drops. The
phenomenon gets worse in larger models.

and video understanding (Li et al. 2025; Liu et al. 2025c,b;
Wang et al. 2025c), highlighting its potential to support generalizable reasoning across modalities.
Despite the growing interest in RL for LLM and Multimodal LLM (MLLM), several practical challenges remain
unresolved, including slow convergence, training instability, and suboptimal efficiency. Recent studies have proposed
various improvements, such as empirical optimization of
training configurations (Yu et al. 2025), integration of experience replay (Wang et al. 2025a), and refinement of objective functions (Chu et al. 2025b). Other works focus on
reward optimization (Liu et al. 2025a; Ma et al. 2025).
However, most of these approaches remain confined to
the standard RL training paradigm, where trajectories are
uniformly sampled without considering their varying informativeness or difficulty. This static strategy overlooks a crucial insight that not all learning signals are equally valuable,
which leads us to consider a fundamental question: *Can dy-*
*namically prioritizing trajectories provide richer gradient*
*information and lead to more efficient training?*
Motivated by this question, we examine the current RL
training practices and identify two critical yet might underexplored inefficiencies. First, **Advantage Collapsing**
emerges when most computed advantages cluster excessively near zero, drowning out informative signals from trajectories with large-magnitude advantages, resulting in weak


-----

or negligible gradient updates (Fig. 1(a)). Second, **Rollout**
**Silencing** arises as the fraction of rollouts contributing nonzero gradients steadily declines during training (Fig. 1(b)),
leading to wasted computation and underutilization of data.
These two findings demonstrate a pressing need for adaptive mechanisms to prioritize, reuse, and reallocate gradient
exposure toward informative samples.
In this paper, we introduce **Shuffle-R1**, an innovative
framework designed to dynamically prioritize and amplify
critical gradient information. Rather than asking *how to de-*
*sign better rewards*, we try to ask *what data should the*
*model actually update on* during RL fine-tuning. Specifically, Shuffle-R1 introduces two elegantly simple yet highly
effective modules: (1) Pairwise Trajectory Sampling, which
selects high-contrast trajectory pairs with large-magnitude
advantages from an extended rollout pool, concentrating
learning signals to mitigate Advantage Collapsing; and
(2) Advantage-based Batch Shuffle, which dynamically reshapes training batches to prioritize informative trajectories while down-weighting ineffective ones, alleviating Rollout Silencing to improve data utilization. Our framework is
lightweight, modular, and seamlessly integrable with existing RL algorithms. Experiments demonstrate that ShuffleR1 significantly improves model performance across challenging multimodal reasoning tasks, even surpassing GPT4o and Claude-3.7 on MathVerse and MathVista. Moreover,
it achieves competitive performance to GRPO while requiring only half of the training steps.
In summary, our contributions are three-fold: **1)** We reveal two critical yet underexplored limitations that undermine training efficiency in RL fine-tuning for MLLM, i.e.,
Advantage Collapsing and Rollout Silencing. **2)** We propose
Shuffle-R1, a novel and adaptive RL framework that dynamically selects high-contrast trajectories and reshapes training
batches to emphasize informative samples. **3)** Extensive experiments across model scales and both in-domain and outof-domain benchmarks demonstrate the effectiveness and
generalizability of our framework.
### **2 Related Work**
#### **2.1 Large Reasoning Models**

Researchers have explored various approaches to equip
LLM with reasoning ability. Some early studies performed
SFT on complex long chain-of-thought data, leading to performance gains on reasoning tasks (Muennighoff et al. 2025;
Ye et al. 2025; Yue et al. 2024; Guo et al. 2024). However, some claim that SFT merely enables the model to
memorize the format of reasoning steps and long chains of
thought, without fully grasping the ability to reason independently (Chu et al. 2025a; Kang et al. 2024; Ye et al.
2024; Allen-Zhu and Li 2024). Some researchers control the
model to generate structured chain-of-thought instead of free
generation, achieving systematic step-by-step reasoning output (Xu et al. 2025; Wu et al. 2025; Thawakar et al. 2025)
Other works attempted to use test-time scaling like Monte
Carlo Tree Search (MCTS) (Yao et al. 2023; Zhang et al.
2024a; Du et al. 2024; Yao et al. 2024) to facilitate complex
reasoning by actively extending the output of the model.


Recently, models such as OpenAI o1/o3 (OpenAI 2024),
DeepSeek-R1 (Guo et al. 2025), Seed-Thinking (Seed et al.
2025), and Kimi-k1.5 (Team et al. 2025) utilized RL to enable the model to explore independently, stimulating reasoning ability. In particular, DeepSeek-R1-Zero directly conducted RL on pre-trained model without instruction finetuning, with verifiable outcome reward functions to replace
reward models, achieving surprising reasoning ability. The
training algorithms for RL are also constantly being optimized (Yu et al. 2025; Chu et al. 2025b). Our work focuses
on a deeper investigation of the efficiency of RL training and
puts forward an effective solution to improve both the efficiency and performance of RL training.
#### **2.2 Reinforcement Learning for MLLM**

Following the success of DeepSeek-R1, a series of studies
have transplanted RL into the training of MLLM and downstream visual tasks, such as Open Vocabulary Object Detection (Liu et al. 2025c), Reasoning Segmentation (Liu et al.
2025b), Video Understanding (Li et al. 2025), Video Localization (Wang et al. 2025c), etc. These works mainly focus
on the applicability of RL in downstream tasks. Some other
works focusing on improving the general reasoning ability
of MLLM have achieved performance improvement on reasoning tasks by collecting a large amount of high-quality
data (Meng et al. 2025; Huang et al. 2025; Yang et al. 2025;
Zhang et al. 2025; Peng et al. 2025; Tan et al. 2025). These
works mainly focus on the organization of high-quality reasoning data and the balance between SFT and RL in the
training process.
Some researchers who conduct in-depth research on the
RL mechanism have optimized the RL training process from
various aspects, including adding contrastive reward mechanism (Li et al. 2025), actively introducing reflection tokens
during rollouts (Wang et al. 2025a), optimizing the RL objective function and gradient update mechanism (Chu et al.
2025b), introducing more diverse rollouts (Liu et al. 2025a;
Yao et al. 2025), etc. The core objective of these works is to
optimize the RL training process. In our work, we propose a
novel training framework that introduces dynamic and adaptive selection and resampling to queries and rollouts, reshaping data distribution for better training efficiency and model
performance.
### **3 Preliminaries**

**Reinforcement Learning in LLM.** Reinforcement learning is originally introduced as a post-training technique to
align LLM outputs with human preference. Recently, it has
been used in fine-tuning stage to improve the reasoning ability of LLMs. Policy gradient algorithm is the most commonly used RL method, whose objective function is to maximize the expectation of return the model gets from the
environment. Given a query *q*, multiple independent sampled responses *O* = *{o* 1 *, o* 2 *, ..., o* *N* *}* are sampled from the
old policy model *o ∼* *π* *θ* *′* ( *q* ), with rollout size *N* . Subsequently, the corresponding rewards *R* = *{r* 1 *, r* 2 *, ..., r* *N* *}*
are obtained for each response, in our case they are calculated by verifiable reward functions. Advantages *A* [ˆ] =


-----

*{A* *[π]* *[θ]* ( *r* 1 ) *, A* *[π]* *[θ]* ( *r* 2 ) *, ..., A* *[π]* *[θ]* ( *r* *N* ) *}* are calculated to assess
the improvement over the current policy model. Importance
sampling and clipping are introduced to stabilize the training. The core objective function is defined as:

*J* ( *θ* ) = E *q∼D,{o* *i* *}* *Ni* =1 *[∼][π]* *θ* *[′]* [ (] *[·|][q]* [)]


*,*
�min [ *γ* *t* ( *θ* ) *,* clip ( *γ* *t* ( *θ* ) *,* 1 *−* *ϵ,* 1 + *ϵ* )] *A* [ˆ] *i* �

(1)


*|o* *i* *|*
�

*t* =1


1
� *Ni* =1 *[|][o]* *[i]* *[|]*

where


*N*
�

*i* =1


*γ* *t* ( *θ* ) = *[π]* *[θ]* [(] *[o]* *[i]* *[,]* *[t]* *[|][q,]* *[ o]* *[i]* *[,]* *[<t]* [)] (2)

*π* *θ* *′* ( *o* *i,t* *|q, o* *i,<t* ) *[,]*


*A* ˆ *i* = *[r]* *[i]* *[ −]* std [mean] ( *R* ) [(] *[R]* [)] *,* (3)


**(a)** **(b)**

Figure 2: (a) Model accuracy improves with larger rollout sizes. (b) Queries with different difficulties demonstrate
varying accuracy during training, their corresponding roll
(Fig. 2(b)). Both cases fail to generate diverse and informative rollouts throughout training, contributing to Rollout Silencing. Moreover, in standard training pipelines, each rollout is used only once for a single gradient update, limiting
the model to fully exploit valuable rollouts. Consequently, a

|igure ut siz arying uts ha Fig. 2 ive rol encing ut is u|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
||(a 2: (a) M es. (b) Qu accuracy ve differe (b)). Both louts thro . Moreov sed only|) odel acc eries wit during nt divers cases fa ughout t er, in sta once for|||a i i a o n ar s|
||||u h tr it il ra n a|r d a y t i d||

dynamic sampling strategy that can adaptively discard ineffective rollouts and queries, while allowing repeated learning from valuable rollouts is of great importance.
#### **4.2 Pairwise Trajectory Sampling**

To relieve Advantage Collapsing, we seek to select trajectories that offer stronger learning signals. Since increasing
the rollout number enlarges the chance of capturing highadvantage samples, we propose Pairwise Trajectory Sampling (PTS), a data-centric module to selectively amplify
valuable learning signals. Rather than evaluating trajectories in isolation, PTS organizes candidate rollouts into structured contrastive pairs. This pairing mechanism captures
both high and low advantage signals jointly, forming informative “positive-negative” pairs. Only pairs with the largest
advantage contrast are then retained for training. This process ensures that limited update bandwidth is focused on
trajectories that are both diverse and gradient-rich.
Given a query *q* and a rollout size of 2 *N*, the rollout
trajectories group is denoted as *O* = *{o* *i* *}* [2] *i* =1 *[N]* [. The corre-]
sponding reward and advantage sets are *R* = *{r* *i* *}* [2] *i* =1 *[N]* [and]
*A* = *{A* [ˆ] *i* *}* [2] *i* =1 *[N]* [, respectively. To identify informative trajec-]
tory pairs, our proposed pairing mechanism follows a simple
‘max-and-min’ principle by matching the trajectory with the
highest advantage to that with the lowest, the second highest to the second lowest, and so on. We denote the sorted
advantage values in descending order as:

*A* *s* = *{A* [ˆ] ( *i* ) *}* *i* [2] =1 *[N]* *[,]* where *A* [ˆ] (1) *≥* *A* [ˆ] (2) *≥· · · ≥* *A* [ˆ] (2 *N* ) *.* (4)

Based on this ordering, we construct the pairing set as:

*P* = *{* ( *o* ( *i* ) *, o* (2 *N* *−i* +1) ) *}* *i* *[N]* =1 *[.]* (5)

In this scheme, the original 2 *N* rollouts are easily sorted
and reorganized into *N* pairs. The top-ranked pairs typically
consist of trajectories with high-magnitude but oppositesign advantages, forming contrastive pairs akin to “positive

and *ϵ* is clipping hyperparameter to prevent training collapse.
However, current static RL training paradigm still exhibits
limitations. Most of the collected rollouts have advantages
sharply concentrate near zero, providing limited signals for
gradient update. The proportion of rollouts with non-zero
gradient continues to decline throughout the training, introducing invalid updates. These limitations indicate the need
for a more efficient training framework.
### **4 Method**

In this section, we begin by further analyzing the existing
inefficient issues in training. Then, as illustrated in Fig. 3,
we introduce **Shuffle-R1**, which optimizes the training process through two crucial modules: (1) Pairwise Trajectory
Sampling and (2) Advantage-based Batch Shuffle.
#### **4.1 Problem Analysis**

**Advantage Collapsing.** Our probe analysis reveals that,
contrary to the ideal scenario, most rollouts exhibit advantages sharply concentrated around zero in current
RL paradigm, leading to the Advantage Collapsing phenomenon. Such distribution results in weak gradient signals
and hampers effective policy updates, as only rollouts with
high-magnitude advantages can provide significant gradient
signals. Simply increasing the number of rollouts can collect
more amount of valuable rollouts and improve performance
to some extent (Fig. 2(a)), but also substantially increase
the computational overhead without fundamentally resolving the Advantage Collapsing. These findings highlight the
need for a dynamic mechanism that can adaptively identify
and select valuable rollouts to improve training efficiency.

**Rollout Silencing.** We observe that the proportion of rollouts contributing non-zero gradients steadily decreases as
training progresses, resulting in the Rollout Silencing phenomenon. This is primarily caused by the accumulation
of factors such as zero advantages, gradient clipping, and
overlong truncation, which further exposes the limitations
of static sampling paradigm. By tracking queries of varying difficulty, we find that simple queries tend to converge
early, while difficult ones consistently yield low accuracy


-----

Policy Optimization














Figure 3: Overview of our proposed Shuffle-R1. After advantage calculation, we first conduct Pairwise Trajectory Sampling
to obtain valuable trajectory pairs from original rollout pool, then perform Advantage-based Batch Shuffle to reshape the
distribution of valid trajectories in a batch.


negative” samples. In contrast, the bottom-ranked pairs involve trajectories with advantages closer to zero.
As implied by Eq. 1, trajectories with higher absolute advantages contribute more significantly to the gradient update, while those with near-zero advantages have negligible
impact. We apply a simple top- *k* sampling strategy to select
a subset of valid pairs:

*P* *v* = *{* ( *o* ( *i* ) *, o* (2 *N* *−i* +1) ) *}* *i* *[M]* =1 *[,]* *M* = *αN, α ∈* (0 *,* 1) *.* (6)

where *α* is a hyperparameter controlling the sampling ratio
from the pairing set. We only keep the trajectories in the
valid set for gradient update.
By introducing a structured contrastive sampling scheme,
PTS enables more effective trajectory selection from a
broader exploration space without increasing the gradient
computation cost. The contrastive pairing not only filters
out low-signal trajectories, but also sharpen the model’s policy gradient through direct comparison. PTS shifts the focus of RL fine-tuning from uniform exploration to gradientinformed selection, representing a principled step toward
more efficient data usage in RL training.
#### **4.3 Advantage-based Batch Shuffle**

While PTS could mitigate Advantage Collapsing and improve RL training performance, the Rollout Silencing issue remains unresolved. To overcome this issue, we propose Advantage-based Batch Shuffle (ABS) module that dynamically reshapes training batches to prioritize and reinforce high-value samples. Rather than relying on static data
flow, ABS adaptively redistributes trajectories within each
training batch, enabling more frequent updates to trajectories with high learning utility. Built on top of PTS, it serves
to magnify the gradient exposure of informative samples, reshaping the training data distribution to achieve better data
utilization and training efficiency.
Denote a data batch:

*B* = *{p* *[g]* *i* [= (] *[o]* *[g]* *i,* 1 *[,]* [ ˆ] *[A]* *[g]* *i,* 1 *[, o]* *[g]* *i,* 2 *[,]* [ ˆ] *[A]* *[g]* *i,* 2 *[, q]* *[g]* [)] *[}]* *[i]* [=1] *[∼][M,g]* [=1] *[∼][G]* *[,]* (7)

with batch size of *M × G*, In the standard gradient update
process, *B* is sequentially divided into *K* mini-batches, each
contains *MG/K* samples.


In our ABS module, we first assign an importance weight
to each trajectory pair based on the sum of the absolute advantages:
*W* ( *p* *j* ) = *|A* [ˆ] *j,* 1 *|* + *|A* [ˆ] *j,* 2 *|.* (8)
We then define the sampling probability of each pair *p* *j* *∈*
*P* *v* as:

*W* ( *p* *j* )
Φ( *p* *j* ) = � *|kP* =1 *v* *|* *[W]* [(] *[p]* *[k]* [)] *.* (9)

Based on the sampling probability, we perform *S* subsampling from original batch *B*, each sub-sampling has a
capacity of *T* pairs (2 *T* trajectories):

*B* *s* = *{p* *s,t* *}* *t* *[T]* =1 *[,]* s.t. *p* *s,t* *̸* = *p* *s,t* *′* *, ∀* *t ̸* = *t* *[′]* *.* (10)

All the sub-sampling batches are sequentially combined
to form the *reshuffled* batch *B* *[′]* = [�] *[S]* *s* =1 *[B]* *[s]* [. During the ABS]
process, we set *|B* *[′]* *|* = *|B|,* i.e., *S × T* = *MG* to ensure
the reshuffled batch matches the same size as the original
batch. The reshuffled batch will maintain the gradient update
paradigm of the original method.
The ABS module optimizes the learning process through
Advantage-aware shuffling and Inter-sub-batch resampling.
It increases the update frequency of trajectories with higher
advantages, maintains diversity while reinforcing high-value
samples through repeated exposure. Together, these designs
transform each batch into a soft-prioritized structure that
better reflects training signal utility.
### **5 Experiments**
#### **5.1 Experimental Setup**

**Datasets and Benchmarks.** We first conduct our experiments on Geometry3K dataset (Lu et al. 2021) (2 *.* 1 *k* training samples.) and a subset of MMK12 dataset (Meng et al.
2025) containing the same amount of data, to investigate
model performance on limited training resources. Then, we
conduct experiments on larger data scale with MM-Eureka
dataset (Meng et al. 2025), seeking for better model performance. We randomly select 27 *k* samples from MM-Eureka
dataset and mix them with Geometry3K dataset, forming
a total of 30 *k* training corpora. All training samples are in
free-form format.


-----

**Method** **Geo3K** **Math Avg. HallBench ChartQA**

Qwen-3B 25.79 41.71 59.83 73.08
+ GRPO 42.64 46.74 63.09 76.20
+ DAPO 45.09 48.08 **63.24** 76.70
**+ Ours** **47.88** ( **+22.09** ) **48.70** ( **+6.99** ) 63.19 ( **+3.36** ) **77.04** ( **+3.06** )

Qwen-7B 38.12 49.82 65.19 79.84
+ GRPO 52.60 53.13 68.56 80.84
+ DAPO 54.43 54.19 69.29 81.20
**+ Ours** **55.89** ( **+17.77** ) **54.63** ( **+4.81** ) **69.51** ( **+4.32** ) **81.64** ( **+1.80** )

Table 1: Performance of Shuffle-R1 on Geometry3K dataset
compared with GRPO and DAPO.

**Method** **K12** **Math Avg. HallBench ChartQA**

Qwen-3B 42.42 41.71 59.83 73.08
+ GRPO 59.19 48.71 64.14 77.12
+ DAPO 61.42 49.75 65.08 77.00
**+ Ours** **62.22** ( **+19.80** ) **50.05** ( **+8.34** ) **65.72** ( **+5.89** ) **78.28** ( **+5.20** )

Qwen-7B 52.13 49.82 65.19 79.84
+ GRPO 66.15 54.47 67.75 82.48
+ DAPO 68.35 54.52 68.66 82.52
**+ Ours** **68.78** ( **+16.65** ) **55.02** ( **+5.20** ) **69.87** ( **+4.68** ) **82.60** ( **+2.76** )

Table 2: Performance of Shuffle-R1 on K12 dataset compared with GRPO and DAPO.

We first perform evaluation on in-domain test set of
Geometry3K and MMK12. Further, as RL is famous for
its strong generalizability, we evaluate our model’s performance on the following representative visual reasoning benchmarks: MathVerse (Zhang et al. 2024b), MathVision (Wang et al. 2024), WeMath (Qiao et al. 2024), MathVista (Lu et al. 2023), HallusionBench (Guan et al. 2024)
and ChartQA (Masry et al. 2022). These benchmarks span
across math reasoning, visual perception, and chart understanding. We use MathRuler to evaluate questions with freeform ground truths and Gemini-2.0-Flash-001 (Deepmind
2025) to evaluate questions with multi-choice ground truths.

**Implementation Details.** We use EasyR1 (Yaowei et al.
2025) as our training codebase. We choose Qwen2.5-VL3B-Instruct and Qwen2.5-VL-7B-Instruct (Bai et al. 2025)
as base model to verify our method’s generalizability on
model scales. Parameters of vision encoder are kept frozen.
We set update batch size to 128 and rollout batch size ( *G* ) to
512. Rollout temperature is set to 1.0 and learning rate is set
to 1 *e −* 6. All experiments are conducted on 8 *×* H800-80G
GPUs. For PTS, we sample 4 trajectory pairs (8 trajectories)
from 16 rollouts for each query, striking a balance between
training cost and exploration space. For ABS, we set the subsampling batch size ( *T* ) to 256 pairs (512 query-response
trajectories) and the shuffle times ( *S* ) to 8. For evaluation,
we set the temperature to 0.5 and report the average pass@1
accuracy of 8 tests to reduce randomness.
#### **5.2 Main Results**

**Comparison with Representative Algorithms.** We compare model performance of our method with GRPO (Shao


Figure 4: Advantage distribution in a training batch of
GRPO and our framework.

et al. 2024) and DAPO (Yu et al. 2025). As shown in
Tab. 1, after training on the Geometry3K dataset, the 3B
model trained with our method achieved 47.88% accuracy
on in-domain test set, with a 5.2% improvement compared
to model trained with GRPO and 2.7% improvement compared to DAPO. Similarly, for the 7B model, our method led
to 3.3% improvement over GRPO and 1.4% over DAPO,
achieving 55.89% accuracy. The superiority of our framework is further highlighted in out-of-domain benchmarks.
For out-of-domain math reasoning task, the average accuracy is improved by 1.96% on 3B model and 1.5% on 7B
model compared to GRPO, and surpasses DAPO as well.
Similar performance gains are also observed on HallusionBench and ChartQA, where the data gap is even further
from training data, showing strong generalizability of our
framework. Tab. 2 demonstrates similar performance results
on K12 dataset, where consistent performance gains are
achieved on both in-domain and out-of-domain benchmarks,
highlighting our framework’s generalizability on different
data distribution.

**Comparison with RL-based models.** We conduct larger
scale experiments on MM-Eureka dataset. As shown in
Tab. 3, trained with 30 *k* selected data from diverse sources
for 150 steps, our 7B model exhibits a substantial accuracy
gain over the base model (Qwen2.5-VL-7B). Moreover, it
outperforms a series of open-source 7B competitors who
also adopt RL training strategies, e.g. MM-Eureka with direct RL and VLAA-Thinker with RL after cold-start SFT.
Notably, our model achieves competitive or superior performance on several benchmarks compared to leading closesource models, for instance, Claude-3.7-Sonnet (Anthropic
2025) and GPT-4o (Achiam et al. 2024). Under the same setting, our 3B variant also demonstrates strong performance,
even outperforming several 7B models on certain benchmarks. These results highlight the superiority of our proposed approach in boosting the training efficiency in reinforcement learning.

**Efficiency Analysis.** The improvement in model performance mainly stems from better training efficiency. Fig. 2(a)
in previous section shows that, under the same update rollout size, model trained under our framework significantly
outperforms GRPO, indicating more effective utilization
of training data. Advantage distribution analysis in Fig. 4


-----

**Model** **MathVerse** **MathVision** **MathVista** **WeMath** **HallBench** **ChartQA** **Avg.**

*Close-source*

GPT-4o (Achiam et al. 2024) 50.8 30.4 63.8 68.8 55.0  -  o1 (OpenAI 2024) 57.0 60.3 73.9 - - - Gemini-2.0 pro (Deepmind 2025) 67.3 48.1 71.3 - 49.8 - Claude-3.7-Sonnet (Anthropic 2025) 52.0 41.3 66.8 72.6 55.4  -  
*Open-Source SFT*

InternVL-2.5-8B (Chen et al. 2025b) 39.5 17.0 64.5  - 50.1 79.1  InternVL-3-8B (Zhu et al. 2025)  - 29.3 71.6  - 49.9 86.6  Qwen2.5-VL-3B *[∗]* (Bai et al. 2025) 34.8 21.9 58.4 51.7 59.8 73.1 49.9
Qwen2.5-VL-7B *[∗]* (Bai et al. 2025) 42.6 25.8 67.4 63.5 65.2 79.8 57.4

*Cold Start + RL*

R1-VL-7B *[∗]* (Zhang et al. 2025) 40.1 24.3 62.3 59.8 60.9 76.1 53.9
Vision-R1-7B *[∗†]* (Huang et al. 2025) 46.1 - 70.8 - 57.8 83.1 R1-OneVision-7B *[∗]* (Yang et al. 2025) 43.0 24.8 61.2 60.6 66.4 77.8 55.2
OpenVLThinker-7B *[∗]* (Deng et al. 2025) 46.4 24.8 69.7 67.2 59.1 78.4 57.6
VLAA-Thinker-7B *[∗]* (Chen et al. 2025a) 48.9 26.3 69.9 67.7 67.5 80.1 60.1

*Zero RL*

MM-Eureka-Qwen-7B *[∗]* (Meng et al. 2025) 49.6 27.4 70.6 67.4 66.7 79.0 60.1
MMR1-Math-7B *[∗]* (Leng et al. 2025) 39.2 **31.9** 71.5 70.7 69.6 82.0 60.8
ThinkLite-VL-7B *[∗]* (Wang et al. 2025b) 45.2 28.0 72.4 69.3 70.2 82.0 61.2
VL-Rethinker-7B *[∗]* (Wang et al. 2025a) 51.7 29.7 72.0 70.1 69.9 79.0 62.1
NoisyRollout-7B-K12 *[∗]* (Liu et al. 2025a) 50.1 28.0 70.9 70.8 70.1 81.4 62.1

**Shuffle-R1-Qwen-3B (Ours)** 44.2 26.8 70.4 66.5 69.2 79.9 59.5
**Shuffle-R1-Qwen-7B (Ours)** **53.9** 30.0 **77.0** **72.3** **71.0** **84.1** **64.7**

Table 3: Model performance on representative visual reasoning benchmarks. Models marked with ‘ *[∗]* ’ are evaluated using our
own evaluation scripts with vLLM. *[†]* Vision-R1-7B used WeMath and MathVision as training data, its performance on these
benchmarks are omitted. Best performance of RL-only models marked with **Bold**, second best with *underline* .


proves that, PTS effectively mitigates Advantage Collapsing
by increasing the proportion of large-magnitude advantages.
ABS further optimizes the batch composition, enabling the
model to focus on more informative trajectories.
Fig. 5(a) and (b) further probe into training dynamics
and demonstrate that our framework consistently achieves
higher training and validation accuracy, reaching comparable performance as GRPO but with only half the training
steps. Moreover, our framework effectively mitigates the issue of “Rollout Silencing” shown in Fig. 5(c), maintaining a
high token utilization rate across all training stages. Fig. 5(d)
further illustrates the favorable trade-off between training
scale and computational cost of our approach, substantially
expanding the RL exploration space with minimal additional
overhead.
#### **5.3 Ablation Study**

We conduct ablation experiments on Qwen2.5-VL-3BInstruct using Geometry3K, focusing on two objectives: (1)
assessing the contribution of each component in our framework, and (2) validating the impact of key designs.

**Effectiveness of Algorithm Design.** We assess the effectiveness of proposed PTS and ABS modules by successively introduce them to the baseline. As shown in Tab. 4,
in the in-domain Geometry3K test set, PTS demonstrates
significant performance gains, improving the model accuracy from 42.64% to 46.21% (+3.57%). When ABS is in

Table 4: Ablation study on effectiveness of PTS and ABS.

troduced for co-optimization, the model achieves a further 1.67% performance gain, eventually reaching an accuracy of 47.88%. This improvement trend continues in
the out-of-domain evaluation scenario: in the math reasoning task, the full setting (PTS + ABS) achieves an absolute performance improvement of 2.0% compared to GRPO
(48.70% vs. 46.74%, with 41.71% before post-training). In
the ChartQA dataset, which has more significant distributional differences, the joint application of PTS and ABS
still maintains an effective performance gain (77.04% vs.
76.20%, with 73.08% before post-training). Our method also
demonstrates improved performance in HallusionBench.

**Rationality of Pairwise Trajectory Sampling.** One core
mechanism of the PTS lies in the structured contrastive sampling scheme. To verify the effectiveness of this bidirectional
optimization mechanism, we designed the following experiments: (1) one-way positive sampling (only select trajectories with highest advantage); (2) one-way negative sampling


-----

**(a)** **(b)** **(c)** **(d)**

Figure 5: **(a)** : Training accuracy of GRPO and Shuffle-R1. **(b)** : Validation accuracy of GRPO and Shuffle-R1. **(c)** : Token
utilization rate of GRPO and Shuffle-R1. **(d)** : Shuffle-R1 achieves better performance with minimal extra time cost.

**Setting** **Geo3k Math Avg. HallBench ChartQA**

Qwen2.5-VL-3B 25.79 41.71 59.83 73.08
+ GRPO 42.64 46.74 63.09 76.20

*Ablation on PTS*

+ only max 41.26 44.77 63.30 75.64
+ only min 23.36 41.52 60.98 74.36
+ rand. select 43.53 46.62 63.19 76.00
**+ PTS** **46.21** **47.64** **63.40** **76.52**

*Ablation on ABS*


+ rand. shuffle 46.05 47.40 **63.19** 76.60
+ reorder 46.28 47.64 63.09 76.64
**+ ABS** **47.88** **48.80** **63.19** **77.04**

Table 5: Ablation study on rationality of PTS and ABS.

(only select trajectories with lowest advantage); and (3) unbiased random sampling. All settings maintained a consistent sampling ratio (8 valid trajectories from 16 rollouts) to
ensure fairness. We disable ABS since it relies on the pairing
result of PTS. As shown in Tab. 5, model trained with PTS
receives a consistent performance gain on both in-domain
and out-of-domain tasks, while one-way positive and oneway negative sampling show a decline in all tasks, demonstrating the effectiveness and rationality of our design. Unbiased random sampling only receives minor improvement
over baseline, far behind the effectiveness of PTS.

**Rationality of Advantage-based Batch Shuffle.** ABS introduces Advantage-aware shuffling and Inter-sub-batch resampling to reshape the training batch. To validate their
effectiveness, we designed two contrastive experiments:
(1) unbiased shuffle: using uniformly distributed sampling
weights to perform shuffle strategy. and (2) static reorder:
randomly reorder the training batch without shuffle, maintaining the original data distribution. We enable PTS during training as ABS relies on its pairing result. As shown
in Tab. 5, model trained with ABS significantly outperforms
the contrastive settings. The unbiased shuffle setting even
performs worse compared to the PTS-only setting, demonstrating the rationality of ABS. The static reorder setting has
no improvement compared to PTS-only setting, as they have
the same data distribution.


**(a)** **(b)**

Figure 6: Ablation on key hyper parameters. (a): Effect of
sampling ratio *α* . (b): Effect of shuffle times *S* .

**Hyperparameters.** We investigate the impact of two key
hyperparameters in our framework: (1) the sampling ratio
( *α* ) in PTS, and (2) the shuffle times ( *S* ) in ABS. For *α*,
we fix *S* = 8 and test values of 0.25, 0.5, and 0.75 (i.e.,
selecting 4, 8, and 12 samples from 16 rollouts). As shown
in Fig. 6(a), both *α* = 0 *.* 75 and *α* = 0 *.* 5 yield strong performance, while *α* = 0 *.* 25 lags behind. We attribute this
to over-pruning, where filtering out rollouts too aggressively
may reduce data diversity. We choose *α* = 0 *.* 5 to strike a balance between signal quality and computational efficiency.
For *S*, we fix *α* = 0 *.* 5 and vary the shuffle times as 4, 8,
and 16. Fig. 6(b) shows that performance improves with increasing *S*, but saturates beyond *S* = 8. This suggests that
moderate resampling enhances data exposure, but too many
shuffles may offer diminishing returns.
### **6 Conclusion**

In this paper, we propose Shuffle-R1, a simple but effective framework that improves the training efficiency of reinforcement learning of multimodal large language models. Through Pairwise Trajectory Sampling and Advantagebased Batch Shuffle, our framework significantly outperforms representative algorithms and models in both indomain and out-of-domain tasks, demonstrating the value of
data-centric adaptive design. We hope that our motivations,
method, and findings are helpful for further research.


-----

### **References**

Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya,
I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2024. Gpt-4 technical report.
arXiv:2303.08774.

Ahmadian, A.; Cremer, C.; Gall´e, M.; Fadaee, M.; Kreutzer,
J.; Pietquin, O.; Ust¨un, A.; and Hooker, S. 2024. Back to [¨]
basics: Revisiting reinforce style optimization for learning
from human feedback in llms. arXiv:2402.14740.

Allen-Zhu, Z.; and Li, Y. 2024. Physics of language models: Part 3.1, knowledge storage and extraction.
arXiv:2309.14316.

[Anthropic. 2025. Claude 3.7 Sonnet, anthropic.com. https:](https://www.anthropic.com/claude/sonnet)
[//www.anthropic.com/claude/sonnet. 2025-02-24.](https://www.anthropic.com/claude/sonnet)

Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang,
K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl
technical report. arXiv:2502.13923.

Chen, H.; Tu, H.; Wang, F.; Liu, H.; Tang, X.; Du, X.; Zhou,
Y.; and Xie, C. 2025a. SFT or RL? An Early Investigation
into Training R1-Like Reasoning Large Vision-Language
Models. arXiv:2504.11468.

Chen, Z.; Wang, W.; Cao, Y.; Liu, Y.; Gao, Z.; Cui, E.; Zhu,
J.; Ye, S.; Tian, H.; Liu, Z.; et al. 2025b. Expanding performance boundaries of open-source multimodal models with
model, data, and test-time scaling. arXiv:2412.05271.

Chu, T.; Zhai, Y.; Yang, J.; Tong, S.; Xie, S.; Schuurmans,
D.; Le, Q. V.; Levine, S.; and Ma, Y. 2025a. Sft memorizes,
rl generalizes: A comparative study of foundation model
post-training. arXiv:2501.17161.

Chu, X.; Huang, H.; Zhang, X.; Wei, F.; and Wang, Y. 2025b.
GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning. arXiv:2504.02546.

Deepmind, G. 2025. Gemini 2.5: Our most intelligent
[AI model — blog.google. https://blog.google/technology/](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking)
[google-deepmind/gemini-model-thinking-updates-march-](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking)
[2025/#gemini-2-5-thinking. Accessed: 2025-03-25.](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking)

Deng, Y.; Bansal, H.; Yin, F.; Peng, N.; Wang, W.; and
Chang, K.-W. 2025. Openvlthinker: An early exploration
to complex vision-language reasoning via iterative selfimprovement. arXiv:2503.17352.

Du, W.; Wang, H.; Hao, Z.; Wang, C.; Song, Y.; and Cai,
Z. 2024. LLM-Augmented Deep Reinforcement Learning
for Complex Task Planning. In *2024 China Automation*
*Congress (CAC)*, 6593–6598. IEEE.

Guan, T.; Liu, F.; Wu, X.; Xian, R.; Li, Z.; Liu, X.; Wang,
X.; Chen, L.; Huang, F.; Yacoob, Y.; et al. 2024. Hallusionbench: an advanced diagnostic suite for entangled language
hallucination and visual illusion in large vision-language
models. In *Proc. of IEEE Intl. Conf. on Computer Vision*
*and Pattern Recognition*, 14375–14385.

Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.;
Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1:
Incentivizing reasoning capability in llms via reinforcement
learning. arXiv:2501.12948.


Guo, D.; Zhu, Q.; Yang, D.; Xie, Z.; Dong, K.;
Zhang, W.; Chen, G.; Bi, X.; Wu, Y.; Li, Y.; et al.
2024. DeepSeek-Coder: When the Large Language
Model Meets Programming-The Rise of Code Intelligence.
arXiv:2401.14196.

Hu, J. 2025. Reinforce++: A simple and efficient approach
for aligning large language models. arXiv:2501.03262.

Huang, W.; Jia, B.; Zhai, Z.; Cao, S.; Ye, Z.; Zhao, F.;
Xu, Z.; Hu, Y.; and Lin, S. 2025. Vision-r1: Incentivizing
reasoning capability in multimodal large language models.
arXiv:2503.06749.

Kang, K.; Setlur, A.; Ghosh, D.; Steinhardt, J.; Tomlin, C.;
Levine, S.; and Kumar, A. 2024. What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
arXiv:2411.07681.

Leng, S.; Wang, J.; Li, J.; Zhang, H.; Hu, Z.; Zhang, B.;
Zhang, H.; Jiang, Y.; Li, X.; Wang, F.; Rong, Y.; Sun, A.;
and Lu, S. 2025. MMR1: Advancing the Frontiers of Mul[timodal Reasoning. https://github.com/LengSicong/MMR1.](https://github.com/LengSicong/MMR1)
2025-03-11.

Li, X.; Yan, Z.; Meng, D.; Dong, L.; Zeng, X.; He, Y.; Wang,
Y.; Qiao, Y.; Wang, Y.; and Wang, L. 2025. VideoChat-R1:
Enhancing Spatio-Temporal Perception via Reinforcement
Fine-Tuning. arXiv:2504.06958.

Liu, X.; Ni, J.; Wu, Z.; Du, C.; Dou, L.; Wang, H.; Pang, T.;
and Shieh, M. Q. 2025a. NoisyRollout: Reinforcing Visual
Reasoning with Data Augmentation. arXiv:2504.13055.

Liu, Y.; Peng, B.; Zhong, Z.; Yue, Z.; Lu, F.; Yu, B.; and Jia,
J. 2025b. Seg-zero: Reasoning-chain guided segmentation
via cognitive reinforcement. arXiv:2503.06520.

Liu, Z.; Sun, Z.; Zang, Y.; Dong, X.; Cao, Y.; Duan, H.; Lin,
D.; and Wang, J. 2025c. Visual-rft: Visual reinforcement
fine-tuning. arXiv:2503.01785.

Lu, P.; Bansal, H.; Xia, T.; Liu, J.; Li, C.; Hajishirzi, H.;
Cheng, H.; Chang, K.-W.; Galley, M.; and Gao, J. 2023.
MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. In *The 3rd Workshop on*
*Mathematical Reasoning and AI at NeurIPS’23* .

Lu, P.; Gong, R.; Jiang, S.; Qiu, L.; Huang, S.; Liang, X.; and
Zhu, S.-c. 2021. Inter-GPS: Interpretable Geometry Problem
Solving with Formal Language and Symbolic Reasoning. In
*Annual Meeting of the Association for Computational Lin-*
*guistics*, 6774–6786.

Ma, Y.; Du, L.; Shen, X.; Chen, S.; Li, P.; Ren, Q.; Ma,
L.; Dai, Y.; Liu, P.; and Yan, J. 2025. One RL to See
Them All: Visual Triple Unified Reinforcement Learning.
arXiv:2505.18129.

Masry, A.; Do, X. L.; Tan, J. Q.; Joty, S.; and Hoque, E.
2022. ChartQA: A Benchmark for Question Answering
about Charts with Visual and Logical Reasoning. In *Annual*
*Meeting of the Association for Computational Linguistics*,
2263–2279.

Meng, F.; Du, L.; Liu, Z.; Zhou, Z.; Lu, Q.; Fu, D.; Han, T.;
Shi, B.; Wang, W.; He, J.; et al. 2025. MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with Rule-based
Reinforcement Learning. arXiv:2503.07365.


-----

Muennighoff, N.; Yang, Z.; Shi, W.; Li, X. L.; Fei-Fei, L.;
Hajishirzi, H.; Zettlemoyer, L.; Liang, P.; Cand`es, E.; and
Hashimoto, T. 2025. s1: Simple test-time scaling. *arXiv*
*preprint arXiv:2501.19393* .

OpenAI. 2024. OpenAI o1 System Card — ope[nai.com. https://cdn.openai.com/o1-system-card.pdf. Ac-](https://cdn.openai.com/o1-system-card.pdf)
cessed: 2024-09-12.

Peng, Y.; Zhang, G.; Zhang, M.; You, Z.; Liu, J.; Zhu, Q.;
Yang, K.; Xu, X.; Geng, X.; and Yang, X. 2025. LMMR1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL. arXiv:2503.07536.

Qiao, R.; Tan, Q.; Dong, G.; Wu, M.; Sun, C.; Song, X.;
GongQue, Z.; Lei, S.; Wei, Z.; Zhang, M.; et al. 2024. Wemath: Does your large multimodal model achieve humanlike mathematical reasoning? arXiv:2407.01284.

Seed, B.; Yuan, Y.; Yue, Y.; Wang, M.; Zuo, X.; Chen, J.;
Yan, L.; Xu, W.; Zhang, C.; Liu, X.; et al. 2025. Seedthinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv:2504.13914.

Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang,
H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath:
Pushing the limits of mathematical reasoning in open language models. arXiv:2402.03300.

Tan, H.; Ji, Y.; Hao, X.; Lin, M.; Wang, P.; Wang, Z.; and
Zhang, S. 2025. Reason-rft: Reinforcement fine-tuning for
visual reasoning. arXiv:2503.20752.

Team, K.; Du, A.; Gao, B.; Xing, B.; Jiang, C.; Chen, C.; Li,
C.; Xiao, C.; Du, C.; Liao, C.; et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv:2501.12599.

Thawakar, O.; Dissanayake, D.; More, K.; Thawkar, R.;
Heakl, A.; Ahsan, N.; Li, Y.; Zumri, M.; Lahoud, J.; Anwer, R. M.; et al. 2025. Llamav-o1: Rethinking step-by-step
visual reasoning in llms. arXiv:2501.06186.

Wang, H.; Qu, C.; Huang, Z.; Chu, W.; Lin, F.; and Chen,
W. 2025a. VL-Rethinker: Incentivizing Self-Reflection
of Vision-Language Models with Reinforcement Learning.
arXiv:2504.08837.

Wang, K.; Pan, J.; Shi, W.; Lu, Z.; Ren, H.; Zhou, A.; Zhan,
M.; and Li, H. 2024. Measuring multimodal mathematical
reasoning with math-vision dataset. In *Proc. of Advances in*
*Neural Information Processing Systems*, volume 37, 95095–
95169.

Wang, X.; Yang, Z.; Feng, C.; Lu, H.; Li, L.; Lin, C.-C.;
Lin, K.; Huang, F.; and Wang, L. 2025b. SoTA with Less:
MCTS-Guided Sample Selection for Data-Efficient Visual
Reasoning Self-Improvement. arXiv:2504.07934.

Wang, Y.; Xu, B.; Yue, Z.; Xiao, Z.; Wang, Z.; Zhang, L.;
Yang, D.; Wang, W.; and Jin, Q. 2025c. Time-R1: PostTraining Large Vision Language Model for Temporal Video
Grounding. arXiv:2503.13377.

Wu, Q.; Yang, X.; Zhou, Y.; Fang, C.; Song, B.; Sun, X.; and
Ji, R. 2025. Grounded Chain-of-Thought for Multimodal
Large Language Models. arXiv:2503.12799.
Xu, G.; Jin, P.; Hao, L.; Song, Y.; Sun, L.; and Yuan, L. 2025.
Llava-o1: Let vision language models reason step-by-step.
arXiv:2411.10440.


Yang, Y.; He, X.; Pan, H.; Jiang, X.; Deng, Y.; Yang, X.;
Lu, H.; Yin, D.; Rao, F.; Zhu, M.; et al. 2025. R1-onevision:
Advancing generalized multimodal reasoning through crossmodal formalization. arXiv:2503.10615.

Yao, H.; Huang, J.; Wu, W.; Zhang, J.; Wang, Y.; Liu,
S.; Wang, Y.; Song, Y.; Feng, H.; Shen, L.; et al.
2024. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search.
arXiv:2412.18319.

Yao, H.; Yin, Q.; Zhang, J.; Yang, M.; Wang, Y.; Wu, W.; Su,
F.; Shen, L.; Qiu, M.; Tao, D.; et al. 2025. R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO. arXiv:2505.16673.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao,
Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate problem solving with large language models. In *Proc.*
*of Advances in Neural Information Processing Systems*, volume 36, 11809–11822.

Yaowei, Z.; Junting, L.; Shenzhi, W.; Zhangchi, F.; Dongdong, K.; and Yuwen, X. 2025. EasyR1: An Efficient,
[Scalable, Multi-Modality RL Training Framework. https:](https://github.com/hiyouga/EasyR1)
[//github.com/hiyouga/EasyR1.](https://github.com/hiyouga/EasyR1)
Ye, T.; Xu, Z.; Li, Y.; and Allen-Zhu, Z. 2024. Physics of
language models: Part 2.1, grade-school math and the hidden reasoning process. In *Proc. of Intl. Conf. on Learning*
*Representations* .

Ye, Y.; Huang, Z.; Xiao, Y.; Chern, E.; Xia, S.; and
Liu, P. 2025. LIMO: Less is More for Reasoning.
arXiv:2501.19393.

Yu, Q.; Zhang, Z.; Zhu, R.; Yuan, Y.; Zuo, X.; Yue, Y.;
Fan, T.; Liu, G.; Liu, L.; Liu, X.; et al. 2025. DAPO:
An open-source llm reinforcement learning system at scale.
arXiv:2503.14476.

Yue, X.; Qu, X.; Zhang, G.; Fu, Y.; Huang, W.; Sun, H.;
Su, Y.; and Chen, W. 2024. MAmmoTH: Building Math
Generalist Models through Hybrid Instruction Tuning. In
*Proc. of Intl. Conf. on Learning Representations* .

Zhang, D.; Zhoubian, S.; Hu, Z.; Yue, Y.; Dong, Y.; and
Tang, J. 2024a. Rest-mcts*: Llm self-training via process
reward guided tree search. In *Proc. of Advances in Neural*
*Information Processing Systems*, volume 37, 64735–64772.

Zhang, J.; Huang, J.; Yao, H.; Liu, S.; Zhang, X.; Lu, S.; and
Tao, D. 2025. R1-vl: Learning to reason with multimodal
large language models via step-wise group relative policy
optimization. arXiv:2503.12937.
Zhang, R.; Jiang, D.; Zhang, Y.; Lin, H.; Guo, Z.; Qiu, P.;
Zhou, A.; Lu, P.; Chang, K.-W.; Qiao, Y.; et al. 2024b. Mathverse: Does your multi-modal llm truly see the diagrams in
visual math problems? In *Proc. of European Conference on*
*Computer Vision*, 169–186. Springer.
Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Duan,
Y.; Tian, H.; Su, W.; Shao, J.; et al. 2025. InternVL3: Exploring Advanced Training and Test-Time Recipes for OpenSource Multimodal Models. arXiv:2504.10479.


-----

### **A Pseudo Code**

Here, we provide the pseudo code of Shuffle-R1 for readers
to better understand the pipeline flow, and for better transparency in algorithm understanding and reproducibility.

Al g orithm 1: Shuffle-R1 Workflow

1: **Input:** *Q* (queries), *π* *θ* (policy), 2 *N* (rollouts per query), *α*
(sampling ratio), *S* (shuffle rounds)
2: **Output:** Optimized batch *B* *[′]* for gradient update
3: Initialize global batch *B ←∅*
4: **for** each *q ∈Q* **do** *▷* **PTS Phase**
5: Generate *{o* *i* *}* *i* [2] =1 *[N]* *[∼]* *[π]* *[θ]* [(] *[q]* [)]
6: Compute *{A* [ˆ] *i* *}* via *R* = *RewardFunc* ( *{o* *i* *}, q* )
7: Sort pairs: *{* ( *o* ( *i* ) *, o* (2 *N* *−i* +1) ) *}* *i* *[N]* =1 *[←]* [MaxMinPair][(] *[{][A]* [ ˆ] *[i]* *[}]* [)]
8: Retain top- *⌊αN* *⌋* pairs: *B* *q* *←{* ( *o* ( *k* ) *, o* (2 *N* *−k* +1) ) *}* *[⌊]* *k* *[αN]* =1 *[⌋]*
9: Aggregate: *B ←B ∪B* *q*
10: **end for**
11: Compute *W* *j* = *|A* [ˆ] *p* *|* + *|A* [ˆ] *q* *| ∀* ( *o* *p* *, o* *q* ) *∈B* *▷* **ABS Phase**
12: Calculate *P* *j* = *W* *j* */* [�] *W* *k* for weighted sampling
13: *B* *[′]* *←* ShuffleSample( *B, P* *j* *, S* ) with *S × T* = *|B|*
14: **return** *B* *[′]* *▷* **Jointly optimized training data**
### **B Prompt Design**

We use a “Thinking prompt” to explicitly control the output format of the model, which requires the model to output its thinking process within special tokens <think> and
</think>, and mark the final answer with \boxed{}. In
practice, we keep the system prompt of Qwen2.5-VL (Bai
et al. 2025), and insert the “Thinking prompt” at the beginning of user message. We keep the training and evaluation
prompt in the same format. The full structure of instruction
prompt is as follows:
### **C Experiment Settings**

We report details of our training and evaluation settings
here, including reward function design, main hyperparameters and computing resources.

**Reward Calculation.** We adopt a combination of format
reward and accuracy reward as the final reward in reinforcement learning. The format reward and accuracy reward are
calculated as follows:


Hyperparameters Value

max pixels 1000000
min pixels 262144
max prompt length 2048
max response length 2048
rollout batch size 512
global batch size 128
learning rate 1e-6
optimizer AdamW
rollout temperature 1.0
rollout top p 0.99
evaluation temperature 0.5
rollout group number 16
PTS sampling ratio ( *α* ) 0.5
ABS shuffle times ( *S* ) 8
KL coefficient 0.0
vision encoder frozen

Table 6: Hyperparameter settings.


*r* acc =


1 *,* if answer = ground truth
(12)
0 *,* if answer *̸* = ground truth
�


The final reward is the weighted sum of above rewards:

*r* overall = 0 *.* 1 *× r* format + 0 *.* 9 *× r* acc (13)

Format reward is assigned to a smaller weight since response formatting is easy to learn.

**Hyperparameters.** We use EasyR1 (Yaowei et al. 2025)
as our training framework. Full hyperparameter settings during training is shown in Tab. 6. For experiments on Geometry3K and K12 with *∼* 2 *.* 1 *k* training samples, we set
the training steps to 80. For the joint training experiments
( *∼* 30 *k* training samples), we increase the training steps to
150 due to extended data size. Other hyperparameters that
are not mentioned are kept to default values of EasyR1.

**Computing Resources.** All experiments are conducted on
8 *×* NVIDIA H800-80G-SXM GPUs.
### **D Dataset Collection**

For training dataset, we choose Geometry3K (Lu et al. 2021)
and MMK12 dataset. Geometry3K dataset is a high quality real-world geometry problem solving dataset, containing
2.1K training samples and 601 test samples. It contains a
wide range of geometry problems with varying levels of difficulty. The text problems are very compact with most of
the geometric conditions represented in images, making it
suitable for our training. MMK12 dataset is introduced by
MM-Eureka (Meng et al. 2025), which contains 16k math
reasoning training samples. The training samples have both
geometric and non-geometric problems and have been carefully examined and manually filtered to ensure quality. The
test set of MMK12 further includes other STEM problems
such as physics, biology and chemistry. During our training,
we used a randomly selected subset of MMK12 provided
by NoisyRollout (Liu et al. 2025a) (referred to as ‘K12’ in
the paper to distinguish it from full set of MMK12), which


*r* format =


1 *,* if format is *correct* (11)
0 *,* if format is *incorrect*
�


-----

Figure 7: **Top** : Examples of Geometry3K. **Bottom** : Examples of MMK12.

has 2.1k samples with the same size as Geometry3K. All the
training samples are in free-form format. Examples of training data shown in Fig. 7.
To fully evaluate model’s reasoning ability, apart from
in-domain test set from training dataset, we select several representative benchmarks to examine model’s performance on math reasoning, visual perception, and chart
understanding tasks. For math reasoning task, we choose
MathVerse (Zhang et al. 2024b), MathVision (Wang et al.
2024), WeMath (Qiao et al. 2024) and MathVista (Lu et al.
2023). We choose HallusionBench (Guan et al. 2024) and
ChartQA (Masry et al. 2022) for visual perception and chart
understanding tasks respectively. We believe that a good RL
algorithm can not only improve in-domain performance, but
also have the potential to promote robust generalization to
out-of-domain tasks. Examples of evaluation data shown in
Fig. 8.
### **E Evaluation Metrics**

We adopt pass@1 accuracy as evaluation metric. During
evaluation, we set the decoding temperature to 0.5 and perform 8 independent runs, reporting the average pass@1 accuracy as final metric. The choice of temperature doesn’t
degrade model performance, and the averaged accuracy reduces the randomness, resulting in a more stable and reliable
evaluation. Prompt format for evaluation is kept identical to
training prompt. Evaluation settings of our model and all the
reproduced results in main paper are kept the same to ensure
a fair comparison.
For evaluation of MathVerse, MathVision, WeMath,
MathVista and ChartQA, we employ Gemini-2.0-Flash001 (Deepmind 2025) to first extract the predicted answer


Figure 8: **Top** : Examples of math reasoning tasks. **Middle** :
Examples of visual perception tasks. **Bottom** : Examples of
chart understanding tasks.

from model response then compare it with ground truth.
Fig. 9 and Fig. 10 demonstrate the extraction and verification
prompt for Gemini. Specifically, we report accuracy on WeMath under loose mode, and overall accuracy on MathVerse
(including all sub categories: Text Dominant, Text Lite, Vision Intensive, Vision Dominant and Vision Only).
### **F More Experimental Results**

**Detailed Model Performance.** We provide a more detailed performance of models trained on Geometry3K and
K12 dataset in Tab. 7, reporting model performance on each
out-of-domain benchmarks as a supplement to “Math Avg.”
columns in main paper. Trained with only 2 *.* 1 *k* data, both
the 3B and 7B model demonstrate significant performance
gains.

**Comparison with more RL algorithms.** We provide
additional comparisons with more representative RL algorithms, i.e. RLOO (Ahmadian et al. 2024) and REINFORCE++ (Hu 2025). We conduct experiments on



-----

Please read the following example.

Then output the answer extracted from the model response directly. No "Extracted answer:" in your

answer.

Hint: Please answer the question requiring an integer answer and provide the final value,

e.g., 1, 2, 3, at the end.

Question: Which number is missing?

Model response: The number missing in the sequence is 14.

Extracted answer: 14

Hint: Please answer the question requiring a floating-point number with one decimal place and provide

the final value,

e.g., 1.2, 1.3, 1.4, at the end.

Question: What is the fraction of females facing the camera?

Model response: The fraction of females facing the camera is 0.6,

which means that six out of ten females in the group are facing the camera.

Extracted answer: 0.6

Hint: Please answer the question requiring a floating-point number with two decimal places and provide

the final value,

e.g., 1.23, 1.34, 1.45, at the end.

Question: How much money does Luca need to buy a sour apple candy and a butter-scotch candy?

(Unit: $)

Model response: Luca needs $1.45 to buy a sour apple candy and a butterscotch candy.

Extracted answer: 1.45

Hint: Please answer the question requiring a Python list as an answer and provide the final list,

e.g., [1, 2, 3], [1.2, 1.3, 1.4], at the end.

Question: Between which two years does the line graph saw its maximum peak?

Model response: The line graph saw its maximum peak between 2007 and 2008.

Extracted answer: [2007, 2008]

Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.

Question: What fraction of the shape is blue?

Choices: (A) 3/11 (B) 8/11 (C) 6/11 (D) 3/5

Model response: The correct answer is (B) 8/11.

Extracted answer: B

**QUESTION**

Model response: **PREDICTION**

Extracted answer:

Figure 9: Extraction prompt for Gemini.

Below are two answers to a **math question/chart understanding question** . Question is [Question],

[Standard Answer] is the standard answer to the question, and [Model_answer] is the answer extracted
from a model's output to this question.  Determine whether these two answers are consistent.

Please note that only when the [Model_answer] completely matches the [Standard Answer] means they

are consistent. For non-multiple-choice questions, if the meaning is expressed in the same way, it is also

considered consistent, for example, 0.5m and 50cm.

If they are consistent, Judgement is 1; if they are different, Judgement is 0.

[Question]: Write the set of numbers represented on the number line in interval notation.

[Standard Answer]: (-2,1]

[Model_answer]: Extracted Answer: \((-2, 1)\)

**Judgement: 0**

[Question]: As shown in the figure, circle O has a radius 1.0, if angle BAC = 60.0, then the length of BC

is ()

Choices:
A:2 B:2√{{3}} C:√{{3}} D:2√{{2}}

[Standard Answer]: C

[Model_answer]: B:2√{{3}}

**Judgement: 0**

[Question]: Find the domain and range of the function f using interval notation.

[Standard Answer]: domain: [-4, 0) and range: (-3, 1]

[Model_answer]: Range: \((-4, 1]\)

**Judgement: 0**

[Question]: As shown in the figure, circle O has a radius 1.0, if angle BAC = 60.0, then the length of BC

is ()

Choices:
(A):2 (B):2√{{3}} (C):√{{3}} (D):2√{{2}}

[Standard Answer]: (C)

[Model_answer]: C

**Judgement: 1**

Please output the judgement score directly with no explanation.

[Question]: **QUESTION**

[Standard Answer]: **ANSWER**

[Model_answer]: **PREDICTION**

Judgement:

Figure 10: Score prompt for Gemini.


Qwen2.5-VL-3B-Instruct with Geometry3K dataset. All the
experiment settings are kept the same as GRPO and DAPO
in main paper. The final model performance is shown in
Tab. 8. Our framework outperform these algorithms by a
large margin in both in-domain and out-of-domain tasks.

**Comparison with Direct Experience Replay.** We conduct a comparative study by replacing ABS with a prioritized experience replay mechanism. Experience replay
maintains a decoupled buffer of past samples, whereas ABS
adopts an online, in-place shuffle strategy to dynamically reshape the data distribution. As shown in Fig. 11(a) and (b),
in the later stages, the experience replay setting exhibits a
plateau in training accuracy and even a drop in validation accuracy, indicating potential overfitting to stale samples. This
suggests that prioritized experience replay may overly emphasize historical trajectories, leading to suboptimal convergence. Moreover, ABS proves to be more effective in mitigating the Rollout Silencing.
### **G Case Study**

Here we provide some qualitative case study of Shuffle-R1’s
reasoning outputs.
Fig. 12 and Fig. 13 show two improved cases on math related tasks. In case 1, the base model has an error in visual
information parsing, referring to angles not existed in the
figure, resulting in a reasoning fault. The RL model correctly
parsed and solved the geometry problem. In case 2, the base
model misused a geometric theorem, leading to wrong answer, while the RL model correctly solved the problem with
accurate theorem.
Fig. 14 demonstrates improved reasoning ability in RL
model, where the base model has accurate perception about
the chart but has reasoning error in CoT. The RL model can
not only accurately parse the visual information, but also
perform correct reasoning to finally reach the right answer.
Case 4 in Fig. 15 further demonstrates that RL model
also has better visual perception ability compared to base
model. The figure from HallusionBench has been artificially
inserted with an image of a hen, which only occupies a very
small region of the original image. This modification has resulted in a perception error and in base model, but the RL
model can accurately identify the inserted image.


-----

**(a)** **(b)** **(c)**

Figure 11: Performance of Shuffle-R1 compared with directly applying Prioritized Experience Replay.

Table 7: Detailed performance on out-of-domain benchmarks of models trained on Geometry3K and K12 data. Highest accuracy
marked in **Bold** .

Table 8: Performance of Shuffle-R1 on Geometry3K dataset compared with RLOO and REINFORCE++. Highest accuracy
marked in **Bold** .


-----

Figure 12: A case study on math related task.





Figure 13: Another case study on math related task.


-----

Figure 14: A case study on chart understanding task.





Figure 15: A case study on visual perception task.


-----


