## Tractable Sharpness-Aware Learning of Probabilistic Circuits
### Hrithik Suresh [*1], Sahil Sidheekh [*2], Vishnu Shreeram M.P [1], Sriraam Natarajan [2], Narayanan C. Krishnan [1]

1 Mehta Family School of Data Science and Artificial Intelligence, Department of Data Science, Indian Institute of
Technology Palakkad, Kerala, India
2 Erik Jonsson School of Engineering & Computer Science, The University of Texas at Dallas, Richardson, TX, USA


Abstract

Probabilistic Circuits (PCs) are a class of generative models
that allow exact and tractable inference for a wide range of
queries. While recent developments have enabled the learning
of deep and expressive PCs, this increased capacity can often
lead to overfitting, especially when data is limited. We analyze PC overfitting from a log-likelihood-landscape perspective and show that it is often caused by convergence to sharp
optima that generalize poorly. Inspired by sharpness aware
minimization in neural networks, we propose a Hessianbased regularizer for training PCs. As a ke y contribution,
we s ~~h~~ o ~~w~~ ~~th~~ at th ~~e~~ tra ~~ce~~ o ~~f~~ ~~th~~ e ~~He~~ s ~~sia~~ n o ~~f~~ ~~th~~ e ~~lo~~ g ~~-like~~ lih ~~o~~ o ~~d~~ ~~–~~
a ~~s~~ h ~~arp~~ nes ~~s~~ p ~~ro~~ x ~~y~~ ~~th~~ a ~~t~~ is ~~ty~~ p ~~ic~~ ally ~~in~~ tra ~~cta~~ b ~~le~~ i ~~n~~ ~~d~~ e ~~ep~~ n ~~eu~~ ~~-~~
ral ~~n~~ e ~~tw~~ o ~~rk~~ s–ca ~~n~~ ~~b~~ e ~~c~~ o ~~m~~ p ~~uted~~ ef ~~fc~~ i ien ~~tly~~ fo ~~r~~ ~~P~~ C ~~s~~ . Min ~~im~~ izin ~~g~~ th ~~is~~ ~~H~~ es ~~s~~ i ~~a~~ n ~~trace~~ ~~in~~ d ~~uce~~ s a g ~~radien~~ t-n ~~o~~ rm- ~~bas~~ ed reg ~~u~~  lariz ~~er~~ ~~that~~ y ~~ields~~ ~~sim~~ p ~~le~~ clo ~~s~~ e ~~d~~ -f ~~orm~~ p ~~ara~~ m ~~ete~~ r ~~u~~ ~~p~~ d ~~ate~~ s fo ~~r~~
EM, ~~an~~ d in ~~te~~ ~~g~~ ra ~~tes~~ s ~~eamles~~ sly ~~with~~ ~~g~~ ra ~~d~~ ie ~~nt~~ ~~bas~~ ed ~~le~~ arn ~~ing~~
m ~~e~~ t ~~ho~~ d ~~s~~ . Ex ~~perim~~ ents o ~~n~~ syn ~~th~~ etic ~~a~~ n ~~d~~ re ~~al-w~~ o ~~rld~~ ~~d~~ a ~~tas~~ ets
d e m o n s tr a te t ~~h~~ a ~~t~~ o u r m e t ~~h~~ o d co nsis ~~t~~ en tly g u ~~id~~ es ~~PC~~ s to wa rd
flatter minima, improves generalization performance.
### Introduction

Probabilistic generative models are fundamental to modern
machine learning, offering a principled framework for reasoning under uncertainty by modeling data as samples from
an unknown underlying distribution. While deep generative
models—such as GANs (Goodfellow et al. 2014), VAEs
(Kingma and Welling 2014), and Normalizing Flows (Papamakarios et al. 2021)—have excelled in generating high
fidelity samples, they sacrifice the ability to do exact inference tractably. This limits their usefulness when downstream
tasks require calibrated probabilities. In contrast, Probabilistic Circuits (PCs) (Choi, Vergari, and den Broeck 2020)
have emerged as a unifying framework that imposes structural constraints to guarantee efficient and exact inference
for a rich set of queries (Vergari et al. 2021), while retaining
enough expressivity for real-world applications such as constrained generation (Zhang et al. 2023, 2024), image inpainting (Liu, Niepert, and den Broeck 2024), lossless compression (Liu, Mandt, and den Broeck 2022), multi-modal fusion
(Sidheekh et al. 2025), and Neurosymbolic-AI (Ahmed et al.
2022; Ahmed, Chang, and Van den Broeck 2023; Loconte
et al. 2023; Karanam et al. 2025).

  - These authors contributed equally.



10 [4]


|Standard Training|Col2|With Hessi|an-Trace Regularizer|
|---|---|---|---|
|101||||


~~1~~ 0

8

6

4

2

0

2






3 . 5

3 . 0

2 . 5

2 . 0

1.5




2 0 2


2 0 2


Figure 1: Visualization of the 2D (top) and 1D (bottom)
loss-landscape (NLL) near the converged parameters of a
PC trained with (right) and without (left) our Hessian trace
regularizer on a 2D dataset. Standard training falls into a
narrow, sharp basin, while the regularized model settles in
flatter minima that generalizes better. The bar plots in the
bottom figures depict the top-15 eigenvalues of the hessian at
the converged point. The lower eigen spectrum on the right
quantifies the reduced sharpness achieved by our method.

Recent works have therefore pushed towards building
deeper and more expressive PCs (Sidheekh and Natarajan
2024), with millions of parameters that can be parallelized
on GPUs for fast and efficient training/inference (Zhang
et al. 2025; Peharz et al. 2020). However, similar to neural networks, deeper and more expressive PC architectures
are increasingly prone to overfitting, especially when trained
on limited/noisy data. Standard parameter-learning methods
can often converge to sharp local optima, leading to poor
generalization. Such sharp minima, characterized by high
curvature, have been extensively studied in deep neural networks, leading to the development of sharpness-aware optimization methods (Foret et al. 2021; Kwon et al. 2021) that


-----

explicitly target flatter minima to enhance generalization.
However, to the best of our knowledge, sharpness-aware
learning strategies remain relatively unexplored for PCs. We
aim to bridge this gap through our work, by studying the geometry of the PC log-likelihood landscape. Our key insight
is that the structural properties of a PC permit efficient and
exact computation of second-order geometric information.
In particular, we show that the trace of the Hessian of the
log-likelihood—which serves as a measure of surface curvature and a proxy for sharpness– can be computed efficiently
in time linear in the number of parameters and the dataset
size. This is in stark contrast to deep neural networks, where
such exact Hessian computations are intractable in general.
Leveraging this insight, we introduce a Hessian trace regularizer that integrates easily into both gradient-based and
EM-based training of PCs. Crucially, for EM, we derive the
closed-form update rule for the sum node parameters, making our approach scalable and easy to integrate into existing
training pipelines. To provide an intuitive picture of what
our approach accomplishes, we visualize the loss landscape
around the converged parameters of a PC trained with and
without the Hessian trace regularizer in Figure 1, using the
filter-normalized projection technique of Li et al. (2018).
The regularized model settles in a broader and flatter optima
compared to standard training, verifying our claim that Hessian trace minimization steers the optimization away from
sharp valleys, which in turn delivers stronger generalization.
Overall, in this work, we make the following contributions:
1. We derive a closed form expression for the exact full
Hessian of the log-likelihood for tree-structured PCs and
show that it can be computed tractably.
2. For general (DAG-structured) PCs, we establish that although the full Hessian can be intractable, its trace remains exactly computable in time linear in both the number of parameters and dataset size, providing the first
practical curvature measure for large-scale PCs.
3. We introduce a novel sharpness-aware regularizer for
learning PCs, derived from this Hessian trace.
4. We show that while directly minimizing the Hessian trace
via EM leads to a cubic update equation, we can reformulate this objective into an equivalent gradient norm minimization problem, resulting in a quadratic equation
with closed-form parameter updates.
5. We conduct exhaustive experiments on multiple synthetic and real-world datasets to show that our regularizer enforces convergence to flatter optima and helps
reduce overfitting, especially in limited data settings.
### Background and Preliminaries

Definition 1. A Probabilistic Circuit p is a parameterized
directed acyclic graph (DAG) with a unique root node n r
that compactly encodes a joint probability distribution over a
set of random variables X = {X 1, . . ., X d }. It is composed
of three types of nodes: Input nodes (leaf) representing simple univariate distributions over a single variable, Sum nodes
(internal) that compute a weighted sum of its children’s output, and Product nodes (internal) that compute a product
of its children’s output. Formally, each node n in the DAG


computes a distribution p n, defined recursively as follows:


p n (x) =





f n (x), if n is an input node
� c∈in(n) [p] [c] [(][x][)][,] if n is a product node


� c∈in(n) [p] [c] [(][x][)][,] if n is a product node

� c∈in(n) [θ] [nc] [ ·][ p] [c] [(][x][)][,] if n is a sum node


c∈in(n) [θ] [nc] [ ·][ p] [c] [(][x][)][,] if n is a sum node


where f n is a univariate input distribution (e.g., Bernoulli,
Gaussian, etc), in(n) denotes the children of n and θ nc is
the weight parameter on the edge (n, c), such that ∀c ∈
in(n) θ nc ∈ (0, 1] and [�] c∈in(n) [θ] [nc] [ = 1][.]

Definition 2 (Scope). The scope function φ associates to
each node in the PC a subset of X, i.e., φ(n) ¦ X, over
which it defines a distribution. For each non-terminal node
n, φ(n) = ∪ c∈in(n) φ(c). The scope of the root n r is X
The sum and product nodes represent convex mixtures
and factorized distributions over the scopes of their children, respectively. A PC is evaluated bottom up and the joint
distribution is computed as the output of its root node, i.e.
p(x) = p n r (x). The size of p, denoted |p|, is the number of
edges in its DAG. We make the common assumption that p
contains alternating sum and product node layers. This formalism subsumes several classes of tractable models such as
arithmetic circuits (Darwiche 2003), sum-product networks
(Poon and Domingos 2011), PSDDs (Kisa et al. 2014) and
cutset networks (Rahman, Kothalkar, and Gogate 2014).
To achieve tractability for exact marginal (MAR), conditional (CON) and maximum-a-posteriori (MAP) inference, a
PC has to satisfy certain structural properties (Choi, Vergari, and den Broeck 2020), some of which are:

Definition 3 (Smoothness). A PC is smooth if the children of every sum node n have the same scope: ∀c 1, c 2 ∈
in(n), φ(c 1 ) = φ(c 2 ).
Definition 4 (Decomposability). A PC is decomposable if
the children of every product node n have disjoint scopes:
∀c 1, c 2 ∈ in(n), φ(c 1 ) ∩ φ(c 2 ) = ∅.
Definition 5 (Determinism). Define the support supp(n) of
a PC node n as the set of complete variable assignments
x ∈ val(X) for which p n (x) > 0. A PC is deterministic
if the children of every sum node n have disjoint support:
∀c 1, c 2 ∈ in(n), c 1 ̸= c 2, supp(c 1 ) ∩ supp(c 2 ) = ∅.
Smoothness ensures that each sum node represents a
valid mixture, while decomposability allows integrals (or
sums) to factorize recursively for tractable MAR and CON
inference. Adding determinism further makes MAP inference tractable. However enforcing structural properties often reduces the model’s expressivity. Thus, recent works
have aimed to increase their expressivity by efficiently scaling them using tensorized implementations (Peharz et al.
2019, 2020; Liu, Ahmed, and den Broeck 2024; Loconte
et al. 2025), borrowing inductive biases from deep generative models (Sidheekh, Kersting, and Natarajan 2023; Liu,
Zhang, and den Broeck 2023; Correia et al. 2023; Gala
et al. 2024) and relaxing structural assumptions (Loconte
et al. 2024; Loconte, Mengel, and Vergari 2025; Wang and
Van den Broeck 2025).
Regardless of these advances, learning the PC parameters is predominantly achieved using one of two standard paradigms: stochastic gradient based optimization or


-----

expectation-maximization (EM). As differentiable computational graphs, PCs allow efficient computation of gradients
of the likelihood (or log-likelihood) w.r.t their parameters via
backpropagation. Thus stochastic gradient descent (SGD)
and its variants can be used directly to learn their parameters. Alternatively, one can view each sum node as introducing a latent categorical variable indexing its child edges and
apply EM to optimize the incomplete data log-likelihood. In
this framework, the E-step computes posterior distributions
over edges, while the M-step updates the weights in closed
form, given the posterior. Notably, both SGD and EM admit
a unified formulation that can be understood in terms of the
notion of circuit flow (Liu and den Broeck 2021).
Definition 6 (Circuit Flow). The flow associated with the
nodes of a PC is defined in the following recursive manner


through the circuit in a single pass to obtain better calibration and out-of-distribution detection. Liu and den Broeck
(2021) proposed data softening, which replaces each training example with a locally blurred distribution and an entropy regularizer on the circuit’s global output distribution.
However, this requires solving a non-linear equation via
Newton’s method at each sum node, leading to added implementation and computational complexity during training.
In parallel, the deep-learning community has demonstrated that convergence to sharp minima–regions of high
curvature–often correlates with poor generalization, leading to the development of sharpness-aware optimization
strategies. While the idea that flatter minima can help unlock higher generalization capability in deep neural networks (DNNs) has been around for a long time (Hochreiter and Schmidhuber 1997), recent works have shown how
to achieve this in practice using sharpness aware minimization (SAM) (Foret et al. 2021), which solves a local minimax problem to find parameter updates robust to worst-case
perturbations. Extensions such as Adaptive SAM (ASAM)
(Kwon et al. 2021) have also been proposed to adaptively
scale the perturbations using curvature information. We
posit that the relevance of sharpness-aware techniques
extends beyond DNNs to PCs, where training objectives
like log-likelihood maximization are often non-convex and
prone to overfitting in overparameterized regimes. In such
settings, sharpness-aware learning can not only offer a principled means to promote solutions that generalize better, but
the structure inherent in PCs can enable computing such curvature information exactly and efficiently.
### Sharpness-Aware Learning for PCs

The Hessian matrix–the second order partial derivatives of a
loss function–has long been used as a natural way to quantify flatness, as its eigenvalues capture the curvature along
different directions. B¨ottcher and Wheeler (2024) used dominant eigenvectors to visualize the loss landscape of DNNs
and distinguish sharp minima from flat ones, while Chaudhari et al. (2017) used this idea to guide DNNs towards wider
optima. More recently, Kaur, Cohen, and Lipton (2023) proposed using the largest eigenvalue as a flatness metric, while
Sankar et al. (2021) developed a layer-wise Hessian tracebased regularizer. However, as computing the full Hessian
is intractable for DNNs, most methods rely on implicit curvature estimates, such as rank-1 approximations (Martens,
Sutskever, and Swersky 2012) and the Hutchinson trace estimator (Hutchinson 1990). In contrast, as we detail below,
the structured DAG of a PC permits exact and efficient curvature computation, enabling a true-sharpness aware regularizer without resorting to costly approximations.
#### Full Hessian Computation for Tree-Structured PCs

A tree-structured PC (in short TS-PC) is one where every
non-root node in its DAG has exactly one parent, and hence
there is a unique path from the root (n r ) to any node (n).
Our first result is that for a TS-PC, the Hessian of the loglikelihood with respect to the parameters can be computed
tractably. Figure 2 illustrates a typical n r –n path in a TS-PC.


F n (x) =





1, if n is the root node
� F c (x), if n is a input/sum node


� F c (x), if n is a input/sum node

c∈pa(n)

p n ( x )

� θ nc (x) [F] [c] [(][x][)][,] if n is a product node


p n ( x )

� θ nc p c (x)

c∈pa(n)


p n ( x )

p c (x) [F] [c] [(][x][)][,] if n is a product node


The flow associated with an edge (n, c) is defined
p c ( x )
as F nc (x) = θ nc p n (x) [F] [n] [(][x][)][. In the EM interpretation,]
F nc (x) corresponds to the expected count of how often
the edge (nc) is used (E-step). The M-step then maximizes [�] c∈in(n) [F] [nc] [(][x][) log][ θ] [nc] s.t. � c∈in(n) [θ] [nc] [ = 1][,]

which for a mini-batch D i yields the closed form update
θ nc [mini] = � j∈in( � n x ) ∈ � D [i] x [ F] ∈ [nc] D [i] [(][ F][x][)] [nj] [(][x][)] [. To smooth out mini-batch]

noise, a running average is often used: θ nc [new] [= (1] [−] [α][)][ θ] nc [old] [+]
α θ nc [mini] [,] α ∈ [0, 1]. Under gradient-based learning, flows
have the interpretation that F nc (x) = θ nc ∂ lo g ∂θ P ncn r ( x ) . Com
puting the flows requires only a single forward-backward
pass of the PC, and has been proven effective for learning
PCs with hundreds of millions of parameters at scale (Liu,
Ahmed, and den Broeck 2024).
However, such large and expressive PCs can overfit when
data is limited, prompting regularization strategies that
adapt ideas from both deep learning and graphical models. Probabilistic dropout (Peharz et al. 2019) randomly
masks inputs and sum-node children during training to
simulate missing data and mixture uncertainty, reducing
co-adaptation among sub-circuits. Pruning and re-growing
(Dang, Liu, and den Broeck 2022) has been suggested to
remove redundant sub-networks, yielding sparser models
that generalize better. Classical Laplacian smoothening on
the sum-node weights have also been applied to PCs (Liu
and den Broeck 2021), although naive Laplace priors can
bias the mixtures when child supports are imbalanced. Shih,
Sadigh, and Ermon (2021) observed that deep PCs can have
tens of millions of parameters and proposed a hypernetwork that generates sum-node weights from low dimensional embeddings, thereby reducing the free parameters
while preserving expressivity. Recent work has also tried to
exploit the tractability of PCs to propose customized regularizers. Ventola et al. (2023) adapted the idea of Monte
Carlo dropout (Gal and Ghahramani 2016) to PCs by deriving tractable dropout inference that propagates variances


c∈in(n) [F] [nc] [(][x][) log][ θ] [nc] s.t. �


� x∈D [i] [ F] [nc] [(][x][)]

� j∈in(n) � x∈D [i] [ F]


�


j∈in(n) �



[nc]

x∈D [i] [ F] [nj] [(][x][)] [. To smooth out mini-batch]


noise, a running average is often used: θ nc [new] [= (1] [−] [α][)][ θ] nc [old] [+]
α θ nc [mini] [,] α ∈ [0, 1]. Under gradient-based learning, flows
have the interpretation that F nc (x) = θ nc ∂ lo g ∂θ P n r ( x ) . Com

-----

�㗅 = �㗃 �㗅 = �㗅 �㗅 = 1 �㗅 = 0


and correspondingly, the gradient of the likelihood at the
root node with respect to θ nc is


∂P n r ( x )


L
�


� θ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=1


�㗰 !% �㗰 !# �㗰 !!


�㗰 () = �㗰 ["]


n r = P c (x)

∂θ nc


�㗅 ' = �㗄 !% �㗄 !% �㗄 !%&! �㗄 !#$! �㗄 !# �㗄 !# �㗄 !! �㗄 !! �㗅= �㗄 "! �㗅= �㗄 !"

Figure 2: A typical path structure in a tree-structured PC.

Let S i [l] [(][x][)][ and][ P] [ l] i [(][x][)][ denote the outputs of the][ i] [th] [ sum and]
product nodes at level l, respectively. Note that x would contain only the subset of variables defined in the scope of the
node. We ignore it in the notation for clarity. To see how this
structure yields closed-form Hessian entries, consider the
task of expressing the gradient of the root likelihood P n r (x)
w.r.t a mixing weight θ nc . We can unfold the computation
along a unique path n r →· · · → P i [l] [→] [S] j [l] [→· · ·][ n][ →] [c][.]
At each product node P i [l] [on this path, the contribution of its]
other children, i.e. those not on the path enters as a multiplicative factor. We will refer to the product of such sibling
outputs as the product complement, as defined below:

Definition 7 (Product Complement). The product complement of a product node P i [l] [with respect to one of its children]
S j [l][−][1] is defined as:


l
P
ij [(][x][) =] �


S k [l][−][1] (x)


k∈in(P i [l] [)]
k≠ j

All such product complements in a PC can be computed in
a single forward pass. Recall that a single forward-backward
pass also computes the circuit flow F n (x) (Def. 6) for each
node n and F nc (x) for each edge (n, c), which can be used
to compute the gradients. The unique-path property of a TSPC collapses the summations in the circuit flow recursion
into a simpler chain, resulting in compact expressions for
the flow and gradient, as presented below.

Lemma 1. Consider the unique path from the root n r to a
sum node n in a TS-PC: n r → P 1 [L] → S 1 [L][−][1] →· · · →
P 1 [1] [→] [S] 1 [0] [=][ n][, as shown in Figure 2. Then the flow at node]
n is given by:


Proof. The proof is a straight forward application of the flow
defined on an edge.

For notational simplicity we have labeled every sum and
product-node along our canonical path by index “1”. In a
general TS-PC path, the nodes at layer l would carry their
own index i l, but the same formulas hold by replacing each
“1” with the appropriate i l . The key insight from the above
corollary is that the partial derivative of the likelihood with
respect to θ nc factorizes into exactly the product complements and weights along the unique path from n to the root,
and does not depend on any sum-node outputs on that path.
Let θ nc and θ n ′ c ′ be the weights associated with two distinct edges in a TS-PC. Due to the tree-structure, n and n [′]
either lie on the same path from the root node or share exactly one deepest common ancestor, let us call it q. Thus, the
pair (θ nc, θ n ′ c ′ ) belongs to one of the following three cases:
1) Sum pair: if q is a sum node. 2) Product pair: if q is a
product node. 3) Path pair: if the θ nc and θ n ′ c ′ lie on the
same path from the root. Our main result shows that the second derivative of root likelihood w.r.t the PC parameters can
be expressed in closed form for each of the above cases.

∂ [2] P nr ( x )
Theorem 1. The mixed second derivative
∂θ n′ c′ ∂θ nc [of the]
output of a tree-structured PC with respect to any two parameters θ nc and θ n [′] c [′] equals
0, ∂P∂θ n r nc ( x ) ∂P∂θ n n r′ ( c x ′ ) if it is a sum pair

, if it is a product pair

 θ 1 [k] [P] [ k] 1 [(][x][)] � � Ll=k+1 [θ] 1 [l] [P][¯] [ l] 11 [(][x][)] �


0, if it is a sum pair
∂P n r ( x ) ∂P n r ( x )


∂P n r ( x )


∂θ nc


∂θ n ′ c ′





nc n c

, if it is a product pair
θ 1 [k] [P] [ k] 1 [(][x][)] � � Ll=k+1 [θ] 1 [l] [P][¯] [ l] 11 [(][x][)] �


1

, if it is a path pair with

θ n [′] c [′] [·][ ∂P] ∂θ [n] [r] nc [(] [x] [)]

θ n ′ c ′ closer to the root


P 1 [1] [(] [x] [)]
F n (x) = θ 1 [1]
P n r (x)


L
� θ 1 [j] [P][¯] [ j] 11 [(][x][)]

j=2


Proof. The proof involves unrolling the flow at a sum node
using the tree structure of a TS-PC and the notion of product
complements of the product nodes along the path from the
node to the root, and is provided in the appendix.

Corollary 1. The flow associated with the edge (n, c)
parametrized by θ c [n] [( =][ θ] 1 [0] [in Figure 2) is given by:]


where P 1 [k] [denotes the deepest common product node, and]
we follow our canonical notation for the path to the root.

Proof. Deferred to the appendix.

Theorem 1 can be further extended to obtain equally simple expressions for the Hessian of the log-likelihood which
is typically used as the objective for training PCs, as follows:

Proposition 1. If (θ nc, θ n ′ c ′ ) is a sum pair, then


Proof. Deferred to the appendix.

Thus, the mixed second derivative of the log-likelihood
for a sum pair factorizes into the product of the corresponding first-order derivatives. A special case is when the sum


∂ [2] lo g P n r ( x )



[2] lo g P n r ( x ) = − [F] [nc] [(] [x] [)]

∂θ n ′ c ′ ∂θ nc θ nc


F n ′ c ′ ( x )

(1)
θ n ′ c ′


θ nc


P c ( x )
F nc (x) = θ nc
P n r (x)


L
� θ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=1


-----

pair corresponds to the same parameter (θ nc = θ n ′ c ′ ), in
which case the double derivative equals the square of the
gradient of the log-likelihood with respect to θ nc .

Proposition 2. If (θ nc, θ n ′ c ′ ) is a product pair with P 1 [k] [being]
their deepest common ancestor, then


P n r (x) [F] [nc] [(] [x] [)]

[2] lo g P n r ( x ) = θ nc

∂θ n ′ c ′ ∂θ nc θ [k] [P] [ k] [(][x][)] � L

�


∂ [2] lo g P n r ( x )



[nc] [(] [x] [)] F n ′ c ′ ( x )

θ nc θ n ′ c ′


θ n ′ c ′


θ 1 [k] [P] [ k] 1 [(][x][)] � � Ll=k+1 [θ] 1 [l] [P][¯] [ l] 11 [(][x][)] �



[nc] [(] [x] [)] F n ′ c ′ ( x )

θ nc θ n ′ c ′


− [F] [nc] [(] [x] [)]


θ n ′ c ′


Proof. Deferred to the appendix.

Proposition 3. If (θ nc, θ n ′ c ′ ) is a path pair, with θ n ′ c ′ closer
to the root node, then


∂ [2] lo g P n r ( x )



[2] lo g P n r ( x ) = [∂] [2] [ lo] [g] [ P] [n] [r] [(] [x] [)]

∂θ n ′ c ′ ∂θ nc ∂θ nc ∂θ n ′ c ′


∂θ nc ∂θ n ′ c ′


1

=
θ n ′ c ′ [·][ F] [nc] θ nc [(] [x] [)]



[nc] [(] [x] [)] − [F] [nc] [(] [x] [)]

θ nc θ nc


θ nc


F n ′ c ′ ( x )

θ n ′ c ′


Proof. Deferred to the appendix.

Since each entry of the full Hessian depends only on
the circuit flow, the mixing parameters and the product
complements–all of which can be computed efficiently–the
Hessian as a whole can likewise be evaluated tractably.
#### Hessian for General (DAG-Structured) PCs

To understand whether the tractability of Hessian computation extends to general DAG-structured PCs, we examine
its diagonal and off-diagonal entries separately. Our analysis suggests that while the former can be computed efficiently, the latter can suffer from a combinatorial explosion.
We present the results and defer proofs to the supplementary.

Proposition 4. The diagonal entry of the Hessian of the loglikelihood of a general PC w.r.t a parameter θ nc is given by:


of the two nodes, i.e., d [∗] = min(depth(n), depth(n [′] )) −
depth(deepest common ancestors). Then, the number of
paths connecting n and n [′] can grow as O(w [d] [∗] ). This exponential growth in the number of shared paths indicates that,
in such densely connected PCs, computing off-diagonal entries of the Hessian becomes computationally intractable.
#### A Tractable Sharpness Regularizer for PCs

Although the full Hessian can be intractable for arbitrary
PCs, its trace remains efficiently computable, and serves as a
scalar measure of the overall curvature of the log-likelihood
surface–large values indicating sharper optima, while lower
values correspond to flatter optima (Keskar et al. 2017; Foret
et al. 2021; Kwon et al. 2021). Thus, reducing the Hessian
trace during training can serve as an effective regularization strategy. While the full Hessian can be computed efficiently and potentially incorporated as a regularizer for treestructured PCs, we focus instead on its trace, as it is both
simpler to compute and applicable to the general class of
PCs. Crucially, for any PC (not just tree-structured), the absolute trace (ignoring absolute henceforth) simplifies to the
sum of squared partial derivatives:


� 2


Tr �∇ [2] log P n r (x)� = �

n,c

=
�

n,c


∂ lo g P n r ( x )
� ∂θ nc


F nc ( x )
� θ nc


� 2


This enables sharpness-aware regularization using
only first-order derivatives that can be computed using the edge flows, while still promoting flatter solutions during training. A simple way to incorporate
this into gradient-based learning is to add the Hessian
trace as a regularizer R(θ, x) = � n,c [(][F] [nc] [(][x][)][/θ] [nc] [)] [2]


x∈D [log][ P] [n] [r] [(][x][)] + λ [�]


∂ [2] lo g P n r ( x )


∂ g [2] Pθ ncn r ( x ) = − � F nc θ nc ( x )


� 2


θ nc


Proof. Deferred to the appendix.

From Proposition 4, we observe that computing the trace
only requires access to the edge flows and the mixing parameters. From Definition 6, all edge flows can be computed with a single forward and backward pass through the
circuit, making the flow computation linear in time with
respect to the number of edges. Consequently, the overall
trace computation is also linear in time. However, for general
PCs, we conjecture that computing the off-diagonal entries
of the Hessian is intractable due to the exponential number of dependency paths between parameters. Concretely,
consider a PC where each internal node (except the root
and its immediate children) has up to w parents. Suppose
that sum nodes n and n [′] share w deepest common ancestors at the same depth, and let d [∗] denote the number of layers between these deepest common ancestors and the lower


This enables sharpness-aware regularization using
only first-order derivatives that can be computed using the edge flows, while still promoting flatter solutions during training. A simple way to incorporate
this into gradient-based learning is to add the Hessian
trace as a regularizer R(θ, x) = � n,c [(][F] [nc] [(][x][)][/θ] [nc] [)] [2]

to the negative log-likelihood, yielding the objective
min θ − [�] x∈D [log][ P] [n] [r] [(][x][)] + λ [�] x∈D [R][(][θ, x][)][.][ Since]

R(θ, x) depends only on the local flows and weights, its
gradients can be computed with a forward-backward pass,
making integration with optimizers like SGD or Adam
straightforward. However, EM is often preferred over gradient descent to learn PCs as it achieves faster convergence
(Desana and Schn¨orr 2017). Thus, we next discuss how to
integrate this curvature penalty into EM-based learning.

Sharpness-Aware EM. To endow EM with sharpness
awareness, we propose to add the Hessian-trace regularizer
into the M-step by constraining the sum squared gradients
at each sum node. More formally, the M-step optimization
is now carried out under two constraints: (1) the parameters
at each sum node must lie on the probability simplex, i.e.,
� c∈in(n) [θ] [nc] [ = 1][, and (2) the trace of the Hessian is upper]

bounded, i.e., [�] c∈in(n) [(][F] [nc] [(][x][)][/θ] [nc] [)] [2] [ f][ m][ for some][ m][.]

The resulting parameter update takes the following form:

Proposition 5. The EM update for a parameter θ nc at a sum
node n, under a Hessian trace-based sharpness regularizer,
is the solution to the cubic equation:

λ θ nc [3] [−] [F] [nc] [(][x][)][ θ] nc [2] [−] [2][ µF] [nc] [(][x][)] [2] [ = 0][,] (2)


-----

Table 1: Percentage improvements in test negative log-likelihood (∆NLL), reduction in overfitting (∆DoF ), and percentage
decrease in the loss-surface sharpness (∆Sharp) for an Einsum Network on the synthetic manifold datasets, comparing
models with Hessian-trace regularization against vanilla counterparts. Values are averaged across 5 independant runs.

Dataset 1% 5% 10% 50% 100%

∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp

bent lissajous 46.65 65.62 93.41 19.45 28.23 52.87 18.16 22.21 56.44 1.27 2.01 34.58 0.65 −0.06 31.25
helix 32.38 62.87 91.80 17.92 35.29 57.51 10.85 5.48 21.48 8.93 −1.90 13.76 7.33 −1.76 15.71
interlocked circles 41.19 68.16 88.06 26.54 39.14 75.72 18.65 14.55 46.30 2.20 1.14 34.15 1.02 0.40 41.59
knotted 52.66 59.88 91.01 26.68 42.31 65.46 17.06 9.42 31.12 5.67 1.23 25.44 10.07 −0.01 21.57
pinwheel 57.29 61.85 93.85 22.62 23.27 66.14 13.24 17.58 75.54 3.70 6.14 76.26 0.38 0.77 79.01
spiral 62.11 69.82 92.23 34.03 37.47 67.09 22.62 20.49 61.48 19.83 1.38 12.66 18.35 3.78 23.72
twisted eight 49.03 70.20 75.09 13.65 22.31 63.21 11.22 12.54 51.52 2.72 2.42 32.11 −1.22 −0.04 40.85
two moons 54.92 67.30 88.47 27.99 30.24 56.63 19.72 17.88 34.10 16.22 −0.13 30.68 3.80 0.85 28.31

Mean 49.53 65.71 89.24 23.61 32.28 63.08 16.44 15.02 47.25 7.57 1.54 32.45 5.05 0.49 35.25


where F nc (x) is the expected edge flow, and λ, µ are
Lagrange multipliers for the normalization and trace constraints, respectively.

Proof. Deferred to the appendix.

Incorporating the Hessian trace into the EM update thus
yields, at each sum node, a cubic equation in the parameters
that must be solved exactly. However, solving such cubic
equations can be computationally expensive and may yield
multiple real roots, negative values, or even complex solutions, making the update process unstable or infeasible. To
overcome this, we exploit a key property of PC gradients:
the partial derivative of the log-likelihood with respect to
a sum node parameter θ nc takes the form [F] [nc] θ nc [(] [x] [)] [, which is]

always non-negative as both F nc (x) and θ nc are positive.
Consequently, the squared gradient is a monotonic function
of the gradient itself, thus minimizing the gradient suffices
to reduce its square. This allows us to directly penalize the
gradient as a surrogate for reducing the trace, resulting in a
simpler quadratic update with a closed-form solution.

Theorem 2. The EM parameter update at sum node n, under
the gradient regularized objective is given by:


4.0

3.5

3.0

2.5

2.0

1.5

1.0

0.5


0 50 100 150 200

Epoch


2 3 4 5 6 7
Model Depth


10 [3]

10 [2]

10 [1]


1.2

1.0

0.8

0.6

0.4

0.2

0.0


θ nc = [F] [nc] [(] [x] [)] [ +] �


F nc ( x ) [2] + 4λ µ F nc ( x )

2λ,


where F nc (x) denotes the flow along the edge from sum
node n to its child c, and λ, µ g 0 are the Lagrange multipliers corresponding to the normalization and regularization
constraints, respectively.

Proof. Deferred to the appendix.
### Experiments and Results

We organize our empirical evaluation to answer the following four research questions.

(Q1) Do large, expressive PCs overfit on limited data, and
does sharpness, defined via Hessian-trace capture this?

(Q2) Is our derived sum-squared-gradient expression for
the Hessian-trace correct and efficient to compute?


Figure 3: [Left] Training and validation negative loglikelihood of EinsumNet on a 2D spiral dataset across
epochs, with Hessian-trace computed by our sum-squaredgradients formula and torch autograd. [Right] Time taken for
computing the Hessian trace as the network depth grows.

(Q3) Does the proposed sharpness aware-learning framework reduce overfitting and improve generalization?

(Q4) What effect does the regularization strength µ have?

Setup. To answer these, we conducted experiments on 8
synthetic 2D/3D manifold datasets (Sidheekh et al. 2022;
Sidheekh, Kersting, and Natarajan 2023) as well as the 20
standard binary density estimation benchmark (Van Haaren
and Davis 2012; Bekker et al. 2015). To show that our approach applies broadly across different PC model classes
and implementations, we integrated it into two widely used
PC frameworks—Einsum Networks (Peharz et al. 2020) and
PyJuice (Liu, Ahmed, and den Broeck 2024), evaluating it
on different structural settings. For synthetic datasets, we
use a fixed RAT-SPN (Peharz et al. 2019) architecture with
10 input-distributions, sum-nodes and num-repetitions. For
the binary datasets, we adopt the Hidden Chow-Liu Tree
(HCLT) structure from PyJuice, with a latent size of 100 to
increase model capacity. To simulate limited data settings
where overfitting can happen, we train each model on random subsets of each dataset at fractions {1%, 5%, 10%, 50%
and 100% }. To show applicability across learning methods,
we integrated our regularizer into two settings: (1) gradient

-----

Table 2: Percentage improvements in test negative log-likelihood (∆NLL), reduction in overfitting (∆DoF ), and percentage
decrease in the loss-surface sharpness (∆Sharp) for a PyJuice HCLT model on the binary density estimation datasets,
comparing models with Hessian-trace regularization against vanilla counterparts. Values represent the mean over 5 runs.

Dataset 1% 5% 10% 50% 100%

∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp ∆NLL ∆DoF ∆Sharp

accidents 1.40 6.94 15.61 −1.88 1.54 16.54 −1.67 0.67 12.33 −0.50 0.04 12.03 −0.29 0.01 3.39
ad 2.11 1.24 56.79 3.20 3.54 78.44 1.26 2.65 27.21 −2.26 1.42 10.80 −1.85 0.43 9.31
baudio 9.39 24.87 9.33 −0.58 1.30 3.06 −0.56 0.35 5.17 −0.20 0.00 1.19 −0.11 0.00 0.88
bbc 10.45 3.52 32.12 15.75 19.70 25.16 8.75 19.57 28.21 −0.23 1.32 11.47 −0.37 0.25 10.74
bnetflix 3.07 10.71 −1.65 −0.38 0.29 2.94 −0.31 0.12 2.68 −0.02 0.00 0.30 −0.01 0.00 0.14
book 9.56 6.11 74.56 1.47 4.15 29.80 −0.60 1.09 21.63 −0.83 0.03 10.23 −0.48 0.02 7.63
c20ng 11.56 13.28 31.06 0.78 3.03 25.84 0.06 0.84 10.53 −0.28 0.01 6.30 −0.12 0.01 3.54
cr52 10.54 9.76 35.21 3.68 12.54 −2.54 −0.10 4.04 3.47 −0.57 0.24 3.45 −0.31 0.04 2.65
cwebkb 9.05 4.05 30.95 14.17 26.46 30.52 3.83 12.36 28.79 −0.63 0.39 11.21 −0.46 0.11 5.29
dna 29.45 15.54 3.87 2.59 6.88 13.93 0.10 2.30 8.95 −0.89 0.25 8.81 −0.47 0.07 3.91
jester 19.22 28.28 25.52 0.24 3.81 −2.60 −0.71 0.36 −3.30 −0.24 0.04 0.88 −0.12 0.00 −0.89
kdd 0.04 1.51 11.96 0.11 0.16 3.50 0.10 0.08 7.49 −0.02 0.00 3.65 0.00 0.00 2.20
kosarek 1.93 6.84 23.70 −0.76 0.60 13.33 −0.42 0.27 7.90 −0.20 0.03 7.51 −0.07 0.00 5.66
msnbc −0.10 0.03 0.35 0.02 −0.01 0.04 −0.02 0.00 0.07 −0.01 0.00 0.06 0.00 0.00 0.06
msweb 1.11 3.75 33.03 −0.23 0.79 18.57 −0.29 0.45 7.33 −0.08 0.04 10.80 −0.03 0.03 6.54
nltcs 0.18 2.31 7.20 −0.94 0.35 4.80 −0.68 0.06 2.52 −0.08 0.00 −1.04 −0.07 0.00 −0.58
plants 1.59 7.19 13.72 −2.41 0.95 10.34 −1.63 0.38 8.02 −0.56 0.02 3.56 −0.28 0.01 3.50
pumsb star 2.04 4.83 8.62 −2.76 1.09 16.12 −1.69 0.51 8.68 −0.50 0.05 5.50 −0.29 0.02 4.33
tmovie 15.15 15.90 16.97 12.01 28.27 −3.17 0.02 8.32 9.83 −1.42 0.34 3.55 −0.82 0.12 11.08
tretail 4.55 9.60 31.95 −0.25 0.44 21.81 −0.10 0.12 10.93 −0.03 0.00 0.69 −0.02 0.00 1.44

Mean 7.12 8.81 23.04 2.19 5.79 15.32 0.27 2.73 10.42 −0.48 0.21 5.55 −0.31 0.06 4.04


based learning for Einsum Networks (using Adam) and (2)
EM-based learning for PyJuice HCLTs (using our quadratic
closed-form updates). We defer further experimental results
and details to the supplementary material.

(Q1) Overfitting and Sharpness: To show that deep PCs
indeed overfit when data is scarce, we plot the train and
validation negative log-likelihoods (NLL) for an EinsumNet trained on a 5% data for 2D spiral distribution in Figure 3[left]. We see that the train-NLL continues to decrease,
while the val-NLL rises, indicating overfitting and a widening generalization gap. Crucially, the value of sharpness,
computed via Hessian-trace also grows in tandem with this
gap, peaking when overfitting occurs. This confirms that
sharp minima that correlate with overfitting do exist in PCs,
and that the Hessian-trace is capable of detecting them.

(Q2) Correctness & Efficiency of Hessian-Trace Computation. We empirically validated the correctness of our
Hessian trace derivation using Einsum Networks, which
support full Hessian evaluation via PyTorch’s automatic differentiation. Figure 3[left] shows an exact match between
the Hessian trace computed using our proposed sum-ofsquared-gradients (SSG) formula and the one obtained directly via autograd on the 2D spiral dataset, where full Hessian computation was feasible without exceeding memory
limits. This confirms the correctness of our derivation. Figure 3[right] also compares the computation time of the Hessian trace using autograd and our closed-form SSG formula.
We see that as the model depth increases, autograd’s runtime
suffers from an exponential blow up, while our SSG formula
scales only linearly, and is thus a more practical and accurate
way to analyze the curvature, even for deep PCs.


(Q3) Gains from Sharpness-Aware Learning. To study
the effect our sharpness regularizer has in learning better
PCs, we measured the performance improvement achieved
by our regularized model as compared to its vanilla counterpart using three metrics
(1) The relative reduction in Test-NLL [∆NLL(%)]

(2) The reduction in the degree of overfitting [∆DoF (%)],
where DoF = [NLL] [test] [−] [NLL] [train]

|NLL train |
(3) The relative reduction in sharpness [∆Sharp(%)], as
measured by our Hessian-trace formula, at convergence.

Table 1 reports the mean results over five runs for an EinsumNet trained on the synthetic 2D/3D manifold datasets at
varying training-set fractions. We observe that in the lowest
data setting, on average, our regularizer cuts overfitting by
up to 65 %, flattens the loss surface by 89%, and boosts test
log-likelihood by upto 49%. Although the absolute gains diminish with more data, they remain positive across all fractions on average, demonstrating that trace minimization consistently guides the learning toward better-generalizing optima. Table 2 presents analogous results for PyJuice HCLTs
on real-world binary datasets using our regularized EM.
Again, in the lowest-data regime we observe a 7 % improvement in test NLL, an 8% reduction in overfitting, and a 23%
decrease in sharpness, on average. As dataset size grows,
these gains plateau—and at the highest data fractions we
record a marginal (< 0.5%) drop in test NLL. This is expected when overfitting is negligible as the regularizer may
push the parameters away from an otherwise sufficient optimum. But even in these settings, our method continues to reduce overfitting and sharpness, confirming its effectiveness
at steering PCs toward flatter, more robust solutions.


-----

×10 [2]


×10 [2]


×10 [3]


1

1

0.1

Regularization Strength (µ)

Figure 4: Ablation showing the effect of µ on the validation
negative log-likelihood, degree of overfitting and sharpness.

(Q4) Ablation on µ. To study the sensitivity of our framework to the regularization strength, we trained a PyJuice
HCLT using the 5% split of DNA binary dataset, for varying values of µ and tracked changes in Validation-NLL, degree of overfitting and sharpness (in Figure 4). We observe
that even a small µ ∈ (0, 0.1] is sufficient to capture most of
the gains-reducing overfitting and curvature. Larger µ values
yield only marginal gains at the cost of slight underfitting.
Thus, our framework is robust to the choice of µ in a broad
mid-range and we select it based on validation performance.
### Conclusion

In this work, we introduced a new direction to study the
training of PCs through the lens of the log-likelihood surface geometry. We derived a closed-form expression for the
exact full Hessian of the log-likelihood in tree-structured
PCs and demonstrated its tractability. For general DAGstructured PCs, we showed that while the full Hessian
can be intractable, its trace remains exactly and efficiently
computable– offering the first scalable curvature measure
for training large PCs. Building on this, we designed a
novel regularizer whose equivalent gradient-norm formulation yields closed-form quadratic updates, enabling efficient
optimization. Our experiments confirmed that our approach
steers training toward flatter minima and reduces overfitting,
especially in low-data regimes. Overall, our work opens up
a promising new direction for studying PCs. We forsee future work investigating the log-likelihood landscape to identify the presence of asymmetric valleys analogous to those
observed in DNNs, developing a theoretical framework for
understanding convergence in over-parameterized PCs, and
designing alternative optimization strategies that leverage
tractable second-order geometric information.
### Acknowledgements

SN and SS gratefully acknowledge the generous support
by the AFOSR award FA9550-23-1-0239, the ARO award
W911NF2010224 and the DARPA Assured Neuro Symbolic
Learning and Reasoning (ANSR) award HR001122S0039.
CK and HS gratefully acknowledge Anagha Sabu for the
discussions related to the work and CK, HS, and VS thank
IIT Palakkad for the access to Madhava Cluster.

### References

Ahmed, K.; Chang, K.-W.; and Van den Broeck, G. 2023.
Semantic strengthening of neuro-symbolic learning. In International Conference on Artificial Intelligence and Statistics, 10252–10261. PMLR.

Ahmed, K.; Teso, S.; Chang, K.; den Broeck, G. V.; and
Vergari, A. 2022. Semantic Probabilistic Layers for NeuroSymbolic Learning. In Koyejo, S.; Mohamed, S.; Agarwal,
A.; Belgrave, D.; Cho, K.; and Oh, A., eds., Advances in
Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022.

Bekker, J.; Davis, J.; Choi, A.; Darwiche, A.; and Van den
Broeck, G. 2015. Tractable learning for complex probability queries. Advances in Neural Information Processing Systems, 28.

B¨ottcher, L.; and Wheeler, G. 2024. Visualizing highdimensional loss landscapes with Hessian directions. Journal of Statistical Mechanics: Theory and Experiment.

Chaudhari, P.; Choromanska, A.; Soatto, S.; LeCun, Y.; Baldassi, C.; Borgs, C.; Chayes, J. T.; Sagun, L.; and Zecchina,
R. 2017. Entropy-SGD: Biasing Gradient Descent Into Wide
Valleys. In 5th International Conference on Learning Representations, 2017.

Choi, Y.; Vergari, A.; and den Broeck, G. V. 2020. Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models.

Correia, A. H.; Gala, G.; Quaeghebeur, E.; De Campos, C.;
and Peharz, R. 2023. Continuous mixtures of tractable probabilistic models. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 37, 7244–7252.

Dang, M.; Liu, A.; and den Broeck, G. V. 2022. Sparse
Probabilistic Circuits via Pruning and Growing. In Koyejo,
S.; Mohamed, S.; Agarwal, A.; Belgrave, D.; Cho, K.; and
Oh, A., eds., Advances in Neural Information Processing
Systems 35: Annual Conference on Neural Information Processing Systems, 2022.

Darwiche, A. 2003. A Differential Approach to Inference in
Bayesian Networks. Journal of the ACM, (3): 280–305.

Desana, M.; and Schn¨orr, C. 2017. Learning Arbitrary SumProduct Network Leaves with Expectation-Maximization.
arXiv:1604.07243.

Foret, P.; Kleiner, A.; Mobahi, H.; and Neyshabur, B. 2021.
Sharpness-aware Minimization for Efficiently Improving
Generalization. In 9th International Conference on Learning Representations, 2021.

Gal, Y.; and Ghahramani, Z. 2016. Dropout as a Bayesian
Approximation: Representing Model Uncertainty in Deep
Learning. In Balcan, M.; and Weinberger, K. Q., eds., Proceedings of the 33nd International Conference on Machine
Learning, 2016, JMLR Workshop and Conference Proceedings, 1050–1059.

Gala, G.; de Campos, C. P.; Peharz, R.; Vergari, A.; and
Quaeghebeur, E. 2024. Probabilistic Integral Circuits. In
Dasgupta, S.; Mandt, S.; and Li, Y., eds., International Conference on Artificial Intelligence and Statistics, 2024, Proceedings of Machine Learning Research, 2143–2151.


-----

Goodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A. C.; and Bengio,
Y. 2014. Generative Adversarial Nets. In Ghahramani, Z.;
Welling, M.; Cortes, C.; Lawrence, N. D.; and Weinberger,
K. Q., eds., Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, 2014, 2672–2680.

Hochreiter, S.; and Schmidhuber, J. 1997. Flat minima. Neural computation, (1): 1–42.

Hutchinson, M. F. 1990. A stochastic estimator of the
trace of the influence matrix for laplacian smoothing splines.
Communications in Statistics - Simulation and Computation.

Karanam, A.; Mathur, S.; Sidheekh, S.; and Natarajan, S.
2025. A Unified Framework for Human-Allied Learning of
Probabilistic Circuits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 17779–17787.

Kaur, S.; Cohen, J.; and Lipton, Z. C. 2023. On the Maximum Hessian Eigenvalue and Generalization. In Proceedings of Machine Learning Research.

Keskar, N. S.; Mudigere, D.; Nocedal, J.; Smelyanskiy, M.;
and Tang, P. T. P. 2017. On Large-Batch Training for Deep
Learning: Generalization Gap and Sharp Minima. In 5th International Conference on Learning Representations, 2017.

Kingma, D. P.; and Welling, M. 2014. Auto-Encoding Variational Bayes. In Bengio, Y.; and LeCun, Y., eds., 2nd International Conference on Learning Representations, 2014.

Kisa, D.; Broeck, G. V. D.; Choi, A.; and Darwiche, A. 2014.
Probabilistic sentential decision diagrams. In Proceedings of
the International Conference on Knowledge Representation
and Reasoning.

Kwon, J.; Kim, J.; Park, H.; and Choi, I. K. 2021.
ASAM: Adaptive Sharpness-Aware Minimization for ScaleInvariant Learning of Deep Neural Networks. In Meila, M.;
and Zhang, T., eds., Proceedings of the 38th International
Conference on Machine Learning, 2021, Proceedings of Machine Learning Research, 5905–5914.

Li, H.; Xu, Z.; Taylor, G.; Studer, C.; and Goldstein, T. 2018.
Visualizing the Loss Landscape of Neural Nets. In Bengio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, 6391–6401.

Liu, A.; Ahmed, K.; and den Broeck, G. V. 2024. Scaling
Tractable Probabilistic Circuits: A Systems Perspective. In
Forty-first International Conference on Machine Learning,
2024.

Liu, A.; and den Broeck, G. V. 2021. Tractable Regularization of Probabilistic Circuits. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds.,
Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems,
2021, 3558–3570.

Liu, A.; Mandt, S.; and den Broeck, G. V. 2022. Lossless
Compression with Probabilistic Circuits. In International
Conference on Learning Representations.


Liu, A.; Niepert, M.; and den Broeck, G. V. 2024. Image Inpainting via Tractable Steering of Diffusion Models. In The
Twelfth International Conference on Learning Representations, 2024.

Liu, A.; Zhang, H.; and den Broeck, G. V. 2023. Scaling Up
Probabilistic Circuits by Latent Variable Distillation. In The
Eleventh International Conference on Learning Representations, 2023.

Loconte, L.; Di Mauro, N.; Peharz, R.; and Vergari, A. 2023.
How to turn your knowledge graph embeddings into generative models. Advances in Neural Information Processing
Systems, 36: 77713–77744.

Loconte, L.; Mari, A.; Gala, G.; Peharz, R.; de Campos, C.;
Quaeghebeur, E.; Vessio, G.; and Vergari, A. 2025. What is
the Relationship between Tensor Factorizations and Circuits
(and How Can We Exploit it)? Transactions on Machine
Learning Research.

Loconte, L.; Mengel, S.; and Vergari, A. 2025. Sum of
squares circuits. In Proceedings of the AAAI Conference on
Artificial Intelligence, 18, 19077–19085.

Loconte, L.; Sladek, A. M.; Mengel, S.; Trapp, M.; Solin,
A.; Gillis, N.; and Vergari, A. 2024. Subtractive Mixture
Models via Squaring: Representation and Learning. In The
Twelfth International Conference on Learning Representations, 2024.

Martens, J.; Sutskever, I.; and Swersky, K. 2012. Estimating
the Hessian by Back-propagating Curvature. In Proceedings
of the 29th International Conference on Machine Learning,
2012.

Papamakarios, G.; Nalisnick, E. T.; Rezende, D. J.; Mohamed, S.; and Lakshminarayanan, B. 2021. Normalizing
Flows for Probabilistic Modeling and Inference. Journal of
Machine Learning Research, 57:1–57:64.

Peharz, R.; Lang, S.; Vergari, A.; Stelzner, K.; Molina, A.;
Trapp, M.; den Broeck, G. V.; Kersting, K.; and Ghahramani, Z. 2020. Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits. In Proceedings of the
37th International Conference on Machine Learning, 2020,
Proceedings of Machine Learning Research, 7563–7574.

Peharz, R.; Vergari, A.; Stelzner, K.; Molina, A.; Trapp, M.;
Shao, X.; Kersting, K.; and Ghahramani, Z. 2019. Random
Sum-Product Networks: A Simple and Effective Approach
to Probabilistic Deep Learning. In Globerson, A.; and Silva,
R., eds., Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, 2019, Proceedings of Machine Learning Research, 334–344.

Poon, H.; and Domingos, P. M. 2011. Sum-Product Networks: A New Deep Architecture. In Cozman, F. G.; and
Pfeffer, A., eds., Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, 2011, 337–
346.

Rahman, T.; Kothalkar, P.; and Gogate, V. 2014. Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of chow-liu trees. In Machine Learning
and Knowledge Discovery in Databases: European Conference, 2014, 630–645. Springer.


-----

Sankar, A. R.; Khasbage, Y.; Vigneswaran, R.; and Balasubramanian, V. N. 2021. A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to
Regularization. In Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, The Eleventh
Symposium on Educational Advances in Artificial Intelligence, 9481–9488.

Shih, A.; Sadigh, D.; and Ermon, S. 2021. HyperSPNs:
Compact and Expressive Probabilistic Circuits. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and
Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, 2021, 8571–8582.

Sidheekh, S.; Dock, C. B.; Jain, T.; Balan, R.; and Singh,
M. K. 2022. VQ-Flows: Vector quantized local normalizing
flows. In Uncertainty in Artificial Intelligence, 1835–1845.
PMLR.

Sidheekh, S.; Kersting, K.; and Natarajan, S. 2023. Probabilistic Flow Circuits: Towards Unified Deep Models for
Tractable Probabilistic Inference. In Evans, R. J.; and Shpitser, I., eds., Uncertainty in Artificial Intelligence, 2023,
Proceedings of Machine Learning Research, 1964–1973.

Sidheekh, S.; and Natarajan, S. 2024. Building Expressive
and Tractable Probabilistic Generative Models: A Review.
In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, 2024, 8234–8243.

Sidheekh, S.; Tenali, P.; Mathur, S.; Blasch, E.; Kersting, K.;
and Natarajan, S. 2025. Credibility-Aware Multimodal Fusion Using Probabilistic Circuits. In The 28th International
Conference on Artificial Intelligence and Statistics.

Van Haaren, J.; and Davis, J. 2012. Markov network structure learning: A randomized feature generation approach.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, 1148–1154.

Ventola, F.; Braun, S.; Yu, Z.; Mundt, M.; and Kersting, K.
2023. Probabilistic circuits that know what they don’t know.
In Evans, R. J.; and Shpitser, I., eds., Uncertainty in Artificial Intelligence, 2023, Proceedings of Machine Learning
Research, 2157–2167.

Vergari, A.; Choi, Y.; Liu, A.; Teso, S.; and Van den Broeck,
G. 2021. A compositional atlas of tractable circuit operations for probabilistic inference. Advances in Neural Information Processing Systems, 34: 13189–13201.

Wang, B.; and Van den Broeck, G. 2025. On the relationship between monotone and squared probabilistic circuits.
In Proceedings of the AAAI Conference on Artificial Intelligence, 20, 21026–21034.

Zhang, H.; Dang, M.; Peng, N.; and den Broeck, G. V.
2023. Tractable Control for Autoregressive Language Generation. In Krause, A.; Brunskill, E.; Cho, K.; Engelhardt,
B.; Sabato, S.; and Scarlett, J., eds., International Conference on Machine Learning,2023, Proceedings of Machine
Learning Research, 40932–40945.

Zhang, H.; Dang, M.; Wang, B.; Ermon, S.; Peng, N.; and
Van den Broeck, G. 2025. Scaling Probabilistic Circuits via


Monarch Matrices. In Proceedings of the 42th International
Conference on Machine Learning.

Zhang, H.; Kung, P.-N.; Yoshida, M.; Van den Broeck, G.;
and Peng, N. 2024. Adaptable logical control for large language models. Advances in Neural Information Processing
Systems, 37: 115563–115587.


-----

### Supplementary Material: Tractable Sharpness-Aware Learning of Probabilistic Circuits A Proofs for Theoretical Results

In this section we provide complete, self-contained proofs for all of the key lemmas, corollaries and theorems stated in the
main paper. We begin by showing how the unique path structure of a Tree-Structured Probabilistic Circuit (TS-PC) allows us to
“unroll” circuit flows into products of edge weights and product complements. We then derive closed-form expressions for the
first and second derivatives in TS-PCs, before turning to the tractable Hessian trace computation for general (non-tree) PCs.

Notation and Preliminaries. Recall that for any PC node n and input x we write F n (x) for its circuit flow (Definition 6 in
the main text), and for any sum-node parameter ¹ nc the corresponding edge flow is

P c ( x )
F nc (x) = ¹ nc
P n (x) [F] [n] [(][x][)][.]

We will also use the notion of a product complement and product double complement as defined below:

Definition 1 (Product Complement). Let P i [l] [be a product node at layer][ l][ with children][ {][S] j [l][−][1] } j∈in(P li [)] [. Its product complement]
with respect to child S j [l][−][1] is defined as

l
P ij [(][x][) =] � S k [l][−][1] (x) .

k∈in(P i [l] [)][, k][̸][=][j]

Definition 2 (Product Double Complement). The product double complement of a product node P i [l] [with respect to two of its]
children S j [l][−][1] and S k [l][−][1] is defined as:


l
P

ijk [(][x][) =] �

m∈in(P i [l] [)]
m≠ j,k


S m [l][−][1] [(][x][)] (1)


In other words, the product double complement of a product node with respect to two of its children is the product of the outputs
of all the other children except the two under consideration.
#### A.1 Full Hessian Computation for Tree Structured PCs

Lemma 1. Consider a TS-PC path from the root to the sum node n as depicted in the Figure 1a. Then, the flow of n is given by


P 1 [1] [(] [x] [)]
F n (x) = ¹ 1 [1]
P n r (x)


L
� ¹ 1 [j] [P][¯] [ j] 11 [(][x][)] (2)

j=2


Proof. In a tree-structured PC, every non-root node has a unique path from the root. Let

n r → P 1 [L] → S 1 [L][−][1] →· · · → P 1 [1] [→] [S] 1 [0] [=][ n]

be such a canonical path to a sum node n = S 1 [0] [, assuming][ n][ to be at level][ 0][ and the root node][ n] [r] [to be at level][ L][ as illustrated]
in Figure 1a. For notational simplicity we have labeled every sum and product-node along our canonical path by index “1”. In
a general TS-PC path, the nodes at layer l would carry their own index i l, but the same results and derivations hold by replacing
each “1” with the appropriate i l .


-----

#### �㗅= �㗃 �㗅= �㗅

#### �㗅 ' = �㗄 !%



#### �㗄 !% �㗄 !%&! �㗄 !#$! �㗄 !# �㗄 !# �㗄 !! �㗄 !! �㗅 = �㗄 !" �㗅 = �㗄 !"

#### �㗰 () = �㗰 !"

**.**

**.**

**.**


**.**

**.**

**.**



**.**

**.**

**.**


#### �㗰 !# �㗰 !!



#### �㗅= 1 �㗅= 0


(a) A typical (unique) path structure from the root node n r (= S 1 [L] [)][ to a sum node][ n][(=][ S] 1 [0] [)][ in a tree structured PC. We group the alternating]
sum and product node layers into a single joint level l for ease of analysis.





**.**

**.**










**.**

**.**



(d) Path pair


**.**

**.**


**.**

**.**

(b) Sum pair (c) Product pair


Figure 1: (a) The unique path from the root n r to a sum node n in a Tree-Structured PC (TS-PC). (b–d) The three possible
relationships between a pair of edges (¹ nc, ¹ n ′ c ′ )—sum pair, product pair, and path pair—used in our Hessian derivation.


-----

We can unroll the flow at n using the notion of product complements of the product nodes along the path from n to n r . By
definition of a flow,
F n (x) =F c (x) = F P 11 [(][x][)] (sinceP 1 [1] [is the unique parent of][ n][)]

=¹ 1 [1] PS 11 [1][1] [(] [(] [x][x][)] [)] [ F] [S] 1 [1] [(][x][)]

=¹ 1 [1] PS 11 [1][1] [(] [(] [x][x][)] [)] [ ¹] 1 [2] PS 11 [2][2] [(] [(] [x][x][)] [)] [ . . . ¹] 1 [k] PS 11 [k][k] [(] [(] [x][x][)] [)] [ F] [S] 1 [k] [(][x][)]


j∈in(P 1 [2] [)] [ S] j [1] [(][x][)]

S 1 [2] [(][x][)] . . . ¹ 1 [k]


�


j∈in(P 1 [k] [)] [ S] j [k][−][1] (x)

S 1 [k] [(][x][)] F S 1k [(][x][)]


=¹ 1 [1] PS 11 [1][1] [(] [(] [x][x][)] [)] [ ¹] 1 [2]


�


=¹ 1 [1] PS 11 [k][1] [(] [(] [x][x][)] [)]

=¹ 1 [1] SP 11 [L][1] [(] [(][x][x] [)] [)]


k
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)][F] S 1 [k] [(][x][)]

l=2


L
� ¹ 1 [l] [P][¯] 11 [ l] [(][x][)][F] S 1 [L] [(][x][)]

l=2


=¹ 1 [1] PP n1 [1] r [(] ( [x] x [)] )


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=2


Corollary 1. Under the same TS-PC path structure, the flow associated with the edge (nc) that is parametrized by ¹ nc (=¹ 1 [0] [in]
Figure 1a) is given by


P c ( x )
F nc (x) = ¹ nc
P n r (x)


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)] (3)

l=1


and hence, the gradient of the likelihood at the root node with respect to the parameter ¹ c [n] [can be written as]


∂P n r ( x ) =P c (x)

∂¹ nc


∂P n r ( x )


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)] (4)

l=1


Proof. The proof is a straight forward application of Lemma 1 and the flow defined on an edge:


F nc (x) = ¹ nc . ∂ lo g P n r ( x )
� ∂¹ nc


�


The key insight from the above corollary is that the derivative of the likelihood with respect to a mixing coefficient contains
only weights and product complements at every layer; i.e., it does not include the output of the sum nodes along the path from
the root to the node n. Let ¹ nc and ¹ n ′ c ′ be the weights associated with two distinct edges in a TS-PC. Then, the pair (¹ nc, ¹ n ′ c ′ )
belongs to one of the three following pairs as illustrated in Figure 1:

 - Sum pair (Figure 1b): If the deepest common ancestor from the root node is a sum node.

 - Product pair (Figure 1c): If the deepest common ancestor from the root node is a product node.

 - Path pair (Figure 1d): If the edges corresponding to ¹ nc and ¹ n ′ c ′ lies on the same path from the root to a terminal node of
the TS-PC.

Now, depending on how the two edges relate in the tree, we show that the mixed second partial derivatives take one of the
following three closed forms.
Theorem 1. Let ¹ nc, ¹ n ′ c ′ be two distinct sum-node weights in a TS-PC. Then, the double derivative of the likelihood of a PC
with respect to (¹ nc, ¹ n ′ c ′ ) can be defined as:


1

, if ¹ nc and ¹ n ′ c ′ are path pairs where ¹ n ′ c ′ is closer to the root node

¹ n ′ c ′ [·][ ∂P] ∂¹ [n] [r] nc [(] [x] [)]


0, if ¹ nc and ¹ n ′ c ′ are sum pairs
∂P n r ( x ) ∂P n r ( x )


∂ [2] P nr ( x ) =
∂¹ n ′ c ′ ∂¹ nc





nc n [′] c [′]

¹ 1 [k] [P] [ k] 1 [(][x][)] � � Ll=k+1 [¹] 1 [l] [P][¯] 11 [ l] [(][x][)] �, if ¹ nc and ¹ n ′ c ′ are product pairs and P 1 [k] [denotes the common product node]


∂¹ nc


∂¹ n [′] c [′]


-----

Proof. Each case follows by differentiating the product form from Corollary 1 once more and observing which factors depend
on which parameter. More formally, from Corollary 1 we have,


∂P n r ( x ) = P c (x)

∂¹ nc


∂P n r ( x )


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=1



- Sum Pair: Let S 1 [k] [be the deepest common ancestor of][ ¹] [nc] [and][ ¹] [n] [′] [c] [′] [. As illustrated in Figure 1b, let us use][ ³i][ to denote the]
level of nodes in the path from S 1 [k] [to][ n][. Then, the derivative can be written as]


�


�


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k


∂P n r ( x ) = P c (x)

∂¹ nc


∂P n r ( x )


³u
� ¹ 1 [l] [P][¯] 11 [ l] [(][x][)]
� l=³1


³u
�
� l=³


� �� � ~~�~~ � ~~�~~ �

Term 1 Term 2


Clearly, Term 1 consists only the parameters and product complements in the path from n to S 1 [k] [. Therefore, it is independent]
of ¹ n ′ c ′ . Term 2 consists of product complements from S 1 [k] [to the root node, in which][ S] 1 [k] [(][x][)][ is absent, making it also]
independent of ¹ n ′ c ′ . Thus,
∂ [2] P nr ( x )
∂¹ nc ∂¹ n ′ c ′ [= 0]

- Product Pair: Let P 1 [k] [be the deepest common ancestor of][ ¹] [nc] [and][ ¹] [n] [′] [c] [′] [. As illustrated in Figure 1c let us use][ ³i][ to denote]
the level of nodes in the path from P 1 [k] [to][ n][ and][ ´i][ to denote the level of nodes in the path from][ P] [ k] 1 [to][ n] [′] [. Then we can split]
the partial derivative as


¹ 1 [k] [P][¯] [ k] 11 [(][x][)]
~~�~~ �� �

Term k


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1


L
�
� l=k


�


�


∂P n r ( x ) = P c (x)

∂¹ nc


³u
� ¹ 1 [l] [P][¯] 11 [ l] [(][x][)]
� l=³1


³u
�
� l=³


~~�~~ �� �

Term n to k − 1


~~�~~ � ~~�~~ �
Term k + 1 to root


Note that in a product complement, the output of the sum node along the path is absent. Therefore, the output of the sum
nodes along the path from level k + 1 to the root is absent in the Term k + 1 to root. Thus, P [¯] 11 [k] [is the only term that depends]
on ¹ n ′ c ′ . Thus,


∂ [2] P n r ( x ) = P c (x)
∂¹ n ′ c ′ ∂¹ nc

= P c (x)


³u
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=³1

³u
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=³1


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
�� l=k+1


L
�
�� l=k


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
�� l=k+1


L
�
�� l=k


∂P [¯] 11 [k] [(] [x] [)]
¹ 1 [k] (5)
∂¹ n ′ c ′


¹ 1 [k] [P][¯¯] [ l] 112 [(][x][)]


 P c ′ (x) (6)




�

�







´v
� ¹ 1 [l] [P][¯] 11 [ l]

l=´1


³u

[)] � ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]

�� l=³1


³u

[)] �

�� l=³1


� [] 





´v
�


� ¹ 1 [l] [P][¯] [ l] 11

l=´1


¯

 P 112 [l] [(][x][)][P] [c] [(][x][)][P] [c] [′] [(][x][)] (7)

 [¯]


= ¹ 1 [k]


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1


L
�
� l=k


The partial derivatives of P n r (x) with respect to ¹ nc and ¹ n ′ c ′ can be written as:


�


¹ 1 [k] [P][¯] [ k] 11 [(][x][)]
~~�~~ �� �

Term k


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1


∂P n r ( x ) = P c (x)

∂¹ nc


³u
� ¹ 1 [l] [P][¯] 11 [ l] [(][x][)]
� l=³1


³u
�
� l=³


�


~~�~~ �� �

Term n to k − 1


~~�~~ � ~~�~~ �

Term k+1 to root


¹ 1 [k] [P][¯] 12 [ k] [(][x][)]
~~�~~ � ~~�~~ �

Term k







∂P n r ( x ) = P c [′] [(][x][)]

∂¹ n ′ c ′







´v
�


� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=´1


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1


�


~~�~~ �� � ~~�~~ �� �
Term n’ to k-1 Term k+1 to root

Thus, we can simplify the double derivative above as:


~~�~~ �� �

Term n’ to k-1


∂ [2] P n r ( x ) =
∂¹ n ′ c ′ ∂¹ nc


¹ 1 [k] [P] [ k] 1 [(][x][)] �� Ll=k+1 [¹] 1 [l] [P][¯] [ l] 11 [(][x][)] �


∂P n r ( x ) ∂P n r ( x )

∂¹ nc ∂¹ n [′] c [′]


-----

Note that for both sum pairs and product pairs, we do not need to consider the order of derivatives, as their formulas are
symmetric (i.e., interchanging ¹ nc and ¹ n [′] c [′] does not affect the result). However, for path pairs, this symmetry is absent, so
we will show that the second derivative yields the same value regardless of the order of differentiation.

- Path Pair: Without loss of generality, Let n [′] be the node nearest to the root. Then we can divide the derivative as


� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=j


L
� ¹ 1 [l] [P][¯] 11 [ l] [(][x][)]
� l=k+1


L
�
� l=k


�



[)]  ¹ n ′ c ′ ¯P 11 [k] [(][x][)]





[)] 


∂P n r ( x ) = P c (x)

∂¹ nc





k−1



�
 l=j


Clearly the derivative is a linear function of ¹ n ′ c ′, so taking its derivative results in


(8)


�


∂ [2] P n r ( x ) = P c (x)
∂¹ n ′ c ′ ∂¹ nc


k




k−1



�
 l=j


� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]

l=j


¯

 P 11 [k] [(][x][)]




L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1


L
�
� l=k+1


= 1 ∂P n r ( x )

(9)

¹ n ′ c ′ ∂¹ nc


Now, let us consider the case when the order of derivation is reversed. Then the derivative is


∂P n r ( x ) = P c [′] (x)

∂¹ n ′ c ′


∂P n r ( x )


L
� ¹ 1 [l] [P][¯] 11 [ l] [(][x][)]

l=k+1


Here P c ′ (x) is the only term dependent on ¹ nc . So,


∂ [2] P n r ( x )
∂¹ nc ∂¹ n ′ c ′ [=]

=


L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1

L
� ¹ 1 [l] [P][¯] [ l] 11 [(][x][)]
� l=k+1


∂P c ′ ( x )

∂¹ nc

�


P¯ 11 [k] [(][x][)]
�


� ¹ 1 [l] [p][¯] [l] 11 [(][x][)]

l=j





k−1



�
 l=j


 P c (x)




= 1 ∂P n r ( x )

(10)

¹ n [′] c [′] ∂¹ nc


Now that we have established the closed form expressions for the double derivative of the likelihood function, we can proceed
to derive the double derivative of log-likelihood.

Proposition 1. If ¹ nc and ¹ n ′ c ′ is a sum pair, then


∂ [2] lo g P n r ( x )


(11)
�


∂¹ [2] lo n g [′] c P [′] ∂¹ n r nc ( x ) = − � F nc ¹ nc ( x )


¹ nc


F n ′ c ′ ( x )
�� ¹ n [′] c [′]


In other words, the mixed double derivative of the log-likelihood with respect to the sum pair parameters is the negative product
of the derivative of the log-likelihood with respect to the individual parameters.


Proof. We have,
∂lo g P n r ( x )


g P n r ( x ) = 1 ∂P n r ( x )

∂¹ nc P n r (x) ∂¹ nc


∂¹ nc P n r (x) ∂¹ nc

Now, by applying the product rule,


∂ [2] lo g P n r ( x )


∂ [2] P n r ( x )

∂¹ nc ¹ n [′] c [′] [−] [∂P] ∂¹ [n] [r] nc [(] [x] [)]



[2] lo g P n r ( x ) = 1 ∂ [2] P n r ( x )

∂¹ n [′] c [′] ¹ nc P n r (x) ∂¹ nc ¹ n [′] c [′]


2

1
� P n r (x) �


∂P n r ( x )

∂¹ n [′] c [′]


∂¹ nc


� 2


∂P n r ( x )

∂¹ n [′] c [′]


1
� P n r (x)


= − [∂P] [n] [r] [(] [x] [)]

∂¹ nc


= − [∂P] [n] [r] [(] [x] [)]


= − [∂lo] [g] [P] [n] [r] [(] [x] [)]

∂¹ nc


∂lo g P n r ( x )

∂¹ n ′ c ′


= − F nc ( x ) F n ′ c ′ ( x )
� ¹ nc �� ¹ n ′ c ′


�


-----

where we use the fact from Theorem 1 that [∂] [2] [P] [nr] [(] [x] [)] = 0 and the relation between Flows and log-likelihood derivative.

∂¹ n ′ c ′ ∂¹ nc

Furthermore, as a special case when ¹ n ′ c ′ = ¹ nc, we get


∂ [2] lo g P n r ( x )


∂ g [2] P¹ ncn r ( x ) = − � F nc ¹ nc ( x )


¹ nc


2

(12)
�


Proposition 2. If ¹ nc and ¹ n ′ c ′ is a product pair, then

∂P n r ( x ) ∂P n r ( x )


∂ [2] lo g P n r ( x ) = 1

∂¹ n ′ c ′ ¹ nc P n r (x)


∂¹ nc ∂¹ n [′] c [′] − [∂] [lo] [g] [ P] [n] [r] [(] [x] [)]

¹ 1 [k] [P] [ k] 1 [(][x][)] �� Ll=k+1 [¹] 1 [l] [P][¯] [ l] 11 [(][x][)] � ∂¹ nc


∂¹ nc


∂¹ n [′] c [′]


∂¹ nc


∂ lo g P n r ( x ) (13)

∂¹ n ′ c ′


Proof. We have,

From Theorem 1 we have,

Therefore,


∂ [2] lo g P n r ( x )



[2] lo g P n r ( x ) = 1 ∂ [2] P n r ( x )

∂¹ n ′ c ′ ¹ nc P n r (x) ∂¹ n ′ c ′ ¹ nc


∂¹ n ′ c ′ ¹ nc


− [∂P] [n] [r] [(] [x] [)] ∂P n r ( x )

∂¹ nc ∂¹ n ′ c ′


2

1
� P n r (x) �


∂P n r ( x )

∂¹ n ′ c ′


∂ [2] lo g P n r ( x ) =

∂¹ n ′ c ′ ¹ nc


¹ 1 [k] [P] [ k] 1 [(][x][)] �� Ll=k+1 [¹] 1 [l] [P][¯] [ l] 11 [(][x][)] �


∂P n r ( x )

∂¹ nc


∂P n r ( x )

∂¹ n ′ c ′


∂ [2] lo g P n r ( x ) = 1

∂¹ n ′ c ′ ¹ nc P n r (x)


¹ 1 [k] [P] 1 [ k] [(][x][)] � � Ll=k+1 [¹] 1 [l] [P][¯] 11 [ l] [(][x][)] �


∂P n r ( x )

∂¹ nc


− [∂] [lo] [g] [ P] [n] [r] [(] [x] [)]

∂¹ nc


∂ lo g P n r ( x )

∂¹ n ′ c ′


= P n r (x) [∂lo] [g] ∂¹ [P] nc [n] [r] [(] [x] [)]


∂lo g P n r ( x )


n r ∂¹ nc ∂¹ n [′] c [′] − [∂] [lo] [g] [ P] [n] [r] [(] [x] [)]

¹ 1 [k] [P] [ k] 1 [(][x][)] � � Ll=k+1 [¹] 1 [l] [P][¯] [ l] 11 [(][x][)] � ∂¹ nc


∂¹ nc


∂¹ n [′] c [′]


∂¹ nc


∂ lo g P n r ( x )

∂¹ n ′ c ′


= P n r (x) [F] [nc] ¹ nc [(] [x] [)]


F n ′ c ′ ( x )


n r

¹ nc ¹ n ′ c ′ − [F] [nc] [(] [x] [)]

¹ 1 [k] [P] [ k] 1 [(][x][)] �� Ll=k+1 [¹] 1 [l] [P][¯] [ l] 11 [(][x][)] � ¹ nc


¹ nc


¹ n ′ c ′


¹ nc


F n ′ c ′ ( x )

¹ n ′ c ′


Proposition 3. If ¹ nc and ¹ n ′ c ′ is a path pair, with ¹ n ′ c ′ closer to the root node, then


∂ [2] lo g P n r ( x )



[2] [ lo] [g] [ P] [n] [r] [(] [x] [)] = 1 F nc ( x )

∂¹ nc ∂¹ n ′ c ′ ¹ n ′ c ′ [·] � ¹ nc



[2] lo g P n r ( x ) = [∂] [2] [ lo] [g] [ P] [n] [r] [(] [x] [)]

∂¹ n ′ c ′ ∂¹ nc ∂¹ nc ∂¹ n ′ c ′


¹ nc


− F nc ( x ) F n ′ c ′ ( x )
� � ¹ nc �� ¹ n ′ c ′


(14)
�


Proof. We have,
∂ [2] lo g P n r ( x )


� 2


1
� P n r (x)



[2] lo g P n r ( x ) = 1 ∂P n r ( x ) − [∂P] [n] [r] [(] [x] [)]

∂¹ n ′ c ′ ¹ nc P n r (x) ∂¹ n ′ c ′ ¹ nc ∂¹ nc


∂P n r ( x )

∂¹ n ′ c ′


∂¹ nc


From Theorem 1 we have, [∂] [2] [P] [nr] [(] [x] [)]


1

[∂] [2] [P] [nr] [(] [x] [)] = [∂P] [n] [r] [(] [x] [)]

∂¹ n ′ c ′ ∂¹ nc ¹ n ′ c ′ [·] ∂¹ nc


∂¹ n ′ c ′ ∂¹ nc ¹ n ′ c ′ ∂¹ nc

Therefore,
∂ [2] lo g P n r ( x ) 1 1



[n] [r] [(] [x] [)] − [∂P] [n] [r] [(] [x] [)]

∂¹ nc ∂¹ nc


� 2


1
� P n r (x)



[2] ∂¹lo n g [′] P c [′] n ¹ r nc ( x ) = P n r 1(x) ¹ n 1 [′] c [′] [·][ ∂P] ∂¹ [n] [r] nc [(] [x] [)]


∂P n r ( x )

∂¹ n [′] c [′]


∂¹ nc


-----

1

=
¹ n ′ c ′ [·][ ∂lo] [g] ∂¹ [P] nc [n] [r] [(] [x] [)]



[g] [P] [n] [r] [(] [x] [)] − [∂lo] [g] [P] [n] [r] [(] [x] [)]

∂¹ nc ∂¹ nc


∂¹ nc


∂lo g P n r ( x )

∂¹ n ′ c ′


= 1 F nc ( x )
¹ n ′ c ′ [·] � ¹ nc


� − � F nc ¹ nc ( x )


F n ′ c ′ ( x )
�� ¹ n ′ c ′


�

#### A.2 Tractable Hessian Trace Computation for a General (Non-Tree Structured) PCs

Proposition 4. The diagonal entry of the Hessian of the log-likelihood with respect to ¹ nc for a general PC is given by:


∂ [2] lo g P n r ( x )


∂¹ g P nc [2] n r ( x ) = − � ∂ lo g ∂¹ P ncn r ( x )


2

.
�


∂¹ nc


Proof. As P n r (x) is linear in ¹ nc, its second derivative with respect to ¹ nc is zero. Applying the chain rule, we obtain:
∂ lo g P n r ( x ) 1 [ ∂P] [n] [r] [(] [x] [)]


g P n r ( x ) = 1

∂¹ nc P n r (x) [·][ ∂P] ∂¹ [n] [r] nc [(] [x] [)]


∂¹ nc .


Differentiating again and using the fact that [∂] [2] [P] [n] [r] [(] [x] [)] = 0, we get:

∂¹ nc [2]


∂ [2] lo g P n r ( x )


∂¹ g P nc [2] n r ( x ) = − � ∂ lo g ∂¹ P ncn r ( x )


∂¹ nc


2
= − F nc ( x )
� � ¹ nc


2

.
�


Thus, the Hessian trace is given by:


Tr�∇ [2] log P n r (x)� = − �

(n,c)


� F nc ¹ nc ( x )


2

,
�


which can be computed in one forward–backward pass in O(|P | |D|) time using edge flows as outlined in Algorithm 1.

Al g orithm 1: Exact Hessian Trace Com p utation for General PCs usin g Ed g e-Flows

Require: Probabilistic circuit PC with edges E, parameters P = {¹ nc } (n,c)∈E ; Dataset D = {x [(][i][)] } [N] i=1
Ensure: |Tr�∇ [2] log P PC (D)�|
1: Initialize: abs trace ← 0
2: for each data point x in D do - N iterations
3: Forward pass: compute node outputs p n (x) ∀ nodes n in PC - Runs in time O(|E|) = O(|P |)
4: Backward pass (compute edge-flows):
5: Initialize all node-flows F n ← 0, and edge-flows F e ← 0
6: Set F root ← 1
7: for each node n in topological order from root to leaves do - Equals making one pass over all edges
8: if n is a sum node then
9: for each child edge e = (n → c) do

10: F e ← F n × ¹ e × p [p] n [c] [(] ( [x] x [)] ) // Compute Edge Flow

11: F c ← F c + F e // Update Node Flow
12: end for
13: else if n is a product node then
14: for each child edge e = (n → c) do
15: P n,c (x) ← p n (x)/p c (x) - Precomputed in the forward pass
16: F e ← F n × P n,c (x) // Compute Edge Flow
17: F c ← F c + F e // Update Node Flow
18: end for

19: end if
20: end for - Runs in time O(|E|) = O(|P |)
21: Accumulate trace contribution:
22: for each edge e ∈ E do

2
23: abs trace += �F e /¹ e �

24: end for - Runs in time O(|E|) = O(|P |)
25: end for - Thus, total over the N points, complexity = O(|P ||D|)
26: return abs trace


-----

#### A.3 Sharpness-Aware Regularization for PCs

In this section we show how to incorporate our tractable Hessian-trace penalty into the EM updates for sum-node parameters.
Recall that at each sum node n, vanilla EM maximizes


L n (¹ n· ) = �


� ¹ nc = 1,

c


� F nc (x) log ¹ nc s.t. �

c∈ch(n) c


where F nc (x) denotes the edge-flows (“soft counts”).

Proposition 5. The EM update for a parameter ¹ nc associated with a sum node n, under a Hessian trace sharpness regularizer
is the solution to the cubic equation:
¼ ¹ nc [3] [−] [F] [nc] [(][x][)][ ¹] nc [2] [−] [2][ µF] [nc] [(][x][) = 0][,] (15)

where F nc (x) is the expected edge-flow along edge (n.c), and ¼, µ are the Lagrange multipliers for the normalization and trace
constraints, respectively.


Proof. To extend vanilla EM with our Hessian trace regularizer and discourage sharp optima, we further update the objective
L n by constraining the Hessian trace,

F nc ( x ) 2

� � ¹ � f m,


� F nc ¹ ( x )


2
f m,
�


c∈ch(n)


¹ nc


which upper-bounds the curvature at n. Thus, for each parameter ¹ n,c we now solve the regularized objective:


¹ n [∗] - [= arg max]
¹ n·


� F nc (x) log ¹ nc

c∈ch(n)


subject to � ¹ nc = 1,

c∈ch(n)


F nc ( x )

c∈ � ch(n) � ¹ nc


2

f m
�


The Lagrangian formulation for the above constrained maximization objective can be written as:





 c∈ [�] ch(


2
− m
�




L(¹ n·, ¼, µ) = � F nc (x) log ¹ nc − ¼

c∈ch(n)





 c∈ [�] ch(


¹ nc − 1
c∈ [�] ch(n)


 − µ





c∈ch(n)


F nc ( x )
� ¹ nc


Taking the partial derivative with respect to ¹ nc and setting to 0, we get



[1] − ¼ + µ [2][F] [nc] [(] [x] [)] [2]

¹ nc ¹ nc [3]


∂L
= F nc (x) [1]
∂¹ nc ¹ nc


= 0
¹ nc [3]


Thus resulting in the cubic equation, (assuming ¹ nc - 0),

¼ ¹ nc [3] [−] [F] [nc] [(][x][)][ ¹] nc [2] [−] [2][µF] [nc] [(][x][)] [2] [ = 0]

Solving a cubic at every node and every iteration can be costly and numerically unstable. Fortunately, because our Hessian-trace
penalty is itself a sum of (F nc /¹ nc ) [2], one can equivalently bound it by directly bounding the gradients


∂ lo g P n r ( x )


g P n r ( x ) = [F] [nc] [(] [x] [)]

∂¹ nc ¹ nc


f r,
¹ nc


which leads to the following quadratic update.

Theorem 2. The EM update for a parameter ¹ nc associated with a sum node n, regularized by a constraint on the gradient, is
given by:


¹ nc = [F] [nc] [(] [x] [)] [ +] �


F nc ( x ) [2] + 4¼ µ F nc ( x )

2¼,


where F nc (x) denotes the flow along the edge from sum node n to its child c, and ¼, µ g 0 are the Lagrange multipliers
corresponding to the normalization and regularization constraints, respectively.


-----

Proof. We now maximize the expected log-likelihood subject to the constraints that the parameters lie on the probability
simplex and the simpler constraint that the local gradient is upper bounded. Thus the regularized objective can be expressed as:


¹ n [∗] - [= arg max]
¹ n·


� F nc (x) log ¹ nc

c∈ch(n)


subject to � ¹ nc = 1,

c∈ch(n)


n r )

∂¹ nc


�

c∈ch(n)


F nc ( x )


nc ( x ) f m (using the identity F nc (x) = ¹ nc ∂ lo g P n r ( x )

¹ nc ∂¹ nc


Forming the Lagrangian and applying KKT conditions leads to the quadratic equation:

¼ ¹ nc [2] [−] [F] [nc] [(][x][)][ ¹] [nc] [−] [µF] [nc] [(][x][) = 0][.]


The discriminant of this equation is:
F nc (x) [2] + 4¼µF nc (x),
which is nonnegative for F nc (x), ¼, µ g 0. Thus, complex roots cannot occur. As for the existence of multiple roots, observe
that: F nc (x) [2] f F nc (x) [2] + 4¼µF nc (x), as ¼ g 0 and µ g 0. Taking square roots:

�F nc (x) [2] f � F nc (x) [2] + 4¼µF nc (x)

F nc (x) − ~~�~~ F nc (x) [2] + 4¼µF nc f 0

As we constrain the parameters to take only non-negative values, we can disregard the negative root of the equation. Hence, the
unique parameter update equation is given by:


F nc (x) [2] f �


F nc (x) [2] + 4¼µF nc (x)


F nc (x) − ~~�~~


¹ nc = [F] [nc] [(] [x] [)] [ +] ~~�~~


2¼


F nc ( x ) [2] + 4¼ µ F nc ( x )


Although our derivation in Theorem 2 is presented for the single-sample (“stochastic”) case, the same closed-form update
extends immediately to mini-batch or full-batch EM by simply summing the expected edge-flows F nc (x) over all samples in
the batch (see lines 3–5 of Algorithm 2). To reduce the variance introduced by small batches, we apply a running-average
smoothing: ¹ nc [new] = (1 − ³)¹ nc [old] [+][ ³][¹][˜] [nc] [where][ ³][ ∈] [[0][,][ 1]][ and][ ˜][¹] [nc] [denotes the roots of the quadratic equation as given by]
Theorem 2. In the KKT system, ¼ appears only as a normalization multiplier and cannot be solved in closed form. In practice
we simply fix ¼ = 1 and then renormalize each sum-node’s parameters to lie on the simplex (see line 11 of Algorithm 2).

Al g orithm 2: Shar p ness-Aware EM for General PCs

Require: Probabilistic circuit PC with sum-node parameters P = {¹ nc } (n,c)∈E ; Dataset D = {x [(][i][)] } [N] i=1 [; Regularization]
weight µ; Simplex Constraint weight ¼; Smoothing factor ³; #Epochs E
Ensure: Updated sum node parameters P under the sharpness regularized objective
1: Initialize: for each sum-node n, set ¹ n· uniformly on its simplex
2: for epoch in 1 . . . E do - until convergence/max epoch E
3: E-step: Compute expected edge flows
4: Run forward–backward passes on PC over D
5: Obtain {F nc (x)} (n,c)∈E, x∈D - Runs in O(|P ||D|)
6: M-step: Apply sharpness-aware update
7: for all sum-nodes n do - independent per node
8: for all child edges (n → c) do


// Theorem. 2
2 ¼


x [F] [nc] [(][x][) +] ���


2
x [F] [nc] [(][x][)] � + 4 ¼ µ �


x [F] [nc] [(][x][)]


9: ¹˜ nc ←


�


10: end for
11: Normalize: ∀c ∈ ch(n) set ¹ [˜] nc ← ¹ [˜] nc / [�] c∈ch(n) [¹][˜] [nc] // project onto simplex
12: end for
13: ¹ ← (1 − ³) ¹ + ³ ¹ [˜] // running-average smoothing to account for noise
14: end for

15: return P ≡ ¹


-----

### B Experimental Setup

In this section, we provide the details pertaining to the datasets, model architectures, training procedures, hyperparameters, and
implementation that underlie the experiments reported in the main paper.
#### B.1 Datasets

Interlocked-Circles Helix Bent-Lissajous Twisted-Eight Knotted

Spiral Pinwheel Two Moons

Figure 2: Visualizations of the 8 synthetic 3D (top) and 2D (bottom) manifold data distributions used in our empirical analysis.

Low Dimensional Data. We first consider 8 synthetic data distributions over 3D and 2D manifolds. These datasets have
been shown to be challenging to learn for several generative models (Sidheekh et al. 2022; Sidheekh, Kersting, and Natarajan
2023), and their low dimensionality allows us to verify the correctness of our derivations by computing the full hessian trace
via Pytorch autograd. Figure 2 provides a visualization of these datasets. For each manifold, we sample 1000 datapoints each
for training, validation and testing.

Real World Data. To study the applicability of our framework in more complex, higher dimensional real world domains,
we consider the standard suite of 20 binary density estimation benchmark (Van Haaren and Davis 2012; Bekker et al. 2015).
These include small to large domains such as `nltcs` (16 variables), `msnbc` (17 variables), up to `ad` (1556 variables). Table 1
summarizes the number of variables as well as the number of datapoints present in the train, validation and test split for each of
these 20 datasets.
#### B.2 Model Architectures

To demonstrate the applicability of our framework across different PC structures and implementations, we embed our sharpnessaware regularizer into two popular Probabilistic Circuit frameworks for each dataset type:

1. Einsum Networks (Peharz et al. 2020) for our experiments on synthetic data, which uses a Random-Tensorized structure
introduced in RAT-SPNs (Peharz et al. 2019) for the PC, and combines the sum and product operations into a tensorized
einsum operation that is more efficient and scalable on GPUs. We employ the “binary” graph type (variable bipartitions) to
generate the random structure. The hyperparameters defining the model structure and capacity include 
  - `num vars` : The number of observed random variables. We set this to 2 (or 3) for our synthetic 2D (or 3D) manifolds.

  - `num input distributions` : The number of input leaf distributions per variable. We set this to 10, yielding 10
Gaussians per input dimension for our synthetic 2D and 3D datasets.

  - `num sums` : The number of sum nodes per layer. We set this to 10.

  - `num repetitions` : The number of replicas, which we also set to 10.

  - `depth` : The total number of layers (sum and product alterations) we set this to 1 for our synthetic experiments as it is the
max depth possible for 2D/3D data.


-----

Table 1: Overview of the 20 binary density estimation datasets, showing the number of variables and the number of instances
in the training, validation, and test splits.

Dataset Name #vars #train #valid #test

nltcs 16 16181 2157 3236

msnbc 17 291326 38843 58265

kdd 65 180092 19907 34955
plants 69 17412 2321 3482
baudio 100 15000 2000 3000
jester 100 9000 1000 4116
bnetflix 100 15000 2000 3000
accidents 111 12758 1700 2551

tretail 135 22041 2938 4408
pumsb star 163 12262 1635 2452
dna 180 1600 400 1186

kosarek 190 33375 4450 6675

msweb 294 29441 3270 5000

tmovie 500 4524 1002 591

book 500 8700 1159 1739

cwebkb 839 2803 558 838

cr52 889 6532 1028 1540
c20ng 910 11293 3764 3764
bbc 1058 1670 225 330

ad 1556 2461 327 491

2. PyJuice (Liu, Ahmed, and den Broeck 2024), for our experiments on the binary density estimation datasets. We use the
Hidden Chow-Liu Tree structure (Liu and den Broeck 2021; Liu, Ahmed, and den Broeck 2024) which is a generative
probabilistic model that extends the classical Chow-Liu tree by introducing latent (hidden) variables to model complex
dependencies among observed variables more effectively. The tree topology is learned from data using maximum-likelihood
(Chow-Liu algorithm). The observed variables are then pushed to the leaves, introducing latent variables to occupy the
internal nodes, forming a latent tree structure. As a result, the learned structure can vary across datasets, adapting to the
underlying statistical relationships. The latent size ( `num latent` ) refers to the number of states each hidden variable can
take, and it serves as a key hyperparameter that controls the model’s capacity. We set `num latent` = 100 for all our binary
density estimation datasets.
#### B.3 Training and Implementation Details

To simulate low data settings where PCs can overfit and study the effect the regularizer can have, we train the models on multiple
random subsets of the official training set for all the datasets, at fractions {1%, 5%, 10%, 50%, 100%}—always evaluating on
the same held-out validation and test partitions. We used the same model architecture and experimental setup for the base model
and the regularized version for a fair comparison, and repeated each experiment across 5 independent trials, setting the random
seed to be equal to the trial number.

Einsum Networks (Gradient-Based Learning). For the eight synthetic low-dimensional datasets, we employed the
RAT-SPN architecture implemented in the Einsum Networks library. We used `num input distributions` =10,
`num sum nodes` =10, `num repetitions` =10, and `depth` =1 (one sum and one product layer per repetition). We optimized
the parameters using an Adam optimizer with a learning rate=10 [−][1], batch size=200, for 200 epochs, using the objective


L reg (¹) = − �


� log P PC (x) + µ �

x∈D train n,c


2

,
�


n,c


� F nc ¹ nc ( x )


where the second term is our Hessian-trace regularizer and setting µ = 0 recovers the unregularized objective. We set ¼ = 1 and
selected µ based on the validation performance (negative log-likelihood) over the grid {0.01, 0.05 0.1, 0.5, 1.0}. In addition
to the fixed grid, we also tried an adaptive schedule by setting µ adaptive = » [DoF] - ³ [g] g [data] reg [,][ at the end of every epoch, where]


DoF = 100 ∗ � NLL val −NLL train � � denotes the degree of overfitting and g data = ∥∇ ¹ L∥ 2, g reg = ∥∇ ¹ R∥ 2, denotes the
� [�] �� NLL train �� �

norm of the gradients of the log-likelihood objective and the regularizer respectively. We used » = 1.05 and ³ = 1.0 to balance
the magnitudes of the data and regularizer gradients during learning and to amplify µ when overfitting worsens.


-----

PyJuice HCLTs (EM-Based Learning). For the 20 standard binary density benchmarks (Van Haaren and Davis 2012; Lowd
and Davis 2014), we fit Hidden Chow-Liu Trees using PyJuice with latent size `num latents` =100. The structure is learned
once from the training set via Chow-Liu, then converted to an HCLT with each observed variable X i connected to a categorical
latent Z i of size 100. We ran EM for 100 epochs with smoothing factor ³ EM = 0.1 and batch size=200. In the regularized
M-step, we used the quadratic-update formula in Theorem 2. We selected µ from the same fixed grid {0.01, 0.05 0.1, 0.5, 1.0}
using the validation performance and also experimented with a layer-wise mean-flow schedule: after each E-step we computed
the mean edge-flow in each layer ℓ, F [¯] ℓ = |E 1 ℓ | � e∈E ℓ [F] [e] [, and set][ µ] [ℓ] [= ¯][F] [ℓ] [. As the magnitude of the circuit flows decrease with]

depth, this schedule prevents over-regularization of shallow layers or under-regularization of deep ones.

Compute. We used 4 NVIDIA L4 GPUs each with 24 GB memory to run our experiments. Across roughly 700 total runs (8
synthetic datasets × 5 fractions × 5 trials + 20 binary datasets × 5 fractions × 5 trials) the total GPU time was ≈ 192 hours. The
code will be publicly released upon acceptance to ensure reproducibility.
### C Additional Results
#### C.1 Empirical Motivation: Sharpness and Overfitting in PCs

Figure 3: Left: Degree of overfitting (DoF) vs. number of latent variables, for three training fractions (10%, 25%, 50%).
Overfitting grows with model capacity and is worst in low-data regimes. Right: Sharpness (blue, left axis) and Degree of
Overfitting (DoF) (red, right axis) as a function of latent size at 50% training data. Higher Hessian-trace (sharpness) aligns with
larger generalization gaps, motivating our sharpness-aware regularization framework.

First, we present additional results that concretely motivate our approach, by empirically examining the relationship between
sharpness and generalization in PCs. Specifically, we consider a PyJuice HCLT with identical architectures trained on the same
dataset (we use a synthetic Gaussian mixture model as the ground truth distribution) and compare them in terms of overfitting,

measured by the relative generalization gap (or degree of overfitting): = [L] [train] [ −L] [test], where L train and L test denote the log
|L train |

likelihoods on the training and test data respectively. As shown in Figure 3, we observe that models converging to sharper
optima—quantified by the trace of the Hessian of the log-likelihood—exhibit greater overfitting. This suggests that minimizing
curvature-based sharpness could serve as an effective regularization strategy to reduce overfitting in PCs.
#### C.2 Extended Quantitative Results

Tables 2 and 3 summarize the the final test-set negative log-likelihood and the sharpness of the train negative log-likelihood
surface at the converged points achieved by an Einsum Network trained with and without our Hessian-trace regularizer using
gradient descent on the eight synthetic manifold datasets. Similarly, Tables 4 and 5 report the results for PyJuice HCLTs trained
on the 20 binary benchmarks using EM with and without our regularizer. In both settings, we observe that adding the regularizer
guides convergence to flatter minima and helps achieve better generalization, especially in the lower data regimes.
#### C.3 Visualization of Loss Landscapes

To illustrate how our Hessian-trace regularizer reshapes the optimization geometry, we train EinsumNet on two synthetic benchmarks ( `pinwheel` and `two moons` ) using gradient descent with and without our regularization, and plot the loss landscape
at the converged points using the visualization algorithm proposed by (Li et al. 2018) in Figures 4-5. Specifically, for each
converged solution ¹ [∗], we compute and visualize the following four diagnostics:

 - 1D Loss Surface: L(¹ [∗] + ³u) plotted against the scalar ³, where u is a random unit vector in parameter space.


-----

Table 2: Final negative log-likelihoods on the test split achieved by Einsum Network trained with and without the Hessiantrace regularizer using gradient descent on the synthetic 2D and 3D manifold datasets. Values represent mean over 5 independent trials.

Dataset 1% 5% 10% 50% 100%

Base Regularized Base Regularized Base Regularized Base Regularized Base Regularized

bent lissajous 11.482 5.973 3.686 2.957 3.277 2.670 2.176 2.147 2.007 1.993
helix 9.996 6.730 3.894 3.192 2.902 2.575 1.991 1.802 1.590 1.475
interlocked circles 10.652 6.257 5.014 3.682 3.862 3.141 2.563 2.507 2.456 2.431
knotted 10.869 5.145 5.472 4.000 3.822 3.164 2.235 2.103 1.915 1.719
pinwheel 8.914 3.814 3.700 2.854 2.842 2.464 2.355 2.266 2.247 2.239
spiral 9.663 3.682 3.932 2.629 2.725 2.047 1.497 1.218 1.492 1.244
twisted eight 10.622 5.425 4.067 3.475 3.098 2.742 2.162 2.102 2.051 2.075
two moons 8.639 3.902 3.184 2.260 2.379 1.902 1.730 1.447 1.450 1.393

Table 3: Sharpness of the train log-likelihood surface at the converged parameters of an Einsum Network trained with and
without the Hessian-trace regularizer using gradient descent on the synthetic 2D and 3D manifold datasets. Values represent
mean over 5 independent trials.

Dataset 1% 5% 10% 50% 100%

Base Regularized Base Regularized Base Regularized Base Regularized Base Regularized

bent lissajous 1.279 0.084 1.048 0.480 1.319 0.579 1.723 1.124 1.123 0.770
helix 1.145 0.097 1.008 0.423 1.233 0.968 2.104 1.811 1.288 1.082
interlocked circles 1.142 0.138 1.137 0.276 1.237 0.665 1.826 1.205 1.118 0.652
knotted 1.016 0.084 1.262 0.439 1.359 0.935 2.121 1.580 1.373 1.077
pinwheel 1.112 0.068 1.066 0.362 1.010 0.238 1.527 0.368 0.862 0.180
spiral 1.191 0.129 1.273 0.428 1.285 0.596 2.094 1.754 1.220 0.892
twisted eight 1.113 0.289 1.019 0.383 1.168 0.562 1.766 1.199 1.068 0.633
two moons 1.048 0.124 0.900 0.393 1.201 0.776 1.725 1.160 1.128 0.813

 - 2D Loss Surface: L(¹ [∗] + ³u + ´v) over a grid of (³, ´), with u, v two orthonormal directions.

 - 2D Contour Plot: Iso-contours of the 2D loss surface to highlight valley geometry.

 - Hessian Spectrum: Histogram of the top-10 eigenvalues of the Hessian, quantifying the local curvature.

In each figure, the top row corresponds to the unregularized model and the bottom row corresponds to the model trained with
our regularizer. From left to right, the columns correspond to the four diagnostics above. We can observe a visibly flatter 1D/2D
surface and a smaller Hessian spectrum under our regularization framework, which indicates that it guides convergence to a
flatter, more stable minima.
#### C.4 Learning Curves and Sharpness

To gain deeper insights into how the Hessian-trace regularization affects the training process, we plot the training and validation
negative log-likelihood (NLL) together with the sharpness over epochs for both gradient-based and EM-based learning. Each
figure displays results for a single dataset under varying fractions of the training data, comparing the unregularized baseline (left
column) against our Hessian-trace regularized model (right column). Figure 6 illustrates the case of the 2D `spiral` manifold
trained with gradient descent on an Einsum Network. Under the lower data regimes, the baseline overfits and the sharpness
increases. In contrast, the regularized model attains a smoother decrease in both train and validation NLL, with sharpness
remaining lower than the base model throughout training. As data size increases, the gap narrows, but regularization still
yields flatter optima and modest generalization gains. Similar trends hold across other synthetic manifolds (e.g., `pinwheel`,
`two moons`, `helix` ), as shown in Figures 7–9. We also observe analogous dynamics for EM-based learning of PyJuice HCLTs
on the real-world binary benchmarks. We provide a visualization of the same for a few of the binary datasets in Figures 14- 17.
### References

Bekker, J.; Davis, J.; Choi, A.; Darwiche, A.; and Van den Broeck, G. 2015. Tractable learning for complex probability queries.
Advances in Neural Information Processing Systems, 28.

Li, H.; Xu, Z.; Taylor, G.; Studer, C.; and Goldstein, T. 2018. Visualizing the Loss Landscape of Neural Nets. In Bengio,
S.; Wallach, H. M.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, 6391–6401.


-----

Table 4: Final negative log-likelihoods on the test split achieved by the PyJuice HCLT models trained with and without the
Hessian-trace regularizer using EM on the binary density estimation benchmark. Values represent mean over 5 independent
trials.

Dataset 1% 5% 10% 50% 100%

Base Regularized Base Regularized Base Regularized Base Regularized Base Regularized

accidents 34.462 33.978 31.004 31.585 29.623 30.117 29.935 30.085 30.400 30.489

ad 113.631 111.227 64.832 62.758 47.492 46.896 26.138 26.717 23.182 23.616

baudio 51.133 46.328 42.741 42.988 42.565 42.802 42.199 42.282 42.362 42.407

bbc 865.477 775.021 413.546 348.391 318.145 290.308 262.896 263.503 261.152 262.113
bnetflix 63.970 62.004 59.488 59.716 59.225 59.407 59.219 59.233 59.050 59.059
book 76.562 69.243 43.391 42.753 38.554 38.785 37.876 38.191 37.635 37.815
c20ng 284.407 251.542 181.128 179.716 174.452 174.348 174.086 174.574 162.192 162.389
cr52 207.083 185.249 114.139 109.935 101.660 101.759 95.856 96.398 96.311 96.614

cwebkb 433.524 394.291 231.935 199.075 182.514 175.492 162.642 163.660 161.588 162.333

dna 314.576 221.919 91.664 89.294 84.991 84.906 82.339 83.069 82.280 82.665
jester 79.907 61.741 56.220 56.081 55.459 55.850 54.800 54.870 54.770 54.801
kdd 2.398 2.397 2.301 2.299 2.269 2.267 2.245 2.245 2.194 2.194

kosarek 14.150 13.876 12.254 12.347 12.077 12.127 11.697 11.721 11.649 11.658

msnbc 6.573 6.580 6.563 6.562 6.547 6.549 6.580 6.580 6.551 6.551

msweb 12.147 12.012 11.014 11.039 10.784 10.815 10.652 10.660 10.603 10.606

nltcs 6.588 6.576 6.342 6.401 6.281 6.324 6.316 6.321 6.230 6.235
plants 16.674 16.410 15.256 15.624 15.117 15.362 15.071 15.155 14.980 15.023
pumsb star 33.633 32.948 28.041 28.814 27.864 28.335 27.860 27.999 27.762 27.842
tmovie 141.179 119.796 78.834 69.353 62.023 62.012 56.704 57.511 56.263 56.723

tretail 14.133 13.489 11.291 11.318 11.210 11.220 11.140 11.144 11.167 11.169

Table 5: Sharpness of the train log-likelihood surface at the converged parameters of PyJuice HCLT models trained with and
without the Hessian-trace regularizer using EM on the binary density estimation benchmark. Values represent mean over 5
independent trials.

Dataset 1% 5% 10% 50% 100%

Base Regularized Base Regularized Base Regularized Base Regularized Base Regularized

accidents 5.418 4.569 5.377 4.481 1.551 1.351 0.170 0.148 0.078 0.075

ad 50.032 21.350 41.110 8.823 77.544 56.467 43.930 39.190 11.437 10.407

baudio 4.468 4.049 1.226 1.189 0.978 0.927 0.186 0.183 0.052 0.051

bbc 87.328 59.199 39.811 29.792 19.994 14.352 21.743 19.306 5.773 5.153
bnetflix 4.011 4.076 1.098 1.066 0.824 0.801 0.173 0.173 0.044 0.044
book 41.330 10.331 20.406 14.350 5.624 4.393 0.689 0.617 0.482 0.445
c20ng 25.661 17.686 6.732 4.972 4.399 3.930 2.473 2.318 0.664 0.641
cr52 29.593 19.021 12.734 13.124 18.422 17.791 3.298 3.191 0.918 0.894

cwebkb 33.709 23.239 18.840 13.088 21.762 15.457 403.455 358.781 79.846 75.722

dna 45.327 43.570 12.163 10.469 5.780 5.263 1.173 1.069 0.575 0.552
jester 6.775 8.878 5.182 5.290 1.391 1.439 0.351 0.344 0.087 0.087
kdd 0.439 0.383 3.401 3.281 1.170 1.069 0.038 0.036 0.008 0.008

kosarek 3.619 2.714 1.590 1.377 0.412 0.379 0.147 0.135 0.046 0.043

msnbc 0.267 0.266 0.037 0.037 0.023 0.023 0.010 0.010 0.003 0.003

msweb 5.472 3.633 2.291 1.842 0.389 0.360 0.862 0.657 0.221 0.206

nltcs 2.768 2.569 10.136 9.650 2.813 2.743 0.122 0.124 0.031 0.031
plants 3.410 2.942 1.888 1.693 0.538 0.494 0.144 0.139 0.636 0.614
pumsb star 6.899 6.301 18.034 15.130 5.338 4.860 0.237 0.224 0.253 0.241
tmovie 15.935 13.224 32.657 33.657 10.971 9.874 2.751 2.659 1.684 1.416

tretail 22.972 15.087 1.800 1.390 18.382 16.292 0.824 0.821 0.186 0.183

Liu, A.; Ahmed, K.; and den Broeck, G. V. 2024. Scaling Tractable Probabilistic Circuits: A Systems Perspective. In Forty-first


-----

8

6

4

2

0

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
||Tr Va|ain lid||||



3.50

3.25

3.00

2.75

2.50

2.25

2.00

1.75


3 2 1 0 1 2 3

|Col1|Col2|Col3|Col4|Col5|Train|Col7|
|---|---|---|---|---|---|---|
||||||Valid||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||



3 2 1 0 1 2 3


1D Loss Surface 2D Loss Surface 2D Loss Surface Contour Hessian Eigen-Values

10 [4]

10 [3]


10 [2]

10 [2]

10 [1]


2 4 6 8 10

Eigen-index k

2 4 6 8 10

Eigen-index k


Figure 4: Visualization of the loss landscape geometry and Hessian spectrum for EinsumNet trained on the `pinwheel` dataset
using gradient descent. The top row shows the unregularized model and the bottom row the model trained with our Hessiantrace regularizer. From left to right: (1) one-dimensional loss surface along a random direction in parameter space, ³ denotes
the distance from the converged point. (2) two-dimensional loss surface over a grid of perturbations; (3) contour plot of the 2D
loss surface; (4) histogram of Hessian eigenvalues computed at the converged solution. Regularization results in convergence
to a flatter valley on the loss surface, validated by the Hessian eigen-spectrum.

International Conference on Machine Learning, 2024.

Liu, A.; and den Broeck, G. V. 2021. Tractable Regularization of Probabilistic Circuits. In Ranzato, M.; Beygelzimer, A.;
Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems, 2021, 3558–3570.

Lowd, D.; and Davis, J. 2014. Improving Markov network structure learning using decision trees. Journal of Machine Learning
Research.

Peharz, R.; Lang, S.; Vergari, A.; Stelzner, K.; Molina, A.; Trapp, M.; den Broeck, G. V.; Kersting, K.; and Ghahramani, Z.
2020. Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits. In Proceedings of the 37th International
Conference on Machine Learning, 2020, Proceedings of Machine Learning Research, 7563–7574.
Peharz, R.; Vergari, A.; Stelzner, K.; Molina, A.; Trapp, M.; Shao, X.; Kersting, K.; and Ghahramani, Z. 2019. Random SumProduct Networks: A Simple and Effective Approach to Probabilistic Deep Learning. In Globerson, A.; and Silva, R., eds.,
Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, 2019, Proceedings of Machine Learning
Research, 334–344.

Sidheekh, S.; Dock, C. B.; Jain, T.; Balan, R.; and Singh, M. K. 2022. VQ-Flows: Vector quantized local normalizing flows. In
Uncertainty in Artificial Intelligence, 1835–1845. PMLR.

Sidheekh, S.; Kersting, K.; and Natarajan, S. 2023. Probabilistic Flow Circuits: Towards Unified Deep Models for Tractable
Probabilistic Inference. In Evans, R. J.; and Shpitser, I., eds., Uncertainty in Artificial Intelligence, 2023, Proceedings of
Machine Learning Research, 1964–1973.
Van Haaren, J.; and Davis, J. 2012. Markov network structure learning: A randomized feature generation approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, 1148–1154.


-----

8

6

4

2

0

2

3.75

3.50

3.25

3.00

2.75

2.50

2.25

2.00

1.75


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
||Train Valid|||


3 2 1 0 1 2 3

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|Train||||
|Valid||||



3 2 1 0 1 2 3


1D Loss Surface 2D Loss Surface 2D Loss Surface Contour Hessian Eigen-Values

10 [4]

10 [3]


10 [2]

10 [1]


2 4 6 8 10

Eigen-index k

2 4 6 8 10

Eigen-index k


Figure 5: Visualization of the loss landscape geometry and Hessian spectrum for EinsumNet trained on the `two moons`
dataset using gradient descent. The top row shows the unregularized model and the bottom row the model trained with our
Hessian-trace regularizer. From left to right: (1) one-dimensional loss surface along a random direction in parameter space,
³ denotes the distance from the converged point. (2) two-dimensional loss surface over a grid of perturbations; (3) contour
plot of the 2D loss surface; (4) histogram of Hessian eigenvalues computed at the converged solution. Regularization results in
convergence to a flatter valley on the loss surface, validated by the Hessian eigen-spectrum.


-----

Train NLL Val NLL Sharpness

Regularized

10

1.0

5

0.5

0

0.0

epoch

Regularized

4

1.0


epoch

Regularized

epoch

Regularized


10

5


1% Data

5% Data

10% Data

50% Data

100% Data


1

3

2

|Col1|Col2|epoch Base|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||
||||||



0 50 100 150 200

epoch


0

4

2

3

2

1

3

2


Base

epoch

Base

epoch

Base

epoch

Base


2

3

2

1

3

2


1

epoch

Regularized

3

2

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


Figure 6: Learning curves and sharpness across training epochs for EinsumNet trained on the `spiral` dataset using
gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left
column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness

1.0

5

0.5

0

0.0

1.0

3

0.5

2


Regularized

epoch

Regularized


5
1% Data

0

3

5% Data

2


Base

epoch

Base


2.0

epoch

Regularized

3.5

3.0

2.5

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


1

epoch

Base


1

epoch

Regularized


3
10% Data

2

3.5


epoch

Base


epoch

Regularized


0.0

1.0

0.5

0.0

1.5

1.0

0.5

0.0

0.5

0.0


3

2

3.5

3.0

2.5


50% Data

100% Data


3.0

2.5

2.0

epoch

3.5

3.0

2.5

|Col1|Col2|Base|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1.5

1.0

0.5

0.0

0.5

0.0


Figure 7: Learning curves and sharpness across training epochs for EinsumNet trained on the `pinwheel` dataset using
gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left
column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


5
1% Data

0

3


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


5% Data

10% Data

50% Data

100% Data


2

1

3

2

1

3

2

3

2


Base

epoch

Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


5

0

3

2

1

3

2

1

3

2

3

2


Figure 8: Learning curves and sharpness across training epochs for EinsumNet trained on the `twomoons` dataset using
gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left
column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness

10

1.0

5

0.5

0


Regularized


1% Data


Base

10

5

0

5

epoch

Base


5

epoch

Regularized

4

2

epoch

Regularized

4

2

epoch

Regularized

4

2

epoch

Regularized

4

2

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


4

5% Data

2

4
10% Data

2

4
50% Data

2

4

100% Data

2


epoch

Base

epoch

Base

|Col1|Col2|epoch Base|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||



0 50 100 150 200

epoch


0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


Figure 9: Learning curves and sharpness across training epochs for EinsumNet trained on the `helix` dataset using
gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left
column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


10


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1.5

1.0

0.5

0.0

1.0

0.5

0.0


10

0

4

2

4

2

4

2

5

4

3

2


1% Data


0

4

5% Data

2

4
10% Data

2

4
50% Data

2

5

4


100% Data


3

2


Base

epoch

Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1.5

1.0

0.5

0.0

1.0

0.5

0.0


Figure 10: Learning curves and sharpness across training epochs for EinsumNet trained on the `bentlissajous`
dataset using gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %,
100 %). The left column depicts the base model without regularization and the right column corresponds to our Hessian-trace
regularized counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


10

5


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1

0

1.0

0.5

0.0


10

5

0

5

4

2

4

2

5

4

3

5

4

3


1% Data


0

5

4

5% Data

2

4
10% Data

2

5


Base

epoch

Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1

0

1.0

0.5

0.0


50% Data

100% Data


4

3

5

4

3


Figure 11: Learning curves and sharpness across training epochs for EinsumNet trained on the `interlockedcircles`
dataset using gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %).
The left column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized
counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


10

5


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1

0

1.0

0.5

0.0


10

5

0

5

4

2

4

2

4

2

5

4

3

2


1% Data


0

5

4

5% Data

2

4
10% Data

2

4
50% Data

2

5

4


100% Data


3

2


Base

epoch

Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1

0

1.0

0.5

0.0


Figure 12: Learning curves and sharpness across training epochs for EinsumNet trained on the `twistedeight` dataset
using gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The
left column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized
counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness

Regularized


epoch

Regularized


10


10

5

0

6

4

2


0

epoch

Regularized

4

2

epoch

Regularized

4

2

epoch

Regularized

4

2

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||



0 50 100 150 200

epoch


Base

epoch

Base


1% Data

5% Data


5

0

6

4

2


0

epoch

Base


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


4
10% Data

2

4
50% Data

2

4
100% Data

2


epoch

Base

epoch

|Col1|Col2|Base|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||



0 50 100 150 200

epoch


1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

2

1

0

1.0

0.5

0.0


Figure 13: Learning curves and sharpness across training epochs for EinsumNet trained on the `knotted` dataset using
gradient-descent. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left
column depicts the base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness

Regularized


60

40

70

60

50

40

70

60

50


epoch

Regularized

epoch

Regularized


40

epoch

Regularized

70

60

50

epoch

Regularized

70

60

50

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||



0 20 40 60 80 100

epoch


Base

epoch

Base

epoch

Base


1% Data

5% Data

10% Data

50% Data

100% Data


60

40

70

60

50

40

70

60

50


40

epoch

Base

70

60

50

epoch

Base

70

60

50

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||



0 20 40 60 80 100

epoch


6

5

4

1.4

1.2

1.0

1.0

0.8

0.25

0.20

0.05

0.04


6

5

4

1.4

1.2

1.0

1.0

0.8

0.25

0.20

0.05

0.04


Figure 14: Learning curves and sharpness across training epochs for HCLT trained on the `baudio` dataset using EM.
Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left column depicts the
base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded
regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|epoch Base|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||



0 20 40 60 80 100

epoch


4

3

2.0

1.8

1.6

1.4

0.6

0.5

0.4

0.16

0.14

0.12

0.7

0.6

0.5


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||



0 20 40 60 80 100

epoch


4

3

2.0

1.8

1.6

1.4

0.6

0.5

0.4

0.16

0.14

0.12

0.7

0.6

0.5


50

40

30

20

40

30

20

50

40

30

20

40

30

20

50

40

30

20


1% Data

5% Data

10% Data

50% Data

100% Data


50

40

30

20

40

30

20

50

40

30

20

40

30

20

50

40

30

20


Figure 15: Learning curves and sharpness across training epochs for HCLT trained on the `plants` dataset using EM.
Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left column depicts the
base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded
regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


Base

epoch

Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||



0 25 50 75 100

epoch


5.0

4.5

4.0

1.2

1.0

0.9

0.8

0.18

0.16

0.0475

0.0450

0.0425

0.0400


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||
||||||||||||||||||||||



0 25 50 75 100

epoch


5.0

4.5

4.0

1.2

1.0

0.9

0.8

0.18

0.16

0.0475

0.0450

0.0425

0.0400


70

60

50

70

65

60

70

65

60

70

65

60

70

65

60


1% Data

5% Data

10% Data

50% Data

100% Data


70

60

50

70

65

60

70

65

60

70

65

60

70

65

60


Figure 16: Learning curves and sharpness across training epochs for HCLT trained on the `bnetflix` dataset using EM.
Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left column depicts the
base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded
regions represent the standard deviation across 5 independent trials.


-----

Train NLL Val NLL Sharpness


60
1% Data

40

80

60


Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

epoch

Regularized

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||
|||||||||||||||||||



0 20 40 60 80 100

epoch


10

8

6

4

6

4

2.0

1.5

0.20

0.15

0.10

0.08

0.06


5% Data

10% Data

50% Data

100% Data


40

80

60

40

80

60

40

80

60

40


Base

epoch

Base

epoch

Base

epoch

Base

epoch

Base

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||||||||||||||



0 20 40 60 80 100

epoch


10

8

6

4

6

4

2.0

1.5

0.20

0.15

0.10

0.08

0.06


60

40

80

60

40

80

60

40

80

60

40

80

60

40


Figure 17: Learning curves and sharpness across training epochs for HCLT trained on the `accidents` dataset using
EM. Each row corresponds to a different fraction of the training data (1%, 5 %, 10 %, 50 %, 100 %). The left column depicts the
base model without regularization and the right column corresponds to our Hessian-trace regularized counterpart. The shaded
regions represent the standard deviation across 5 independent trials.


-----

