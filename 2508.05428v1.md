## **Group Causal Policy Optimization for Post-Training Large Language Models**
### Ziyin Gu [12*], Jingyao Wang [12*], Ran Zuo [3], Chuxiong Sun [12], Zeen Song [12], Changwen Zheng [12], Wenwen Qiang [12†]

1 Institute of Software, Chinese Academy of Sciences, Beijing, China
2 University of the Chinese Academy of Sciences, Beijing, China
3 Communication University of China, Beijing, China


**Abstract**

Recent advances in large language models (LLMs) have
broadened their applicability across diverse tasks, yet specialized domains still require targeted post-training. Among existing methods, Group Relative Policy Optimization (GRPO)
stands out for its efficiency, leveraging groupwise relative rewards while avoiding costly value function learning. However, GRPO treats candidate responses as independent, overlooking semantic interactions such as complementarity and
contradiction. To address this challenge, we first introduce a
Structural Causal Model (SCM) that reveals hidden dependencies among candidate responses induced by conditioning on a final integrated output—forming a collider structure.
Then, our causal analysis leads to two insights: (1) projecting
responses onto a causally-informed subspace improves prediction quality, and (2) this projection yields a better baseline
than query-only conditioning. Building on these insights, we
propose Group Causal Policy Optimization (GCPO), which
integrates causal structure into optimization through two key
components: a causally-informed reward adjustment and a
novel KL-regularization term that aligns the policy with a
causally-projected reference distribution. Comprehensive experimental evaluations demonstrate that GCPO consistently
surpasses existing methods—including GRPO—across multiple reasoning benchmarks.
### **Introduction**

Recent advances in large language models (LLMs) have
significantly broadened their application potential, demonstrating remarkable capabilities in general tasks (Lai et al.
2025; Zhao et al. 2025; Minaee et al. 2024b; Jaech et al.
2024). However, fully harnessing their practical effectiveness, particularly in specialized domains, requires focused
post-training adjustments (Tie et al. 2025). While foundational pre-training establishes linguistic fluency and general reasoning, supplementary methods such as reinforcement learning with human feedback (RLHF) (Bai et al.
2022) are essential for adapting LLMs to specific applications and aligning their outputs with human preferences and
ethical norms. Among these approaches, the recently proposed Group Relative Policy Optimization (GRPO) (Shao

  - These authors contributed equally.

  - Corresponding author. Email: qiang.ww0922@gmail.com
Copyright © 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.


**(a) Supportive Chains** **(b) Conflicting Chains**

Figure 1: Examples of (a) Supportive chains: A1 provides
precise computation, A2 offers geometric insight, and A3
quickly verifies the result via Pythagorean triple recognition;
combined, they robustly lead to the optimal answer 13; (b)
Conflicting chains: B1 yields 7 due to calculation errors, B2
outputs 169 by omitting the square root, and B3 misinterprets the question to give 17; their conclusions conflict, and
mixing them with correct paths introduces errors.

et al. 2024) has garnered considerable attention due to its
significant reduction in computational overhead and memory requirements. By introducing a scalable and efficient
training mechanism, GRPO has demonstrated substantial
performance gains on many benchmarks (Guo et al. 2025).
While GRPO introduces an efficient mechanism by estimating advantages through groupwise relative rewards, it
adopts a simplifying assumption: all candidate responses
within a group are treated as independent and unrelated. This
design choice helps reduce computational complexity and
makes the method more scalable, especially when compared
to traditional value-based approaches like PPO (Schulman
et al. 2017; Ouyang et al. 2022). However, in many realworld reasoning tasks, responses generated for the same input often contain rich semantic connections. For an example
shown in Figure 1, some responses may complement each
other by covering different aspects of the problem, jointly
forming a more complete reasoning chain; others may contradict each other, revealing logical conflicts or alternative
interpretations. These interactions—whether supportive or






**A1**

**A2**

**A3**







**B1**

**B2**

**B3**


-----

conflicting—are not captured in the current formulation of
GRPO. As a result, although GRPO successfully leverages
relative reward signals within a group, it may overlook valuable information encoded in the relationships between responses. Incorporating such intra-group dynamics could enable models to better understand the structure of the output
space, leading to more nuanced learning and potentially improved alignment with human reasoning preferences.
To address this challenge from a causal perspective, we
introduce a Structural Causal Model (SCM) that explicitly
captures the relationships between the original query and
the generated candidate responses. Specifically, consider a
scenario in Figure 2 where a user inputs a query into the
LLM, resulting in multiple independently generated candidate answers. Initially, these candidate outputs seem unrelated since each is generated independently based solely on
the query. However, if we subsequently use these candidate
answers collectively to produce a final, refined response, we
unintentionally create a collider structure. In causal inference terms (Pearl, Glymour, and Jewell 2016; Pearl 2009),
a collider is a scenario where two or more independent variables influence a common variable, such that conditioning
on this common variable makes these previously independent variables become interdependent. In our case, candidate responses are initially independent when conditioned
solely on the query. But when these responses jointly influence a final integrated output, conditioning on this final
result (the collider) introduces dependencies among the candidate responses. Practically speaking, knowing the content
of the final integrated response can reveal previously hidden
relationships among candidate answers. For example, one
candidate response might provide context missing from another, forming a complementary relationship; another might
present contradictory logic, creating a conflicting relationship. Recognizing and explicitly modeling these colliderinduced relationships might help the model better leverage
hidden structural patterns within generated answers.
Formally, our causal analysis (refer to Section: Causal
Analysis and Motivation for more details) provides a rigorous theoretical basis for this intuition. Specifically, Theorem 1 indicates that when the query-response generation
process follows a collider structure, predicting an output
based on a causally adjusted baseline—that is, the projection of the original predictions onto a subspace that respects this collider structure—will consistently yield improved accuracy. In other words, rather than directly predicting responses based solely on independent evaluations, incorporating a causally informed adjustment significantly enhances prediction performance. Moreover, Corollary 2 complements this by showing that even the original query-based
predictions can benefit from incorporating this causally projected baseline. Intuitively, this can be thought of as adding
a causal “lens” through which predictions are viewed, enabling the model to correct latent biases or misunderstandings that arise from ignoring structural dependencies.
Motivated by these causal insights, we propose a novel
optimization method called Group Causal Policy Optimization (GCPO). Unlike GRPO, which evaluates each candidate
response purely based on its reward relative to the group


average, GCPO explicitly incorporates causal relationships
within the group of generated outputs. Guided by Theorem 1
and Corollary 2, GCPO introduces two major adjustments
to the original GRPO framework: (1) a causally-adjusted reward mechanism, and (2) a novel KL-divergence regularization term that explicitly considers causal structures. First,
the reward mechanism in GCPO is enhanced by projecting
each candidate response onto a causally-informed baseline.
Practically, the reward of each candidate answer is adjusted
based on how closely it aligns with this causally projected
reference. Intuitively, this approach rewards responses that
are not only individually strong but also structurally coherent with other responses. Second, to further encourage
structural consistency, GCPO introduces an additional KLdivergence regularization term. Specifically, during training,
we first compute the model’s output distribution conditioned
solely on the query. Next, we calculate a causally-adjusted
distribution that captures interdependencies among candidate responses. The sum of these two components forms
a new reference distribution, representing the model’s corrected belief after considering group-level causal structures.
By minimizing the KL-divergence between the model’s current output and this causally-informed reference, GCPO explicitly guides the model towards structurally consistent predictions. To further illustrate intuitively, the original GRPO
method measures divergence by comparing the current policy model to a standard reference model trained without
causal adjustments. GCPO, however, measures this divergence against a structurally enhanced baseline, explicitly encouraging the policy model to conform to inferred dependencies among candidate responses. The main contributions
of this paper can be summarized as:

 - Causal insight into candidate dependencies. We establish that conditioning on a final integrated output induces a collider structure among candidate responses.
Theoretically, we prove that projecting predictions onto
a causally-informed subspace reduces test error, offering
a more reliable baseline than query-only conditioning.

 - A causality-aware policy optimization method. We propose GCPO, which enhances GRPO with a causallyadjusted reward and a KL regularizer aligned to a projected reference distribution. This enables structurally
consistent and semantically robust policy updates.

 - Consistent gains across benchmarks. Experiments on
math and code reasoning tasks show that GCPO consistently outperforms GRPO. Ablations confirm the critical
role of both proposed components.
### **Related Work**

In recent years, LLMs have made remarkable progress on a
wide range of tasks, including question answering (Bottou,
Curtis, and Nocedal 2018; Bai et al. 2024), code generation
(Sadik and Govind 2025; Wang et al. 2025b), and mathematical reasoning (Minaee et al. 2024a; Wang et al. 2025a;
Muennighoff et al. 2025). However, achieving optimal performance on specialized tasks often requires targeted posttraining adaptation (Tie et al. 2025). Common approaches
such as Supervised Fine-Tuning (Raffel et al. 2020; Devlin


-----

et al. 2019) and Instruction Tuning (Sanh et al. 2022; Chung
et al. 2022; Ouyang et al. 2022) use labeled data or instructional examples to align model outputs with specific objectives, delivering strong results. Nevertheless, these posttraining methods are prone to exposure bias and may generalize poorly to novel scenarios (Touvron et al. 2023; Ballon,
Algaba, and Ginis 2025).
To address these limitations, reinforcement learning (RL)
has been adopted to tailor LLMs for domain-specific applications and align their outputs w ~~i~~ t ~~h~~ ~~h~~ uman pre ~~f~~ erences an ~~d~~
ethical standards (Ouyang et al. 2022; Bai et al. 2022). Under RL-based strategies, GRPO (Shao et al. 2024) has garnered widespread attention with its efficiency in lowering
computational and memory burdens. It introduces a scalable
group-wise optimization framework, where policy updates
leverage relative advantages within groups of candidate responses. This design enables flexible integration of process
rewards and preference signals, resulting in great performance. Building upon GRPO, a number of variants have
been proposed, leveraging process-level reward estimation,
adaptive reward shaping, and regularization strategies to further improve efficiency and generalization. Specifically, LCR1 (Cheng et al. 2025) employs a novel combination of a
length reward for overall conciseness and a compress reward
that is specifically designed to remove the invalid portion of
the thinking process. GVPO (Zhang et al. 2025) incorporates
the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment
with the optimal policy. Dr.GRPO (Liu et al. 2025) improves
token efficiency while maintaining reasoning performance.
L2T (Wang et al. 2025a) proposes an information-theoretic
reinforcement fine-tuning framework for LLMs to make the
models achieve optimal reasoning with fewer tokens.
However, these exist RL-based policy optimization methods often treat candidate responses as independent, thus ignoring the rich structural and causal relationships that are
embedded in the interrelationships among responses. To address this, in this work, we propose GCPO that explicitly
models and leverages intra-group dependencies to improve
the general coherence and reasoning capability of LLMs.
### **Causal Analysis and Motivation**

This section begins by introducing an SCM. Based on this
foundation, we construct a causal analysis framework to
evaluate the quality of reasoning strategies in LLMs. We
conclude by outlining the motivation that informs the design
of the proposed approach.
#### **Causal Analysis**

Consider an SCM illustrated in Figure 2. Here, the variable *q* represents the original input query. The variables
*y* 0 *, y* 1 *, · · ·, y* *n−* 1 respectively denote the corresponding outputs obtained by independently feeding the same query *q*
into the function *π* . The variable *y* *n* is a new output derived
by feeding *q, y* 0 *, y* 1 *, · · ·, y* *n−* 1 into the *π* . Consequently, the
SCM includes causal paths: *{q →* *y* *i* *→* *y* *n* *}* *[n]* *i* =0 *[−]* [1] [and]
*q →* *y* *n* . In addition, the path *q →{y* 0 *, y* 1 *, · · ·, y* *n−* 1 *}* forms
a fork structure, while the path *{y* 0 *, y* 1 *, · · ·, y* *n−* 1 *} →* *y* *n*





Figure 2: The SCM under our setting. *q* is the input query,
*{y* 0 *, · · ·, y* *n−* 1 *}* represents the set of outputs obtained by
feeding *q* into a LLM *n* times, and *y* *n* denotes the final output produced by inputting *{q, y* 0 *, · · ·, y* *n−* 1 *}* into a LLM.

forms a collider structure (Pearl 2009). These structures lead
to two conditional independence relations (Pearl, Glymour,
and Jewell 2016): conditioned on *q*, the variables in *{y* *i* *}* *[n]* *i* =0 *[−]* [1]
are mutually independent; however, conditioned additionally on *y* *n*, these variables become mutually dependent.
From a Bayesian perspective, when a model is trained
to optimality by minimizing the cross-entropy loss or mean
squared error loss, it can be viewed as estimating the conditional expectation of the output distribution given the input context (Zhang and Bowman 2018; Goodfellow, Bengio, and Courville 2016; Bengio et al. 2003). More precisely, if the function *π* is trained using cross-entropy loss
and achieves optimality, then statistical decision theory implies that *π* ( *X* ) = E[ *Y | X* ], where *X* refers to the input
in a general sense, and *Y* refers to the corresponding output. For clarity of distinction, let *π* denote a function that
outputs a probability, similar to the formulation used during
training based on cross-entropy loss minimization, and let
*π* *[∗]* denote its Bayesian optimal counterpart. Then, we have
*π* ( *y* 0 *| q* ) = *p* *π* ( *y* 0 *| q* ) and *π* *[∗]* ( *q* ) = E[ *y* 0 *| q* ]. Based on the
conditional independence relations discussed in the previous
paragraph, the following conclusion can be drawn:

E [ *π* *[∗]* ( *x* ) *−* *π* *[∗]* ( *q* ) *| q, y* 1: *n−* 1 ]
= E [E [ *y* 0 *| x* ] *−* E [ *y* 0 *| q* ] *|q, y* 1: *n−* 1 ] (1)
= E [ *y* 0 *| q, y* 1: *n−* 1 ] *−* E [ *y* 0 *| q, y* 1: *n−* 1 ] = 0 *,*

where *x* = *{q, y* 1 *, · · ·, y* *n* *}*, *y* 1: *n−* 1 = *{y* 1 *, · · ·, y* *n−* 1 *}* .
It is important to note that the variables *y* 0 *, y* 1 *, · · ·, y* *n−* 1
are used as generic placeholders. In other words, Equation (1) still holds when *y* 0 is exchanged with any *y* *i* *∈*
*{y* 1 *, · · ·, y* *n−* 1 *}* . All subsequent results in this section follow this property, and we will not reiterate it in the follows.
Let *F* denote the space of square-integrable functions, we
can obtain that *π* *[∗]* *∈F* . Let Φ be a functional operator acting
on *π* *[∗]* ( *x* ), defined as the following:

Φ *· π* *[∗]* ( *x* ) = E[ *π* *[∗]* ( *x* ) *|q, y* 1 *, · · ·, y* *n−* 1 ] *.* (2)

Based on this, we define a causal-related mapping Ψ as: Ψ =
Id *−* Φ, where Id denotes the identity mapping. Noting that
the output of Φ can be viewed as the image space, while the
output of Ψ corresponds to the kernel space associated with
Φ. Meanwhile, let *X* be the random variable of the query and


-----

*Y* be the random variable of the answer, assume *X × Y ∼*
*p* ( *X, Y* ) where *p* ( *X, Y* ) is the joint probability distribution
of *X* and *Y* . Given *π* 1 *[∗]* *[, π]* 2 *[∗]* *[∈F]* [,][ ∆] [is defined as:]

∆( *π* 1 *[∗]* *[, π]* 2 *[∗]* [) =] *p* ( *X,Y* E ) [[] *[Y][ −]* *[π]* 1 *[∗]* [(] *[X]* [)]]

*−* E 2 [(] *[X]* [)]] *[ .]* (3)
*p* ( *X,Y* ) [[] *[Y][ −]* *[π]* *[∗]*

Equation (3) can be interpreted as the test error or the expected risk. Then, the following conclusion can be drawn:

**Theorem 1** *Given the condition of Equation (1) and the*
*SCM shown in Figure 2, for ∀π* *[∗]* *∈F, the following holds:*

∆( *π* *[∗]* ( *x* ) *,* Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* )) *≥* 0 *.* (4)

The proof of Theorem 1 is presented in the Appendix. We
provide an intuitive understanding of Theorem 1. First, the
collider structure makes us realize that, although some variables may appear independent on the surface, they could potentially be dependent through a common influence. If this
relationship is not captured, it may affect the accuracy of the
LLM. Second, *π* *[∗]* ( *x* ) is tasked with predicting an outcome
based on the input. From a causal perspective, *π* *[∗]* ( *x* ) serves
as a generative function. If we know that the data generation
process follows a collider structure, we can project the hypothesis space formed by all *π* *[∗]* ( *x* ) onto a subspace formed
by those *π* *[∗]* ( *x* ) that can recognize the collider structure. This
is akin to adding a pair of “glasses” to *π* *[∗]* ( *x* ), helping it identify latent dependencies that are not immediately apparent,
thereby improving its predictive accuracy on new data. Furthermore, *π* *[∗]* ( *q* ) represents the model’s initial prediction in
the absence of the collider structure’s influence, and it can be
viewed as a preliminary estimate of the input. By incorporating this initial estimate, we can further optimize the model,
ensuring that the final output does not merely rely on the
preliminary estimate but fully considers the inherent structure of the data. Similarly, the follows can also be drawn:

**Corollary 2** *Given the condition of Equation (1) and the*
*SCM shown in Figure 2, for π* *[∗]* *and* Ψ *, the following holds:*

∆( *π* *[∗]* ( *q* ) *,* Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* )) *≥* 0 *.* (5)

The proof of Corollary 2 is provided in the Appendix.
Since the intuitive interpretation of Corollary 2 closely parallels that of Theorem 1, we omit a redundant explanation.
Together, Theorem 1 and Corollary 2 suggest that when the
query generation process involves a collider structure, it is
possible to project the hypothesis space of an LLM onto a
subspace that better aligns with this structure. By incorporating a baseline function, the model can be further optimized.
This approach leverages conditional independence relations
encoded in the causal graph, thereby improving the generalization capability of the LLM and enabling more stable and
reliable performance on unseen queries.
#### **Motivation Analysis**

GRPO has been widely adopted for post-training LLMs due
to its efficiency and simplicity. It treats the model as a policy and optimizes it by comparing relative rewards among
candidate responses generated for the same query. However,


GRPO assumes that all candidates are independent, overlooking potential semantic interactions such as complementarity or contradiction (see Figure 1). This limits the reward
signal expressiveness and may hinder LLMs generalization.
From the above causal analysis, while candidate responses are independently sampled from the query, they often influence a final integrated output, thus forming a collider structure. Under this structure, responses become conditionally dependent when the final output is observed. Our
theoretical findings (Theorem 1 and Corollary 2) show that
projecting predictions onto a causally-informed subspace,
expressed as Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* ), indeed leads to consistently
lower test error than using *π* *[∗]* ( *q* ) or *π* *[∗]* ( *x* ) alone.
This insight motivates a principled revision of GRPO’s
preference mechanism. Instead of favoring candidates
purely based on relative rewards, we can additionally consider their alignment with the causally projected output.
This adjustment allows the model to exploit latent dependencies among responses, encouraging structurally coherent
and semantically accurate outputs. Furthermore, we can introduce a causal regularization term that aligns the policy
with a causally-informed reference distribution. Together,
these changes form the basis of the following proposed
GCPO, a causality-aware optimization framework that enhances model performance by integrating structural reasoning signals into the learning process.
### **The Proposed Method**

In this section, we propose GCPO, a new post-training algorithm for LLMs. GCPO can be viewed as a variant of GRPO,
with the primary differences lying in two aspects: the relative advantage function and the KL Divergence.
#### **A Brief Introduction to GRPO**

For a given query *q*, GRPO samples a set of outputs
*{y* 0 *, y* 1 *, · · ·, y* *n−* 1 *}* from the old policy *π* *θ* old . It then updates
the policy *π* *θ* by maximizing the objective:

*J* GRPO = E [ *q∼P,{y* 0 *,y* 1 *,···,y* *n−* 1 *}* ]


where *π* ref is a fixed reference policy and often set to *π* *θ* old .
The relative advantage *A* *i* is calculated within each sampled
group to capture the comparative quality of outputs:

*A* *i* = [ *r* *i* *−* mean( *r* 0 *, · · ·, r* *n−* 1 )] */* std( *r* 0 *, · · ·, r* *n−* 1 ) *,* (8)


1

*n*


*n−* 1
�

*i* =0


1
*T* *i*


*T* *i*
� *{* [min( *R* *i,j* ( *θ* ) *A* *i* *,* Ξ *ij* *· A* *i* )]

*j* =0

*−βD* KL ( *π* *θ* *∥* *π* ref ) *},*


(6)


where *ϵ* and *β* are hyperparameters, Ξ *ij* = clip( *R* *i,j* ( *θ* ) *,* 1 *−*
*ϵ,* 1 + *ϵ* ), clip( *·* ) is a truncation function ensuring stable updates, *P* denotes the distribution of queries. Because an LLM generates a output *y* *i* = ( *y* *i,* 1 *, . . ., y* *i,T* *i* )
token-by-token in an autoregressive manner, where *T* *i* denotes the token length of *y* *i*, thus, *R* *i,j* ( *θ* ) and *D* KL ( *·* ) are
also calculated in a token-by-token manner. Then, the KL
divergence term *D* KL ( *π* *θ* *∥* *π* ref ) is computed as:


*π* ref ( *y* *i* *,j* *|q,* *y* *i* *,* *<* *j* )


*π* ref ( *y* *i* *,j* *|q,* *y* *i* *,* *<* *j* ) *[y]* *[i]* *[,]* *[<]* *[j]* [)]

*π* *θ* ( *y* *i,j* *|q, y* *i,<j* ) *[−]* [log] *[ π]* *π* [ref] *θ* ( [(] *y* *[y]* *i,j* *[i]* *[,j]* *|q, y* *[|][q,]* *i,<j* )



*[,]* (7)

*π* *θ* ( *y* *i,j* *|q, y* *i,<j* ) *[−]* [1] *[,]*


-----

where *r* *i* = reward( *y* *i* ) combines task-specific accuracy and
formatting rewards, while mean( *·* ) and std( *·* ) are the mean
and standard deviation over the reward group. At last, the
importance ratio *R* *i,j* ( *θ* ) is defined as:

*π* *θ* ( *y* *i,j* *|q, y* *i,<j* ) */π* *θ* old ( *y* *i,j* *|q, y* *i,<j* ) *.* (9)
#### **Details of the Proposed GCPO**

For a query *q*, GCPO also samples a group of outputs
*{y* 0 *, y* 1 *, · · ·, y* *n−* 1 *}* from the old policy *π* *θ* old . Different from
GRPO, GCPO then input *q* and *{y* 0 *, y* 1 *, · · ·, y* *n−* 1 *}* into the
old policy *π* *θ* old for *n* times to obtain a final outputs *y* *n* and
*{y* *n,i* *}* *[n]* *i* =1 *[−]* [1] [. GCPO optimizes the policy model] *[ π]* *[θ]* [ by maxi-]
mizing the following objective:

*J* GCPO = E [ *q∼P,{y* 0 *,y* 1 *,···,y* *n* *},{y* *n,i* *}* *ni* =1 *−* 1 []]


*T* *i*
� *{* [min( *R* *i,j* ( *θ* ) *B* *i* *,* Ξ *ij* *· B* *i* )]

*j* =0


1

*n*


*n−* 1
�

*i* =0


1
*T* *i*


(10)


Let *Z* [¯] *i* = mean( *Z* *i,* 1 *, · · ·, Z* *i,n* ), which can be regarded
as a Monte Carlo approximation of the output representation of *π* *θ* *[∗]* [(] *[x]* [)][ for] *[ y]* *[i]* [;] **[ Step 3:]** [ Approximating][ Φ] *[ ·][ π]* *[∗]* [(] *[x]* [)][ for]
*y* *i* *∈{y* *i* *}* *i* *[n]* =0 *[−]* [1] [. According to the definition of][ Φ][, we first]
define *y* *n,* 0 = *y* *n* and *x* *i,j* = *{q, y* 0 *, · · ·, y* *n,j* *} \ {y* *i* *}*,
where *j ∈{* 0 *, · · ·, n −* 1 *}* . We repeat “Step 2” for the
set *{x* *i,j* *}* *[n]* *j* =0 *[−]* [1] [to obtain the corresponding representations]
*{Z* [¯] *i,j* *}* *j* *[n]* =0 *[−]* [1] [. The average of it is denoted by][ ¯] *[Z]* *[′][i]* [, which serves]
as a Monte Carlo approximation of the output representation
of Φ *·π* *[∗]* ( *x* ) for *y* *i* . **Step 4:** Approximating Ψ *·π* *[∗]* ( *x* )+ *π* *[∗]* ( *q* ).
Combining the previous steps, *Z* [¯] *i* *−* *Z* [¯] *[′]* *i* + ¯ *z* serves as a
Monte Carlo approximation of the output representation of
Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* ). **Step 5:** Finally, Υ *i* is calculated by:

Υ *i* = *α ·* cos( *z* *i* *,* *Z* [¯] *i* *−* *Z* [¯] *[′]* *i* + ¯ *z* ) *,* (11)

where *α* is a scaling hyperparameter, and cos( *·, ·* ) denotes
the cosine similarity between two vectors.
**Design of KL Divergence** . The design of *D* KL ( *π* *θ* *∥*
*π* ref *[′]* [)][ is directly inspired by Theorem 1. According to Equa-]
tion (4), the expected risk of *π* *[∗]* ( *x* ) is higher than that of
Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* ). This suggests that the output generated
by Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* ) may have better quality than the one
produced by *π* *[∗]* ( *x* ). Since *π* represents the probability distribution over output tokens, while *π* *[∗]* corresponds to the actual
generated content, a natural way to improve the performance
of *π* is to encourage its output distribution to align with that
of Ψ *· π* ( *x* ) + *π* ( *q* ). This motivates the use of KL divergence
as a regularization term. Specifically, for *D* KL ( *π* *θ* *∥* *π* ref *[′]* [)][,]
the definition follows a procedure similar to Equation (7):

*n−* 1

*π* ref *[′]* [(] *[x]* *[i]* [)] *π* ref *[′]* [(] *[x]* *[i]* [)]

� *i* =0 [ *π* *θ* ( *y* *i,j* *|x* *i* *, y* *i,<j* ) *[−]* [log] *π* *θ* ( *y* *i,j* *|x* *i* *, y* *i,<j* ) *[−]* [1]] *[,]* [ (12)]

where *π* ref *[′]* [(] *[x]* *[i]* [) = Ψ] *[ ·][ π]* [(] *[y]* *[i,j]* *[|][x]* *[i]* *[, y]* *[i,<j]* [) +] *[ π]* [(] *[y]* *[i,j]* *[|][q, y]* *[i,<j]* [)][.]
Based on the analysis in the previous paragraph, we derive
the following approximations: (1) Φ *· π* ( *y* *i,j* *| x* *i* *, y* *i,<j* ) can
be approximated by [�] *l* *[n]* =0 *[−]* [1] *[π]* [(] *[y]* *[i,j]* *[ |][ x]* *[i]* *[, y]* *[n,l]* *[, y]* *[i,<j]* [)][, and (2)]
the analytical expression of Ψ *· π* ( *y* *i,j* *| x* *i* *, y* *i,<j* ) + *π* ( *y* *i,j* *|*
*q, y* *i,<j* ) can be approximated by the following equation:


*−βD* KL ( *π* *θ* *∥* *π* ref ) *} −* *κD* KL ( *π* *θ* *∥* *π* ref *[′]* [)] *[,]*

where *κ* is a hyper-parameter, *B* *i* is the newly proposed relative advantage function, *π* ref *[′]* [is the newly proposed pre-]
defined policy model, and *D* KL ( *π* *θ* *∥* *π* ref *[′]* [)][ is the newly pro-]
posed regularizer. In the following, we conduct an in-depth
study of the terms *B* *i* and *D* KL ( *π* *θ* *∥* *π* ref *[′]* [)][.]
**Design of Relative Advantage Function** . Formally, we
define *B* *i* = *A* *i* *·* Υ *i*, where *A* *i* is computed in the same
way as in GRPO. We next describe the procedure for designing and computing Υ *i* . The design of Υ *i* is inspired by
Corollary 2. According to Equation (2), when we focus on
the answer variable *y* 0, the expected risk of the output from
*π* *[∗]* ( *q* ) is higher than that of Ψ *·π* *[∗]* ( *x* )+ *π* *[∗]* ( *q* ). This suggests,
conservatively, that the output quality of Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* )
is better than that of *π* *[∗]* ( *q* ). Based on this observation, the
advantage value for each candidate answer corresponding to
a given query can be designed as follows: the closer the candidate is to the output of Ψ *· π* *[∗]* ( *x* ) + *π* *[∗]* ( *q* ), the higher the
advantage value it receives.
The proposed approach faces two practical challenges
during implementation: (1) how to approximate Ψ *· π* *[∗]* ( *x* ) +
*π* *[∗]* ( *q* ); and (2) how to measure the similarity between model
outputs. Since *π* *[∗]* represents concrete generated content,
which typically includes both intermediate reasoning steps
and the final answer, we propose to approximate Ψ *·* *π* *[∗]* ( *x* )+
*π* *[∗]* ( *q* ) using the feature representation of the output. Then,
we measure similarity based on the cosine distance between
these feature representations. The detailed procedure is as
follows: **Step 1:** Approximating *π* *θ* *[∗]* [(] *[q]* [)][. Given a answer] *[ y]* *[i]* [,]
we define *o* *i* as the combination of *y* *i* and the intermediate reasoning steps leading to it. We then feed *o* *i* back into
*π* *θ*, and extract the hidden representation of the final token
from the last layer as the feature representation of *o* *i*, denoted by *z* *i* . Then, let ¯ *z* = mean( *z* 0 *, · · ·, z* *n−* 1 ), which can
be regarded as a Monte Carlo approximation of the output
representation of *π* *θ* *[∗]* [(] *[q]* [)][;] **[ Step 2:]** [ Approximating] *[ π]* *θ* *[∗]* [(] *[x]* [)][ for]
*y* *i* *∈{y* *i* *}* *[n]* *i* =0 *[−]* [1] [. Because that] *[ π]* *θ* *[∗]* [(] *[x]* [) =][ E][ [] *[y]* [0] *[ |][ x]* []][, when we]
focus on *y* *i*, it equals to that *y* 0 is exchanged with *y* *i*, and the
condition *x* is exchanged with *x* *i* = *{q, y* 0 *, · · ·, y* *n* *} \ {y* *i* *}* .
We then feed *x* *i* into *π* *θ* for *n* times to obtain the corresponding outputs *{O* *i,j* *}* *[n]* *j* =1 [and representations] *[ {][Z]* *[i,j]* *[}]* *[n]* *j* =1 [.]


*n−* 1
*π* ( *y* *i,j* *| x* *i* *, y* *i,<j* ) *−* �


� *π* ( *y* *i,j* *| x* *i* *, y* *n,l* *, y* *i,<j* )

*l* =0


*i,<j* *l* =0 *y* *i,j* *i* *, y* *n,l* *, y* *i,<j* (13)

+ *π* ( *y* *i,j* *| q, y* *i,<j* ) *.*


Note that, similar to GRPO, the computation in Equation
(12) is also carried out in a token-wise manner. Finally, the
training process is also similar to GRPO. In the appendix,
the overall procedure of the GCPO training is illustrated
through the pseudocode.
### **Experiments**

In this section, we conduct comprehensive experiments and
ablation studies on multiple reasoning benchmarks to evaluate the effectiveness of our proposed method.
#### **Experimental Settings**

We conduct evaluations of our method across several reasoning benchmarks, including AIME24-25, AMC, MATH500


-----

Table 1: Pass@1 performance on various math reasoning benchmarks. We compare base models trained with different finetuning approaches. The best results are highlighted in **bold** .

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
|||||HumanEval|0-shot Pas|s@1|
|||||HumanEval|5-shot Pas|s@1|

|Col1|Col2|Col3|Col4|Col5|Col6|Accura Best (|cy = 2)|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Accura Best (|cy = 0.06)|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||


Cosine similarity Euclidean distance Gaussian distance
Distance Metric


76

74

72

70

68

66

64

62

60



58.0

57.5

57.0

56.5

56.0




57.5

57.0

56.5

56.0

55.5

55.0


(c) Evaluation of similarity metric


58.0

57.5

57.0

56.5

56.0

55.5

55.0

54.5

54.0


55.5

0.02 0.03 0.04 0.05 0.06 0.07 0.08

(b) Ablation Study of *κ*


w/o Fine-tuning w/ GRPO w/ GVPO w/ Dr.GRPO w/ GCPO
Methods


Figure 3: Performance analysis on code reasoning tasks.
We record the 1-shot and 5
shot results on HumanEval.


54.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5

(a) Ablation Study of *α*


Figure 4: Ablation Study. (a) and (b) shows the results for parameter sensitivity, i.e., hyperparameter *α* and *κ* . (c) shows the evaluation of similarity metric in Υ *i* .


(Hendrycks et al. 2021), MinervaMATH (Lewkowycz et al.
2022), and HumanEval (Chen et al. 2021). Our experiments use DeepScaleR-1.5B-Preview and DeepSeek-R1Distill-Qwen-1.5B, and DeepSeekR1-Distill-Qwen-7B, and
Qwen2-7B-Instruct as base models. We compare our method
against classic and SOTA reinforcement learning methods,
including GRPO (Shao et al. 2024), GVPO (Zhang et al.
2025), ReST-MCTS (Zhang et al. 2024), and Dr.GRPO (Liu
et al. 2025). DeepScaleR-1.5B-Preview, having been previously fine-tuned on 40k math QA pairs, is further fine-tuned
on 919 AIME problems from 1989 to 2023. DeepSeek-R1Distill-Qwen-1.5B is fine-tuned on a random subset of 4,000
QA pairs from NuminaMath (Li et al. 2024). Following
(Wang et al. 2025a), all training and evaluation stages are
constrained to a token budget of 16,384. We adopt a learning rate of 1e *−* 6, weight decay of 0.01, and batch size of
256. All experiments are conducted on A100 GPU clusters.
#### **Performance Analysis**

**Performance on Mathematical Reasoning Tasks.** We
evaluate our method against all baseline approaches across


a comprehensive set of benchmarks, including AIME
2024, AIME 2025, AMC 2023, MATH500, and MinervaMATH. We compare three widely used base models,
including DeepScaleR-1.5B-Preview, DeepSeek-R1-DistillQwen-1.5B, and DeepSeek-R1-Distill-Qwen-7B. We report
the pass@1 accuracy following (Wang et al. 2025a). Table 1 shows the pass@1 performance. From the results,
we can observe that across all settings, GCPO consistently
achieves the best average performance, outperforming both
the base models and all competitive baselines. Specifically, for DeepScaleR-1.5B-Preview, GCPO delivers an average improvement of 2.3% over the base model. Similar trends are observed for DeepSeek-R1-Distill-Qwen-1.5B
and DeepSeek-R1-Distill-Qwen-7B, where GCPO yields
average improvements of 2.5% and 2.2%, respectively.
Compared to SOTA RL baselines such as GRPO and GVPO,
GCPO offers stronger gains on more challenging datasets
(e.g., over 1% on AIME and MinervaMATH), highlighting its effectiveness in capturing complex reasoning patterns
through causally informed optimization. Notably, the performance margins between GCPO and the strongest baseline


-----

Table 2: Ablation study of the components within GCPO. We report Pass@1 on five benchmarks. Removing either the advantage
weighting or KL divergence degrades overall performance, indicating both are essential.


are largest on the hardest benchmarks, indicating that the
benefits of causal group structure modeling become more
pronounced as task complexity increases. Moreover, the
consistent improvements demonstrate that GCPO is robust
to model scale and architecture, providing a generalizable
fine-tuning strategy for math reasoning tasks.

**Performance on Code Reasoning Tasks.** Further, for
code reasoning tasks, we run different fine-tuning pipelines
on Qwen2-7B-Instruct and evaluate them using the standard HumanEval protocol. The results are summarized in
Figure 3, which reports both 0-shot and 5-shot Pass@1 accuracies for each method. From the results, we can observe that across both evaluation settings, GCPO achieves
the strongest performance among all compared methods. In
particular, GCPO attains a 0-shot Pass@1 of 65.1% and a 5shot Pass@1 of 72.0%, outperforming the foundation model
by 2.9% and 5.5%, respectively. Relative to other policy
optimization methods, such as GRPO and GVPO, GCPO
consistently delivers higher accuracy. Notably, the gap between GCPO and previous methods becomes even more
pronounced in the multi-shot evaluation, highlighting the
advantage of incorporating causal projection and structureaware regularization in leveraging contextual information
and enabling compositional reasoning. These results further
demonstrate the effectiveness of the proposed GCPO.

**Visualization Analysis** Given the substantial computational cost of training LLMs, maintaining stable training dynamics is crucial. To assess this, we use the gradient norm
(as a proxy for policy variance) to measure training stability. We record the gradient norm during training for both the
baseline methods and the proposed GCPO. The results, presented in the Appendix (Figure 1), demonstrate that GCPO
achieves the highest stability, with the gradient norm remaining consistently steady throughout training.
#### **Ablation Study**

We conduct a series of ablation studies to evaluate the contribution of each component within our method, the best parameterization and implementation choices, etc.

**The effect of different components.** To assess the effect
of each component in GCPO, we conduct ablation studies.
Specifically, we consider two alternative configurations: (i)
removing the advantage weighting term (i.e., reusing *A* *i* for
all *i* ); and (ii) removing the additional KL divergence term
(i.e., setting *κ* = 0). Notably, the overall contribution of
our reward formulation has already been substantiated in Table 1. Here, we focus on isolating the impact of these individual mechanisms. The ablation results are shown in Fig

ure 2. We can observe that both terms are critical for LLM
reasoning. These findings underscore the advantages of our
design and the effectiveness of GCPO.

**Parameter sensitivity.** We select the hyperparameters
of GCPO based on a systematic evaluation of reasoning performance. Specifically, we conduct a grid search
over the hyperparameter *α* and the KL regularization
coefficient *κ* to identify the optimal configuration for
our method. For *α*, which controls the influence of the
causal projection factor, we explore a range of values:

[0 *,* 0 *.* 5 *,* 1 *,* 1 *.* 4 *,* 1 *.* 8 *,* 2 *,* 2 *.* 4 *,* 2 *.* 8 *,* 3 *,* 3 *.* 4]. For *κ*, which balances
the KL regularization strength, we first perform a coarse grid
search over [0 *.* 02 *,* 0 *.* 04 *,* 0 *.* 06 *,* 0 *.* 08] with a step size of 0 *.* 02,
and subsequently conduct a finer search within the promising interval using a step size of 0 *.* 01. For each configuration
of ( *α, β* ), we record the Pass@1 performance. As shown
in Figure 4(a)-(b), model accuracy initially increases with
larger values of *α* and *κ*, but plateaus or slightly degrades
when these values become too large; the best results are consistently achieved with *α* = 2 and *β* = 0 *.* 06. These values
are thus adopted as our default hyperparameter settings.

**Evaluation of metric for** Υ *i* According to Eq. 11, we
compute Υ *i* by calculating the cosine similarity *cos* ( *·* ). To
evaluate the impact of this metric on performance, we conduct an ablation study comparing different similarity measures, including cosine similarity, Euclidean distance, and
Gaussian distance. The results are shown in Figure 4(c).
With the introduction of the hyperparameter *α*, the performance differences among various similarity measures are
negligible. We ultimately select cosine similarity as the default metric, primarily because it allows for more flexible
and convenient tuning of *α* (See the Appendix for details).
### **Conclusion**

In this paper, we present GCPO, a novel post-training
method that integrates causal structure into policy optimization for large language models. Building on the limitations of GRPO, GCPO addresses the overlooked interdependencies among groupwise candidate responses by modeling them through an SCM. Our analysis reveals that
conditioning on a final integrated response induces a collider structure, which in turn exposes latent dependencies among originally independent candidates. Guided by
this insight, GCPO introduces two key components: (1) a
causally-adjusted reward mechanism that projects individual responses onto a structurally coherent subspace, and (2)
a KL-divergence regularization term that aligns the policy
with a causally-informed reference distribution. Extensive


-----

experiments across multiple reasoning benchmarks demonstrate that GCPO substantially outperforms existing baselines, confirming the benefits of incorporating causal reasoning into groupwise optimization. Our findings underscore the importance of structural awareness in reinforcement learning for LLM post-training and suggest promising
directions for future work on causality-aware RLHF.
### **References**

Bai, H.; Zhou, Y.; Cemri, M.; Pan, J.; Suhr, A.; Levine, S.;
and Kumar, A. 2024. DigiRL: Training In-The-Wild DeviceControl Agents with Autonomous Reinforcement Learning.
arXiv:2406.11896.

Bai, Y.; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.;
Jones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon,
C.; et al. 2022. Training a Helpful and Harmless Assistant
with Reinforcement Learning from Human Feedback. *arXiv*
*preprint arXiv:2204.05862* .

Ballon, M.; Algaba, A.; and Ginis, V. 2025. The Relationship Between Reasoning and Performance in Large Language Models–o3 (mini) Thinks Harder, Not Longer. *arXiv*
*preprint arXiv:2502.15631* .

Bengio, Y.; Ducharme, R.; Vincent, P.; and Jauvin, C. 2003.
A neural probabilistic language model. *Journal of machine*
*learning research*, 3(Feb): 1137–1155.

Bottou, L.; Curtis, F. E.; and Nocedal, J. 2018. Optimization Methods for Large-Scale Machine Learning.
arXiv:1606.04838.

Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. D. O.;
Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman,
G.; et al. 2021. Evaluating large language models trained on
code. *arXiv preprint arXiv:2107.03374* .

Cheng, Z.; Chen, D.; Fu, M.; and Zhou, T. 2025. Optimizing Length Compression in Large Reasoning Models. *arXiv*
*preprint arXiv:2506.14755* .

Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.;
Fedus, W.; Li, E.; Wang, X.; Hu, X.; Roberts, A.; Mehta,
H.; Wei, J.; Chandu, K. R.; Gritsenko, A.; Piantanida, P.;
Chowdhery, A.; Clark, J. H.; Schick, T.; Dwivedi-Yu, J.; Yu,
J.; Shi, K.; Li, X.; Ippolito, D.; Zhou, D.; Ainslie, J.; Firat,
O.; Lu, Y.; Dean, J.; Le, Q. V.; and Chi, E. H. 2022. Scaling Instruction-Finetuned Language Models. *arXiv preprint*
*arXiv:2210.11416* .

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In *Proceedings of the 2019 Con-*
*ference of the North American Chapter of the Association*
*for Computational Linguistics: Human Language Technolo-*
*gies, Volume 1 (Long and Short Papers)*, 4171–4186.

Goodfellow, I.; Bengio, Y.; and Courville, A. 2016. *Deep*
*learning*, volume 1.

Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.;
Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1:
Incentivizing reasoning capability in llms via reinforcement
learning. *arXiv preprint arXiv:2501.12948* .


Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart,
S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring
mathematical problem solving with the math dataset. *arXiv*
*preprint arXiv:2103.03874* .

Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; El-Kishky,
A.; Low, A.; Helyar, A.; Madry, A.; Beutel, A.; Carney,
A.; et al. 2024. Openai o1 system card. *arXiv preprint*
*arXiv:2412.16720* .

Lai, H.; Liu, X.; Gao, J.; Cheng, J.; Qi, Z.; Xu, Y.; Yao, S.;
Zhang, D.; Du, J.; Hou, Z.; et al. 2025. A Survey of PostTraining Scaling in Large Language Models. In *Proceedings*
*of the 63rd Annual Meeting of the Association for Computa-*
*tional Linguistics (Volume 1: Long Papers)*, 2771–2791.

Lewkowycz, A.; Andreassen, A.; Dohan, D.; Dyer, E.;
Michalewski, H.; Ramasesh, V.; Slone, A.; Anil, C.; Schlag,
I.; Gutman-Solo, T.; et al. 2022. Solving quantitative reasoning problems with language models. *Advances in Neural*
*Information Processing Systems*, 35: 3843–3857.

Li, J.; Beeching, E.; Tunstall, L.; Lipkin, B.; Soletskyi, R.;
Huang, S.; Rasul, K.; Yu, L.; Jiang, A. Q.; Shen, Z.; et al.
2024. Numinamath: The largest public dataset in ai4maths
with 860k pairs of competition math problems and solutions.
*Hugging Face repository*, 13: 9.

Liu, Z.; Chen, C.; Li, W.; Qi, P.; Pang, T.; Du, C.; Lee, W. S.;
and Lin, M. 2025. Understanding r1-zero-like training: A
critical perspective. *arXiv preprint arXiv:2503.20783* .

Minaee, S.; Mikolov, T.; Nikzad, N.; Chenaghlu, M.; Socher,
R.; Amatriain, X.; and Gao, J. 2024a. Large Language Models: A Survey. *arXiv preprint arXiv:2402.06196* .

Minaee, S.; Mikolov, T.; Nikzad, N.; Chenaghlu, M.; Socher,
R.; Amatriain, X.; and Gao, J. 2024b. Large language models: a survey (2024). *URL https://arxiv. org/abs/2402.06196*,
7(8): 9.

Muennighoff, N.; Yang, Z.; Shi, W.; Li, X. L.; Fei-Fei, L.;
Hajishirzi, H.; Zettlemoyer, L.; Liang, P.; Cand`es, E.; and
Hashimoto, T. 2025. s1: Simple test-time scaling. *arXiv*
*preprint arXiv:2501.19393* .

Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. *arXiv preprint arXiv:2203.02155* .

Pearl, J. 2009. *Causality* . Cambridge university press.

Pearl, J.; Glymour, M.; and Jewell, N. P. 2016. *Causal in-*
*ference in statistics: A primer* . John Wiley & Sons.

Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer. *Journal of Machine Learning Research*,
21(140): 1–67.

Sadik, A. R.; and Govind, S. 2025. Benchmarking LLM for
Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3.
arXiv:2504.16027.

Sanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika,
L.; Alyafeai, Z.; Chaffin, A.; Srinivasan, A.; Yong, Z. X.;
Kim, T.; Crowell, E. S.; Kudugunta, S.; Sharma, A.; Ong,
R.; Sharma, S.; Lo, A.; Bari, M. S.; Xu, C.; Thakker, U.;


-----

Dey, M.; Desai, S.; Sangwan, R.; Geng, X.; Arora, D.; Ram,
D.; Wang, H.; Chandu, K.; Kashyap, A.; Tan, S.; Gotmare,
A. D.; Swabha, S.; Phang, J.; Chan, H. P.; Urbanek, J. H.;
Gururangan, S.; d. S. Clemente, M. V.; McMahan, B.; Albanie, S.; Welbl, J.; Liu, Q.; Malmi, E.; Jean, S.; Kuo, J. T.;
Jiang, M. T.-J.; Xu, Y.; Conneau, A.; McCoy, R. T.; Taylor,
S.; Smith, N. A.; Zettlemoyer, L.; Ruder, S.; Yogatama, D.;
Cho, K.; and Rush, A. M. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. In *International*
*Conference on Learning Representations (ICLR)* .

Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
*arXiv preprint arXiv:1707.06347* .

Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang,
H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath:
Pushing the limits of mathematical reasoning in open language models. *arXiv preprint arXiv:2402.03300* .
Tie, G.; Zhao, Z.; Song, D.; Wei, F.; Zhou, R.; Dai, Y.; Yin,
W.; Yang, Z.; Yan, J.; Su, Y.; et al. 2025. A survey on posttraining of large language models. *arXiv e-prints*, arXiv–
2503.

Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; et al. 2023. LLaMA: Open and Efficient Foundation Language Models. *arXiv preprint arXiv:2302.13971* .
Wang, J.; Qiang, W.; Song, Z.; Zheng, C.; and Xiong, H.
2025a. Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs. arXiv:2505.10425.
Wang, N.; Yao, B.; Zhou, J.; Hu, Y.; Wang, X.; Guan, N.;
and Jiang, Z. 2025b. Insights from Verification: Training a
Verilog Generation LLM with Reinforcement Learning with
Testbench Feedback. arXiv:2504.15804.

Zhang, D.; Zhoubian, S.; Hu, Z.; Yue, Y.; Dong, Y.; and
Tang, J. 2024. Rest-mcts*: Llm self-training via process reward guided tree search. *Advances in Neural Information*
*Processing Systems*, 37: 64735–64772.

Zhang, K.; Hong, Y.; Bao, J.; Jiang, H.; Song, Y.; Hong,
D.; and Xiong, H. 2025. GVPO: Group variance policy
optimization for large language model post-training. *arXiv*
*preprint arXiv:2504.19599* .

Zhang, K. W.; and Bowman, S. R. 2018. Language modeling teaches you more syntax than translation does: Lessons
learned through auxiliary task analysis. *arXiv preprint*
*arXiv:1809.10040* .

Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.;
Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2025. A survey of large language models.

### **Proofs**
#### **Proofs of Theorem 1**

We interpret all functions as square-integrable elements of
*L* [2] (Ω), where Ω is the sample space with probability measure P induced by *p* ( *X* *[′]* *, Y* *[′]* ). We define Φ as a conditional
expectation operator:

Φ *f* := E[ *f | q, y* 1: *n−* 1 ] *.* (14)

Its function is to project any function *f ∈* *L* [2] (Ω) onto
the function subspace corresponding to the sub- *σ* -algebra
spanned by the variables *q, y* 1: *n−* 1 . Since conditional expectation is an orthogonal projection in *L* [2], this implies:
(i) Range: all *σ* ( *q, y* 1 *n−* 1 )-measurable functions with finite
second moment. (ii) Kernel: all functions orthogonal to this
subspace. Thus, Φ is a non-expansive, self-adjoint, idempotent projection operator. Similarly, Ψ = Id *−* Φ is a projection onto the orthogonal complement.
Let *f* = *π* *[∗]* ( *y* 0 *| x* ) and define its projection version as:

*f* proj := Ψ *f* + *π* *[∗]* ( *y* 0 *| q* ) (15)

where *π* *[∗]* ( *y* 0 *| q* ) is the initial estimate under the unconditional output history information. Our goal is to prove that:

∆( *f, f* proj ) := E[( *Y* *[′]* *−* *f* ) [2] ] *−* E[( *Y* *[′]* *−* *f* proj ) [2] ] *≥* 0 *.* (16)

According to the conditional independence structure implied
by formula (1) in the original text, under the conditions of
given *q* and *y* 1: *n−* 1 :

E[ *π* *[∗]* ( *y* 0 *| x* ) *| q, y* 1: *n−* 1 ] = *π* *[∗]* ( *y* 0 *| q* ) *,* (17)

That is Φ *f* = *π* *[∗]* ( *y* 0 *| q* ). Combined with Ψ = Id *−* Φ, we
can get:

*f* = Φ *f* + Ψ *f* = *π* *[∗]* ( *y* 0 *| q* ) + Ψ *f* = *f* proj *.* (18)

This means that *f* and *f* proj are actually equal. This yields
∆= 0 under ideal conditions. However, this result becomes
non-trivial in practice when *π* *[∗]* deviates from the perfect
Bayesian estimator due to finite data or model approximation. In order to obtain a non-trivial generalized inequality,
we further use geometric methods to expand the error term
and reveal the structural return.
Next, let *Y* *[′]* := *y* 0 be the prediction target, *f* := *π* *[∗]* ( *y* 0 *|*
*x* ) be the original predictor, and *f* proj := Ψ *f* + *π* *[∗]* ( *y* 0 *| q* )
be the projection form. Consider the squared error:

∆( *f, f* proj ) = *∥Y* *[′]* *−* *f* *∥* [2] *−∥Y* *[′]* *−* *f* proj *∥* [2] *.* (19)

Since *f* = *f* proj +(Φ *f −π* *[∗]* ( *y* 0 *| q* )), we write it as *Y* *[′]* *−f* =
*Y* *[′]* *−* *f* proj *−* (Φ *f −* *π* *[∗]* ( *y* 0 *| q* )). Thus, we have:

*∥Y* *[′]* *−* *f* *∥* [2] = *∥Y* *[′]* *−* *f* proj *∥* [2] + *∥* Φ *f −* *π* *[∗]* ( *y* 0 *| q* ) *∥* [2]

(20)
+ 2 *⟨Y* *[′]* *−* *f* proj *,* Φ *f −* *π* *[∗]* ( *y* 0 *| q* ) *⟩.*

However, since Φ *f −* *π* *[∗]* ( *y* 0 *| q* ) = 0, the cross term disappears, so ∆( *f, f* proj ) = *∥* Φ *f −* *π* *[∗]* ( *y* 0 *| q* ) *∥* [2] = 0. If the
model *π* *[∗]* is not a perfect Bayesian optimal estimator, or disturbances are introduced during training, then Φ *f ̸* = *π* *[∗]* ( *y* 0 *|*
*q* ), and we have:

∆( *f, f* proj ) = *∥* Φ *f −* *π* *[∗]* ( *y* 0 *| q* ) *∥* [2] *≥* 0 *.* (21)

Therefore, we have ∆( *f, f* proj ) *≥* 0, completing the proofs.


-----

#### **Proofs of Corollary 2**

We aim to show that the following inequality holds under
the same structural assumptions as Theorem 1:

∆( *π* *[∗]* ( *y* 0 *| q* ) *,* Ψ *· π* *[∗]* ( *y* 0 *| x* ) + *π* *[∗]* ( *y* 0 *| q* )) *≥* 0 *.* (22)

To this end, we interpret all functions as square-integrable
elements of *L* [2] (Ω), where Ω is the sample space equipped
with the probability measure P induced by the joint distribution *p* ( *X* *[′]* *, Y* *[′]* ) of input-output pairs.
We define Φ as the conditional expectation operator
Φ *f* := E[ *f | q, y* 1 *n−* 1 ], which projects any *f ∈* *L* [2] (Ω)
onto the subspace of functions measurable with respect to
the *σ* -algebra generated by *q, y* 1 *n−* 1 .
By the standard properties of conditional expectation in
*L* [2], this operator is an orthogonal projection, meaning it is
self-adjoint, idempotent, and non-expansive, and its range
and null space are orthogonal complements.
We further define Ψ := Id *−* Φ as the projection onto the
orthogonal complement of Range(Φ).
Let *f* := *π* *[∗]* ( *y* 0 *| x* ) be the Bayes-optimal prediction based on the full input *x* = *q, y* 1 *, . . ., y* *n−* 1, and let
*b* := *π* *[∗]* ( *y* 0 *| q* ) denote the model’s prediction when only
the query *q* is observed. We then define the causally projected prediction as *f* proj := Ψ *f* + *b*, which augments the
baseline estimate *b* with the residual component of *f* that is
orthogonal to *σ* ( *q, y* 1 *n−* 1 ).
Our goal is to evaluate the difference in squared prediction
error between *b* and *f* proj, defined by

∆( *b, f* proj ) := E[( *Y* *[′]* *−* *b* ) [2] ] *−* E[( *Y* *[′]* *−* *f* proj ) [2] ] *.* (23)

To compute this quantity, we note that since *f* proj = Ψ *f* +
*b*, we have

*Y* *[′]* *−* *b* = ( *Y* *[′]* *−* *f* proj ) + Ψ *f,* (24)

and thus, by expanding the square norm, it follows that

*|Y* *[′]* *−* *b|* [2] = *|Y* *[′]* *−* *f* proj + Ψ *f* *|* [2]

= *|Y* *[′]* *−* *f* proj *|* [2] + *|* Ψ *f* *|* [2] + 2 *⟨Y* *[′]* *−* *f* proj *,* Ψ *f* *⟩.*
(25)

Subtracting *|Y* *[′]* *−* *f* proj *|* [2] from both sides yields the regret
difference

∆( *b, f* proj ) = *∥* Ψ *f* *∥* [2] + 2 *⟨Y* *[′]* *−* *f* proj *,* Ψ *f* *⟩.* (26)

Since *f* proj = Ψ *f* + *b*, we further have *Y* *[′]* *−* *f* proj =
*Y* *[′]* *−* *b −* Ψ *f*, and so

*⟨Y* *[′]* *−f* proj *,* Ψ *f* *⟩* = *⟨Y* *[′]* *−b−* Ψ *f,* Ψ *f* *⟩* = *⟨Y* *[′]* *−b,* Ψ *f* *⟩−∥* Ψ *f* *∥* [2] *.*
(27)
Substituting back, we obtain:

∆( *b, f* proj ) = *∥* Ψ *f* *∥* [2] + 2( *⟨Y* *[′]* *−* *b,* Ψ *f* *⟩−∥* Ψ *f* *∥* [2] )

(28)
= *−∥* Ψ *f* *∥* [2] + 2 *⟨Y* *[′]* *−* *b,* Ψ *f* *⟩.*

Now, we invoke the conditional independence result established in Equation [˜] (1), which implies that E[ *Y* *[′]* *|q, y* 1 *n−* 1 ] =
*π* *[∗]* ( *y* 0 *| q* ) = *b*, and hence

E[ *Y* *[′]* *−* *b | q, y* 1: *n−* 1 ] = 0 *.* (29)


As a result, the inner product *⟨Y* *[′]* *−* *b,* Ψ *f* *⟩* = E[( *Y* *[′]* *−*
*b* )Ψ *f* ] = 0, since Ψ *f* is orthogonal to the *σ* ( *q, y* 1 *n−* 1 )measurable subspace to which *Y* *[′]* *−b* belongs in expectation.
Therefore, we conclude that

∆( *b, f* proj ) = *∥* Ψ *f* *∥* [2] *≥* 0 *,* (30)

which completes the proof.
### **Pseudo-code of GCPO**

The pseudo-code of our proposed GCPO is shown in Algorithm 1.

Al g orithm 1: Pseudo-Code of GCPO

**Require:** Initial policy *π* *θ* ; prompt distribution *D* ; hyperparameters *α*, *β*, and *κ*
1: **for** step = 1 **to** *n* **do**
2: Sample a batch *D* *b* from *D*
3: Set old policy *π* *θ* old *←* *π* *θ*
4: **for** each query *q ∈* *D* *b* **do**
5: Sample group *{y* 0 *, y* 1 *, · · ·, y* *n−* 1 *} ∼* *π* *θ* old ( *·|q* )
6: Sample group *{y* *n,i* *}* *i* *[n]* =0 *[−]* [1] *[∼]* *[π]* *[θ]* old [(] *[·|][q,][ {][y]* *[i]* *[}]* *[n]* *i* =0 *[−]* [1] [)]
7: **for** each *y* *i* **do**
8: Construct *x* *i* = *{q, y* 0 *, . . ., y* *n−* 1 *} \ {y* *i* *}*
9: Compute reward *r* *i* and advantage *A* *i* via Eq.(8)
10: Compute causal factor Υ *i* via Eq.(11)
11: Obtain the relative advantage *B* *i* = *A* *i* *·* Υ *i*
12: Compute *D* KL ( *π* *θ* *∥* *π* ref )
13: Construct *π* ref *[′]* [and compute] *[ D]* [KL] [(] *[π]* *[θ]* *[ ∥]* *[π]* ref *[′]* [)]
14: **end for**

15: **end for**
16: Update *π* *θ* via the GCPO objective in Eq.(10)
17: **end for**

18: **return** *π* *θ*
### **Benchmark Datasets**

This section provides a brief overview of the datasets used
in our experiments. Broadly, the benchmarks fall into two
categories: (i) reasoning tasks for mathematical derivation,
including AIME24-25, AMC, MATH500 (Hendrycks et al.
2021), MinervaMATH (Lewkowycz et al. 2022); and (ii)
reasoning tasks for code generation, i.e., HumanEval (Chen
et al. 2021). The composition and characteristics of each
benchmark are summarized as follows.
**AIME24-25** contains 30 fill-in-the-blank questions drawn
from the 2024 and 2025 American Invitational Mathematics Examinations (15 questions per year). These problems
are generally more challenging than those in AMC, covering number theory, combinatorics, geometry, and algebra.
**AMC** includes 975 multiple-choice questions from 39
AMC competitions, with 25 questions each for AMC10 (targeted at students up to 10th grade) and AMC12 (up to 12th
grade). The problems range from basic algebra and geometry to introductory topics in probability and combinatorics,
offering a diverse set of tasks for evaluating LLM reasoning.
**MATH500** is a subset of 500 problems randomly sampled
from the full MATH dataset. It spans seven mathematical do

-----

mains, including prealgebra, algebra, number theory, geometry, intermediate algebra, and precalculus. Each problem is
accompanied by a step-by-step solution and a difficulty label
ranging from 1 to 5, allowing for fine-grained assessment of
mathematical reasoning performance.

**MinervaMATH** contains 12,500 high school-level
competition-style math problems. Each question includes
detailed solution steps and covers a broad curriculum from
prealgebra to precalculus.

**HumanEval** consists of 164 Python programming tasks
designed to evaluate the correctness of code generated by
language models. Each task includes a function signature
and a natural language description, requiring the model to
produce a working implementation. Evaluation is based on
the *Pass* @ *k* metric, which measures the proportion of times
the generated code passes all test cases within *k* attempts.
### **Implementation Details**

Our implementation is based on the TRL codebases,
with custom modifications. For model initialization, we
directly load base models from Hugging Face, including DeepScaleR-1.5B-Preview, DeepSeek-R1-DistillQwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and Qwen27B-Instruct. Unless otherwise stated, we follow the official evaluation protocols of each benchmark and report
maj@4 scores across different models. For certain mathematical tasks, DeepScaleR-1.5B-Preview is first fine-tuned
on a dataset of 40,000 math problems and solutions, and
then further fine-tuned on 919 AIME problems from 1989
to 2023. For DeepSeek-R1-Distill-Qwen-1.5B, we fine-tune
using a random subset of 4,000 problem-solution pairs sampled from the NuminaMath dataset. All fine-tuning is performed with a maximum token budget of 16,384 tokens,
which also serves as the evaluation constraint. The training configuration is as follows: we set the learning rate to
1 *.* 0 *×* 10 *[−]* [6], employ a cosine learning rate scheduler with a
warm-up ratio of 0.1, and use a batch size of 256. The maximum prompt length is 4,096 tokens, and the maximum generation length is 16,384 tokens. Each model is trained for up
to 10 epochs, with early stopping typically at 1 epoch. We
enable vLLM acceleration by setting the ‘use vllm’ flag to
True, with GPU memory utilization capped at 80%. Mixedprecision training is employed using BF16. The regularization coefficients *α* and *κ* are set to 2 and 0.06, respectively,
based on grid search results.

All experiments were conducted on A100 clusters, i.e.,
a high-performance GPU cluster consisting of multiple interconnected nodes, each equipped with 8× NVIDIA A100SXM4 40GB GPUs and an AMD EPYC 7742 64-core CPU,
with 512 GB RAM per node. The cluster supports multinode distributed training via NCCL and InfiniBand, enabling efficient fine-tuning of large-scale models such as 7B
GCPO, with DeepSpeed 0.13.1 plus ZeRO-3 for large model
optimization. All training is performed with mixed-precision
(bf16) under a Slurm-based job scheduling environment.

### **Additional Experiments and Discussion**
#### **Full Results of Comparison**

We conduct comprehensive experiments across a diverse
set of reasoning benchmarks in the main text, including
mathematical tasks (AIME24–25, AMC, MATH500, MinervaMATH) and code generation (HumanEval). The experiments are carried out using several open-source base
models such as DeepScaleR-1.5B-Preview, DeepSeek-R1Distill-Qwen-1.5B/7B, and Qwen2-7B-Instruct. GCPO is
compared against a range of strong baselines, including
GRPO, GVPO, ReST-MCTS, and Dr.GRPO. The results in
the main text show that GCPO consistently outperforms all
baselines across benchmarks.
To provide a more comprehensive evaluation, in this section, we conduct comparison experiments with the SOTA
test-time methods, e.g., MCTS, to explore whether reasoning effective fine-tuning avoids the need for test-time compute, or further gains can be achieved. Specifically, we directly introduce MCTS on models trained based on GRPO,
Dr.GRPO, and GCPO. Then, we record the accuracy of these
models before and after introducing the corresponding methods. The results are shown in Table 3. From the results, we
can observe that GCPO still achieves the best performance.
In addition, while these test-time methods provide certain
performance improvements, they still fail to surpass GCPO
even when combined with RL-based training baselines. This
demonstrates the irreplaceability of the causal module introduced by GCPO, i.e., it uncovers inter-group relationships
that were overlooked by previous work and guides the model
to learn them for further reasoning optimization.
#### **Training Stability**

Given the substantial computational cost of training LLMs,
maintaining stable training dynamics is crucial to ensure
convergence efficiency and avoid catastrophic failures during optimization. Unstable updates can lead to divergent
behavior, increased variance in model performance, and
wasted computational resources. To quantitatively assess
training stability, we adopt the gradient norm as a proxy
for policy variance, following standard practice in reinforcement learning. A stable gradient norm suggests consistent
updates to the model parameters, whereas large fluctuations
may indicate unstable or overly aggressive policy shifts. As
shown in Figure 7, GCPO exhibits the most stable training behavior among all compared methods, with its gradient
norm remaining nearly constant across training iterations.
This indicates that the policy updates are well-regulated,
likely due to the regularization effect introduced by the
causal term in GCPO. In contrast, GRPO suffers from pronounced oscillations in gradient norm, reflecting unstable
dynamics that may hinder reliable convergence. These results underscore the importance of incorporating causal constraints into the learning process.
#### **Computational Overhead Analysis**

To assess the training efficiency of different methods, we
normalize the computational cost of GRPO to 1 *×* as a baseline, and compare all other methods accordingly. The cost is


-----

**Question:** **How many 4-digit numbers with distinct digits are divisible by 5?**


**Flip a fair coin until two consecutive heads appear. What is the**
**Question:** **expected number of flips?**












(a) Example 1: 4-digit numbers divisible by 5

**A triangle has sides 10, 17, and x (x is an integer). The area is 60.**
**Question:**

**How many x are possible?**


(b) Example 2: Expected number of consecutive heads

**Find the least positive integer n such that 3n is a perfect square**
**Question:**

**and 2n is a perfect cube.**












(c) Example 3: Triangle with integer sides and area 60

**A box has 4 red, 5 blue, 6 green balls. Two are drawn without**
**Question:**
**replacement. What is the probability they are different colors?**


(d) Example 4: Numbers that are both perfect squares and cubes

**How many positive integers n ≤ 1000 satisfy that n^2 − 1 is**
**Question:**

**divisible by 24?**












(e) Example 5: Probability of drawing balls

**How many surjective (onto) functions are there from a 6-**
**Question:** **element set to a 3-element set?**


(f) Example 6: Divisibility filtering

**How many lattice paths from (0,0) to (6,6) using steps (1,0) and**
**Question:**

**(0,1) never go above the line y = x?**












(g) Example 7: Counting surjective mappings


(h) Example 8: Lattice paths that stay within bounds


Figure 5: Qualitative analysis of side-by-side rollouts. For each question, we show a GRPO (left) and a GCPO chain (right).
GRPO lacks cross-component causal coordination, leading to systematic errors (e.g., missing modular constraints, miscounting
Catalan paths). GCPO organizes multi-aspect constraints within identical step budgets, resolves the errors, and returns the
correct answers across topics ranging from number theory and geometry to probability, combinatorics, and lattice-path counting.


-----

Table 3: Pass@1 performance on various math reasoning benchmarks.

27.0


46.0

45.5

45.0

44.5

44.0

43.5

43.0


q x - (x) + - (q)
Input Configuration


q x - (x) + - (q)
Input Configuration


48.0

47.5

47.0

46.5

46.0

45.5

45.0

44.5

44.0


26.5

26.0

25.5

25.0

24.5



(a) Evaluation on GRPO


(b) Evaluation on GCPO


Figure 6: Ablation results under GRPO and GCPO frameworks using three input configurations.

1000


24.0

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
||||G|CPO||
|||||||
||Dr.GRP|O||GVP|O|
|||GRPO||||
|||||||


0.8 0.9 1.0 1.1 1.2 1.3 1.4
Training Cost (N × )


800



600

400

200

0

0 200 400 600 800 1000
Step

Figure 7: The norm of the gradient during training.

measured in terms of total GPU hours under matched training schedules and hardware settings. The results are shown
in Figure 8. We find that GCPO incurs a modest increase
in training cost ( 1 *.* 18 *×* ) due to the incorporation of a KLregularized causal objective. Despite this slight overhead,
GCPO consistently outperforms all baselines across benchmarks, yielding the highest gains in reasoning accuracy and
robustness. This indicates a favorable trade-off between cost
and performance. In summary, GCPO offers a great balance
between efficiency and effectiveness, delivering superior results with only a modest increase in training overhead compared to the GRPO baseline.
#### **More Ablation Studies**

In the ablation study of the main text, we verify that both
terms are crucial to the observed improvements. Further,


Figure 8: Trade-off performance of different methods.

sensitivity analysis identifies optimal hyperparameters at
*α* = 2 and *β* = 0 *.* 06, with performance remaining stable in
a reasonable range. These results demonstrate that GCPO is
robust and effective across reasoning tasks of varying complexity and modality.
In this subsection, we further conduct a ablation study
to examine the predictive effects of different input configurations, i.e., the original query *q*, the input *x* =
*q, y* 1 *, . . ., y* *n−* 1, and the causally projected input Ψ *·π* *[∗]* ( *x* )+
*π* *[∗]* ( *q* ) (as mentioned in Section Causal Analysis). The experiment is conducted under both GRPO and GCPO frameworks by feeding the three different input configurations into
the trained models and evaluating their performance. The results are shown in Figure 6. From the results, we can observe
that under GRPO, prediction quality varies significantly: *q*
alone leads to the poorest performance, *x* provides moderate
improvement, while the causally projected input achieves
the best results. This confirms the benefit of leveraging
collider-aware representations. In contrast, GCPO yields
consistently high and stable performance across all inputs,
indicating that its causal objective effectively internalizes
the relevant dependencies, making it robust to input variation.
#### **Qualitative Analysis**

To better illustrate the impact of GCPO on model behavior,
we conduct a qualitative analysis comparing outputs before
and after applying GCPO. Specifically, we randomly sam

-----

ple a set of math reasoning problems from the benchmark
datasets. For each problem, we visualize the outputs generated by models trained with GRPO and GCPO, respectively. We observe that, for certain complex reasoning tasks,
the GCPO-trained model is able to explore multiple aspects
of the problem, reflect on intermediate steps from different
perspectives, and ultimately arrive at the correct solution. In
contrast, the GRPO-trained model often follows a relatively
rigid line of reasoning, which can lead to errors.
Figure 5 presents representative examples. These qualitative results suggest that GCPO enables the model to capture inter-path relationships and reason more effectively by
integrating diverse viewpoints during inference. For example, in Figure 5(b), GRPO offers a heuristic guess without
modeling the underlying process. GCPO distinguishes three
Markov states, i.e., no previous head, one previous head, and
termination, and sets up linear equations that couple transition probabilities with remaining expectations. Solving the
system yields the exact expectation of 6 flips, demonstrating
tight coordination between probabilistic and algebraic reasoning. In Figure 5(e), although GRPO correctly enumerates 74 favorable pairs, it proceeds to “simplify” the fraction
and erroneously converts 105 into 52.5, corrupting the final
answer. GCPO completes the combinatorial count and then
explicitly checks the greatest common divisor, confirming
that 74/105 is already in the lowest terms and preserving
numerical integrity. Also, in Figure 5(f), GRPO considers
only the modulus-8 requirement that n be odd, ignoring the
modulus-3 condition, and consequently overestimates the
solution set. GCPO decomposes the problem into two modular layers, i.e., oddness (mod 8) and non-multiplicity by 3
(mod 3), and performs inclusion exclusion counting to arrive at the exact total of 334 integers. Therefore, GCPO systematically integrates local conditions and cross-component
interactions, ensuring logical consistency and robust error
correction throughout the reasoning chain.


-----

