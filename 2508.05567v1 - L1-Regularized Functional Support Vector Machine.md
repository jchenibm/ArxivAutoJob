## - L 1 REGULARIZED F UNCTIONAL S UPPORT V ECTOR M ACHINE


**Bingfan Liu**
Department of Statistics and Acturial Science
University of Waterloo
Waterloo, ON, Canada
```
    b242liu@uwaterloo.ca

```

**Peijun Sang** *[∗]*
Department of Statistics and Acturial Science
University of Waterloo
Waterloo, ON, Canada
```
   peijun.sang@uwaterloo.ca

```

August 8, 2025
#### **A BSTRACT**

In functional data analysis, binary classification with one functional covariate has been extensively
studied. We aim to fill in the gap of considering multivariate functional covariates in classification. In
particular, we propose an *L* 1 -regularized functional support vector machine for binary classification.
An accompanying algorithm is developed to fit the classifier. By imposing an *L* 1 penalty, the
algorithm enables us to identify relevant functional covariates of the binary response. Numerical
results from simulations and one real-world application demonstrate that the proposed classifier
enjoys good performance in both prediction and feature selection.

***K*** **eywords** B-splines *·* Feature selection *·* Functional data classification *·* More *·* Multivariate functional features *·*
Optimization
#### **1 Introduction**

In many real-world applications, data are collected in the form of repeated signals over a continuum such as space or
time for each individual subject. Functional data analysis (FDA), as a collection of powerful statistical tools to deal with
such data, has been extensively studied over the past few decades. In FDA, each trajectory with repeated measurements
is treated as a sample path of a continuous stochastic process, though only discretized observations are available due
to practical limitations. When exploring the relationship between a functional covariate and a scalar response, the
effect of the functional covariate is typically modeled as a coefficient function, rather than a finite-dimensional vector
as in multivariate data analysis. The issue of the curse of dimensionality can thus be successfully circumvented by
incorporating regularization on the function. This strategy has been widely applied to fields like neuroscience [ 1 ],
spectrometry [4], and speech analysis [2].

Binary classification has been extensively studied in the literature on machine learning. Among binary classifiers,
support vector machines (SVM) have received considerable attention in both theory and applications; see [ 5 ], [ 22 ], [ 28 ],

[21], [23] and references therein.

In the context of FDA, [ 3 ] applied an SVM to binary classification where only one univariate trajectory was considered
for each subject. In particular, each univariate trajectory is first projected onto a finite-dimensional subspace spanned by
pre-determined basis functions, e.g. B-spline or Fourier basis functions. After the projection, the infinite-dimensional
classification problem is converted to a finite-dimensional one. Classification can then be conducted through the
classical SVM on the finite-dimensional projection scores.

In the aforementioned work, only one functional covariate is accounted for in classification. Nevertheless, it is quite
common that multiple trajectories are accessible for each subject in practice. For example, [ 24 ] considered a large

*∗* Corresponding author.
This paper is published in Statistics and its Interface, (3), 349-356. https://dx.doi.org/10.4310/22-SII773
Thanks to Henri Begleiter at the Neurodynamics Laboratory at the State University of New York Health Center at Brooklyn for
making this EEG signal data of alcoholism, namely "EEG Database Dataset", publicly available.


-----

A PREPRINT                        - A UGUST 8, 2025

number of flight parameters such as aircraft speed, accelerations and warning signals, each of which was treated
as a functional covariate for every flight, to identify important ones that are associated with the landing risk. [ 11 ]
proposed generalized functional linear models, as well as a penalized likelihood method, and applied this framework to
six magnetic resonance imaging indices, each of which was treated as a functional covariate, to identify risk factors
for multiple sclerosis, which is a neurological disease affecting the central nervous system. To classify the label
of each subject and select relevant functional covariates, we propose an *L* 1 -regularized functional support vector
machine ( *L* 1 -fSVM) for a binary response with multiple functional covariates. Instead of projecting each trajectory to
a pre-specified set of basis functions, this classifier estimates a coefficient function for each trajectory via regression
splines. The effect of each functional covariate is represented by its projection onto the coefficient function. Then we
build an SVM classifier on these projection scores. Instead of the commonly used *L* 2 penalty, we impose an *L* 1 penalty
in the SVM to achieve feature selection. To fit *L* 1 -fSVM, we develop an iterative updating algorithm to update the
coefficient functions and the vector in the SVM separately. The performance of our classifier in prediction accuracy and
feature selection is examined under various simulation settings. Simulation studies demonstrate that *L* 1 -fSVM slightly
outperforms some existing functional classifiers in terms of both prediction accuracy and feature selection.

The rest of the article is organized as follows. In Section 2, we propose the *L* 1 -fSVM classifier and develop an algorithm
to fit this classifier. In Section 3, empirical studies are conducted to demonstrate the performance of the proposed
classifier. Section 4 concludes the paper.
#### **2 Methodology**

**2.1** **Review of** *L* 1 **-regularized Support Vector Machine with Scalar Covariates**

An SVM separates different classes of samples using a hyper-plane that maximizes the margin from the hyper-plane to
the support vectors [ 5 ]. Suppose a training set consists of *n* pairs of binary observations. In particular, for *i* = 1 *, . . ., n*
let *x* *i* = ( *x* *i* 1 *, ..., x* *ip* ) [T] *∈* R *[p]* and *y* *i* *∈{−* 1 *,* 1 *}* denote a *p* -dimensional feature and a binary class label of the *i* th
subject, respectively. The canonical form of the hard-margin SVM problem can be formulated as:

min 1 2 [such that] *[ y]* *[i]* [(] *[α]* [0] [+] *[ x]* T *i* *[α]* [)] *[ ≥]* [1] *[, i]* [ = 1] *[, ..., n,]*
*α* 0 *,α* 2 *[∥][α][∥]* [2]

where *α* 0 *∈* R and *α* = ( *α* 1 *, . . ., α* *p* ) [T] *∈* R *[p]* are used to define the hyper-plane. By introducing a set of slack
variables { *ξ* *i* *}* *[n]* *i* =1 [, the SVM classifier can be adapted to classify non-linearly separable classes [] [14] []. The corresponding]
optimization problem becomes


1
min 2 [+] *[ λ]*
*α* 0 *,α* 2 *[∥][α][∥]* [2]


*n*
� *ξ* *i* (1)

*i* =1


such that *y* *i* ( *α* 0 + *x* *[T]* *i* *[α]* [)] *[ ≥]* [1] *[ −]* *[ξ]* *[i]* *[, ξ]* *[i]* *[≥]* [0] *[, i]* [ = 1] *[, ..., n,]*

where *λ* denotes a cost parameter.

In practice, not all features are predictive of the binary response. Thus it is desirable to identify relevant features for
better interpretation and to enhance prediction accuracy. To achieve this goal, [ 8 ] proposed to replace the term *∥α∥* 2 [2]
with *∥α∥* 1 in (1) in order to obtain a sparse solution. Consequently, the minimization problem of (1) is converted to


*α* min 0 *,α* *[∥][α][∥]* [1] [ +] *[ λ]*


*n*
� *ξ* *i*

*i* =1


such that *y* *i* ( *α* 0 + *x* T *i* *[α]* [)] *[ ≥]* [1] *[ −]* *[ξ]* *[i]* *[,]* *ξ* *i* *≥* 0 *, i* = 1 *, . . ., n.*

By the same arguments in Chapter 12 of [ 20 ], we can show that it is equivalent to the following optimization problem:


*α* min 0 *,α* *[∥][α][∥]* [1] [ +] *[ λ]*


*n*
� *{* 1 *−* *y* *i* *f* ( *x* *i* ) *}* + (2)

*i* =1


where *f* ( *x* ) = *α* 0 + [�] *[p]* *j* =1 *[α]* *[j]* *[x]* *[j]* [ denotes the decision function and] [ (1] *[ −]* *[yf]* [)] [+] [ = max(0] *[,]* [ 1] *[ −]* *[yf]* [)] [ denotes the hinge]
loss function. Given an estimate of *α* 0 and *α*, a new observation *x* will be assigned the label of sign( *f* [ˆ] ( *x* )) . Various
algorithms have been proposed in the literature to solve this minimization problem; see [8], [6] and [7] for example.

Due to the non-smoothness of the hinge loss function, some extensions such as the squared hinge loss function [ 18 ] or
the Huberized version of the squared hinge loss function [ 17 ] were proposed. According to [ 17 ], compared with the

2


-----

A PREPRINT                        - A UGUST 8, 2025

squared hinge loss, the Huberized version of the squared hinge loss leads to a more robust classifier. In this paper, we
consider using the squared hinge loss when solving the minimization problem at (2):


*α* min 0 *,α* *[∥][α][∥]* [1] [ +] *[ λ]*


*n*
�[ *{* 1 *−* *y* *i* *f* ( *x* *i* ) *}* + ] [2] *,* (3)

*i* =1


for simplicity. This loss was also adopted in [16], [12], [15] and references therein.

**2.2** *L* 1 **-regularized Support Vector Machine with Functional Covariates**

In this article, the problem of primary interest is to discriminate multiple functional data, where the label of each subject
is still binary, but the covariates become multivariate random functions. Let *X* ( *t* ) = ( *X* 1 ( *t* 1 ) *, . . ., X* *p* ( *t* *p* )) [T] denote
a vector of *p* random functions evaluated at *t* = ( *t* 1 *, . . ., t* *p* ) *∈* [�] *[p]* *j* =1 [I] *[j]* [, and] *[ Y][ ∈{−]* [1] *[,]* [ 1] *[}]* [ denote a binary response.]
Note that I *j* ’s could vary from one functional covariate to another. Let *{* ( *x* *i* ( *t* ) *, y* *i* ) : *i* = 1 *, . . ., n}* denote *n* i.i.d
observations on ( *X* ( *t* ) *, Y* ) . We aim to build a classifier that can discriminate different samples by leveraging the *p*
functional covariates and identify which functional covariates are relevant to the response.

By replacing the linear combination with a suitable form for functional covariates, we obtain a functional version of the
*L* 1 -regularized SVM:


*α* min 0 *,α* *[∥][α][∥]* [1] [ +] *[ λ]*


*n*
�

*i* =1


2
� *{* 1 *−* *y* *i* *f* ( *x* *i* ) *}* + � *,* (4)


where the decision function is


*f* ( *x* *i* ( *t* )) = *α* 0 +


*p*
� *α* *j*

*j* =1


*β* *j* ( *t* ) *x* *ij* ( *t* )d *t.* (5)

� I *j*


Note that we utilize *α* *j* � I *j* *[β]* *[j]* [(] *[t]* [)] *[x]* *[ij]* [(] *[t]* [)d] *[t]* [ to characterize the effect of the] *[ j]* [th functional covariate of the] *[ i]* [th subject.]

In practice, we can only observe discrete observations for each feature rather than its whole sample path. However,
by assuming that the sample path of each functional covariate is continuous, information on the whole trajectory can
be inferred from dense observations. This illustrates the essential difference between FDA and high-dimensional
methodologies. With denser observations, more data can be leveraged to recover the whole trajectory, and thus
the estimation accuracy can be enhanced. In contrast, high-dimensional approaches may suffer from the curse of
dimensionality due to denser observations. In addition, compared with the expression of *f* in (3), we have an extra
multiplier *α* *j* to identify relevant functional covariates in *f* to avoid directly penalizing *β* *j* ’s. But this strategy would
lead to the issue of identifiability of *α* *j* and *β* *j* . To address this issue, without loss of generality, we assume that
I *j* = [0 *,* 1] and *β* *j* (0) = 1 for *j* = 1 *, . . ., p* . Consequently, feature selection can be achieved by estimating *α* *j* ’s from
the minimization problem of (4).

Next, we introduce a B-spline representation for the decision function defined in (5) . Let *{B* 1 *, . . ., B* *K* *}* denote
*K* cubic B-spline basis functions on [0 *,* 1] . We approximate each coefficient function by *β* *j* ( *t* ) = *c* [T] *j* *[B]* [(] *[t]* [)] [, where]
*c* *j* = ( *c* *j* 1 *, . . ., c* *jK* ) [T] *∈* R *[K]* and *B* ( *t* ) = ( *B* 1 ( *t* ) *, . . ., B* *K* ( *t* )) [T] . This treatment has been widely adopted in the
literature of FDA, and interested readers can refer to [ 10 ] for more details. It should be noted that there are three main
advantages of using this representation. Firstly, due to the property of locally compact support, i.e., B-spline basis
functions are vanishing in most subintervals defined by the knots, calculation of the integral in (5) can be facilitated.
Secondly, we only need to fix *c* *j* 1 to be 1 to entertain the constraint *β* *j* (0) = 1 for *j* = 1 *, . . ., p* . Lastly, when *β* *j*
satisfies certain smoothness conditions, e.g., the Hölder condition, the B-spline approximation can attain desirable
accuracy by Lemma 5 in [19].

With the B-spline representation, the decision function in (5) is approximated by


*K*
� *c* *jk* *B* *k* ( *t* ) *x* *ij* ( *t* )d *t*

*k* =1


*f* ( *x* *i* ( *t* )) *≈* *α* 0 +

= *α* 0 +

= *α* 0 +


*p*
� *α* *j*

*j* =1


*p*

T

� *α* *j* ( *c* *j* *[u]* *[ij]* [)] *[,]* (6)

*j* =1


� 01


*p*
� *α* *j*

*j* =1


*K*
� *c* *jk*

*k* =1


1

*B* *k* ( *t* ) *x* *ij* ( *t* )d *t*

� 0


1 1 T
where *u* *ij* = �� 0 *[B]* [1] [(] *[t]* [)] *[x]* *[ij]* [(] *[t]* [)d] *[t, . . .,]* � 0 *[B]* *[K]* [(] *[t]* [)] *[x]* *[ij]* [(] *[t]* [)d] *[t]* � and each component in *u* *ij* can be approximated via a finite
Riemann sum.

3


-----

A PREPRINT                        - A UGUST 8, 2025

**2.3** **Algorithm**

Now we present the details to solve the minimization problem at (4) . Let *α* = ( *α* 1 *, . . ., α* *p* ) [T] and *c* = ( *c* [T] 1 *[, . . ., c]* [T] *p* [)] [T] [. A]
coordinate descent algorithm is employed to iteratively update the parameters *α* and *c* .

Given an initial value of *c* *j*, denoted by *c* [(0)] *j* for *j* = 1 *, . . ., p*, we update the values of *α* 0 and *α*, denoted by *α* 0 [(0)] and
*α* [(0)], in the first step. At this step, functional features are first transformed to scalar features via (6) . Hence, solving (4)
is converted to solving an *L* 1 -regularized SVM problem. We employ the coordinate descent algorithm proposed in [ 12 ]
and [ 15 ] to address this optimization problem as shown in (3) . Then we fix the values of *α* 0 and *α* to be *α* 0 [(0)] and *α* [(0)],
respectively, and update the value of *c* *j* for *{j* : *α* *j* [(0)] = 0 *̸* *,* 1 *≤* *j ≤* *p}* using the gradient descent. These procedures are
iterated until convergence, i.e., some stopping criteria are satisfied.

More specifically, define the Lagrangian function for (4) with constraint (5) as



*[j]* 






 +


*n*

*ℓ* ( *c* ) = *∥α∥* 1 + *λ* �

*i* =1

and the index of the support vectors as













 [1] *[ −]* *[y]* *[i]*


 *α* 0 +


*p*

T

� *α* *j* *u* *ij* *[c]* *[j]*

*j* =1


*p*
�


*,*


 2








 *α* 0 +





 *>* 0


 *[.]*


*I* *s* =





 *[i]* [ : 1] *[ −]* *[y]* *[i]*


*p*

T

� *α* *j* *u* *ij* *[c]* *[j]*

*j* =1


*p*
�


Then the partial derivatives of *ℓ* ( *c* ) with respect to *c* *j* for *j ∈* *A* *α* := *{j* : *α* *j* *̸* = 0 *,* 1 *≤* *j ≤* *p}* are


*∂ℓ* *∂ℓ*
*∂c* *j* = �0 *,* *∂c* *[∂ℓ]* *j* 2 *, . . .,* *∂c* *jK*


� T


� *y* *i* *α* *j* *u* *ij* 2

*i∈I* *s*


*p*

T

� *α* *j* *u* *ij* *[c]* *[j]*

*j* =1


*p*
�











 *[, . . .,]*





 [1] *[ −]* *[y]* *[i]*


 *α* 0 +





=





0 *, −* 2 *λ* �
 *i∈I* *s*


T


















*−* 2 *λ* � *y* *i* *α* *j* *u* *ijK*

*i∈I* *s*





 [1] *[ −]* *[y]* *[i]*


 *α* 0 +


*p*
� *α* *j* *u* *[T]* *ij* *[c]* *[j]*

*j* =1


*p*
�


*.*


We update *c* *j* for those *j* ’s in the active set *A* *α* by the gradient descent:


*c* *j* ( *r* +1) = *c* [(] *j* *[r]* [)] *−* *η* *∂c* *[∂ℓ]* *j*


*,*

����� *c* [(] *[r]* [)]
*j*


where *η* is the learning rate and *c* [(] *j* *[r]* [)] denotes the value of *c* *j* at the *r* th iteration. Note that at each iteration the first
element of *c* *j* is fixed to be 1.

The algorithm stops when either the maximum number of iterations, *maxiter*, is reached or

max *{|α* 0( *r* +1) *−* *α* ˆ 0 [(] *[r]* [)] *[|][,]* [ max] 1 *≤j≤p* *[|][α]* *[j]* ( *r* +1) *−* *α* *j* [(] *[r]* [+1)] *|,*

1 max *≤j≤p* *[∥][c]* *[j]* ( *r* +1) *−* *c* [(] *j* *[r]* [)] *[∥]* [1] *[} ≤]* *[ϵ,]*

where *∥a∥* 1 denotes the *L* 1 -norm of a vector *a* and *ϵ* is a pre-determined tiny constant. Algorithm 1 details the
implementations of the *L* 1 -regularized functional SVM.
#### **3 Numerical Studies**

**3.1** **Simulation Studies**

This subsection is to demonstrate the finite sample performance of the proposed classification method. We are
particularly interested in the classifier’s prediction accuracy and its performance in feature selection.

4


-----

A PREPRINT                        - A UGUST 8, 2025

**Algorithm 1:** Coordinate Descent for solvin g the minimization p roblem (4) in *L* 1 -fSVM

**Initialization**
*α* 0 [(0)] = 1 if # *{y* *i* = 1 *} ≥* # *{y* *i* = *−* 1 *}*, else ˆ *α* 0 [(0)] = *−* 1
*α* [(0)] = (0 *, ...,* 0) [T]

*c* [(0)] *j* = (1 *, ...,* 1) [T] for 1 *≤* *j ≤* *p*, find *u* *ij*
*η* = learning rate
*ϵ* = tolerance

*maxiter* = maximum iterations

*r* = 0

**while** *True* **do**

**Step 1** :
Fix *c* *j* to be *c* [(] *j* *[r]* [)] and find the value of *u* [T] *ij* *[c]* *[j]* [ for] *[ j]* [ = 1] *[, . . ., p]* [. Obtain the updated coefficients] *[ α]* [0(] *[r]* [+1)] [ and]
*α* [(] *[r]* [+1)] from the *L* 1 -SVM.
**Step 2** :
Fix *α* 0 = *α* 0 [(] *[r]* [+1)] and *α* = *α* [(] *[r]* [+1)], and update *c* *j* [(] *[r]* [+1)] for *j ∈{j|α* *j* [(] *[r]* [+1)] = 0 *̸* *, j* = 1 *, . . ., p}* using the
gradient descent.

*converged* = max *{|α* 0( *r* +1) *−* *α* 0 [(] *[r]* [)] *[|][,]* [ max] 1 *≤j≤p* *[|][α]* *[j]* ( *r* +1) *−* *α* *j* [(] *[r]* [)] *[|][,]*

1 max *≤j≤p* *[∥][c]* *[j]* ( *r* +1) *−* *c* [(] *j* *[r]* [)] *[∥]* [1] *[} ≤]* *[ϵ,]*

**if** *converged == true* **then**

print("Algorithm Converged.")
break
**end**
**if** *r ≥* *maxiter* **then**

print("Reached maximum iterations. Algorithm does not converge.")
break
**end**
**Update** :
*r* = *r* + 1
**end**
**Return** ˆ *α* 0, ˆ *α* and ˆ *c* *j* for *j* = 1 *, . . ., p* .

We adopt the data generation mechanism in [ 13 ] to generate multiple functional covariates for *n* subjects. In particular,
the *j* -th functional covariate of the *i* -th subject, *x* *ij* ( *t* ), is generated from


5
�

*k* =1 �


*b* *ijk* sin 2 *π* (5 *−* *b* *ijk* ) *[t]* *−* *m* *ijk*
� *T* *j* � [��]


1
*x* *ij* ( *t* ) =
100


5 +

�


with
*b* *ijk* *∼* *U* (0 *,* 5) *, m* *ijk* *∼* *U* (0 *,* 2 *π* ) *, T* *j* *∼* *U* (50 *,* 100) *,*
where *U* ( *a, b* ) denotes the uniform distribution on ( *a, b* ) . We fix the domain of all functional features to be the unit time
interval, [0,1]. Additionally, the *j* -th functional feature is observed at *T* *j* equidistant time points. The labels of subjects
are simulated to be balanced such that the number of negative labels is almost equal to that of positive ones in both
training and test sets. Moreover, the binary label of each subject is designed to be related to only the first five functional
covariates as follows:


5

*y* *i* = sign �

� *j* =1

where the first five coefficient functions are :


*β* *j* ( *t* ) *x* *ij* ( *t* )d *t*
�


*,*

�


*β* 1 ( *t* ) = 1 *,* *β* 2 ( *t* ) = 1 *−* 2 *t,* *β* 3 ( *t* ) = sin 5 *t* + *[π]*
� 2

*β* 4 ( *t* ) = exp (2 *t* ) *,* *β* 5 ( *t* ) = log(2 *t* + 1) + 1 *.*

5


*,*
�


-----

A PREPRINT                        - A UGUST 8, 2025

Figure 1: 5-fold cross-validation accuracy with one standard error under *n* = 200 and *p* = 20, given *K* = 5.

Further, random noises are added to the response such that each subject has a 0.1 probability of being mislabelled.

To demonstrate the performance of the proposed classifier in feature selection and prediction accuracy, we compare it
with the group-lasso-based functional logistic regression model (grplFlogit) proposed by [ 11 ]. This alternative method
approximates coefficient functions using a linear combination of basis functions. In contrast to our method, it imposes
a group lasso penalty on the coefficients of basis functions to achieve feature selection. Six simulation scenarios are
designed to assess their performances; the number of subjects in the training set *n ∈{* 100 *,* 200 *}* and the number of
functional features *p ∈{* 10 *,* 20 *,* 40 *}* are considered. Additionally, a test set of size 200 is generated to assess the
prediction accuracy of these two classifiers in each simulation trial, and 100 independent simulation trials are conducted
under each scenario.

There are two tuning parameters, the cost parameter *λ* and the number of basis functions *K* in *L* 1 -fSVM. The cost
parameter controls the sparsity of *α* or the number of selected functional covariates. A smaller *λ* would lead to fewer
functional covariates selected in the final classifier. The number of basis functions, however, determines the smoothness
of the estimated coefficient functions. A smaller *K* would lead to a smoother estimator of the coefficient functions.

A 5-fold cross-validation procedure is employed to select the optimal combination of tuning parameters for both
classifiers. Figure 1 displays how the cross-validation accuracy changes with respect to the cost parameter *λ* given the
number of basis functions *K* = 5 . The highest accuracy is attained at *λ* = 0 *.* 05 . Thus *λ* = 0 *.* 04 is selected since the
corresponding accuracy value of *λ* = 0 *.* 04 is within one standard error of that of *λ* = 0 *.* 05, and it can yield a more
sparse estimate of *α* . Consequently, fewer functional covariates are identified as relevant features of the binary outcome.

Figure 2 depicts the boxplots of the prediction accuracy calculated on the test set for these two classification methods.
For both classifiers, the medians of prediction accuracy are above 80% in all simulation scenarios. Moreover, when
*n* is fixed, the prediction accuracy of both classifiers decreases as *p*, the number of functional covariates, increases.
This finding is consistent with the curse of dimensionality in the setting of multivariate data classification. In contrast,
when *p* is fixed, the prediction accuracy of both classifiers is enhanced when sample size increases. *L* 1 -fSVM compares
favorably with grplFlogit in terms of prediction accuracy in most simulation settings.

6


-----

A PREPRINT                      - A UGUST 8, 2025

Figure 2: Boxplots of test accuracy for L1-fSVM and grplFlogit under different combinations of *n* and *p* .

7


-----

A PREPRINT                        - A UGUST 8, 2025
#### **FP rates FN rates** L 1 -fSVM grplFlogit L 1 -fSVM grplFlogit n = 100, p = 10 .00 (.18) .00 (.40) .40 (.18) .40 (.22) n = 100, p = 20 .07 (.09) .07 (.27) .60 (.16) .40 (.21) n = 100, p = 40 .03 (.06) .06 (.16) .60 (.17) .40 (.17) n = 200, p = 10 .00 (.16) .20 (.33) .40 (.11) .20 (.14) n = 200, p = 20 .07 (.10) .20 (.11) .40 (.10) .20 (.14) n = 200, p = 40 .09 (.04) .20 (.07) .40 (.11) .20 (.15)

Table 1: Medians of the False Positive (FP) rates and False Negative (FN) rates of feature selection for L1-fSVM and
grplFlogit across the 100 simulation runs, with standard error presented in parentheses.

To compare the performance in feature selection of the two classifiers, the false positive (FP) rate and the false negative
(FN) rate are calculated under each simulation scenario. Table 1 summarizes the medians of their FP rates and FN rates
across 100 simulation runs. For both classifiers, when *n* is fixed, as *p* increases, the FP rate increases, while the FN rate
remains almost unchanged. Moreover, the FP rate of *L* 1 -fSVM is smaller than that of grplFlogit regardless of sample
size or the number of functional covariates. In contrast, the FN rate of *L* 1 -fSVM is larger than that of grplFlogit. One
possible reason is that grplFlogit tends to select more functional covariates than *L* 1 -fSVM. Hence the grplFlogit is less
likely to commit a type II error, that is, the grplFlogit has a larger power or a smaller FN rate.

**3.2** **Real Data Analysis**

In this subsection, both *L* 1 -fSVM and grplFlogit were applied to an electroencephalogram (EEG) dataset obtained from
the UCI Machine Learning Repository.

For each subject in the dataset, EEG signals were collected by *p* = 64 electrodes placed at the 64 specific locations on
the head. As explained in [ 26 ], the entire 10/20 international montage and 41 extra sites were utilized according to
the Standard Electrode Position Nomenclature proposed by American Electroencephalographic Association in 1990.
Additionally, one nose electrode, channel “nd", and two bipolar deviations recording electrooculogram, channels “X"
and “Y", were also utilized. Figure 3 [ 27 ] shows the locations of the electrodes, where channels “FPz" and “Oz"
represent the front most channel and the back most channel on the scalp, respectively. During the experiments, each
electrode collected signals with a frequency of 256 Hz for 1 second. Each subject took at most 120 trials where 3
different stimuli were presented. However, only the trials with a single stimulus (S1) were considered in our study.
Signals from the multiple trials with the S1 stimulus were averaged at each frequency for each EEG channel per
participant. The trials that contained missing data were removed from the study. Among all the participants included in
the study, 77 participants were labeled as alcoholic and 45 participants were labeled as nonalcoholic. The main purpose
of this study is to predict the alcoholism status and identify the channels that are associated with alcoholism.

To assess the prediction performance of these two classifiers, *L* 1 -fSVM and grplFlogit, the whole dataset was randomly
divided into a training set of size 98 and a test set of size 24. We employed 5-fold cross-validation with the one-standarderror rule to choose tuning parameters when training these two classifiers. The prediction accuracy was calculated
on the test set for both classifiers. 100 random splits were considered to better evaluate the uncertainty in prediction.
Figure 4 shows the histograms of the prediction accuracy for both classifiers, where the median values of the prediction
accuracy are marked as red vertical lines in these two histograms. These two classifiers achieved almost the same
median prediction accuracy across the 100 splits.

If we chose the mode of the selected tuning parameters across 100 splits, which are *λ* = 0 *.* 05 and *K* = 5 for *L* 1 -fSVM,
we found out that channels “FP2", “F3", “Cz", “Pz", “PO1", “O1", “X", “FT8", “AFz", “CP3", “PO7", “POz", “nd" and
“Y" are identified to be associated with alcoholism. In contrast, taking the same strategy for grplFlogit, channels “F3",
“Cz", “P4", “O2", “X", “AF8", “AFz", “PO7", “PO8", “nd" and “Y" were identified. These channels are mainly located
in the frontal region and the occipital region of the brain, which are related to self-movement and visual function. The
“FT8" channel selected by grplFlogit is located in the temporal region on the right side of the brain, which is related to
the formation of memory. All these functions are associated with alcoholism.

8


-----

A PREPRINT                        - A UGUST 8, 2025

Figure 3: The 61 locations of EEG channels on the scalp according to the Standard Electrode Position Nomenclature,
American Electroencephalographic Association 1990.

Figure 4: The histograms of prediction accuracy of *L* 1 -SVM and grplFlogit for the UCI EEG dataset across 100 splits.
The red vertical lines indicate the median of prediction accuracy.

9


-----

A PREPRINT                        - A UGUST 8, 2025
#### **4 Conclusion**

In this paper, we propose an *L* 1 -regularized functional support vector machine for functional data classification.
Additionally, we develop a coordinate descent-based algorithm to estimate the functional coefficients and select
functional covariates that are relevant to the binary response. Compared with the existing method in the literature, the
proposed classifier achieves better prediction accuracy in simulation settings and shows a comparable performance
in feature selection. We then apply the proposed classifier to an EEG dataset to predict the status of alcoholism. We
identify several channels (regions) on the scalp that may be associated with the status of alcoholism.

[ 29 ] proposed a functional distance-weighted discrimination (DWD) classifier. Let *f* [ˆ] and *f* *[∗]* denote the estimated
functional DWD classifier and the Bayes classifier, respectively. The paper studied the non-asymptotic upper bound on
the excessive risk, which is defined as *L* ( *f, f* [ˆ] *[∗]* ) = *E{ℓ* ( *f* [ˆ] ) *−* *ℓ* ( *f* *[∗]* ) *}* when the loss function *ℓ* satisfies mild conditions.
It is worthwhile to consider the non-asymptotic bound on the excessive classification risk for this proposed classifier.
*q* 1 *q* *[q]*
Actually, the loss function used in the DWD classifier, *ℓ* *q* ( *u* ) = 1 *−* *u* if *u ≤* 1+ *q* [and] *u* *[q]* ( *q* +1) *[q]* [+1] [ otherwise, is pretty]

close to the hinge loss used in the SVM when *q* is sufficiently large. Thus this result can be generalized to a functional
SVM classifier with a smooth penalty, *J* ( *β* ) = � 01 *[{][β]* *′′* ( *t* ) *}* 2 *dt* . However, in our context, there are multiple functional
covariates and the task of functional covariate selection is involved. Consequently, establishing a non-asymptotic error
bound becomes a challenging task. It will be left for future research.
#### **References**

[1] Sennay Ghebreab, Arnold Smeulders, and Pieter Adriaans. Predicting brain states from fMRI data: Incremental
functional principal component regression. Advances in Neural Information Processing Systems, 20, 2007.

[2] Michele Gubian, Francisco Torreira, Helmer Strik, and Lou Boves. Functional Data Analysis as a Tool for
Analyzing Speech Dynamics: A Case Study on the French Word c’était. In Annual Conference of the International
Speech Communication Association, pages 2199–2202, 2009.

[3] Fabrice Rossi and Nathalie Villa. Support vector machine for functional data classification. Neurocomputing,
69(7–9):730–742, 2006.

[4] Xiongtao Dai, Hans-Georg Müller, and Fang Yao. Optimal Bayes classifiers for functional data and density ratios.
Biometrika, 104(3):545–560, 2017.

[5] Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classifiers.
In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152, 1992.

[6] Ji Zhu, Saharon Rosset, Robert Tibshirani, and Trevor Hastie. 1-norm support vector machines. Advances in
Neural Information Processing Systems, 16, 2003.

[7] Jussi Kujala, Timo Aho, and Tapio Elomaa. A Walk from 2-Norm SVM to 1-Norm SVM. In 2009 Ninth IEEE
International Conference on Data Mining, pages 836–841, 2009. doi:10.1109/ICDM.2009.100

[8] Paul S. Bradley and Olvi L. Mangasarian. Feature selection via concave minimization and support vector machines.
In ICML ’98, volume 98, pages 82–90, 1998.

[9] Abdus Salam. Weak and Electromagnetic Interactions. In Nils Svartholm (ed.), Elementary Particle Theory:
Relativistic Groups and Analyticity, Proceedings of the Eighth Nobel Symposium, Aspenäs garden, Lerum, May
19–25, 1968, pages 367–377. Almquist & Wiksell, Stockholm, 1968.

[10] James O. Ramsay and Bernard Silverman. Functional Data Analysis (Second Edition). Springer, New York, 2005.

[11] Jan Gertheiss, Arnab Maity, and Ana-Maria Staicu. Variable selection in generalized functional linear models.
Stat, 2(1):86–101, 2013.

[12] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library for
large linear classification. Journal of Machine Learning Research, 9:1871–1874, 2008.

[13] Gerhard Tutz and Jan Gertheiss. Feature extraction in signal regression: A boosting technique for functional data
regression. Journal of Computational and Graphical Statistics, 19(1):154–174, 2010.

[14] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.

[15] Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. Coordinate descent method for large-scale *ℓ* 2 -loss linear support
vector machines. Journal of Machine Learning Research, 9(7), 2008.

[16] Olvi L. Mangasarian. A finite Newton method for classification. Optimization Methods and Software,
17(5):913–929, 2002.

10


-----

A PREPRINT                        - A UGUST 8, 2025

[17] Saharon Rosset and Ji Zhu. Piecewise linear regularized solution paths. The Annals of Statistics, 35(3):1012–1030,
2007.

[18] Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. Hinge-loss Markov random fields and
probabilistic soft logic. Journal of Machine Learning Research, 18:1–67, 2017.

[19] Charles J. Stone. Additive regression and other nonparametric models. The Annals of Statistics, 13(2):689–705,
1985.

[20] Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, 2nd ed. Springer, New York, 2009.

[21] Le Hoang Thai, Tran Son Hai, and Nguyen Thanh Thuy. Image classification using support vector machine and
artificial neural network. International Journal of Information Technology and Computer Science, 4(5):32–38,
2012.

[22] Nello Cristianini and John Shawe-Taylor et al. An Introduction to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press, 2000.

[23] Raisa Varghese and M. Jayasree. Aspect based sentiment analysis using support vector machine classifier. In
2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI), pages
1581–1586, 2013.

[24] Baptiste Gregorutti, Bertrand Michel, and Philippe Saint-Pierre. Grouped variable importance with random forests
and application to multiple functional data analysis. Computational Statistics & Data Analysis, 90:15–35, 2015.

[25] Kyle Hasenstab, Aaron Scheffler, Donatello Telesca, Catherine A. Sugar, Shafali Jeste, Charlotte DiStefano,
and Damla ¸Sentürk. A multi-dimensional functional principal components analysis of EEG data. Biometrics,
73(3):999–1009, 2017.

[26] Xiao Lei Zhang, Henri Begleiter, Bernice Porjesz, Wenyu Wang, and Ann Litke. Event related potentials during
object recognition tasks. Brain Research Bulletin, 38(6):531–538, 1995.

[27] Youngoh Bae, Byeong Wook Yoo, Jung Chan Lee, and Hee Chan Kim. Automated network analysis to measure
brain effective connectivity estimated from EEG data of patients with alcoholism. Physiological Measurement,
38(5):759, 2017.

[28] Lipo Wang. Support Vector Machines: Theory and Applications. Springer, New York, 2005.

[29] Peijun Sang, Adam B. Kashlak, and Linglong Kong. A reproducing kernel Hilbert space framework for functional classification. Journal of Computational and Graphical Statistics, ja:1–21, 2022.
doi:10.1080/10618600.2022.2138407

11


-----


